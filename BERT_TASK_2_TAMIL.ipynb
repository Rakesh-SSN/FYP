{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_TASK_2_TAMIL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT_TASK_2_TAMIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "88ba74a1-9517-4169-d6f4-e90c5ecbefd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "957fab20-da23-4d0c-fbbf-66ab64e811bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "9caa9e61-a7ce-4ec6-fe65-a84960cbd19f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/tamil/task2tamil.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 3,495\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2913</th>\n",
              "      <td>TAM2919</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>போலீஸ் நிலையத்திலிருந்து வெளியே வந்த போலீசார் ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3070</th>\n",
              "      <td>TAM3076</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>கேரள கடல் பகுதியில் 2 மீனவர்களை சுட்டுக்கொன்ற ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2174</th>\n",
              "      <td>TAM2180</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>அவர் மூலம் \"மறுமையில் இன்பம் அடைய, இப்பிறப்பில...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2563</th>\n",
              "      <td>TAM2569</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>தாக்குதலில் காயமடைந்த ரிபான் பரிசாலில் உள்ள அர...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>TAM0310</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>மாலேகான் குண்டு வெடிப்பில் 8 பேரை விடுதலை செய்...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1621</th>\n",
              "      <td>TAM1627</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>பப்புவா நியூ கினியா மற்றும் நியூசிலாந்து ஆகிய ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1697</th>\n",
              "      <td>TAM1703</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>நாடுகளுக்கிடையே ஏற்படும் கருத்து வேறுபாடுகளை ந...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1803</th>\n",
              "      <td>TAM1809</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>கோவையில் மூவர் கொலைவழக்கில் வழக்கறிஞர் உட்பட இ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1569</th>\n",
              "      <td>TAM1575</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>இந்திய மீனவர்களை சுட்டுக்கொன்ற இத்தாலி கடற்படை...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>TAM0326</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>பணிந்தவர்களும், துணிந்தவர்களும் வாழ்வில் தோற்ற...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "2913         TAM2919  ...  போலீஸ் நிலையத்திலிருந்து வெளியே வந்த போலீசார் ...\n",
              "3070         TAM3076  ...  கேரள கடல் பகுதியில் 2 மீனவர்களை சுட்டுக்கொன்ற ...\n",
              "2174         TAM2180  ...  அவர் மூலம் \"மறுமையில் இன்பம் அடைய, இப்பிறப்பில...\n",
              "2563         TAM2569  ...  தாக்குதலில் காயமடைந்த ரிபான் பரிசாலில் உள்ள அர...\n",
              "309          TAM0310  ...  மாலேகான் குண்டு வெடிப்பில் 8 பேரை விடுதலை செய்...\n",
              "1621         TAM1627  ...  பப்புவா நியூ கினியா மற்றும் நியூசிலாந்து ஆகிய ...\n",
              "1697         TAM1703  ...  நாடுகளுக்கிடையே ஏற்படும் கருத்து வேறுபாடுகளை ந...\n",
              "1803         TAM1809  ...  கோவையில் மூவர் கொலைவழக்கில் வழக்கறிஞர் உட்பட இ...\n",
              "1569         TAM1575  ...  இந்திய மீனவர்களை சுட்டுக்கொன்ற இத்தாலி கடற்படை...\n",
              "325          TAM0326  ...  பணிந்தவர்களும், துணிந்தவர்களும் வாழ்வில் தோற்ற...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 3,495\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2913</th>\n",
              "      <td>TAM2919</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>போலீஸ் நிலையத்திலிருந்து வெளியே வந்த போலீசார் ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3070</th>\n",
              "      <td>TAM3076</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>கேரள கடல் பகுதியில் 2 மீனவர்களை சுட்டுக்கொன்ற ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2174</th>\n",
              "      <td>TAM2180</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>அவர் மூலம் \"மறுமையில் இன்பம் அடைய, இப்பிறப்பில...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2563</th>\n",
              "      <td>TAM2569</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>தாக்குதலில் காயமடைந்த ரிபான் பரிசாலில் உள்ள அர...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>TAM0310</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>மாலேகான் குண்டு வெடிப்பில் 8 பேரை விடுதலை செய்...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1621</th>\n",
              "      <td>TAM1627</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>பப்புவா நியூ கினியா மற்றும் நியூசிலாந்து ஆகிய ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1697</th>\n",
              "      <td>TAM1703</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>நாடுகளுக்கிடையே ஏற்படும் கருத்து வேறுபாடுகளை ந...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1803</th>\n",
              "      <td>TAM1809</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>கோவையில் மூவர் கொலைவழக்கில் வழக்கறிஞர் உட்பட இ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1569</th>\n",
              "      <td>TAM1575</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>இந்திய மீனவர்களை சுட்டுக்கொன்ற இத்தாலி கடற்படை...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>TAM0326</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>பணிந்தவர்களும், துணிந்தவர்களும் வாழ்வில் தோற்ற...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "2913         TAM2919  ...  போலீஸ் நிலையத்திலிருந்து வெளியே வந்த போலீசார் ...\n",
              "3070         TAM3076  ...  கேரள கடல் பகுதியில் 2 மீனவர்களை சுட்டுக்கொன்ற ...\n",
              "2174         TAM2180  ...  அவர் மூலம் \"மறுமையில் இன்பம் அடைய, இப்பிறப்பில...\n",
              "2563         TAM2569  ...  தாக்குதலில் காயமடைந்த ரிபான் பரிசாலில் உள்ள அர...\n",
              "309          TAM0310  ...  மாலேகான் குண்டு வெடிப்பில் 8 பேரை விடுதலை செய்...\n",
              "1621         TAM1627  ...  பப்புவா நியூ கினியா மற்றும் நியூசிலாந்து ஆகிய ...\n",
              "1697         TAM1703  ...  நாடுகளுக்கிடையே ஏற்படும் கருத்து வேறுபாடுகளை ந...\n",
              "1803         TAM1809  ...  கோவையில் மூவர் கொலைவழக்கில் வழக்கறிஞர் உட்பட இ...\n",
              "1569         TAM1575  ...  இந்திய மீனவர்களை சுட்டுக்கொன்ற இத்தாலி கடற்படை...\n",
              "325          TAM0326  ...  பணிந்தவர்களும், துணிந்தவர்களும் வாழ்வில் தோற்ற...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "300f9b59-44a9-472f-9d64-7ee0bda7e0cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]\n",
        "print(df.label)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       1\n",
            "1       1\n",
            "2       1\n",
            "3       1\n",
            "4       1\n",
            "       ..\n",
            "3490    0\n",
            "3491    0\n",
            "3492    0\n",
            "3493    0\n",
            "3494    0\n",
            "Name: label, Length: 3495, dtype: int64\n",
            "0       1\n",
            "1       1\n",
            "2       1\n",
            "3       1\n",
            "4       1\n",
            "       ..\n",
            "3490    0\n",
            "3491    0\n",
            "3492    0\n",
            "3493    0\n",
            "3494    0\n",
            "Name: label, Length: 3495, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "51230b79-fa5c-4b48-d49e-f7eb6968181f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n",
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "0902e7a8-583b-4193-8d0e-e819c16ebb79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Tokenized:  ['ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'பி', '##ர', '##ச', '##ார', '##ம', 'ச', '##ெ', '##ய', '##தா', '##ர', '.', '<', 'eo', '##l', '>', 'தி', '.', 'மு', '.', 'க', '.', ',', 'வ', '##ே', '##ட', '##பா', '##ள', '##ர', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'சி', '##ன', '##ன', 'ச', '##ே', '##ல', '##ம', 'பகுதி', '##ய', '##ில', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'ஓ', '##ட', '##டு', 'ச', '##ே', '##க', '##ரி', '##த', '##தா', '##ர', '.']\n",
            "Token IDs:  [1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119]\n",
            " Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Tokenized:  ['ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'பி', '##ர', '##ச', '##ார', '##ம', 'ச', '##ெ', '##ய', '##தா', '##ர', '.', '<', 'eo', '##l', '>', 'தி', '.', 'மு', '.', 'க', '.', ',', 'வ', '##ே', '##ட', '##பா', '##ள', '##ர', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'சி', '##ன', '##ன', 'ச', '##ே', '##ல', '##ம', 'பகுதி', '##ய', '##ில', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'ஓ', '##ட', '##டு', 'ச', '##ே', '##க', '##ரி', '##த', '##தா', '##ர', '.']\n",
            "Token IDs:  [1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "05a1b5bc-16d5-47a4-90fa-f8e8c6a9d2ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Token IDs: [101, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119, 102]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Token IDs: [101, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "f94b069b-ea98-40f8-e0da-6925e2c0c935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  584\n",
            "Max sentence length:  584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "868621e9-63a8-4380-90cf-69babe683b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "e4995f59-779f-4b73-9335-2af6c6668007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 20% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "# print(train_inputs)\n",
        "# print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "\n",
        "print(labels)\n",
        "#print(train_masks)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 0 0 0]\n",
            "[1 1 1 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "faa1c657-481e-465a-d372-dd5d3ed75821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "outputId": "320ef85c-88c5-4549-f7fc-0dbd41243c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (3, 768)\n",
            "classifier.bias                                                 (3,)\n",
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (3, 768)\n",
            "classifier.bias                                                 (3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "outputId": "5837f604-d154-4ce6-e6a4-98d9ea0c1e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     88.    Elapsed: 0:00:09.\n",
            "  Batch    80  of     88.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 1.01\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.56\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     88.    Elapsed: 0:00:09.\n",
            "  Batch    80  of     88.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 0.88\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     88.    Elapsed: 0:00:09.\n",
            "  Batch    80  of     88.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 0.75\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     88.    Elapsed: 0:00:09.\n",
            "  Batch    80  of     88.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 0.66\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.71\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     88.    Elapsed: 0:00:09.\n",
            "  Batch    80  of     88.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 1.01\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.56\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     88.    Elapsed: 0:00:09.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "outputId": "052fa950-10db-400b-e7c1-5b7fc935f043",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task2tamil-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "#print(len(labels))\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "#print(len(prediction_labels))\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 1,400\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "outputId": "717f9c77-6b04-4e2f-ff82-571af6e008a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  #print(outputs)\n",
        "  # print(logits)\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,400 test sentences...\n",
            "    DONE.\n",
            "*************************\n",
            "44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/task2tamil-results.txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "outputId": "37fd5d55-b699-4a1d-9fdc-372f1788d067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] and predictions[i][j][0]>predictions[i][j][2]):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] and predictions[i][j][1]>predictions[i][j][2]):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "      #print(\" Semi Paraphrase     \",end=\"\")\n",
        "      #count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "      x=2\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)\n",
        "  \n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(0)\t 1\t [ 2.0915835  -1.7771685  -0.30726063] \t0\ttrue\n",
            "(2)\t 2\t [ 0.29309115 -0.8439448   0.73045516] \t2\ttrue\n",
            "(2)\t 3\t [ 0.24668713 -1.3226161   1.4189271 ] \t2\ttrue\n",
            "(0)\t 4\t [-0.14421271 -1.1690549   1.5036641 ] \t2\tfalse\n",
            "(2)\t 5\t [ 1.1610926 -1.3651131  0.2439659] \t0\tfalse\n",
            "(0)\t 6\t [ 1.2071744 -1.4449202  0.2885272] \t0\ttrue\n",
            "(2)\t 7\t [ 1.4556674  -1.5364262   0.15103035] \t0\tfalse\n",
            "(2)\t 8\t [ 0.82079715 -1.3950704   0.78429294] \t0\tfalse\n",
            "(1)\t 9\t [ 1.872558   -1.6465803  -0.23875472] \t0\tfalse\n",
            "(2)\t 10\t [ 2.1500604 -1.4962417 -0.6838003] \t0\tfalse\n",
            "(1)\t 11\t [ 0.6052795  -0.8924595   0.41521093] \t0\tfalse\n",
            "(0)\t 12\t [ 1.5552229  -1.7273681   0.27354503] \t0\ttrue\n",
            "(0)\t 13\t [ 1.8301245  -1.426344   -0.44245228] \t0\ttrue\n",
            "(0)\t 14\t [ 1.8730568  -1.624358   -0.21405761] \t0\ttrue\n",
            "(2)\t 15\t [ 1.6523587  -1.6110507   0.07482579] \t0\tfalse\n",
            "(0)\t 16\t [ 1.0629541  -0.11992678 -1.1454648 ] \t0\ttrue\n",
            "(0)\t 17\t [-0.4313938  -0.03872401  0.61613053] \t2\tfalse\n",
            "(0)\t 18\t [ 1.2080497  -0.7444352  -0.64512223] \t0\ttrue\n",
            "(0)\t 19\t [ 0.47891006 -0.5647076   0.05148641] \t0\ttrue\n",
            "(2)\t 20\t [ 0.80051243 -1.2422793   0.61868984] \t0\tfalse\n",
            "(0)\t 21\t [ 1.9981974  -1.6356527  -0.27102414] \t0\ttrue\n",
            "(0)\t 22\t [ 0.7501167 -0.8615614  0.1388952] \t0\ttrue\n",
            "(2)\t 23\t [ 0.5703757  -0.7230493   0.19243076] \t0\tfalse\n",
            "(2)\t 24\t [ 1.7961823  -1.6247172  -0.08182617] \t0\tfalse\n",
            "(1)\t 25\t [ 2.0122356  -1.6722715  -0.33843377] \t0\tfalse\n",
            "(2)\t 26\t [ 0.63387656 -1.2331775   0.77776575] \t2\ttrue\n",
            "(1)\t 27\t [ 0.7920052  -0.34512836 -0.35119346] \t0\tfalse\n",
            "(2)\t 28\t [ 0.47252074 -0.92787194  0.6004647 ] \t2\ttrue\n",
            "(2)\t 29\t [ 1.2165219  -1.3303652   0.25076663] \t0\tfalse\n",
            "(2)\t 30\t [ 1.7107766  -1.6764809   0.02047966] \t0\tfalse\n",
            "(0)\t 31\t [ 1.6457872 -0.3829117 -1.2870083] \t0\ttrue\n",
            "(2)\t 32\t [ 1.0130893 -1.369564   0.4818493] \t0\tfalse\n",
            "(2)\t 33\t [ 0.9314867  -1.3911967   0.53774995] \t0\tfalse\n",
            "(0)\t 34\t [ 2.0222642  -1.619046   -0.28304446] \t0\ttrue\n",
            "(1)\t 35\t [ 1.7458957  -1.5858759  -0.12714754] \t0\tfalse\n",
            "(1)\t 36\t [-1.8596145   1.572826    0.08493143] \t1\ttrue\n",
            "(0)\t 37\t [ 1.5102295  -1.1397547  -0.44257614] \t0\ttrue\n",
            "(0)\t 38\t [ 1.552941   -1.5894142   0.16278154] \t0\ttrue\n",
            "(0)\t 39\t [ 1.44146    -1.1020237  -0.38325953] \t0\ttrue\n",
            "(1)\t 40\t [ 2.210802   -1.4732631  -0.77642393] \t0\tfalse\n",
            "(1)\t 41\t [ 2.210802   -1.4732631  -0.77642393] \t0\tfalse\n",
            "(2)\t 42\t [ 0.05402883 -0.1816826   0.21811745] \t2\ttrue\n",
            "(2)\t 43\t [ 0.45003393 -1.0167785   0.7096752 ] \t2\ttrue\n",
            "(2)\t 44\t [-1.3853899   1.5034745  -0.37921447] \t1\tfalse\n",
            "(2)\t 45\t [ 1.3688821  -1.261523   -0.05115395] \t0\tfalse\n",
            "(1)\t 46\t [-1.8134857   1.5587423   0.06316966] \t1\ttrue\n",
            "(2)\t 47\t [ 1.3795131  -1.3782722   0.04768234] \t0\tfalse\n",
            "(1)\t 48\t [-1.6070784  0.601229   1.2113681] \t2\tfalse\n",
            "(2)\t 49\t [ 1.7016257 -1.324796  -0.3422403] \t0\tfalse\n",
            "(1)\t 50\t [-1.3700992   0.42569864  1.3613392 ] \t2\tfalse\n",
            "(1)\t 51\t [ 1.6694397  -1.6105362  -0.02982013] \t0\tfalse\n",
            "(1)\t 52\t [ 1.6694397  -1.6105362  -0.02982013] \t0\tfalse\n",
            "(1)\t 53\t [-0.5389743 -0.8376693  1.6465219] \t2\tfalse\n",
            "(2)\t 54\t [ 2.043019  -1.4809855 -0.5893371] \t0\tfalse\n",
            "(0)\t 55\t [ 1.8322731  -1.5895442  -0.17765911] \t0\ttrue\n",
            "(1)\t 56\t [ 1.0246251  -0.56973517 -0.29926598] \t0\tfalse\n",
            "(0)\t 57\t [ 2.0724332 -1.2746602 -0.9132586] \t0\ttrue\n",
            "(2)\t 58\t [ 1.841199   -1.7160803  -0.14981912] \t0\tfalse\n",
            "(0)\t 59\t [ 2.4023395 -0.9408325 -1.3424166] \t0\ttrue\n",
            "(1)\t 60\t [-1.4738907  0.9409184  0.8723935] \t1\ttrue\n",
            "(1)\t 61\t [-1.6573274   1.6070254  -0.21248055] \t1\ttrue\n",
            "(1)\t 62\t [ 0.02762969 -1.0278794   1.1022882 ] \t2\tfalse\n",
            "(1)\t 63\t [ 1.3548814  -1.491051    0.21599726] \t0\tfalse\n",
            "(1)\t 64\t [ 1.3548814  -1.491051    0.21599726] \t0\tfalse\n",
            "(0)\t 65\t [ 1.055744  -1.4068366  0.3785866] \t0\ttrue\n",
            "(0)\t 66\t [ 1.6086186  -1.5343703   0.01407903] \t0\ttrue\n",
            "(2)\t 67\t [ 1.4655538  -1.574475    0.19533446] \t0\tfalse\n",
            "(2)\t 68\t [ 2.1394615  -1.3693819  -0.72596174] \t0\tfalse\n",
            "(2)\t 69\t [-1.9201391   1.5961872   0.44229686] \t1\tfalse\n",
            "(2)\t 70\t [ 1.8612084  -1.6495451  -0.19990307] \t0\tfalse\n",
            "(0)\t 71\t [ 2.5938916 -1.357529  -1.2947415] \t0\ttrue\n",
            "(0)\t 72\t [ 2.0289698 -1.6793576 -0.4140593] \t0\ttrue\n",
            "(2)\t 73\t [ 2.185925  -1.5828968 -0.6301555] \t0\tfalse\n",
            "(0)\t 74\t [ 2.3029964  -1.6851567  -0.61150837] \t0\ttrue\n",
            "(2)\t 75\t [ 2.551355  -1.090644  -1.3551482] \t0\tfalse\n",
            "(2)\t 76\t [ 1.8882314  -1.5710338  -0.32026324] \t0\tfalse\n",
            "(1)\t 77\t [ 1.6332471  -1.4444653  -0.09017886] \t0\tfalse\n",
            "(2)\t 78\t [ 1.437487  -1.5908612  0.2420285] \t0\tfalse\n",
            "(0)\t 79\t [ 0.9762838  -1.0803179  -0.02965619] \t0\ttrue\n",
            "(2)\t 80\t [ 2.497311  -1.535436  -0.9861027] \t0\tfalse\n",
            "(0)\t 81\t [ 1.7852818  -1.5872803  -0.19894777] \t0\ttrue\n",
            "(1)\t 82\t [-1.5157034   0.79298604  0.8933875 ] \t2\tfalse\n",
            "(1)\t 83\t [-1.5157034   0.79298604  0.8933875 ] \t2\tfalse\n",
            "(2)\t 84\t [-0.42636722  1.0691065  -0.77528954] \t1\tfalse\n",
            "(0)\t 85\t [ 1.7853261  -1.6338209  -0.07632504] \t0\ttrue\n",
            "(2)\t 86\t [ 1.2175103 -1.5611235  0.4456748] \t0\tfalse\n",
            "(1)\t 87\t [ 0.10607028 -0.59535414  0.74896896] \t2\tfalse\n",
            "(1)\t 88\t [ 0.10607028 -0.59535414  0.74896896] \t2\tfalse\n",
            "(1)\t 89\t [ 2.2344542 -1.6202129 -0.685243 ] \t0\tfalse\n",
            "(1)\t 90\t [ 2.2344542 -1.6202129 -0.685243 ] \t0\tfalse\n",
            "(0)\t 91\t [ 2.2424905  -1.63284    -0.60658294] \t0\ttrue\n",
            "(2)\t 92\t [ 1.8091954  -1.7229121  -0.02152692] \t0\tfalse\n",
            "(1)\t 93\t [ 1.7756783  -1.245093   -0.60045135] \t0\tfalse\n",
            "(1)\t 94\t [ 1.7756783  -1.245093   -0.60045135] \t0\tfalse\n",
            "(0)\t 95\t [ 1.7621008  -1.3980092  -0.36404204] \t0\ttrue\n",
            "(1)\t 96\t [-0.13824227 -0.6491915   0.92799103] \t2\tfalse\n",
            "(2)\t 97\t [ 1.5116725  -1.6874632   0.32996428] \t0\tfalse\n",
            "(0)\t 98\t [ 2.171515   -1.6631781  -0.43067732] \t0\ttrue\n",
            "(0)\t 99\t [ 1.1863962  -1.5226855   0.36740732] \t0\ttrue\n",
            "(0)\t 100\t [ 1.0456258  -1.3082303   0.43011916] \t0\ttrue\n",
            "(1)\t 101\t [-1.6662062  0.6479604  1.2516164] \t2\tfalse\n",
            "(1)\t 102\t [ 1.9887649 -1.4322745 -0.4025288] \t0\tfalse\n",
            "(1)\t 103\t [ 1.9887649 -1.4322745 -0.4025288] \t0\tfalse\n",
            "(2)\t 104\t [-1.6461847   1.1515092   0.73962104] \t1\tfalse\n",
            "(2)\t 105\t [ 1.3380214 -1.5361418  0.3636175] \t0\tfalse\n",
            "(2)\t 106\t [ 1.7652007 -1.6497542 -0.126862 ] \t0\tfalse\n",
            "(1)\t 107\t [-1.4161577  1.3514096  0.1481847] \t1\ttrue\n",
            "(2)\t 108\t [ 1.7815539e+00 -1.5875307e+00  8.5581536e-04] \t0\tfalse\n",
            "(0)\t 109\t [ 2.150735   -1.6237034  -0.56570894] \t0\ttrue\n",
            "(0)\t 110\t [ 1.8748144  -1.7915703  -0.09752063] \t0\ttrue\n",
            "(1)\t 111\t [-1.4714409   1.516172   -0.03335129] \t1\ttrue\n",
            "(1)\t 112\t [ 1.4161206  -1.5026733   0.13745813] \t0\tfalse\n",
            "(0)\t 113\t [ 2.4596195 -0.9888334 -1.3796171] \t0\ttrue\n",
            "(2)\t 114\t [-0.81770086  0.8532803   0.08716628] \t1\tfalse\n",
            "(2)\t 115\t [ 2.0986056  -1.6773062  -0.46201992] \t0\tfalse\n",
            "(2)\t 116\t [ 1.4283088  -1.5857369   0.13085668] \t0\tfalse\n",
            "(1)\t 117\t [ 2.330721  -1.6642524 -0.6831611] \t0\tfalse\n",
            "(1)\t 118\t [ 0.192778   -0.90571004  0.97699237] \t2\tfalse\n",
            "(1)\t 119\t [ 2.125628   -1.7008685  -0.34333035] \t0\tfalse\n",
            "(1)\t 120\t [ 2.125628   -1.7008685  -0.34333035] \t0\tfalse\n",
            "(1)\t 121\t [ 1.6490284  -1.6576875   0.00983123] \t0\tfalse\n",
            "(2)\t 122\t [ 1.4100037 -1.2623706 -0.1982775] \t0\tfalse\n",
            "(2)\t 123\t [ 1.5035866  -1.6333345   0.16402987] \t0\tfalse\n",
            "(0)\t 124\t [-0.9049541 -0.2072334  1.4995846] \t2\tfalse\n",
            "(1)\t 125\t [ 1.4219205  -1.4848765   0.09705144] \t0\tfalse\n",
            "(1)\t 126\t [ 1.4219205  -1.4848765   0.09705144] \t0\tfalse\n",
            "(0)\t 127\t [ 0.22042693 -1.0960664   1.0085816 ] \t2\tfalse\n",
            "(0)\t 128\t [ 2.324208  -0.8974084 -1.3388251] \t0\ttrue\n",
            "(2)\t 129\t [-1.8805246  1.5711578  0.4714282] \t1\tfalse\n",
            "(0)\t 130\t [ 1.5296043  -1.3483578  -0.18355703] \t0\ttrue\n",
            "(0)\t 131\t [ 1.9176636  -1.6401895  -0.26940134] \t0\ttrue\n",
            "(0)\t 132\t [ 1.6595167  -1.5415698  -0.04158137] \t0\ttrue\n",
            "(2)\t 133\t [ 1.3745221 -1.4469274  0.1538208] \t0\tfalse\n",
            "(1)\t 134\t [-1.8589752   1.5990855   0.16034672] \t1\ttrue\n",
            "(2)\t 135\t [-1.5779139  0.5167547  1.323902 ] \t2\ttrue\n",
            "(1)\t 136\t [-1.4927794  1.3051654 -0.0823109] \t1\ttrue\n",
            "(2)\t 137\t [ 1.5890124  -1.596677    0.04956767] \t0\tfalse\n",
            "(0)\t 138\t [ 0.526052   -0.741513    0.27431932] \t0\ttrue\n",
            "(0)\t 139\t [ 1.0544626   0.15187821 -1.193297  ] \t0\ttrue\n",
            "(0)\t 140\t [ 2.0671484 -1.711933  -0.3783896] \t0\ttrue\n",
            "(0)\t 141\t [ 2.3908868 -1.1235396 -1.3358182] \t0\ttrue\n",
            "(2)\t 142\t [ 0.4094194  -1.1885775   0.94087225] \t2\ttrue\n",
            "(1)\t 143\t [-0.8457714 -0.5799266  1.6837584] \t2\tfalse\n",
            "(2)\t 144\t [ 2.083312  -1.5864569 -0.4941219] \t0\tfalse\n",
            "(1)\t 145\t [ 1.3997859  -1.3753027  -0.01738775] \t0\tfalse\n",
            "(0)\t 146\t [ 0.64350057 -0.73264635  0.22776411] \t0\ttrue\n",
            "(2)\t 147\t [ 1.8478174  -1.7430538  -0.05388792] \t0\tfalse\n",
            "(2)\t 148\t [ 1.463149   -1.1056663  -0.31363538] \t0\tfalse\n",
            "(0)\t 149\t [ 1.5021846 -1.5663437  0.154274 ] \t0\ttrue\n",
            "(2)\t 150\t [ 1.951712  -1.7285377 -0.2906037] \t0\tfalse\n",
            "(0)\t 151\t [ 2.4251359 -0.8993795 -1.4599047] \t0\ttrue\n",
            "(0)\t 152\t [ 1.7645252 -1.6902426  0.0586794] \t0\ttrue\n",
            "(1)\t 153\t [-1.4002761   1.5382112  -0.11311903] \t1\ttrue\n",
            "(0)\t 154\t [ 1.7500743  -1.7148753  -0.07525073] \t0\ttrue\n",
            "(0)\t 155\t [ 1.8166078 -1.5628072 -0.2959933] \t0\ttrue\n",
            "(2)\t 156\t [ 1.7554632  -1.6617206  -0.06231089] \t0\tfalse\n",
            "(0)\t 157\t [ 1.1109723  -1.3278795   0.31918564] \t0\ttrue\n",
            "(0)\t 158\t [ 2.4534757 -1.4209162 -1.0981768] \t0\ttrue\n",
            "(2)\t 159\t [ 2.1873984 -1.6538992 -0.5245225] \t0\tfalse\n",
            "(0)\t 160\t [ 1.1291558  -1.4821008   0.44496232] \t0\ttrue\n",
            "(0)\t 161\t [ 2.020564   -1.5613643  -0.45012805] \t0\ttrue\n",
            "(2)\t 162\t [ 1.150971  -1.7257241  0.6122907] \t0\tfalse\n",
            "(1)\t 163\t [-1.7194124  1.6470397 -0.1515926] \t1\ttrue\n",
            "(0)\t 164\t [ 2.0443249  -1.4100266  -0.67190874] \t0\ttrue\n",
            "(2)\t 165\t [ 1.059913   -1.2977171   0.27893478] \t0\tfalse\n",
            "(0)\t 166\t [ 1.1385968  -0.90262246 -0.228574  ] \t0\ttrue\n",
            "(0)\t 167\t [ 1.6076905 -1.3496281 -0.2371713] \t0\ttrue\n",
            "(0)\t 168\t [ 1.5603248  -1.5548097   0.15435378] \t0\ttrue\n",
            "(2)\t 169\t [ 2.3088903 -1.6530915 -0.6416712] \t0\tfalse\n",
            "(0)\t 170\t [ 1.6839333  -1.7311958   0.09606156] \t0\ttrue\n",
            "(1)\t 171\t [ 1.8220296 -1.6344194 -0.1038539] \t0\tfalse\n",
            "(1)\t 172\t [ 1.8220296 -1.6344194 -0.1038539] \t0\tfalse\n",
            "(2)\t 173\t [ 1.1511554  -1.3462591   0.36596102] \t0\tfalse\n",
            "(0)\t 174\t [ 1.9444579 -1.6916558 -0.2947754] \t0\ttrue\n",
            "(0)\t 175\t [ 2.15265    -1.7256011  -0.42124403] \t0\ttrue\n",
            "(0)\t 176\t [ 1.3090576  -0.03129164 -1.2678989 ] \t0\ttrue\n",
            "(2)\t 177\t [ 0.816594  -1.4269253  0.7749822] \t0\tfalse\n",
            "(2)\t 178\t [ 1.4628514  -1.283884   -0.14859985] \t0\tfalse\n",
            "(2)\t 179\t [ 2.0672128 -1.4174886 -0.6948697] \t0\tfalse\n",
            "(0)\t 180\t [ 1.3474424  -1.5025378   0.24074055] \t0\ttrue\n",
            "(0)\t 181\t [ 2.0974965  -1.7339661  -0.32484803] \t0\ttrue\n",
            "(1)\t 182\t [ 1.957233   -1.6192944  -0.23944251] \t0\tfalse\n",
            "(1)\t 183\t [ 1.957233   -1.6192944  -0.23944251] \t0\tfalse\n",
            "(0)\t 184\t [ 2.0025446 -1.5775119 -0.3559314] \t0\ttrue\n",
            "(0)\t 185\t [ 1.2715082  -0.24692427 -0.9897943 ] \t0\ttrue\n",
            "(2)\t 186\t [ 1.82403    -1.602715   -0.17877711] \t0\tfalse\n",
            "(0)\t 187\t [-0.56437624 -0.6888677   1.5033993 ] \t2\tfalse\n",
            "(0)\t 188\t [ 2.1602426 -1.7108473 -0.4735327] \t0\ttrue\n",
            "(0)\t 189\t [ 2.046866  -1.6844219 -0.2658729] \t0\ttrue\n",
            "(2)\t 190\t [ 1.0193948 -1.3670529  0.6013169] \t0\tfalse\n",
            "(1)\t 191\t [-1.8685399   1.0477598   0.94794077] \t1\ttrue\n",
            "(1)\t 192\t [ 1.5088835  -1.4991475   0.07008646] \t0\tfalse\n",
            "(1)\t 193\t [ 1.5088835  -1.4991475   0.07008646] \t0\tfalse\n",
            "(2)\t 194\t [ 1.8329949  -1.5996069  -0.21921243] \t0\tfalse\n",
            "(1)\t 195\t [ 1.0986193 -0.7252106 -0.281536 ] \t0\tfalse\n",
            "(1)\t 196\t [ 1.0986193 -0.7252106 -0.281536 ] \t0\tfalse\n",
            "(1)\t 197\t [ 0.9686356  -0.23304899 -0.66579276] \t0\tfalse\n",
            "(2)\t 198\t [ 2.2298176 -1.5821108 -0.6309706] \t0\tfalse\n",
            "(2)\t 199\t [ 1.2783078  -1.5277647   0.35275915] \t0\tfalse\n",
            "(2)\t 200\t [ 1.5454167  -1.4866419   0.00202159] \t0\tfalse\n",
            "(2)\t 201\t [ 0.50762683 -1.1787585   0.85032153] \t2\ttrue\n",
            "(0)\t 202\t [ 1.3715681  -1.4745427   0.13755168] \t0\ttrue\n",
            "(0)\t 203\t [ 0.99795973  0.39460555 -1.2911137 ] \t0\ttrue\n",
            "(0)\t 204\t [ 1.8952976  -1.6125247  -0.19490512] \t0\ttrue\n",
            "(2)\t 205\t [ 1.7674388  -1.5429976  -0.16688365] \t0\tfalse\n",
            "(2)\t 206\t [-1.6810598  1.3028013  0.5787024] \t1\tfalse\n",
            "(2)\t 207\t [ 1.2747476  -1.3384382  -0.00257333] \t0\tfalse\n",
            "(2)\t 208\t [ 0.9264251  -1.2435201   0.43979862] \t0\tfalse\n",
            "(2)\t 209\t [ 0.66765594 -1.3684012   0.86596495] \t2\ttrue\n",
            "(2)\t 210\t [ 0.6432776  -1.2237669   0.79664814] \t2\ttrue\n",
            "(2)\t 211\t [ 2.1365771 -1.6223241 -0.5175334] \t0\tfalse\n",
            "(0)\t 212\t [ 2.0790286 -1.7368861 -0.3643759] \t0\ttrue\n",
            "(0)\t 213\t [ 1.9072266  -1.6020281  -0.18333772] \t0\ttrue\n",
            "(0)\t 214\t [ 1.3217037 -1.4485584  0.3381686] \t0\ttrue\n",
            "(0)\t 215\t [ 2.3876748  -1.514614   -0.85134125] \t0\ttrue\n",
            "(0)\t 216\t [ 1.4074188  -1.5854934   0.33373436] \t0\ttrue\n",
            "(0)\t 217\t [ 2.013986   -1.6355097  -0.34467235] \t0\ttrue\n",
            "(1)\t 218\t [-0.38178167  0.2620285   0.460512  ] \t2\tfalse\n",
            "(2)\t 219\t [ 1.0952508  -1.3487359   0.31928414] \t0\tfalse\n",
            "(2)\t 220\t [-1.50475     1.2737786   0.35235968] \t1\tfalse\n",
            "(0)\t 221\t [ 1.8610443  -1.7561435  -0.04751085] \t0\ttrue\n",
            "(2)\t 222\t [-0.37785587 -0.9368759   1.6157074 ] \t2\ttrue\n",
            "(0)\t 223\t [ 2.1642299  -1.7225375  -0.48954305] \t0\ttrue\n",
            "(1)\t 224\t [-1.615292    1.4731945   0.13810273] \t1\ttrue\n",
            "(0)\t 225\t [ 1.3440493  -0.37927255 -0.92901933] \t0\ttrue\n",
            "(0)\t 226\t [ 1.8043267 -1.6159589 -0.2183149] \t0\ttrue\n",
            "(2)\t 227\t [-1.3403028  -0.06358634  1.7281705 ] \t2\ttrue\n",
            "(0)\t 228\t [ 1.8000861  -1.7474542   0.05057279] \t0\ttrue\n",
            "(2)\t 229\t [ 0.47916412 -1.0682636   0.69077986] \t2\ttrue\n",
            "(0)\t 230\t [ 1.9932615  -1.7194513  -0.22829603] \t0\ttrue\n",
            "(2)\t 231\t [ 1.9528629  -1.601792   -0.27874613] \t0\tfalse\n",
            "(0)\t 232\t [ 0.7847694 -1.424619   0.7280158] \t0\ttrue\n",
            "(2)\t 233\t [ 0.45071685 -1.041607    0.6376741 ] \t2\ttrue\n",
            "(2)\t 234\t [ 1.5188245  -1.6710964   0.12438826] \t0\tfalse\n",
            "(0)\t 235\t [ 2.2469454 -1.701142  -0.5158197] \t0\ttrue\n",
            "(0)\t 236\t [ 0.58632743 -0.5743886   0.19185905] \t0\ttrue\n",
            "(0)\t 237\t [ 1.5919987  -1.4213792  -0.01887812] \t0\ttrue\n",
            "(1)\t 238\t [ 1.2731118  -1.2568611   0.07537614] \t0\tfalse\n",
            "(2)\t 239\t [ 1.9846672  -1.7189915  -0.16784249] \t0\tfalse\n",
            "(0)\t 240\t [ 2.2155452 -1.6702057 -0.5149295] \t0\ttrue\n",
            "(0)\t 241\t [ 1.8034785  -1.4306427  -0.23560758] \t0\ttrue\n",
            "(2)\t 242\t [ 1.8645746  -1.5784026  -0.22056134] \t0\tfalse\n",
            "(0)\t 243\t [ 2.2613902 -1.5296496 -0.7537593] \t0\ttrue\n",
            "(0)\t 244\t [ 2.3414483  -1.4794545  -0.86058784] \t0\ttrue\n",
            "(2)\t 245\t [ 2.1584778 -1.7485781 -0.3956217] \t0\tfalse\n",
            "(2)\t 246\t [ 1.790806   -1.7408477   0.02263175] \t0\tfalse\n",
            "(0)\t 247\t [ 0.8446128  0.5551501 -1.4221807] \t0\ttrue\n",
            "(0)\t 248\t [ 2.0581312 -1.5145539 -0.4353108] \t0\ttrue\n",
            "(2)\t 249\t [ 1.1446037  -1.3150938   0.22542518] \t0\tfalse\n",
            "(0)\t 250\t [-0.00439782  1.0866278  -1.0122619 ] \t1\tfalse\n",
            "(1)\t 251\t [ 2.213295  -1.5798789 -0.6211565] \t0\tfalse\n",
            "(1)\t 252\t [ 2.213295  -1.5798789 -0.6211565] \t0\tfalse\n",
            "(0)\t 253\t [ 0.24729937 -0.73911494  0.79511124] \t2\tfalse\n",
            "(0)\t 254\t [ 2.0315976  -1.6267232  -0.37627384] \t0\ttrue\n",
            "(1)\t 255\t [ 2.2534313 -1.4814459 -0.7685416] \t0\tfalse\n",
            "(1)\t 256\t [ 2.2534313 -1.4814459 -0.7685416] \t0\tfalse\n",
            "(0)\t 257\t [ 2.2798138 -1.1010706 -1.2073168] \t0\ttrue\n",
            "(2)\t 258\t [-0.43769246 -0.46660367  1.2033306 ] \t2\ttrue\n",
            "(1)\t 259\t [-1.3873674e+00 -7.3990668e-05  1.7168216e+00] \t2\tfalse\n",
            "(1)\t 260\t [-1.2657554   0.9641831   0.58421266] \t1\ttrue\n",
            "(1)\t 261\t [ 2.283667  -1.6317164 -0.5785468] \t0\tfalse\n",
            "(0)\t 262\t [ 1.2639315  -1.5931505   0.38408896] \t0\ttrue\n",
            "(1)\t 263\t [ 1.4958208  -1.5765225   0.10016723] \t0\tfalse\n",
            "(1)\t 264\t [ 1.3936177  -1.5926172   0.16271837] \t0\tfalse\n",
            "(1)\t 265\t [ 0.741501   -1.2975532   0.63227755] \t0\tfalse\n",
            "(0)\t 266\t [ 1.8595924  -0.38860402 -1.4625235 ] \t0\ttrue\n",
            "(0)\t 267\t [ 2.2177234 -1.6855054 -0.5679201] \t0\ttrue\n",
            "(1)\t 268\t [-1.8965188  1.4519566  0.6386055] \t1\ttrue\n",
            "(0)\t 269\t [ 0.47252074 -0.92787194  0.6004647 ] \t2\tfalse\n",
            "(1)\t 270\t [-0.5315442   0.8700495  -0.27104756] \t1\ttrue\n",
            "(2)\t 271\t [-1.7226009  1.2857755  0.7956315] \t1\tfalse\n",
            "(0)\t 272\t [ 2.1340303  -1.7250959  -0.45323786] \t0\ttrue\n",
            "(1)\t 273\t [ 1.0352123  -1.3511966   0.43567944] \t0\tfalse\n",
            "(2)\t 274\t [ 1.4394675  -1.5745016   0.34293774] \t0\tfalse\n",
            "(0)\t 275\t [ 1.4392079 -1.5633956  0.2556766] \t0\ttrue\n",
            "(0)\t 276\t [ 1.4400781  -1.5899436   0.18130343] \t0\ttrue\n",
            "(0)\t 277\t [ 1.7338682  -1.458195   -0.17022054] \t0\ttrue\n",
            "(2)\t 278\t [ 1.9051133  -1.4332849  -0.48172212] \t0\tfalse\n",
            "(1)\t 279\t [ 1.3705342  -1.3996739   0.06013943] \t0\tfalse\n",
            "(1)\t 280\t [-0.40505245 -0.01879586  0.7713961 ] \t2\tfalse\n",
            "(1)\t 281\t [-1.7927176   1.2449826   0.61023986] \t1\ttrue\n",
            "(0)\t 282\t [ 1.5121268  -1.5492321   0.18813594] \t0\ttrue\n",
            "(2)\t 283\t [ 2.1209247  -1.557701   -0.46668544] \t0\tfalse\n",
            "(2)\t 284\t [ 1.8940665  -1.6460992  -0.20425963] \t0\tfalse\n",
            "(2)\t 285\t [ 1.475449   -1.5539931   0.07761572] \t0\tfalse\n",
            "(1)\t 286\t [-1.086089   -0.10319866  1.6301563 ] \t2\tfalse\n",
            "(1)\t 287\t [-1.3041855   1.2135222   0.06850202] \t1\ttrue\n",
            "(2)\t 288\t [ 0.83013654 -1.1982802   0.50951684] \t0\tfalse\n",
            "(0)\t 289\t [ 2.1092985 -1.5903615 -0.5029764] \t0\ttrue\n",
            "(2)\t 290\t [-0.9322506   1.1471893  -0.03038476] \t1\tfalse\n",
            "(1)\t 291\t [-1.8682815   1.4887987   0.18888041] \t1\ttrue\n",
            "(2)\t 292\t [ 2.0195436 -1.5302182 -0.5157357] \t0\tfalse\n",
            "(2)\t 293\t [ 1.120186  -1.4568006  0.4411876] \t0\tfalse\n",
            "(1)\t 294\t [ 1.5933217  -0.16990939 -1.4415401 ] \t0\tfalse\n",
            "(1)\t 295\t [ 2.0374334 -0.9673354 -1.0521556] \t0\tfalse\n",
            "(0)\t 296\t [ 1.6579412  -1.5831395  -0.07003471] \t0\ttrue\n",
            "(0)\t 297\t [ 2.249629   -1.7366496  -0.49637938] \t0\ttrue\n",
            "(0)\t 298\t [ 1.6076041  -1.4871591  -0.02246187] \t0\ttrue\n",
            "(1)\t 299\t [ 1.8123338  -1.5203933  -0.27175143] \t0\tfalse\n",
            "(1)\t 300\t [ 1.8123338  -1.5203933  -0.27175143] \t0\tfalse\n",
            "(1)\t 301\t [ 0.83576393 -1.225175    0.54105896] \t0\tfalse\n",
            "(2)\t 302\t [ 0.62114537 -1.4248687   0.8112599 ] \t2\ttrue\n",
            "(2)\t 303\t [ 0.9105602  -1.4456102   0.69532925] \t0\tfalse\n",
            "(1)\t 304\t [ 1.9525253  -1.7209595  -0.36037478] \t0\tfalse\n",
            "(1)\t 305\t [ 1.9525253  -1.7209595  -0.36037478] \t0\tfalse\n",
            "(0)\t 306\t [ 1.0789707  -1.334922    0.16052647] \t0\ttrue\n",
            "(2)\t 307\t [-1.5899867   1.0622033   0.85296065] \t1\tfalse\n",
            "(2)\t 308\t [ 1.8405863  -1.6290126  -0.13671733] \t0\tfalse\n",
            "(0)\t 309\t [ 1.522123   -1.4619746   0.04821701] \t0\ttrue\n",
            "(2)\t 310\t [ 0.31746507 -1.088492    0.8438157 ] \t2\ttrue\n",
            "(2)\t 311\t [ 1.4492842  -1.1793537  -0.39618018] \t0\tfalse\n",
            "(1)\t 312\t [-0.04147842 -1.0444889   1.3339056 ] \t2\tfalse\n",
            "(2)\t 313\t [ 2.2712774 -1.5159647 -0.8342915] \t0\tfalse\n",
            "(2)\t 314\t [ 2.2563004 -1.3914157 -0.8735418] \t0\tfalse\n",
            "(0)\t 315\t [ 1.7987036  -1.4166666  -0.31295952] \t0\ttrue\n",
            "(2)\t 316\t [ 2.2942753 -1.6689588 -0.5895249] \t0\tfalse\n",
            "(0)\t 317\t [ 2.062501   -1.6768726  -0.34604314] \t0\ttrue\n",
            "(2)\t 318\t [-0.18359797 -1.1343342   1.5546383 ] \t2\ttrue\n",
            "(2)\t 319\t [-1.8987633   1.6662005   0.07476899] \t1\tfalse\n",
            "(2)\t 320\t [ 1.2635113  -1.5768977   0.27800345] \t0\tfalse\n",
            "(0)\t 321\t [ 1.6014634 -1.5523288  0.0039234] \t0\ttrue\n",
            "(0)\t 322\t [ 2.075282  -1.652594  -0.4093452] \t0\ttrue\n",
            "(0)\t 323\t [ 2.0725965  -1.378158   -0.71591794] \t0\ttrue\n",
            "(2)\t 324\t [ 1.4273697  -1.5301402   0.04477784] \t0\tfalse\n",
            "(0)\t 325\t [ 1.2209806  -1.2672228   0.11370888] \t0\ttrue\n",
            "(1)\t 326\t [-0.62243855 -0.30144033  1.1583327 ] \t2\tfalse\n",
            "(1)\t 327\t [ 1.6081895  -1.5122632  -0.05421077] \t0\tfalse\n",
            "(1)\t 328\t [ 1.6081895  -1.5122632  -0.05421077] \t0\tfalse\n",
            "(2)\t 329\t [ 0.9409794  -1.2832023   0.54684883] \t0\tfalse\n",
            "(2)\t 330\t [ 1.9872297  -1.7290987  -0.21852979] \t0\tfalse\n",
            "(0)\t 331\t [ 2.4485042 -1.5154066 -1.0251275] \t0\ttrue\n",
            "(2)\t 332\t [ 1.637224   -1.654518    0.01693368] \t0\tfalse\n",
            "(1)\t 333\t [ 1.0795277 -1.4489822  0.4893795] \t0\tfalse\n",
            "(0)\t 334\t [ 1.7131231 -1.4289147 -0.1146525] \t0\ttrue\n",
            "(2)\t 335\t [-1.5155264   1.0651517   0.77438486] \t1\tfalse\n",
            "(1)\t 336\t [ 0.08014352 -0.05235388  0.10506818] \t2\tfalse\n",
            "(1)\t 337\t [ 0.08014352 -0.05235388  0.10506818] \t2\tfalse\n",
            "(2)\t 338\t [ 2.1564665  -1.5757124  -0.51477605] \t0\tfalse\n",
            "(1)\t 339\t [-1.8227894  1.3656423  0.5779352] \t1\ttrue\n",
            "(2)\t 340\t [ 1.305158 -0.288159 -0.97898 ] \t0\tfalse\n",
            "(1)\t 341\t [ 0.73181754  0.3087853  -0.96551394] \t0\tfalse\n",
            "(1)\t 342\t [ 0.73181754  0.3087853  -0.96551394] \t0\tfalse\n",
            "(2)\t 343\t [ 1.8663305  -1.5369008  -0.20647444] \t0\tfalse\n",
            "(0)\t 344\t [-0.55527055 -0.29710075  1.109066  ] \t2\tfalse\n",
            "(2)\t 345\t [ 1.5607392  -1.6252352   0.16221328] \t0\tfalse\n",
            "(0)\t 346\t [ 2.0629916  -1.5504838  -0.52330637] \t0\ttrue\n",
            "(0)\t 347\t [ 1.5395828  -1.3452597  -0.21590574] \t0\ttrue\n",
            "(2)\t 348\t [ 0.18161516  0.8720907  -1.0946763 ] \t1\tfalse\n",
            "(2)\t 349\t [ 1.3413633  -1.5739566   0.42972395] \t0\tfalse\n",
            "(2)\t 350\t [ 1.0824264  -1.3957363   0.33153996] \t0\tfalse\n",
            "(0)\t 351\t [ 0.7237375 -1.019606   0.5306082] \t0\ttrue\n",
            "(1)\t 352\t [-0.566101   -0.57771313  1.3875151 ] \t2\tfalse\n",
            "(2)\t 353\t [-0.25236753 -0.52694476  0.9911194 ] \t2\ttrue\n",
            "(0)\t 354\t [ 2.3122756 -1.6126939 -0.6772218] \t0\ttrue\n",
            "(0)\t 355\t [ 2.3426776 -1.3316572 -1.0343906] \t0\ttrue\n",
            "(2)\t 356\t [ 1.2842405  -1.3052584  -0.02627893] \t0\tfalse\n",
            "(1)\t 357\t [-1.2104031  -0.08709175  1.6448191 ] \t2\tfalse\n",
            "(1)\t 358\t [-1.842187   1.46035    0.4696432] \t1\ttrue\n",
            "(1)\t 359\t [ 0.84315205 -1.219411    0.5684922 ] \t0\tfalse\n",
            "(1)\t 360\t [ 1.0473306 -1.3280162  0.3688892] \t0\tfalse\n",
            "(1)\t 361\t [ 1.0473306 -1.3280162  0.3688892] \t0\tfalse\n",
            "(0)\t 362\t [ 0.91443014 -0.996815    0.14698191] \t0\ttrue\n",
            "(0)\t 363\t [ 1.8233826 -1.6436002 -0.21794  ] \t0\ttrue\n",
            "(0)\t 364\t [ 0.92368066 -0.6895712  -0.24113512] \t0\ttrue\n",
            "(1)\t 365\t [ 0.6283175  -1.4380275   0.79433465] \t2\tfalse\n",
            "(1)\t 366\t [-1.3759912   1.3470665  -0.07263937] \t1\ttrue\n",
            "(0)\t 367\t [ 1.1149718  -1.3444334   0.36664766] \t0\ttrue\n",
            "(1)\t 368\t [-0.0793134   0.17647746  0.02213836] \t1\ttrue\n",
            "(1)\t 369\t [-0.9944884   1.2475712  -0.20176317] \t1\ttrue\n",
            "(2)\t 370\t [ 1.6406275  -1.5293477  -0.12643842] \t0\tfalse\n",
            "(2)\t 371\t [ 2.2870073 -1.4122778 -0.8451638] \t0\tfalse\n",
            "(1)\t 372\t [ 0.26158085 -0.5211004   0.4322147 ] \t2\tfalse\n",
            "(2)\t 373\t [ 2.1680703  -1.6654099  -0.51583606] \t0\tfalse\n",
            "(2)\t 374\t [ 1.1834788  -1.3976665   0.41229883] \t0\tfalse\n",
            "(2)\t 375\t [ 0.3221398 -0.7458723  0.5893897] \t2\ttrue\n",
            "(2)\t 376\t [-1.7884266   1.4800099   0.13332482] \t1\tfalse\n",
            "(1)\t 377\t [-1.4161414  1.0410262  0.6165271] \t1\ttrue\n",
            "(0)\t 378\t [ 1.7387013  -1.5984721  -0.00596691] \t0\ttrue\n",
            "(0)\t 379\t [ 1.6093317  -1.6399314   0.12361526] \t0\ttrue\n",
            "(0)\t 380\t [ 1.0946032  -0.8709198  -0.14754237] \t0\ttrue\n",
            "(2)\t 381\t [ 0.6447555 -1.0911081  0.5568939] \t0\tfalse\n",
            "(0)\t 382\t [ 2.140905   -1.7126641  -0.48319805] \t0\ttrue\n",
            "(0)\t 383\t [ 0.6524445  -1.1477655   0.52186584] \t0\ttrue\n",
            "(2)\t 384\t [ 2.1455302 -1.3781412 -0.8442147] \t0\tfalse\n",
            "(0)\t 385\t [-0.18422322 -1.1351441   1.4801449 ] \t2\tfalse\n",
            "(0)\t 386\t [ 1.1441239  -1.4607613   0.43059272] \t0\ttrue\n",
            "(2)\t 387\t [ 0.1119792 -1.2840405  1.342086 ] \t2\ttrue\n",
            "(0)\t 388\t [ 2.4422479 -0.9797774 -1.4948776] \t0\ttrue\n",
            "(0)\t 389\t [ 2.1098824  -1.2421471  -0.90769494] \t0\ttrue\n",
            "(0)\t 390\t [ 1.1966482  -1.4050679   0.33205137] \t0\ttrue\n",
            "(0)\t 391\t [ 2.0960922  -1.6812606  -0.38257882] \t0\ttrue\n",
            "(0)\t 392\t [ 0.828031   -1.3573779   0.60334766] \t0\ttrue\n",
            "(0)\t 393\t [ 2.0076103  -1.6808081  -0.27477324] \t0\ttrue\n",
            "(2)\t 394\t [ 0.33132362 -0.7988708   0.5802389 ] \t2\ttrue\n",
            "(0)\t 395\t [ 1.6604404  -1.2249643  -0.51416206] \t0\ttrue\n",
            "(0)\t 396\t [ 1.8498882  -1.5869166  -0.10494114] \t0\ttrue\n",
            "(0)\t 397\t [ 1.9591324  -1.6789082  -0.22881962] \t0\ttrue\n",
            "(0)\t 398\t [ 1.4937886  -1.6577958   0.16313134] \t0\ttrue\n",
            "(0)\t 399\t [ 1.8584744  -1.6612419  -0.19816838] \t0\ttrue\n",
            "(2)\t 400\t [ 1.8972782  -1.707524   -0.11401428] \t0\tfalse\n",
            "(2)\t 401\t [ 2.276361  -1.682617  -0.6664188] \t0\tfalse\n",
            "(2)\t 402\t [-1.9880482  1.5025476  0.557378 ] \t1\tfalse\n",
            "(1)\t 403\t [-1.0013245  -0.53059745  1.7962099 ] \t2\tfalse\n",
            "(2)\t 404\t [-0.74289864  0.5917381   0.3515492 ] \t1\tfalse\n",
            "(2)\t 405\t [ 1.3551816  -1.2993642  -0.01488909] \t0\tfalse\n",
            "(2)\t 406\t [ 1.620356   -1.6269258   0.02306608] \t0\tfalse\n",
            "(1)\t 407\t [ 0.470333   -1.1798719   0.85525113] \t2\tfalse\n",
            "(1)\t 408\t [ 0.28432214 -1.0405165   0.9797528 ] \t2\tfalse\n",
            "(1)\t 409\t [-1.2868496   0.27222437  1.4650654 ] \t2\tfalse\n",
            "(2)\t 410\t [ 1.7508745  -1.5785481  -0.05540315] \t0\tfalse\n",
            "(2)\t 411\t [ 1.9992529  -1.5886062  -0.33960474] \t0\tfalse\n",
            "(1)\t 412\t [ 0.71964985 -1.3039942   0.58318824] \t0\tfalse\n",
            "(1)\t 413\t [ 1.4796373  -1.5217093  -0.00294645] \t0\tfalse\n",
            "(1)\t 414\t [-1.037572   -0.11165731  1.583891  ] \t2\tfalse\n",
            "(1)\t 415\t [-1.5166236   1.5832336  -0.28184247] \t1\ttrue\n",
            "(2)\t 416\t [-1.030574   0.6505046  0.6439963] \t1\tfalse\n",
            "(1)\t 417\t [-1.7534105   1.6194935  -0.04127282] \t1\ttrue\n",
            "(2)\t 418\t [ 1.8790948  -1.6608661  -0.15643527] \t0\tfalse\n",
            "(1)\t 419\t [-1.2174604   0.10636848  1.4600934 ] \t2\tfalse\n",
            "(0)\t 420\t [ 1.4903518 -0.9768906 -0.5394891] \t0\ttrue\n",
            "(2)\t 421\t [ 2.2145061  -1.7159172  -0.45542648] \t0\tfalse\n",
            "(0)\t 422\t [ 1.6254419  -1.6635989   0.13599375] \t0\ttrue\n",
            "(0)\t 423\t [ 2.1072073 -1.5353285 -0.6178428] \t0\ttrue\n",
            "(2)\t 424\t [ 2.1768312  -1.6364404  -0.52658594] \t0\tfalse\n",
            "(0)\t 425\t [ 1.9582498  -1.6685028  -0.24474938] \t0\ttrue\n",
            "(1)\t 426\t [ 1.5823702  -1.4618808  -0.16426586] \t0\tfalse\n",
            "(0)\t 427\t [ 2.3653035 -1.5460933 -0.875157 ] \t0\ttrue\n",
            "(0)\t 428\t [ 2.085596   -1.6652166  -0.38949814] \t0\ttrue\n",
            "(0)\t 429\t [ 1.3645779  -1.3461099   0.02169676] \t0\ttrue\n",
            "(0)\t 430\t [ 1.9294305 -1.5602406 -0.286708 ] \t0\ttrue\n",
            "(2)\t 431\t [ 2.3155622  -1.7543784  -0.52177703] \t0\tfalse\n",
            "(1)\t 432\t [ 2.1672888  -1.7311891  -0.43170652] \t0\tfalse\n",
            "(1)\t 433\t [ 2.1672888  -1.7311891  -0.43170652] \t0\tfalse\n",
            "(2)\t 434\t [-1.2755463   1.086656    0.46235555] \t1\tfalse\n",
            "(0)\t 435\t [ 1.7287666  -1.5974686  -0.05065014] \t0\ttrue\n",
            "(1)\t 436\t [ 1.839359   -1.3165858  -0.48439798] \t0\tfalse\n",
            "(0)\t 437\t [ 2.0005054 -1.6225067 -0.3690305] \t0\ttrue\n",
            "(2)\t 438\t [ 1.3591735  -1.6013933   0.32762185] \t0\tfalse\n",
            "(2)\t 439\t [ 2.1424494 -1.4560217 -0.7287539] \t0\tfalse\n",
            "(0)\t 440\t [ 1.8077698  -1.5251756  -0.24619865] \t0\ttrue\n",
            "(2)\t 441\t [ 1.984889  -1.4953057 -0.5081837] \t0\tfalse\n",
            "(2)\t 442\t [ 1.5847977  -1.3226986  -0.25635773] \t0\tfalse\n",
            "(0)\t 443\t [ 1.7076949  -1.2373222  -0.57027584] \t0\ttrue\n",
            "(0)\t 444\t [ 1.1600605  -0.8316426  -0.49300775] \t0\ttrue\n",
            "(0)\t 445\t [ 0.6101831   0.34268793 -0.91379035] \t0\ttrue\n",
            "(2)\t 446\t [-1.7506479   1.6209743  -0.13693069] \t1\tfalse\n",
            "(1)\t 447\t [ 2.150384  -0.6430583 -1.4735478] \t0\tfalse\n",
            "(1)\t 448\t [ 2.150384  -0.6430583 -1.4735478] \t0\tfalse\n",
            "(2)\t 449\t [ 2.345459  -1.6531787 -0.6601558] \t0\tfalse\n",
            "(0)\t 450\t [ 0.849024   -1.265111    0.42449299] \t0\ttrue\n",
            "(2)\t 451\t [-0.4044947  -0.89916414  1.6525735 ] \t2\ttrue\n",
            "(0)\t 452\t [ 2.1968777 -1.6319541 -0.535531 ] \t0\ttrue\n",
            "(1)\t 453\t [-1.9237205   1.7760547   0.06981184] \t1\ttrue\n",
            "(0)\t 454\t [ 2.079787   -1.7041823  -0.31151038] \t0\ttrue\n",
            "(2)\t 455\t [ 1.6431017  -1.5174776  -0.12649137] \t0\tfalse\n",
            "(2)\t 456\t [ 1.1024843  -1.317652    0.21032917] \t0\tfalse\n",
            "(0)\t 457\t [ 1.74075   -1.7418612  0.0295413] \t0\ttrue\n",
            "(2)\t 458\t [ 1.8714021 -1.3513741 -0.5212996] \t0\tfalse\n",
            "(2)\t 459\t [ 1.7488394  -1.4811323  -0.34402034] \t0\tfalse\n",
            "(2)\t 460\t [ 2.0516963 -1.6495521 -0.3275995] \t0\tfalse\n",
            "(0)\t 461\t [ 1.4656631 -1.5030123  0.0443606] \t0\ttrue\n",
            "(2)\t 462\t [ 1.8748695  -1.550757   -0.17686714] \t0\tfalse\n",
            "(2)\t 463\t [ 0.7382266  0.5508485 -1.222226 ] \t0\tfalse\n",
            "(1)\t 464\t [ 1.5194161  -1.2183512  -0.36846676] \t0\tfalse\n",
            "(0)\t 465\t [ 1.9650348 -1.309765  -0.6612162] \t0\ttrue\n",
            "(1)\t 466\t [ 2.3714037  -1.6160012  -0.78506976] \t0\tfalse\n",
            "(1)\t 467\t [ 2.3714037  -1.6160012  -0.78506976] \t0\tfalse\n",
            "(2)\t 468\t [ 1.9130388  -1.626077   -0.23674576] \t0\tfalse\n",
            "(2)\t 469\t [ 1.2792741  -1.1460575  -0.08608222] \t0\tfalse\n",
            "(0)\t 470\t [ 2.5650606 -1.2683936 -1.2121444] \t0\ttrue\n",
            "(2)\t 471\t [ 0.9997967 -1.4492885  0.5380863] \t0\tfalse\n",
            "(2)\t 472\t [ 0.7986195 -1.2745515  0.5926314] \t0\tfalse\n",
            "(0)\t 473\t [ 2.0449295  -1.4321754  -0.68192685] \t0\ttrue\n",
            "(2)\t 474\t [ 0.21960051 -0.7281312   0.6683084 ] \t2\ttrue\n",
            "(2)\t 475\t [ 0.09373882 -1.1805309   1.248623  ] \t2\ttrue\n",
            "(2)\t 476\t [ 2.0057187  -1.6823905  -0.23704447] \t0\tfalse\n",
            "(2)\t 477\t [ 0.0051403 -1.1714462  1.4037577] \t2\ttrue\n",
            "(0)\t 478\t [ 2.3176594 -0.9229819 -1.3176949] \t0\ttrue\n",
            "(0)\t 479\t [ 2.0773149 -1.5337179 -0.6522051] \t0\ttrue\n",
            "(2)\t 480\t [ 1.14467    -1.3821292   0.38434812] \t0\tfalse\n",
            "(2)\t 481\t [ 0.4522751  -1.0823823   0.56816256] \t2\ttrue\n",
            "(1)\t 482\t [-1.8103441   1.5713362   0.15729918] \t1\ttrue\n",
            "(0)\t 483\t [ 2.0153098 -1.5849452 -0.3926771] \t0\ttrue\n",
            "(2)\t 484\t [ 1.6845962 -1.483222  -0.171775 ] \t0\tfalse\n",
            "(1)\t 485\t [ 1.3792663  -1.4501401   0.10122535] \t0\tfalse\n",
            "(1)\t 486\t [ 1.3792663  -1.4501401   0.10122535] \t0\tfalse\n",
            "(2)\t 487\t [-1.5992802   1.6234983  -0.24136105] \t1\tfalse\n",
            "(0)\t 488\t [ 1.209581  -1.4173456  0.4241065] \t0\ttrue\n",
            "(1)\t 489\t [-1.888033   1.5667996  0.4283803] \t1\ttrue\n",
            "(0)\t 490\t [ 1.8720306  -1.665217   -0.22651435] \t0\ttrue\n",
            "(0)\t 491\t [ 2.1154146 -0.6321295 -1.3569835] \t0\ttrue\n",
            "(1)\t 492\t [-1.4452363   0.17251568  1.5531154 ] \t2\tfalse\n",
            "(1)\t 493\t [ 0.12013278 -1.2640496   1.3150301 ] \t2\tfalse\n",
            "(2)\t 494\t [ 2.1880727  -1.6305411  -0.53814685] \t0\tfalse\n",
            "(0)\t 495\t [ 2.0154123 -1.7032002 -0.3675293] \t0\ttrue\n",
            "(1)\t 496\t [-0.32039794 -0.47730464  1.0942823 ] \t2\tfalse\n",
            "(2)\t 497\t [ 1.3796319 -1.3576671  0.0978889] \t0\tfalse\n",
            "(0)\t 498\t [ 1.665847   -1.4819449  -0.22302519] \t0\ttrue\n",
            "(2)\t 499\t [-0.03919768 -0.7864942   0.9758127 ] \t2\ttrue\n",
            "(2)\t 500\t [ 0.7137505  -1.3008915   0.68652916] \t0\tfalse\n",
            "(0)\t 501\t [ 2.2744865 -1.6907871 -0.4885603] \t0\ttrue\n",
            "(0)\t 502\t [ 1.3404125 -1.517917   0.3318456] \t0\ttrue\n",
            "(0)\t 503\t [ 2.2730618 -1.4990531 -0.7235205] \t0\ttrue\n",
            "(2)\t 504\t [ 0.19341649 -0.8127711   1.066432  ] \t2\ttrue\n",
            "(1)\t 505\t [ 0.09047136 -1.0521873   1.1520792 ] \t2\tfalse\n",
            "(1)\t 506\t [-0.5468477  1.2027103 -0.734926 ] \t1\ttrue\n",
            "(2)\t 507\t [ 0.5354525  -0.9384374   0.48783633] \t0\tfalse\n",
            "(0)\t 508\t [ 1.5696381 -1.6154293  0.2622929] \t0\ttrue\n",
            "(0)\t 509\t [ 2.1967244  -1.522575   -0.62270576] \t0\ttrue\n",
            "(2)\t 510\t [ 1.636388  -1.2393364 -0.4299952] \t0\tfalse\n",
            "(1)\t 511\t [-0.5688954  -0.21537048  0.9707968 ] \t2\tfalse\n",
            "(1)\t 512\t [ 1.3774561  -1.2323512  -0.17199208] \t0\tfalse\n",
            "(1)\t 513\t [ 0.926254   -1.2060102   0.38400096] \t0\tfalse\n",
            "(1)\t 514\t [-1.9961451  1.4422809  0.4827927] \t1\ttrue\n",
            "(2)\t 515\t [ 0.44123945 -1.1389476   0.8030405 ] \t2\ttrue\n",
            "(2)\t 516\t [ 0.58761674 -1.1811794   0.6797813 ] \t2\ttrue\n",
            "(0)\t 517\t [ 1.9083235  -1.563202   -0.16857344] \t0\ttrue\n",
            "(2)\t 518\t [ 1.0634716  -1.2878585   0.37810972] \t0\tfalse\n",
            "(0)\t 519\t [ 1.7928607  -1.7429138  -0.02747612] \t0\ttrue\n",
            "(2)\t 520\t [ 1.2429922 -1.5034016  0.3139296] \t0\tfalse\n",
            "(2)\t 521\t [ 1.475372  -0.7803287 -0.6440214] \t0\tfalse\n",
            "(2)\t 522\t [ 1.1883494  -1.4225026   0.39339826] \t0\tfalse\n",
            "(1)\t 523\t [ 1.2897456 -1.2869084  0.1843657] \t0\tfalse\n",
            "(2)\t 524\t [ 0.14751774  0.61425894 -0.7248219 ] \t1\tfalse\n",
            "(1)\t 525\t [-1.4687599   1.3715001   0.11853383] \t1\ttrue\n",
            "(0)\t 526\t [ 2.173345   -1.6698011  -0.46182132] \t0\ttrue\n",
            "(0)\t 527\t [ 2.4109297 -1.4946486 -0.9039889] \t0\ttrue\n",
            "(2)\t 528\t [ 1.6896261 -1.5832065  0.0516567] \t0\tfalse\n",
            "(2)\t 529\t [ 0.8434336 -1.3747376  0.6109556] \t0\tfalse\n",
            "(2)\t 530\t [ 2.1254659 -1.6701003 -0.3973383] \t0\tfalse\n",
            "(0)\t 531\t [ 0.4076914   0.44774485 -0.7315156 ] \t1\tfalse\n",
            "(0)\t 532\t [ 1.2104127  -1.3223916   0.24127425] \t0\ttrue\n",
            "(0)\t 533\t [-0.70242107  1.2198749  -0.6654704 ] \t1\tfalse\n",
            "(0)\t 534\t [ 1.4098555  -1.2567488  -0.23155187] \t0\ttrue\n",
            "(2)\t 535\t [ 0.8772426 -1.3244308  0.5980808] \t0\tfalse\n",
            "(1)\t 536\t [-1.5603404   1.3995172   0.04397501] \t1\ttrue\n",
            "(0)\t 537\t [ 2.1061964  -1.618186   -0.57299113] \t0\ttrue\n",
            "(2)\t 538\t [ 1.3081167 -1.5244116  0.2002119] \t0\tfalse\n",
            "(0)\t 539\t [ 1.8663179  -1.6632377  -0.12493505] \t0\ttrue\n",
            "(2)\t 540\t [ 0.66733813 -1.5696691   1.0376796 ] \t2\ttrue\n",
            "(1)\t 541\t [ 1.9581027  -1.6533394  -0.18909879] \t0\tfalse\n",
            "(0)\t 542\t [ 0.9941374  -0.75851864 -0.28583393] \t0\ttrue\n",
            "(0)\t 543\t [ 1.7148279  -1.6621289  -0.03000482] \t0\ttrue\n",
            "(0)\t 544\t [-0.9809396  -0.00548527  1.4199557 ] \t2\tfalse\n",
            "(0)\t 545\t [ 2.44236   -1.1465464 -1.256616 ] \t0\ttrue\n",
            "(0)\t 546\t [ 1.5815014  -1.6503965  -0.02257614] \t0\ttrue\n",
            "(2)\t 547\t [ 1.6579739  -1.5122094  -0.14292447] \t0\tfalse\n",
            "(1)\t 548\t [ 2.5267994 -1.1505717 -1.3296068] \t0\tfalse\n",
            "(0)\t 549\t [ 1.5017236  -0.15371875 -1.3168343 ] \t0\ttrue\n",
            "(1)\t 550\t [-1.9148972  1.4989742  0.7042301] \t1\ttrue\n",
            "(1)\t 551\t [-0.81897575 -0.12890142  1.2400582 ] \t2\tfalse\n",
            "(2)\t 552\t [ 1.9825271  -1.6045121  -0.23931283] \t0\tfalse\n",
            "(0)\t 553\t [ 2.5008554 -1.0715737 -1.3667094] \t0\ttrue\n",
            "(2)\t 554\t [-1.7553905  1.5379155  0.1721974] \t1\tfalse\n",
            "(2)\t 555\t [ 1.7343699  -1.6570789  -0.11837712] \t0\tfalse\n",
            "(2)\t 556\t [ 1.3816451  -1.5263525   0.21467179] \t0\tfalse\n",
            "(1)\t 557\t [ 0.48180932 -0.5753498   0.20522997] \t0\tfalse\n",
            "(1)\t 558\t [ 0.9470671  -1.2086736   0.32673147] \t0\tfalse\n",
            "(0)\t 559\t [ 0.60988265 -1.2278038   0.845615  ] \t2\tfalse\n",
            "(2)\t 560\t [-0.22713594 -0.11875059  0.610039  ] \t2\ttrue\n",
            "(2)\t 561\t [ 1.8606142  -1.5892011  -0.26525897] \t0\tfalse\n",
            "(2)\t 562\t [ 1.027049   -0.14680003 -0.85939914] \t0\tfalse\n",
            "(1)\t 563\t [ 0.90592486 -1.3748527   0.58267057] \t0\tfalse\n",
            "(0)\t 564\t [ 1.3549227 -1.4178606  0.1758616] \t0\ttrue\n",
            "(0)\t 565\t [ 2.417466   -1.5945497  -0.72575706] \t0\ttrue\n",
            "(1)\t 566\t [-1.6718062   1.1106426   0.73455083] \t1\ttrue\n",
            "(2)\t 567\t [-1.8469883   1.6452757   0.10444604] \t1\tfalse\n",
            "(2)\t 568\t [ 0.46803853 -1.1964061   0.981014  ] \t2\ttrue\n",
            "(1)\t 569\t [ 0.79249024 -1.1542083   0.43606272] \t0\tfalse\n",
            "(1)\t 570\t [ 0.79249024 -1.1542083   0.43606272] \t0\tfalse\n",
            "(2)\t 571\t [ 0.7643541 -1.4287285  0.7483833] \t0\tfalse\n",
            "(1)\t 572\t [ 2.2204485  -1.4370908  -0.64818245] \t0\tfalse\n",
            "(1)\t 573\t [ 2.2204485  -1.4370908  -0.64818245] \t0\tfalse\n",
            "(2)\t 574\t [-0.68365735 -0.7858577   1.7731463 ] \t2\ttrue\n",
            "(2)\t 575\t [ 1.2599925  -1.5086417   0.25597662] \t0\tfalse\n",
            "(0)\t 576\t [ 1.3926878 -1.5885782  0.2993245] \t0\ttrue\n",
            "(0)\t 577\t [ 1.5372641  -1.2375653  -0.26577544] \t0\ttrue\n",
            "(2)\t 578\t [ 2.6084037 -1.2746336 -1.3553096] \t0\tfalse\n",
            "(2)\t 579\t [ 1.7571155  -1.623556   -0.08876355] \t0\tfalse\n",
            "(2)\t 580\t [ 1.7206997  -1.5081657  -0.18665448] \t0\tfalse\n",
            "(1)\t 581\t [-1.9523867   1.683189    0.10053612] \t1\ttrue\n",
            "(2)\t 582\t [ 1.4186119  -1.1735586  -0.17425658] \t0\tfalse\n",
            "(2)\t 583\t [ 1.1971151 -1.4197384  0.212369 ] \t0\tfalse\n",
            "(0)\t 584\t [ 1.969318  -1.6763316 -0.260228 ] \t0\ttrue\n",
            "(0)\t 585\t [ 1.5043275 -1.2509003 -0.2997339] \t0\ttrue\n",
            "(0)\t 586\t [ 2.2434914 -1.1426015 -1.1002777] \t0\ttrue\n",
            "(0)\t 587\t [-0.07684596 -1.200497    1.3776044 ] \t2\tfalse\n",
            "(0)\t 588\t [ 1.5781726  -1.4086565   0.06586803] \t0\ttrue\n",
            "(0)\t 589\t [ 2.0707407 -1.6738123 -0.3884476] \t0\ttrue\n",
            "(1)\t 590\t [ 2.2837336 -1.6663902 -0.5818436] \t0\tfalse\n",
            "(2)\t 591\t [-0.00923139 -0.80527025  0.9033356 ] \t2\ttrue\n",
            "(0)\t 592\t [ 0.61802846 -1.5233847   0.9869525 ] \t2\tfalse\n",
            "(1)\t 593\t [ 0.0377151 -0.9856509  1.1657113] \t2\tfalse\n",
            "(0)\t 594\t [ 1.3296099  -1.6466818   0.26962176] \t0\ttrue\n",
            "(2)\t 595\t [ 2.5332227 -1.3952851 -1.1639875] \t0\tfalse\n",
            "(0)\t 596\t [ 1.4621814  -1.6948924   0.26538676] \t0\ttrue\n",
            "(2)\t 597\t [ 0.8736171  -0.26813638 -0.69196546] \t0\tfalse\n",
            "(1)\t 598\t [ 1.3501745  -1.5078685   0.21882007] \t0\tfalse\n",
            "(1)\t 599\t [ 1.3501745  -1.5078685   0.21882007] \t0\tfalse\n",
            "(0)\t 600\t [ 1.7956625  -1.6066443  -0.19350982] \t0\ttrue\n",
            "(2)\t 601\t [ 1.8159941  -1.5193397  -0.18368922] \t0\tfalse\n",
            "(2)\t 602\t [-1.8250715   1.6140745   0.05183333] \t1\tfalse\n",
            "(0)\t 603\t [ 1.3261584  -0.813846   -0.47277927] \t0\ttrue\n",
            "(0)\t 604\t [ 1.938358   -0.46997425 -1.4968415 ] \t0\ttrue\n",
            "(0)\t 605\t [ 0.92006195 -1.2887086   0.49782404] \t0\ttrue\n",
            "(0)\t 606\t [ 2.0475373  -1.767948   -0.22439261] \t0\ttrue\n",
            "(2)\t 607\t [ 0.7567346  -1.1799648   0.56350774] \t0\tfalse\n",
            "(0)\t 608\t [ 1.6365209 -0.6961001 -1.0199987] \t0\ttrue\n",
            "(0)\t 609\t [ 1.8634194 -1.6956698 -0.2898915] \t0\ttrue\n",
            "(2)\t 610\t [ 1.5829583 -1.038412  -0.774001 ] \t0\tfalse\n",
            "(0)\t 611\t [ 1.7788309 -1.0852319 -0.84241  ] \t0\ttrue\n",
            "(2)\t 612\t [ 1.3525522  -1.5660688   0.26756886] \t0\tfalse\n",
            "(2)\t 613\t [-1.8953432  1.442406   0.3594183] \t1\tfalse\n",
            "(0)\t 614\t [ 1.598383  -1.6595447  0.168662 ] \t0\ttrue\n",
            "(2)\t 615\t [ 2.1532686  -1.7070851  -0.40247813] \t0\tfalse\n",
            "(2)\t 616\t [ 0.46213922 -0.6591785   0.23860022] \t0\tfalse\n",
            "(1)\t 617\t [ 1.1987082  -0.77900636 -0.32354292] \t0\tfalse\n",
            "(2)\t 618\t [ 0.78239477 -1.2118125   0.59973323] \t0\tfalse\n",
            "(1)\t 619\t [-2.090042    1.6117913   0.42187354] \t1\ttrue\n",
            "(0)\t 620\t [ 0.6371573  -0.40419537 -0.22163789] \t0\ttrue\n",
            "(0)\t 621\t [-0.7432184   1.2891966  -0.71715045] \t1\tfalse\n",
            "(0)\t 622\t [ 2.3097265  -1.6320677  -0.71338165] \t0\ttrue\n",
            "(0)\t 623\t [ 1.1386546   0.15569992 -1.2814398 ] \t0\ttrue\n",
            "(2)\t 624\t [ 2.1098988 -1.4720904 -0.5925695] \t0\tfalse\n",
            "(1)\t 625\t [-1.987799   1.6561666  0.3812879] \t1\ttrue\n",
            "(2)\t 626\t [ 1.5723422  -1.5964714   0.13191347] \t0\tfalse\n",
            "(1)\t 627\t [ 0.94614  -1.149285  0.311521] \t0\tfalse\n",
            "(2)\t 628\t [ 1.4193243  -1.5876169   0.29283154] \t0\tfalse\n",
            "(2)\t 629\t [ 2.076488   -1.6600103  -0.44370314] \t0\tfalse\n",
            "(0)\t 630\t [ 1.9617335 -1.5310701 -0.3621202] \t0\ttrue\n",
            "(0)\t 631\t [ 1.3426642  -0.18848862 -1.0552291 ] \t0\ttrue\n",
            "(0)\t 632\t [ 2.123589   -1.7219416  -0.37886485] \t0\ttrue\n",
            "(0)\t 633\t [ 1.4998562  -0.77133286 -0.67150456] \t0\ttrue\n",
            "(0)\t 634\t [ 1.7073038  -1.3786689  -0.28360668] \t0\ttrue\n",
            "(0)\t 635\t [ 1.8922545 -1.6666245 -0.2053982] \t0\ttrue\n",
            "(2)\t 636\t [ 1.2713463  -1.1727594  -0.11940768] \t0\tfalse\n",
            "(2)\t 637\t [ 2.2736077  -1.6127113  -0.76094615] \t0\tfalse\n",
            "(0)\t 638\t [ 2.1934042  -1.7000359  -0.43076402] \t0\ttrue\n",
            "(2)\t 639\t [ 2.1458642  -1.6997678  -0.41000637] \t0\tfalse\n",
            "(2)\t 640\t [ 2.0623422 -1.6012053 -0.5306019] \t0\tfalse\n",
            "(0)\t 641\t [ 1.2530469 -0.9272648 -0.4653945] \t0\ttrue\n",
            "(2)\t 642\t [ 1.9375528 -1.6103457 -0.2672939] \t0\tfalse\n",
            "(0)\t 643\t [ 1.0401704  -1.1976238   0.12892029] \t0\ttrue\n",
            "(2)\t 644\t [ 1.7969742  -1.6554905  -0.13800253] \t0\tfalse\n",
            "(1)\t 645\t [-1.0841385   1.3852477  -0.44115052] \t1\ttrue\n",
            "(1)\t 646\t [ 1.0377942  -1.258244    0.39397895] \t0\tfalse\n",
            "(1)\t 647\t [ 1.0277752  -0.4664228  -0.50018394] \t0\tfalse\n",
            "(2)\t 648\t [ 1.912303   -1.6444535  -0.14841463] \t0\tfalse\n",
            "(2)\t 649\t [ 2.261292   -0.83096313 -1.3855162 ] \t0\tfalse\n",
            "(1)\t 650\t [ 0.6949842 -1.3513609  0.7655327] \t2\tfalse\n",
            "(1)\t 651\t [ 0.6798661  -1.0579112   0.42607743] \t0\tfalse\n",
            "(2)\t 652\t [ 1.9240535  -1.5776148  -0.35095802] \t0\tfalse\n",
            "(0)\t 653\t [ 2.382457  -1.5932075 -0.9146524] \t0\ttrue\n",
            "(0)\t 654\t [ 2.4968133 -1.0150224 -1.3618443] \t0\ttrue\n",
            "(0)\t 655\t [ 1.6397929 -1.2492045 -0.6804136] \t0\ttrue\n",
            "(0)\t 656\t [ 0.39823964 -0.9253812   0.49102113] \t2\tfalse\n",
            "(1)\t 657\t [-0.24002086 -0.77315664  1.1984062 ] \t2\tfalse\n",
            "(0)\t 658\t [ 1.5595212 -1.482992  -0.0497415] \t0\ttrue\n",
            "(1)\t 659\t [ 2.0898814  -1.6865816  -0.40266415] \t0\tfalse\n",
            "(1)\t 660\t [ 2.0898814  -1.6865816  -0.40266415] \t0\tfalse\n",
            "(1)\t 661\t [ 1.5615734  -1.5213207   0.07505867] \t0\tfalse\n",
            "(0)\t 662\t [ 1.3866962  -1.4808508   0.23025937] \t0\ttrue\n",
            "(2)\t 663\t [-1.6351808  1.1108391  0.8664411] \t1\tfalse\n",
            "(2)\t 664\t [ 1.4565998  -1.6114945   0.23283967] \t0\tfalse\n",
            "(2)\t 665\t [ 0.14896764 -0.23339655  0.09257376] \t0\tfalse\n",
            "(1)\t 666\t [ 2.1252246 -1.5479239 -0.5214672] \t0\tfalse\n",
            "(1)\t 667\t [ 2.1252246 -1.5479239 -0.5214672] \t0\tfalse\n",
            "(0)\t 668\t [ 1.679404  -1.0508322 -0.6994504] \t0\ttrue\n",
            "(0)\t 669\t [ 2.0642214 -1.470571  -0.7286279] \t0\ttrue\n",
            "(0)\t 670\t [ 0.9383223  -1.3090352   0.55891174] \t0\ttrue\n",
            "(2)\t 671\t [ 2.030261  -1.1773876 -0.8409937] \t0\tfalse\n",
            "(0)\t 672\t [-0.00128546  0.01644477  0.14522746] \t2\tfalse\n",
            "(1)\t 673\t [ 1.7471167  -1.5713867  -0.17025803] \t0\tfalse\n",
            "(1)\t 674\t [-1.1585447   1.38108    -0.05907374] \t1\ttrue\n",
            "(0)\t 675\t [ 1.3987669  -1.4620689   0.08121406] \t0\ttrue\n",
            "(2)\t 676\t [ 2.102263   -1.5914758  -0.42777544] \t0\tfalse\n",
            "(2)\t 677\t [ 0.60431206 -0.8412042   0.45792297] \t0\tfalse\n",
            "(0)\t 678\t [-1.5202013   1.6230544  -0.24568295] \t1\tfalse\n",
            "(0)\t 679\t [ 1.8474778  -1.4830986  -0.45929518] \t0\ttrue\n",
            "(2)\t 680\t [ 1.8861625  -1.3216324  -0.41144606] \t0\tfalse\n",
            "(0)\t 681\t [ 2.1318169  -1.7189182  -0.36916095] \t0\ttrue\n",
            "(0)\t 682\t [ 1.5289693 -1.5119249  0.0919219] \t0\ttrue\n",
            "(2)\t 683\t [-0.17421542 -1.133792    1.56484   ] \t2\ttrue\n",
            "(0)\t 684\t [ 1.747278   -1.501835   -0.22382009] \t0\ttrue\n",
            "(0)\t 685\t [ 1.50483    -1.5294064   0.15500483] \t0\ttrue\n",
            "(0)\t 686\t [ 1.2488757  -1.4495599   0.36872482] \t0\ttrue\n",
            "(0)\t 687\t [ 0.24833933 -0.6303842   0.7535044 ] \t2\tfalse\n",
            "(0)\t 688\t [ 1.9622157  -1.5591835  -0.38306093] \t0\ttrue\n",
            "(0)\t 689\t [ 0.63047206 -1.2490038   0.9083208 ] \t2\tfalse\n",
            "(0)\t 690\t [ 1.9945712 -1.5749136 -0.3143267] \t0\ttrue\n",
            "(1)\t 691\t [ 1.6899979  -1.3551058  -0.27468908] \t0\tfalse\n",
            "(2)\t 692\t [ 2.0627117 -1.4197034 -0.6250044] \t0\tfalse\n",
            "(1)\t 693\t [ 1.8337809  -1.3861351  -0.34393847] \t0\tfalse\n",
            "(1)\t 694\t [ 1.8337809  -1.3861351  -0.34393847] \t0\tfalse\n",
            "(1)\t 695\t [-1.9138165  1.51114    0.608546 ] \t1\ttrue\n",
            "(0)\t 696\t [ 2.3105998 -1.6428431 -0.6596267] \t0\ttrue\n",
            "(0)\t 697\t [ 0.46539107 -0.65094686  0.25114205] \t0\ttrue\n",
            "(1)\t 698\t [-2.0580626  1.6426417  0.4065819] \t1\ttrue\n",
            "(1)\t 699\t [-0.49395418 -0.8830752   1.6014009 ] \t2\tfalse\n",
            "(1)\t 700\t [-1.9056123   1.6087601   0.16694297] \t1\ttrue\n",
            "(0)\t 701\t [ 1.6673454  -1.6415125   0.09720691] \t0\ttrue\n",
            "(1)\t 702\t [-0.13651747 -0.6417272   1.0999869 ] \t2\tfalse\n",
            "(2)\t 703\t [ 0.93275076 -1.3099544   0.41687617] \t0\tfalse\n",
            "(2)\t 704\t [ 1.1036042  -1.3382064   0.38122547] \t0\tfalse\n",
            "(2)\t 705\t [ 1.6689466  -1.4435416  -0.16938993] \t0\tfalse\n",
            "(2)\t 706\t [-1.0142335  1.2947334 -0.5355186] \t1\tfalse\n",
            "(1)\t 707\t [-0.6287973  -0.76852596  1.7761798 ] \t2\tfalse\n",
            "(0)\t 708\t [ 2.0661762  -1.6584574  -0.23931243] \t0\ttrue\n",
            "(2)\t 709\t [ 0.92409146 -1.0675973   0.17527726] \t0\tfalse\n",
            "(1)\t 710\t [ 1.5249528  -1.1882067  -0.28355432] \t0\tfalse\n",
            "(2)\t 711\t [ 1.7507194  -1.4872196  -0.33662736] \t0\tfalse\n",
            "(1)\t 712\t [ 1.7996026 -1.6278218 -0.2615429] \t0\tfalse\n",
            "(2)\t 713\t [ 1.9619213 -1.730724  -0.2019808] \t0\tfalse\n",
            "(0)\t 714\t [ 1.8855889  -1.6258739  -0.23234081] \t0\ttrue\n",
            "(1)\t 715\t [ 0.31701532 -0.8048618   0.7904137 ] \t2\tfalse\n",
            "(0)\t 716\t [ 2.3742366 -1.0495809 -1.225754 ] \t0\ttrue\n",
            "(1)\t 717\t [ 0.87979853 -1.3615466   0.57251525] \t0\tfalse\n",
            "(1)\t 718\t [-0.07414559 -0.99630207  1.2155373 ] \t2\tfalse\n",
            "(0)\t 719\t [ 1.9049019 -1.5138    -0.4171899] \t0\ttrue\n",
            "(2)\t 720\t [ 1.2214067 -1.4750067  0.2720074] \t0\tfalse\n",
            "(1)\t 721\t [-1.5829183   0.65081966  1.3181005 ] \t2\tfalse\n",
            "(1)\t 722\t [-1.2130371   1.4764501  -0.39069661] \t1\ttrue\n",
            "(0)\t 723\t [ 2.0117002 -1.5778813 -0.4546805] \t0\ttrue\n",
            "(2)\t 724\t [ 1.5731374  -0.89122045 -0.77657413] \t0\tfalse\n",
            "(0)\t 725\t [ 1.3257403  -0.5175429  -0.97433925] \t0\ttrue\n",
            "(2)\t 726\t [ 1.2422972 -1.1402642 -0.0551887] \t0\tfalse\n",
            "(0)\t 727\t [ 2.3266063 -1.2132791 -1.1345217] \t0\ttrue\n",
            "(0)\t 728\t [ 2.2979062 -1.6696364 -0.6532392] \t0\ttrue\n",
            "(2)\t 729\t [-0.0257024  -0.6140408   0.81754965] \t2\ttrue\n",
            "(1)\t 730\t [ 1.1124686  -1.3095307   0.44120324] \t0\tfalse\n",
            "(0)\t 731\t [ 1.9692714  -1.3501719  -0.69147396] \t0\ttrue\n",
            "(1)\t 732\t [ 1.0183082  -1.1917335   0.23802827] \t0\tfalse\n",
            "(2)\t 733\t [ 0.7135378 -1.2605333  0.6490593] \t0\tfalse\n",
            "(2)\t 734\t [ 2.1666045 -1.6381595 -0.5685019] \t0\tfalse\n",
            "(2)\t 735\t [ 0.73226964  0.36529237 -1.0660604 ] \t0\tfalse\n",
            "(2)\t 736\t [ 0.9526001 -1.2860585  0.4043189] \t0\tfalse\n",
            "(1)\t 737\t [-1.5001163  1.1777264  0.6028435] \t1\ttrue\n",
            "(1)\t 738\t [-1.7225491  0.9793811  0.8030696] \t1\ttrue\n",
            "(2)\t 739\t [-1.808448   1.5530151  0.122519 ] \t1\tfalse\n",
            "(0)\t 740\t [ 1.4335904  -1.1488799  -0.38738158] \t0\ttrue\n",
            "(1)\t 741\t [ 0.89046407 -1.120835    0.3089366 ] \t0\tfalse\n",
            "(2)\t 742\t [ 0.5725417 -1.1361433  0.7843434] \t2\ttrue\n",
            "(1)\t 743\t [-1.98767     1.5922986   0.43336976] \t1\ttrue\n",
            "(1)\t 744\t [-1.5861349   1.4796715  -0.05929801] \t1\ttrue\n",
            "(2)\t 745\t [ 2.4712133 -1.0673765 -1.3318017] \t0\tfalse\n",
            "(2)\t 746\t [ 0.8310255  0.5325683 -1.2638713] \t0\tfalse\n",
            "(1)\t 747\t [ 0.69508845 -1.0816237   0.45407885] \t0\tfalse\n",
            "(1)\t 748\t [ 1.5663688 -1.5849565  0.1741563] \t0\tfalse\n",
            "(1)\t 749\t [ 1.5663688 -1.5849565  0.1741563] \t0\tfalse\n",
            "(1)\t 750\t [-1.9379176   1.7473022   0.09672081] \t1\ttrue\n",
            "(0)\t 751\t [ 2.1068168 -1.2127968 -1.0332794] \t0\ttrue\n",
            "(1)\t 752\t [ 1.4184813  -1.2159771  -0.13163285] \t0\tfalse\n",
            "(2)\t 753\t [ 1.3015579  -1.5010238   0.32050654] \t0\tfalse\n",
            "(0)\t 754\t [ 2.1759472  -0.63857955 -1.4080615 ] \t0\ttrue\n",
            "(2)\t 755\t [ 1.3791095 -1.5973294  0.1991301] \t0\tfalse\n",
            "(1)\t 756\t [ 0.25909373  0.17476177 -0.43432972] \t0\tfalse\n",
            "(1)\t 757\t [-0.5972345   0.10878108  0.65049076] \t2\tfalse\n",
            "(2)\t 758\t [ 1.9488639 -1.5998951 -0.344889 ] \t0\tfalse\n",
            "(2)\t 759\t [ 1.4734416  -1.5526112   0.16548492] \t0\tfalse\n",
            "(0)\t 760\t [ 1.8367405  -1.6979812  -0.05022876] \t0\ttrue\n",
            "(1)\t 761\t [-1.8120013   0.80609846  1.28856   ] \t2\tfalse\n",
            "(1)\t 762\t [ 2.1210437 -1.3376504 -0.8483153] \t0\tfalse\n",
            "(1)\t 763\t [ 2.1210437 -1.3376504 -0.8483153] \t0\tfalse\n",
            "(1)\t 764\t [-1.6469046  0.7052187  1.1911093] \t2\tfalse\n",
            "(2)\t 765\t [-0.28898188 -0.7429662   1.1570235 ] \t2\ttrue\n",
            "(2)\t 766\t [-1.2839704  -0.19311108  1.7780199 ] \t2\ttrue\n",
            "(0)\t 767\t [ 2.1750748  -1.6280745  -0.60172373] \t0\ttrue\n",
            "(2)\t 768\t [ 1.8432397  -1.5426382  -0.23433273] \t0\tfalse\n",
            "(2)\t 769\t [ 1.4478959  -0.00521859 -1.4472561 ] \t0\tfalse\n",
            "(0)\t 770\t [ 1.7773257  -1.5600508  -0.23420087] \t0\ttrue\n",
            "(2)\t 771\t [ 0.02005289 -0.66178256  0.9326869 ] \t2\ttrue\n",
            "(2)\t 772\t [ 1.005748   -1.3446614   0.43570766] \t0\tfalse\n",
            "(1)\t 773\t [ 0.23763965 -1.054597    0.95283896] \t2\tfalse\n",
            "(1)\t 774\t [-2.0086322   1.6616691   0.20383899] \t1\ttrue\n",
            "(2)\t 775\t [ 2.0002666 -1.6191361 -0.2982327] \t0\tfalse\n",
            "(2)\t 776\t [ 0.9152415  -0.5991518  -0.29640484] \t0\tfalse\n",
            "(0)\t 777\t [ 1.4883631 -0.8160473 -0.7547883] \t0\ttrue\n",
            "(0)\t 778\t [ 1.5854031  -1.1978345  -0.46983707] \t0\ttrue\n",
            "(0)\t 779\t [ 2.364246   -0.96746457 -1.3964943 ] \t0\ttrue\n",
            "(2)\t 780\t [-0.21339938  0.50152934 -0.01937671] \t1\tfalse\n",
            "(1)\t 781\t [ 2.1080601 -1.6221663 -0.5062457] \t0\tfalse\n",
            "(0)\t 782\t [ 1.9518296 -1.5913281 -0.2229556] \t0\ttrue\n",
            "(0)\t 783\t [ 1.9353515 -1.6388096 -0.2558617] \t0\ttrue\n",
            "(2)\t 784\t [ 1.9859186  -1.5646917  -0.34565896] \t0\tfalse\n",
            "(2)\t 785\t [ 1.4358071 -1.3594266 -0.0895824] \t0\tfalse\n",
            "(2)\t 786\t [-0.85056996 -0.25847888  1.5129021 ] \t2\ttrue\n",
            "(2)\t 787\t [-1.282038    1.4025327  -0.41681975] \t1\tfalse\n",
            "(0)\t 788\t [-0.91043353 -0.30453798  1.5110132 ] \t2\tfalse\n",
            "(2)\t 789\t [-0.6732383 -0.8064854  1.7635798] \t2\ttrue\n",
            "(0)\t 790\t [ 1.3864104  -1.4936107   0.15837084] \t0\ttrue\n",
            "(2)\t 791\t [ 1.8476487 -1.5172457 -0.4198359] \t0\tfalse\n",
            "(2)\t 792\t [ 0.03053558 -0.67213136  0.8112036 ] \t2\ttrue\n",
            "(2)\t 793\t [ 1.1799895  -1.2597235   0.15942721] \t0\tfalse\n",
            "(0)\t 794\t [ 1.9973296  -1.6509589  -0.39003822] \t0\ttrue\n",
            "(2)\t 795\t [ 2.2332795  -1.7128109  -0.57276386] \t0\tfalse\n",
            "(1)\t 796\t [ 1.9854354  -1.6556057  -0.18250136] \t0\tfalse\n",
            "(1)\t 797\t [ 1.9854354  -1.6556057  -0.18250136] \t0\tfalse\n",
            "(2)\t 798\t [ 1.9489866  -1.5788932  -0.38657707] \t0\tfalse\n",
            "(1)\t 799\t [ 0.639483   -1.1602501   0.66124094] \t2\tfalse\n",
            "(2)\t 800\t [ 0.8778117 -1.3617952  0.6962775] \t0\tfalse\n",
            "(2)\t 801\t [ 2.2568357 -1.6670469 -0.5499645] \t0\tfalse\n",
            "(1)\t 802\t [-1.1615163   0.17861503  1.2728547 ] \t2\tfalse\n",
            "(0)\t 803\t [ 1.9060754 -1.6604164 -0.2620762] \t0\ttrue\n",
            "(2)\t 804\t [ 1.989978   -1.6676363  -0.27841508] \t0\tfalse\n",
            "(1)\t 805\t [ 0.50651366 -0.71849513  0.16203393] \t0\tfalse\n",
            "(2)\t 806\t [ 1.7001892  -1.6242483  -0.04350509] \t0\tfalse\n",
            "(2)\t 807\t [-0.20329507 -1.1666447   1.5643774 ] \t2\ttrue\n",
            "(1)\t 808\t [-1.1906208   0.17965609  1.2656742 ] \t2\tfalse\n",
            "(0)\t 809\t [ 1.5914109  -1.5047129  -0.07160184] \t0\ttrue\n",
            "(2)\t 810\t [ 1.9755926  -1.335054   -0.70147157] \t0\tfalse\n",
            "(1)\t 811\t [ 2.107904  -1.5815207 -0.5775896] \t0\tfalse\n",
            "(1)\t 812\t [-0.24365585 -0.7360423   1.13692   ] \t2\tfalse\n",
            "(0)\t 813\t [-0.36459258 -0.2899336   0.93090314] \t2\tfalse\n",
            "(0)\t 814\t [ 1.4938351  -1.4863547   0.14799911] \t0\ttrue\n",
            "(0)\t 815\t [ 1.9825364  -1.7156245  -0.29820797] \t0\ttrue\n",
            "(2)\t 816\t [ 1.5539411  -1.5384711   0.06761529] \t0\tfalse\n",
            "(2)\t 817\t [ 1.0694021 -1.3398091  0.41395  ] \t0\tfalse\n",
            "(0)\t 818\t [ 2.041613  -1.5755025 -0.4201045] \t0\ttrue\n",
            "(0)\t 819\t [ 2.3771918 -1.3777767 -1.0197744] \t0\ttrue\n",
            "(1)\t 820\t [-1.7261404   1.1399965   0.69669837] \t1\ttrue\n",
            "(1)\t 821\t [-1.7261404   1.1399965   0.69669837] \t1\ttrue\n",
            "(0)\t 822\t [ 2.0458553  -1.5822843  -0.48957276] \t0\ttrue\n",
            "(0)\t 823\t [ 2.3975387 -1.6227489 -0.8394765] \t0\ttrue\n",
            "(1)\t 824\t [ 1.7184435 -1.6059196 -0.0589924] \t0\tfalse\n",
            "(1)\t 825\t [ 1.7184435 -1.6059196 -0.0589924] \t0\tfalse\n",
            "(0)\t 826\t [ 1.8601114  -1.5092491  -0.27473807] \t0\ttrue\n",
            "(1)\t 827\t [ 2.0213096 -1.6967496 -0.2970342] \t0\tfalse\n",
            "(1)\t 828\t [ 2.0213096 -1.6967496 -0.2970342] \t0\tfalse\n",
            "(2)\t 829\t [ 1.5633968  -1.5245589  -0.00597652] \t0\tfalse\n",
            "(2)\t 830\t [ 1.9725947  -1.487007   -0.41645265] \t0\tfalse\n",
            "(0)\t 831\t [ 1.7115842  -1.6508509   0.03604294] \t0\ttrue\n",
            "(0)\t 832\t [ 1.5301095  -1.5577668   0.16106492] \t0\ttrue\n",
            "(2)\t 833\t [ 1.3940359  -1.4922019   0.18400967] \t0\tfalse\n",
            "(1)\t 834\t [-1.860009    1.4808348   0.42598587] \t1\ttrue\n",
            "(1)\t 835\t [-1.246146    0.75904787  0.70463914] \t1\ttrue\n",
            "(2)\t 836\t [ 1.643413   -1.5550169  -0.03972142] \t0\tfalse\n",
            "(0)\t 837\t [ 0.44971183 -1.0758424   0.80148566] \t2\tfalse\n",
            "(1)\t 838\t [-1.3775164   0.21933469  1.4839375 ] \t2\tfalse\n",
            "(0)\t 839\t [ 1.4960163  -1.3305737  -0.19185834] \t0\ttrue\n",
            "(0)\t 840\t [ 1.9201351  -0.49905884 -1.423613  ] \t0\ttrue\n",
            "(0)\t 841\t [ 1.4205962 -1.1489648 -0.3723386] \t0\ttrue\n",
            "(2)\t 842\t [ 0.39938152 -1.0676099   0.6944375 ] \t2\ttrue\n",
            "(2)\t 843\t [ 1.0075974  -1.38019     0.44876322] \t0\tfalse\n",
            "(0)\t 844\t [ 2.2601967  -1.6023228  -0.62307215] \t0\ttrue\n",
            "(1)\t 845\t [ 0.5316512  -0.96179646  0.53386754] \t2\tfalse\n",
            "(2)\t 846\t [ 1.568329   -1.7281343   0.17746918] \t0\tfalse\n",
            "(2)\t 847\t [ 0.49213013 -0.803836    0.514382  ] \t2\ttrue\n",
            "(2)\t 848\t [-0.7562386 -0.6409966  1.7747227] \t2\ttrue\n",
            "(0)\t 849\t [ 1.7376828  -1.6853386   0.09666258] \t0\ttrue\n",
            "(0)\t 850\t [ 1.4815631  -1.2364262  -0.23153423] \t0\ttrue\n",
            "(2)\t 851\t [ 1.9776672 -1.6585652 -0.2781662] \t0\tfalse\n",
            "(2)\t 852\t [-1.8701729   1.6937956   0.31637615] \t1\tfalse\n",
            "(1)\t 853\t [ 1.2827002  -1.1891388   0.00711675] \t0\tfalse\n",
            "(2)\t 854\t [ 1.8478638  -1.6419452  -0.15507802] \t0\tfalse\n",
            "(2)\t 855\t [ 1.9358274  -1.7188929  -0.16583325] \t0\tfalse\n",
            "(1)\t 856\t [ 1.088276   -1.046118    0.13150199] \t0\tfalse\n",
            "(1)\t 857\t [ 1.088276   -1.046118    0.13150199] \t0\tfalse\n",
            "(0)\t 858\t [ 2.1550128  -1.4993782  -0.78659713] \t0\ttrue\n",
            "(0)\t 859\t [ 2.3105059 -1.4639227 -0.8881413] \t0\ttrue\n",
            "(2)\t 860\t [ 2.0413964  -1.6526018  -0.36935875] \t0\tfalse\n",
            "(2)\t 861\t [ 0.2651365  -0.26298732  0.13483159] \t0\tfalse\n",
            "(1)\t 862\t [-0.99356866  0.81551254  0.10697456] \t1\ttrue\n",
            "(1)\t 863\t [-1.8066972  1.2898554  0.5111802] \t1\ttrue\n",
            "(2)\t 864\t [ 1.7375246 -1.2099956 -0.5716062] \t0\tfalse\n",
            "(0)\t 865\t [ 1.1076342  -1.4367255   0.44800442] \t0\ttrue\n",
            "(2)\t 866\t [ 2.0665777  -1.4424621  -0.69389474] \t0\tfalse\n",
            "(1)\t 867\t [-1.3547252   0.88816226  0.8051275 ] \t1\ttrue\n",
            "(0)\t 868\t [ 1.5243217 -1.397639  -0.0032115] \t0\ttrue\n",
            "(2)\t 869\t [ 1.5947293  -1.3207365  -0.27970353] \t0\tfalse\n",
            "(2)\t 870\t [ 1.016312   -1.200576    0.22853395] \t0\tfalse\n",
            "(2)\t 871\t [ 0.58806235 -1.0301661   0.51036376] \t0\tfalse\n",
            "(2)\t 872\t [ 1.2864285  -1.2018493   0.03348975] \t0\tfalse\n",
            "(1)\t 873\t [-0.43643972  0.8004931  -0.29193714] \t1\ttrue\n",
            "(0)\t 874\t [-1.0264229  0.4182963  0.9094497] \t2\tfalse\n",
            "(2)\t 875\t [ 1.1194662  -1.3787985   0.40777817] \t0\tfalse\n",
            "(1)\t 876\t [ 1.8484727  -1.6544946  -0.16883446] \t0\tfalse\n",
            "(1)\t 877\t [ 1.8484727  -1.6544946  -0.16883446] \t0\tfalse\n",
            "(2)\t 878\t [ 1.9745882  -1.7093105  -0.19993162] \t0\tfalse\n",
            "(0)\t 879\t [ 2.131621   -1.4986387  -0.67497027] \t0\ttrue\n",
            "(1)\t 880\t [-0.18846737 -1.008014    1.5045043 ] \t2\tfalse\n",
            "(2)\t 881\t [ 2.10211   -1.6415349 -0.3912372] \t0\tfalse\n",
            "(1)\t 882\t [ 0.8783274  -1.2787439   0.48757794] \t0\tfalse\n",
            "(1)\t 883\t [ 0.8783274  -1.2787439   0.48757794] \t0\tfalse\n",
            "(2)\t 884\t [ 2.2787206  -1.6591682  -0.57887906] \t0\tfalse\n",
            "(0)\t 885\t [ 2.281033  -1.688152  -0.6264727] \t0\ttrue\n",
            "(1)\t 886\t [ 1.6224983  -1.4431324  -0.19576736] \t0\tfalse\n",
            "(1)\t 887\t [ 1.6224983  -1.4431324  -0.19576736] \t0\tfalse\n",
            "(0)\t 888\t [ 2.1388009  -1.3395171  -0.95190287] \t0\ttrue\n",
            "(2)\t 889\t [ 1.3365508  -1.527163    0.31180748] \t0\tfalse\n",
            "(0)\t 890\t [ 1.833069  -1.6100308 -0.2035289] \t0\ttrue\n",
            "(0)\t 891\t [ 2.1617022 -1.5948967 -0.6063286] \t0\ttrue\n",
            "(1)\t 892\t [ 1.1989475 -1.5678786  0.4581279] \t0\tfalse\n",
            "(1)\t 893\t [ 0.8746872  -1.0668842   0.19984837] \t0\tfalse\n",
            "(0)\t 894\t [ 1.5632823  -1.489051   -0.09714314] \t0\ttrue\n",
            "(1)\t 895\t [ 1.9895362  -1.711553   -0.24430116] \t0\tfalse\n",
            "(0)\t 896\t [ 0.02627927 -1.132925    1.2669047 ] \t2\tfalse\n",
            "(1)\t 897\t [-0.15487154 -0.7480941   1.2257745 ] \t2\tfalse\n",
            "(1)\t 898\t [ 1.1442262  -1.5038687   0.46470633] \t0\tfalse\n",
            "(1)\t 899\t [ 2.0654376  -0.65757596 -1.2806913 ] \t0\tfalse\n",
            "(2)\t 900\t [ 2.2440538 -1.5794557 -0.5643022] \t0\tfalse\n",
            "(2)\t 901\t [-1.6446269   1.5587679  -0.10972318] \t1\tfalse\n",
            "(0)\t 902\t [ 2.0510416 -1.5796695 -0.4445007] \t0\ttrue\n",
            "(1)\t 903\t [ 2.237443   -1.6716012  -0.63216054] \t0\tfalse\n",
            "(2)\t 904\t [ 1.3075739  -1.5389675   0.30333564] \t0\tfalse\n",
            "(1)\t 905\t [-1.0614237 -0.4602863  1.8097104] \t2\tfalse\n",
            "(0)\t 906\t [ 2.0102413 -0.5803828 -1.4875083] \t0\ttrue\n",
            "(2)\t 907\t [-1.431678    0.35783085  1.2725512 ] \t2\ttrue\n",
            "(2)\t 908\t [ 0.05247211 -1.2824402   1.3392106 ] \t2\ttrue\n",
            "(2)\t 909\t [ 2.2932286 -1.4532931 -0.9538546] \t0\tfalse\n",
            "(0)\t 910\t [ 2.092774   -1.4779342  -0.73931277] \t0\ttrue\n",
            "(0)\t 911\t [-0.23150545 -1.0773538   1.433138  ] \t2\tfalse\n",
            "(0)\t 912\t [ 1.4654602 -1.333888  -0.1170706] \t0\ttrue\n",
            "(0)\t 913\t [ 1.6170567  -1.448949   -0.13960026] \t0\ttrue\n",
            "(2)\t 914\t [ 2.1373796 -1.5366378 -0.6669993] \t0\tfalse\n",
            "(1)\t 915\t [-1.9626037  1.4757155  0.3760162] \t1\ttrue\n",
            "(2)\t 916\t [ 1.9611601  -1.6617846  -0.23426525] \t0\tfalse\n",
            "(1)\t 917\t [ 0.6070608  -0.91376626  0.48444918] \t0\tfalse\n",
            "(0)\t 918\t [ 2.0105379  -1.602407   -0.41521865] \t0\ttrue\n",
            "(1)\t 919\t [ 1.4071139  -1.3568884   0.03430046] \t0\tfalse\n",
            "(1)\t 920\t [ 1.4071139  -1.3568884   0.03430046] \t0\tfalse\n",
            "(1)\t 921\t [ 1.192016   -1.4530808   0.29602996] \t0\tfalse\n",
            "(2)\t 922\t [ 0.6475862  0.515744  -1.2201988] \t0\tfalse\n",
            "(0)\t 923\t [ 1.3034576 -1.5531341  0.2804965] \t0\ttrue\n",
            "(0)\t 924\t [-0.98062336  0.97787905 -0.02873122] \t1\tfalse\n",
            "(0)\t 925\t [ 1.2710516  -1.4334975   0.22397085] \t0\ttrue\n",
            "(0)\t 926\t [ 1.9855841  -1.7303463  -0.22770795] \t0\ttrue\n",
            "(2)\t 927\t [-1.9982364  1.6714605  0.5616486] \t1\tfalse\n",
            "(1)\t 928\t [ 2.0219433 -1.5440358 -0.5054088] \t0\tfalse\n",
            "(1)\t 929\t [ 2.0219433 -1.5440358 -0.5054088] \t0\tfalse\n",
            "(0)\t 930\t [ 1.909644   -1.617086   -0.22490938] \t0\ttrue\n",
            "(0)\t 931\t [ 2.2825818 -1.5410453 -0.6756074] \t0\ttrue\n",
            "(2)\t 932\t [ 1.2859634  -1.617343    0.35085258] \t0\tfalse\n",
            "(1)\t 933\t [ 1.2038757  -1.2736146   0.20711419] \t0\tfalse\n",
            "(2)\t 934\t [ 1.5476812  -1.5720073   0.02262549] \t0\tfalse\n",
            "(2)\t 935\t [ 1.8511813  -1.5184764  -0.40618122] \t0\tfalse\n",
            "(1)\t 936\t [-0.29826137 -0.6371558   1.1119133 ] \t2\tfalse\n",
            "(0)\t 937\t [ 1.6720729  -1.6007823   0.10567346] \t0\ttrue\n",
            "(0)\t 938\t [ 1.9166819 -1.6888131 -0.1616099] \t0\ttrue\n",
            "(0)\t 939\t [ 2.3163624  -1.633104   -0.69422704] \t0\ttrue\n",
            "(0)\t 940\t [ 1.9851466 -1.734857  -0.1984206] \t0\ttrue\n",
            "(2)\t 941\t [-1.2595202   1.3259397   0.01781484] \t1\tfalse\n",
            "(0)\t 942\t [-0.68118817 -0.36300212  1.3607602 ] \t2\tfalse\n",
            "(2)\t 943\t [ 1.1041371 -1.4008511  0.432407 ] \t0\tfalse\n",
            "(1)\t 944\t [ 1.8535008  -0.45623305 -1.4037918 ] \t0\tfalse\n",
            "(1)\t 945\t [ 1.8535008  -0.45623305 -1.4037918 ] \t0\tfalse\n",
            "(1)\t 946\t [-1.9988595   1.7758187   0.23746775] \t1\ttrue\n",
            "(2)\t 947\t [ 1.0555931 -1.3768626  0.5065158] \t0\tfalse\n",
            "(2)\t 948\t [ 1.3121809 -1.3682655  0.191754 ] \t0\tfalse\n",
            "(2)\t 949\t [-0.4009803 -0.9713859  1.5306693] \t2\ttrue\n",
            "(1)\t 950\t [ 1.2583307 -0.8563012 -0.2539622] \t0\tfalse\n",
            "(2)\t 951\t [ 1.2218884  -1.498188    0.33927104] \t0\tfalse\n",
            "(2)\t 952\t [ 1.3953342  -1.0930097  -0.34524095] \t0\tfalse\n",
            "(0)\t 953\t [ 0.16155572  0.9060581  -1.1480039 ] \t1\tfalse\n",
            "(1)\t 954\t [ 0.8423929  -0.8017167  -0.02604703] \t0\tfalse\n",
            "(0)\t 955\t [ 0.9880457  -1.4020114   0.56170064] \t0\ttrue\n",
            "(2)\t 956\t [-1.4357456  1.1998304  0.6439711] \t1\tfalse\n",
            "(0)\t 957\t [ 1.0027252  -1.3975024   0.42852303] \t0\ttrue\n",
            "(2)\t 958\t [ 1.8281916  -1.5642307  -0.09868774] \t0\tfalse\n",
            "(2)\t 959\t [ 1.8475858  -1.5510157  -0.37688658] \t0\tfalse\n",
            "(1)\t 960\t [ 1.9600881  -1.4579352  -0.49278554] \t0\tfalse\n",
            "(1)\t 961\t [ 1.9600881  -1.4579352  -0.49278554] \t0\tfalse\n",
            "(2)\t 962\t [ 1.991159   -1.7120172  -0.28688112] \t0\tfalse\n",
            "(2)\t 963\t [ 0.7240001 -1.1255506  0.5107963] \t0\tfalse\n",
            "(0)\t 964\t [ 2.2947476 -1.5574318 -0.7424451] \t0\ttrue\n",
            "(0)\t 965\t [ 0.842196   -1.421819    0.77358544] \t0\ttrue\n",
            "(0)\t 966\t [ 2.0879867  -1.6613301  -0.47750387] \t0\ttrue\n",
            "(0)\t 967\t [ 1.5488313  -1.3286114  -0.17019759] \t0\ttrue\n",
            "(0)\t 968\t [ 1.7067353  -1.2425985  -0.43361586] \t0\ttrue\n",
            "(0)\t 969\t [ 2.2783844 -0.8891292 -1.3922743] \t0\ttrue\n",
            "(2)\t 970\t [-0.9630368   0.00176959  1.3073446 ] \t2\ttrue\n",
            "(0)\t 971\t [ 1.0893685  -1.2753623   0.30830678] \t0\ttrue\n",
            "(2)\t 972\t [ 0.39832273 -0.32592967  0.17801356] \t0\tfalse\n",
            "(0)\t 973\t [ 1.3416271  -1.1306219  -0.16406442] \t0\ttrue\n",
            "(2)\t 974\t [ 0.8082881 -1.1657354  0.5460294] \t0\tfalse\n",
            "(0)\t 975\t [ 0.10170858 -0.4688169   0.4830782 ] \t2\tfalse\n",
            "(2)\t 976\t [ 2.108307  -1.5440099 -0.5436188] \t0\tfalse\n",
            "(2)\t 977\t [-1.0282279   0.42616266  0.78119326] \t2\ttrue\n",
            "(2)\t 978\t [ 0.5560119 -1.1821008  0.8372361] \t2\ttrue\n",
            "(1)\t 979\t [ 0.58244646 -1.0185518   0.7226757 ] \t2\tfalse\n",
            "(2)\t 980\t [ 1.5936364 -1.7127997  0.1745701] \t0\tfalse\n",
            "(1)\t 981\t [-1.7311242  1.2272246  0.6153563] \t1\ttrue\n",
            "(1)\t 982\t [-0.49656484 -0.39179716  1.1654265 ] \t2\tfalse\n",
            "(0)\t 983\t [ 0.8354838  -0.9757645   0.08767222] \t0\ttrue\n",
            "(1)\t 984\t [ 2.2095811  -1.4372431  -0.71114147] \t0\tfalse\n",
            "(0)\t 985\t [ 2.4579618 -1.2737595 -1.2011164] \t0\ttrue\n",
            "(2)\t 986\t [-1.838173    0.94776404  0.99425817] \t2\ttrue\n",
            "(0)\t 987\t [ 1.9999537  -1.6544293  -0.46225244] \t0\ttrue\n",
            "(2)\t 988\t [ 1.4926729  -1.5852056   0.12962867] \t0\tfalse\n",
            "(2)\t 989\t [ 0.95379007 -1.3458143   0.54154223] \t0\tfalse\n",
            "(2)\t 990\t [ 0.627409   -0.7561347   0.08329418] \t0\tfalse\n",
            "(0)\t 991\t [ 2.1008968 -1.6712726 -0.4727476] \t0\ttrue\n",
            "(0)\t 992\t [ 1.7190187  -1.2849067  -0.45206654] \t0\ttrue\n",
            "(0)\t 993\t [ 1.2629565  -1.4862047   0.26408163] \t0\ttrue\n",
            "(0)\t 994\t [ 1.7686814  -1.637736   -0.12234847] \t0\ttrue\n",
            "(0)\t 995\t [ 1.499396    0.04861221 -1.4345615 ] \t0\ttrue\n",
            "(1)\t 996\t [-1.2054924  -0.02297436  1.6047446 ] \t2\tfalse\n",
            "(1)\t 997\t [ 0.6158134 -1.113597   0.6811683] \t2\tfalse\n",
            "(2)\t 998\t [ 0.8535587 -1.1848148  0.3698368] \t0\tfalse\n",
            "(1)\t 999\t [-0.5673249  -0.52256894  1.4148287 ] \t2\tfalse\n",
            "(0)\t 1000\t [ 1.900756  -1.4366747 -0.5192491] \t0\ttrue\n",
            "(0)\t 1001\t [ 2.0967064  -1.4639676  -0.60445404] \t0\ttrue\n",
            "(0)\t 1002\t [ 1.8783922  -1.7340441  -0.10559337] \t0\ttrue\n",
            "(1)\t 1003\t [ 0.96988577 -1.4628994   0.5271093 ] \t0\tfalse\n",
            "(1)\t 1004\t [ 0.96988577 -1.4628994   0.5271093 ] \t0\tfalse\n",
            "(2)\t 1005\t [ 1.2675327  -1.5720702   0.41153932] \t0\tfalse\n",
            "(0)\t 1006\t [ 2.0589485  -1.6654515  -0.36386535] \t0\ttrue\n",
            "(2)\t 1007\t [ 1.6924583  -1.6728134  -0.02399466] \t0\tfalse\n",
            "(2)\t 1008\t [-1.9399006   1.7380527   0.14240043] \t1\tfalse\n",
            "(2)\t 1009\t [-1.3536575   0.9168938   0.65282434] \t1\tfalse\n",
            "(2)\t 1010\t [ 1.0830313  -1.3064297   0.43540728] \t0\tfalse\n",
            "(1)\t 1011\t [-0.762969   -0.01018705  1.0190811 ] \t2\tfalse\n",
            "(1)\t 1012\t [-1.2978696  0.4678524  1.2841604] \t2\tfalse\n",
            "(1)\t 1013\t [ 1.9061998  -1.5652535  -0.26894888] \t0\tfalse\n",
            "(0)\t 1014\t [ 2.1678529 -1.6973051 -0.478729 ] \t0\ttrue\n",
            "(0)\t 1015\t [ 1.9634103 -1.6262959 -0.3231282] \t0\ttrue\n",
            "(2)\t 1016\t [ 2.161952  -1.7336947 -0.4346831] \t0\tfalse\n",
            "(0)\t 1017\t [ 2.3638892  -1.6981946  -0.61818147] \t0\ttrue\n",
            "(2)\t 1018\t [-0.94517154  0.71287596  0.48827752] \t1\tfalse\n",
            "(0)\t 1019\t [ 1.1515065 -1.4685774  0.3222169] \t0\ttrue\n",
            "(0)\t 1020\t [ 2.1960814 -1.6275741 -0.608706 ] \t0\ttrue\n",
            "(1)\t 1021\t [-0.20154467  1.0609182  -0.9275061 ] \t1\ttrue\n",
            "(1)\t 1022\t [-0.20154467  1.0609182  -0.9275061 ] \t1\ttrue\n",
            "(0)\t 1023\t [ 1.4313685  -1.3274397  -0.13096301] \t0\ttrue\n",
            "(2)\t 1024\t [ 1.3605434  -1.5697073   0.17898583] \t0\tfalse\n",
            "(1)\t 1025\t [ 0.03874771 -1.0705937   1.262971  ] \t2\tfalse\n",
            "(2)\t 1026\t [ 1.5212216  -1.2455822  -0.14836738] \t0\tfalse\n",
            "(2)\t 1027\t [-1.6586127   1.6059296  -0.14038686] \t1\tfalse\n",
            "(1)\t 1028\t [-1.415265   0.5005357  1.2384547] \t2\tfalse\n",
            "(1)\t 1029\t [-1.5869144   1.551734   -0.25605085] \t1\ttrue\n",
            "(1)\t 1030\t [ 1.075252   -1.4299865   0.44770536] \t0\tfalse\n",
            "(1)\t 1031\t [ 1.075252   -1.4299865   0.44770536] \t0\tfalse\n",
            "(0)\t 1032\t [ 2.268341   -1.6837095  -0.63026595] \t0\ttrue\n",
            "(2)\t 1033\t [ 2.3651116  -1.5418347  -0.88162994] \t0\tfalse\n",
            "(2)\t 1034\t [ 1.1052486  -1.2862598   0.19594233] \t0\tfalse\n",
            "(0)\t 1035\t [ 0.1599888 -1.0620381  1.0352582] \t2\tfalse\n",
            "(0)\t 1036\t [ 1.9768537  -1.5912688  -0.24522567] \t0\ttrue\n",
            "(0)\t 1037\t [ 1.9920925 -1.5606302 -0.347568 ] \t0\ttrue\n",
            "(0)\t 1038\t [ 1.059532   -1.1549401   0.21527895] \t0\ttrue\n",
            "(1)\t 1039\t [ 0.44206932 -1.2590102   0.91312695] \t2\tfalse\n",
            "(1)\t 1040\t [ 0.44206932 -1.2590102   0.91312695] \t2\tfalse\n",
            "(0)\t 1041\t [ 1.7899112 -0.5907579 -1.0972592] \t0\ttrue\n",
            "(0)\t 1042\t [ 0.25776297 -1.0647095   0.87702996] \t2\tfalse\n",
            "(1)\t 1043\t [-1.2153937   0.7053997   0.88676643] \t2\tfalse\n",
            "(0)\t 1044\t [ 2.0672598 -1.37892   -0.7788496] \t0\ttrue\n",
            "(2)\t 1045\t [ 1.8068721  -1.5470895  -0.18974122] \t0\tfalse\n",
            "(1)\t 1046\t [ 0.46633592 -1.1323433   0.80323917] \t2\tfalse\n",
            "(0)\t 1047\t [ 2.4930449 -1.1678452 -1.3003272] \t0\ttrue\n",
            "(0)\t 1048\t [ 2.0811357 -1.6274055 -0.3835964] \t0\ttrue\n",
            "(0)\t 1049\t [ 2.348253  -0.8732883 -1.3800303] \t0\ttrue\n",
            "(1)\t 1050\t [ 1.3767567  -1.5352097   0.07266118] \t0\tfalse\n",
            "(2)\t 1051\t [-0.98892576  1.3281132  -0.44491005] \t1\tfalse\n",
            "(2)\t 1052\t [ 0.80824804 -1.3679079   0.68748987] \t0\tfalse\n",
            "(1)\t 1053\t [ 0.8466173 -1.3778634  0.744821 ] \t0\tfalse\n",
            "(2)\t 1054\t [-0.89570165 -0.55238456  1.7655942 ] \t2\ttrue\n",
            "(0)\t 1055\t [ 1.5991789  -1.4824814  -0.04883166] \t0\ttrue\n",
            "(0)\t 1056\t [ 2.5314364 -1.0776649 -1.3497177] \t0\ttrue\n",
            "(0)\t 1057\t [ 1.8233589  -1.5585623  -0.20073207] \t0\ttrue\n",
            "(2)\t 1058\t [ 0.8128553  -1.1235185   0.46231446] \t0\tfalse\n",
            "(2)\t 1059\t [-0.28346547 -0.67957556  1.1805506 ] \t2\ttrue\n",
            "(2)\t 1060\t [ 2.1017315 -1.716195  -0.2731258] \t0\tfalse\n",
            "(0)\t 1061\t [ 2.0819998  -1.632373   -0.42432746] \t0\ttrue\n",
            "(0)\t 1062\t [ 1.9337407  -1.5163428  -0.39169133] \t0\ttrue\n",
            "(0)\t 1063\t [ 2.5180268 -1.4854031 -1.1362922] \t0\ttrue\n",
            "(1)\t 1064\t [-0.8424818  0.5863318  0.5748863] \t1\ttrue\n",
            "(1)\t 1065\t [ 0.77534807 -1.0959886   0.38779694] \t0\tfalse\n",
            "(1)\t 1066\t [ 0.77534807 -1.0959886   0.38779694] \t0\tfalse\n",
            "(1)\t 1067\t [ 0.10633999  0.6919153  -0.7254907 ] \t1\ttrue\n",
            "(1)\t 1068\t [ 0.10633999  0.6919153  -0.7254907 ] \t1\ttrue\n",
            "(1)\t 1069\t [ 1.1131264  -1.0571033  -0.11864476] \t0\tfalse\n",
            "(0)\t 1070\t [ 1.8509027  -1.7196065  -0.04513918] \t0\ttrue\n",
            "(2)\t 1071\t [ 1.6716217  -1.4677958  -0.17146413] \t0\tfalse\n",
            "(0)\t 1072\t [ 1.6277606 -1.5854212  0.0593308] \t0\ttrue\n",
            "(2)\t 1073\t [ 1.3849226  -1.5848528   0.24984865] \t0\tfalse\n",
            "(0)\t 1074\t [ 1.8450865 -1.5804592 -0.1300943] \t0\ttrue\n",
            "(0)\t 1075\t [ 2.0879972  -1.6843256  -0.35301214] \t0\ttrue\n",
            "(1)\t 1076\t [-1.9209664   1.6980039   0.12226072] \t1\ttrue\n",
            "(2)\t 1077\t [ 1.9492106  -1.5128514  -0.42648876] \t0\tfalse\n",
            "(0)\t 1078\t [ 1.8140516 -0.2683885 -1.4614487] \t0\ttrue\n",
            "(0)\t 1079\t [ 2.2790098 -1.6032267 -0.7286209] \t0\ttrue\n",
            "(2)\t 1080\t [-1.2777673   1.3026713  -0.14850454] \t1\tfalse\n",
            "(0)\t 1081\t [ 1.4802724  -1.4945416   0.09443808] \t0\ttrue\n",
            "(2)\t 1082\t [ 1.6306392  -1.4543709  -0.22390774] \t0\tfalse\n",
            "(0)\t 1083\t [ 1.7206997  -1.5081657  -0.18665448] \t0\ttrue\n",
            "(2)\t 1084\t [ 0.5441679 -1.0409199  0.7856396] \t2\ttrue\n",
            "(0)\t 1085\t [ 1.3470322  -0.8812409  -0.47039184] \t0\ttrue\n",
            "(0)\t 1086\t [ 2.2360876 -1.4667226 -0.7719651] \t0\ttrue\n",
            "(0)\t 1087\t [ 1.4650973  -1.5688735   0.02169871] \t0\ttrue\n",
            "(0)\t 1088\t [ 0.17738406 -0.786523    0.9641797 ] \t2\tfalse\n",
            "(0)\t 1089\t [ 2.1307733 -1.5747209 -0.5080647] \t0\ttrue\n",
            "(0)\t 1090\t [ 1.9780128 -1.4201028 -0.6701154] \t0\ttrue\n",
            "(2)\t 1091\t [-0.7093679  -0.76876813  1.784496  ] \t2\ttrue\n",
            "(2)\t 1092\t [-1.1798384   0.60580873  0.999303  ] \t2\ttrue\n",
            "(2)\t 1093\t [ 1.0200783  -0.4708471  -0.38392183] \t0\tfalse\n",
            "(2)\t 1094\t [ 0.504773   -0.53788453 -0.04479337] \t0\tfalse\n",
            "(2)\t 1095\t [ 1.9516068 -1.6910006 -0.231212 ] \t0\tfalse\n",
            "(0)\t 1096\t [ 1.2775508  -1.1979064  -0.15905844] \t0\ttrue\n",
            "(0)\t 1097\t [ 1.6917317  -0.18915543 -1.3399361 ] \t0\ttrue\n",
            "(0)\t 1098\t [ 0.75672853 -1.107499    0.3341721 ] \t0\ttrue\n",
            "(0)\t 1099\t [-0.33475804  1.1717328  -0.9190085 ] \t1\tfalse\n",
            "(0)\t 1100\t [ 1.3877541  -1.0837024  -0.42501137] \t0\ttrue\n",
            "(2)\t 1101\t [ 1.5076736  -1.5449663   0.09383142] \t0\tfalse\n",
            "(1)\t 1102\t [-1.4854647   0.88342917  0.575285  ] \t1\ttrue\n",
            "(2)\t 1103\t [ 1.7512827  -1.4735626  -0.31638706] \t0\tfalse\n",
            "(1)\t 1104\t [-1.3000792   1.3676995  -0.18673278] \t1\ttrue\n",
            "(1)\t 1105\t [ 0.63067466 -0.4180734   0.02984702] \t0\tfalse\n",
            "(2)\t 1106\t [ 1.478765   -0.8992977  -0.66324484] \t0\tfalse\n",
            "(1)\t 1107\t [ 1.9077848 -1.6186887 -0.3073963] \t0\tfalse\n",
            "(2)\t 1108\t [ 2.2823539 -1.1298653 -1.1405514] \t0\tfalse\n",
            "(0)\t 1109\t [ 1.938624   -1.7382828  -0.14084348] \t0\ttrue\n",
            "(1)\t 1110\t [-1.2049338   0.13773999  1.4125274 ] \t2\tfalse\n",
            "(0)\t 1111\t [ 2.498427 -1.12757  -1.303073] \t0\ttrue\n",
            "(2)\t 1112\t [ 1.7013165  -1.6948339   0.13486506] \t0\tfalse\n",
            "(1)\t 1113\t [-1.0845969  -0.32927945  1.7181157 ] \t2\tfalse\n",
            "(1)\t 1114\t [-0.83741164  0.02548287  1.0606883 ] \t2\tfalse\n",
            "(2)\t 1115\t [ 0.8023156 -1.304221   0.6628853] \t0\tfalse\n",
            "(2)\t 1116\t [ 0.27436107  0.33143523 -0.56008565] \t1\tfalse\n",
            "(1)\t 1117\t [ 1.2096087  -1.4972361   0.23054972] \t0\tfalse\n",
            "(2)\t 1118\t [-1.562381    1.5223323  -0.06036577] \t1\tfalse\n",
            "(2)\t 1119\t [ 2.3135157  -1.6324438  -0.61759394] \t0\tfalse\n",
            "(0)\t 1120\t [ 2.1705234  -1.5645965  -0.54381555] \t0\ttrue\n",
            "(0)\t 1121\t [ 1.3084717  -1.462727    0.26183042] \t0\ttrue\n",
            "(0)\t 1122\t [ 1.8642008  -1.6567146  -0.24662258] \t0\ttrue\n",
            "(0)\t 1123\t [ 2.4020214  -1.6664168  -0.78005713] \t0\ttrue\n",
            "(2)\t 1124\t [ 2.1393514 -1.6847645 -0.5418802] \t0\tfalse\n",
            "(2)\t 1125\t [ 1.0082914  -1.3836074   0.51381856] \t0\tfalse\n",
            "(2)\t 1126\t [ 0.23004822 -1.1113523   1.1420857 ] \t2\ttrue\n",
            "(2)\t 1127\t [-1.8124156  1.1781102  0.9203783] \t1\tfalse\n",
            "(0)\t 1128\t [ 0.7458082  -1.2186351   0.66710895] \t0\ttrue\n",
            "(2)\t 1129\t [-0.09908449 -1.1408534   1.4280958 ] \t2\ttrue\n",
            "(2)\t 1130\t [ 1.2832522  -1.4491843   0.21862069] \t0\tfalse\n",
            "(0)\t 1131\t [ 2.0464923 -1.5003812 -0.6198398] \t0\ttrue\n",
            "(1)\t 1132\t [ 1.5972103  -1.3626142  -0.36531055] \t0\tfalse\n",
            "(2)\t 1133\t [ 1.5878663  -1.4951181  -0.08329903] \t0\tfalse\n",
            "(1)\t 1134\t [ 1.725972   -1.6343937  -0.02593886] \t0\tfalse\n",
            "(1)\t 1135\t [ 1.725972   -1.6343937  -0.02593886] \t0\tfalse\n",
            "(1)\t 1136\t [-1.6904775   1.3762958   0.50251365] \t1\ttrue\n",
            "(1)\t 1137\t [ 1.8506721 -1.1588696 -0.8133127] \t0\tfalse\n",
            "(1)\t 1138\t [ 1.8506721 -1.1588696 -0.8133127] \t0\tfalse\n",
            "(0)\t 1139\t [ 1.9261478 -1.5611439 -0.2000746] \t0\ttrue\n",
            "(0)\t 1140\t [ 2.305436  -1.3799607 -0.9170715] \t0\ttrue\n",
            "(0)\t 1141\t [ 1.7769192 -1.2897607 -0.5636079] \t0\ttrue\n",
            "(2)\t 1142\t [ 1.5183343  -1.579713    0.08906691] \t0\tfalse\n",
            "(2)\t 1143\t [ 1.7439454  -1.3261797  -0.39542112] \t0\tfalse\n",
            "(2)\t 1144\t [ 1.9153166  -1.444936   -0.38194564] \t0\tfalse\n",
            "(0)\t 1145\t [ 1.8204054  -1.4920017  -0.25710237] \t0\ttrue\n",
            "(2)\t 1146\t [ 1.2731118  -1.2568611   0.07537614] \t0\tfalse\n",
            "(1)\t 1147\t [-0.431078   0.0425009  0.6626376] \t2\tfalse\n",
            "(0)\t 1148\t [ 1.6023582  -1.525881    0.08106975] \t0\ttrue\n",
            "(2)\t 1149\t [ 1.0297791 -1.4013016  0.5039954] \t0\tfalse\n",
            "(2)\t 1150\t [ 0.16179973 -0.39430878  0.44452503] \t2\ttrue\n",
            "(1)\t 1151\t [-1.492573    1.3689258  -0.02644575] \t1\ttrue\n",
            "(0)\t 1152\t [ 1.5829583 -1.038412  -0.774001 ] \t0\ttrue\n",
            "(2)\t 1153\t [-0.01306066 -0.7742757   0.9967942 ] \t2\ttrue\n",
            "(1)\t 1154\t [-0.7663168   0.9972011  -0.09399423] \t1\ttrue\n",
            "(0)\t 1155\t [ 2.5461125 -1.2514828 -1.3427807] \t0\ttrue\n",
            "(1)\t 1156\t [-1.9487473   1.6202515   0.43740028] \t1\ttrue\n",
            "(1)\t 1157\t [-1.2797313  -0.08451835  1.6657315 ] \t2\tfalse\n",
            "(2)\t 1158\t [ 1.3210207  -1.513405    0.25199008] \t0\tfalse\n",
            "(0)\t 1159\t [ 2.0372014  -1.7112707  -0.32167733] \t0\ttrue\n",
            "(1)\t 1160\t [ 1.5832211  -1.5291439   0.09727146] \t0\tfalse\n",
            "(0)\t 1161\t [ 1.5806924  -1.5914859   0.08903016] \t0\ttrue\n",
            "(0)\t 1162\t [ 1.5854726 -1.5722601  0.0619785] \t0\ttrue\n",
            "(2)\t 1163\t [ 1.6891819  -1.5077451  -0.09347449] \t0\tfalse\n",
            "(2)\t 1164\t [ 1.8293589  -1.5564195  -0.27047986] \t0\tfalse\n",
            "(0)\t 1165\t [ 1.8201258  -1.5382957  -0.19580944] \t0\ttrue\n",
            "(0)\t 1166\t [ 1.9083906  -1.596397   -0.38863614] \t0\ttrue\n",
            "(1)\t 1167\t [ 2.3027735 -1.59869   -0.6573515] \t0\tfalse\n",
            "(1)\t 1168\t [ 2.3027735 -1.59869   -0.6573515] \t0\tfalse\n",
            "(0)\t 1169\t [ 2.2875402  -1.6841346  -0.55625564] \t0\ttrue\n",
            "(0)\t 1170\t [ 2.2322483 -1.7124507 -0.5643824] \t0\ttrue\n",
            "(2)\t 1171\t [ 2.0109572  -1.6549802  -0.24627747] \t0\tfalse\n",
            "(2)\t 1172\t [ 1.4054505  -1.4798286   0.02196768] \t0\tfalse\n",
            "(2)\t 1173\t [ 1.8228358  -1.6382383  -0.20027272] \t0\tfalse\n",
            "(0)\t 1174\t [ 0.27975783 -1.2908818   1.0529425 ] \t2\tfalse\n",
            "(2)\t 1175\t [ 1.1188538 -1.3813589  0.3097388] \t0\tfalse\n",
            "(0)\t 1176\t [ 1.7270612  -1.5754071  -0.00986137] \t0\ttrue\n",
            "(0)\t 1177\t [ 1.6422472  -1.4501933  -0.04807143] \t0\ttrue\n",
            "(0)\t 1178\t [-0.08956695 -0.81001925  1.1298203 ] \t2\tfalse\n",
            "(0)\t 1179\t [ 1.5323676  -1.639634    0.18311444] \t0\ttrue\n",
            "(0)\t 1180\t [ 0.99631035 -1.2048217   0.16184695] \t0\ttrue\n",
            "(0)\t 1181\t [ 2.1314704  -1.7124611  -0.34022987] \t0\ttrue\n",
            "(1)\t 1182\t [-1.1720009   0.8338847   0.46439222] \t1\ttrue\n",
            "(1)\t 1183\t [ 1.395702   -1.2590631  -0.19875534] \t0\tfalse\n",
            "(2)\t 1184\t [ 0.2829003  -1.0684849   0.87587845] \t2\ttrue\n",
            "(2)\t 1185\t [ 1.5990051  -1.4958593  -0.06976672] \t0\tfalse\n",
            "(0)\t 1186\t [ 1.9931628  -1.5107914  -0.58989406] \t0\ttrue\n",
            "(1)\t 1187\t [ 0.35981384 -0.32031208  0.07536417] \t0\tfalse\n",
            "(2)\t 1188\t [ 1.5645894  -1.5661196   0.10348128] \t0\tfalse\n",
            "(0)\t 1189\t [ 1.8591613  -1.6278936  -0.11684956] \t0\ttrue\n",
            "(2)\t 1190\t [ 1.4317044  -1.4437699   0.12499279] \t0\tfalse\n",
            "(0)\t 1191\t [ 1.1845711 -1.4398707  0.347389 ] \t0\ttrue\n",
            "(1)\t 1192\t [-1.6097609  0.9685153  0.7799475] \t1\ttrue\n",
            "(2)\t 1193\t [ 1.4897407  -1.4996463   0.10695821] \t0\tfalse\n",
            "(1)\t 1194\t [-1.8557352   1.4771183   0.24859637] \t1\ttrue\n",
            "(0)\t 1195\t [ 2.258984  -1.6400458 -0.5497993] \t0\ttrue\n",
            "(0)\t 1196\t [ 2.1305783  -1.7324942  -0.27651024] \t0\ttrue\n",
            "(0)\t 1197\t [-0.14599417  0.06309105  0.24336974] \t2\tfalse\n",
            "(1)\t 1198\t [-1.8299772  1.4871141  0.6613784] \t1\ttrue\n",
            "(2)\t 1199\t [-0.49214864 -0.37623566  1.1736104 ] \t2\ttrue\n",
            "(0)\t 1200\t [ 0.6482675 -1.1041241  0.597898 ] \t0\ttrue\n",
            "(0)\t 1201\t [ 1.9313043  -1.6629499  -0.31878534] \t0\ttrue\n",
            "(1)\t 1202\t [ 0.42119524 -1.0075045   0.68226373] \t2\tfalse\n",
            "(2)\t 1203\t [ 2.1047978  -1.7746812  -0.23285507] \t0\tfalse\n",
            "(1)\t 1204\t [ 1.662907   -1.4815353  -0.05995565] \t0\tfalse\n",
            "(1)\t 1205\t [ 1.662907   -1.4815353  -0.05995565] \t0\tfalse\n",
            "(0)\t 1206\t [ 1.8726918 -1.6743557 -0.1011963] \t0\ttrue\n",
            "(2)\t 1207\t [ 0.91892934 -1.3328685   0.49478632] \t0\tfalse\n",
            "(0)\t 1208\t [ 1.5414656  -0.08322106 -1.43324   ] \t0\ttrue\n",
            "(1)\t 1209\t [-1.5243043   1.4044396  -0.03051822] \t1\ttrue\n",
            "(2)\t 1210\t [ 1.8585025  -1.427841   -0.35807475] \t0\tfalse\n",
            "(1)\t 1211\t [ 1.2904356  -1.4662071   0.18450817] \t0\tfalse\n",
            "(1)\t 1212\t [-1.8522326e+00  1.7618340e+00  1.5712499e-03] \t1\ttrue\n",
            "(0)\t 1213\t [ 1.8560266 -1.388503  -0.4466142] \t0\ttrue\n",
            "(2)\t 1214\t [ 1.8868443  -1.581298   -0.29202092] \t0\tfalse\n",
            "(0)\t 1215\t [ 1.5347769  -1.4831213  -0.24057783] \t0\ttrue\n",
            "(1)\t 1216\t [ 1.885194   -1.6297777  -0.20107712] \t0\tfalse\n",
            "(1)\t 1217\t [ 1.885194   -1.6297777  -0.20107712] \t0\tfalse\n",
            "(0)\t 1218\t [ 2.5664034 -1.2544494 -1.2994568] \t0\ttrue\n",
            "(2)\t 1219\t [ 2.511578   -1.5793525  -0.96034634] \t0\tfalse\n",
            "(1)\t 1220\t [-0.8159288   0.19933711  0.81333697] \t2\tfalse\n",
            "(0)\t 1221\t [ 2.20821   -0.849092  -1.3393764] \t0\ttrue\n",
            "(2)\t 1222\t [ 1.9864517  -1.6875378  -0.29654357] \t0\tfalse\n",
            "(0)\t 1223\t [ 2.3739798 -1.5143969 -0.9129388] \t0\ttrue\n",
            "(2)\t 1224\t [ 0.86143875 -1.5554048   0.6637995 ] \t0\tfalse\n",
            "(1)\t 1225\t [ 1.7801872 -1.6168944 -0.0928943] \t0\tfalse\n",
            "(1)\t 1226\t [ 1.7801872 -1.6168944 -0.0928943] \t0\tfalse\n",
            "(2)\t 1227\t [ 1.7974008  -1.561761   -0.26348114] \t0\tfalse\n",
            "(1)\t 1228\t [ 1.1990047  -1.3817223   0.22200342] \t0\tfalse\n",
            "(2)\t 1229\t [ 1.0447639  -1.4560589   0.49539122] \t0\tfalse\n",
            "(0)\t 1230\t [ 2.6071029 -1.281872  -1.235581 ] \t0\ttrue\n",
            "(0)\t 1231\t [ 2.3948     -1.4762763  -0.96667093] \t0\ttrue\n",
            "(2)\t 1232\t [ 0.9808611 -1.381617   0.6850722] \t0\tfalse\n",
            "(1)\t 1233\t [ 1.44464    -1.7285217   0.34518576] \t0\tfalse\n",
            "(1)\t 1234\t [ 1.44464    -1.7285217   0.34518576] \t0\tfalse\n",
            "(2)\t 1235\t [-0.12397566 -0.9436375   1.2542315 ] \t2\ttrue\n",
            "(0)\t 1236\t [ 1.355981   -1.4043516   0.19812568] \t0\ttrue\n",
            "(0)\t 1237\t [ 1.292497   -1.3534368   0.05072297] \t0\ttrue\n",
            "(0)\t 1238\t [ 2.044972   -1.6335404  -0.42205897] \t0\ttrue\n",
            "(0)\t 1239\t [ 1.8410374  -1.5679744  -0.20547122] \t0\ttrue\n",
            "(1)\t 1240\t [ 1.9051323 -1.2066654 -0.8564987] \t0\tfalse\n",
            "(1)\t 1241\t [ 1.9051323 -1.2066654 -0.8564987] \t0\tfalse\n",
            "(1)\t 1242\t [ 1.661287   -1.5779914   0.08197357] \t0\tfalse\n",
            "(1)\t 1243\t [ 1.661287   -1.5779914   0.08197357] \t0\tfalse\n",
            "(0)\t 1244\t [ 1.7224966  -1.5938791  -0.03111622] \t0\ttrue\n",
            "(2)\t 1245\t [ 1.8095754  -1.6342409  -0.12041768] \t0\tfalse\n",
            "(0)\t 1246\t [ 2.3347702  -1.6604483  -0.65891814] \t0\ttrue\n",
            "(1)\t 1247\t [-2.0095177  1.6401755  0.4497941] \t1\ttrue\n",
            "(1)\t 1248\t [ 2.1814764 -1.488406  -0.776286 ] \t0\tfalse\n",
            "(1)\t 1249\t [ 2.1814764 -1.488406  -0.776286 ] \t0\tfalse\n",
            "(1)\t 1250\t [ 1.640803  -1.430596  -0.2263021] \t0\tfalse\n",
            "(1)\t 1251\t [ 1.640803  -1.430596  -0.2263021] \t0\tfalse\n",
            "(1)\t 1252\t [-1.502545    1.2706245   0.20591708] \t1\ttrue\n",
            "(1)\t 1253\t [-1.8599526   1.6148778   0.05117307] \t1\ttrue\n",
            "(2)\t 1254\t [ 2.3836906 -1.4259044 -0.9977665] \t0\tfalse\n",
            "(1)\t 1255\t [ 1.56502   -1.5992904  0.1648599] \t0\tfalse\n",
            "(2)\t 1256\t [ 1.4340689  -0.95121527 -0.5328598 ] \t0\tfalse\n",
            "(2)\t 1257\t [ 1.7795426  -1.3524271  -0.41677988] \t0\tfalse\n",
            "(1)\t 1258\t [-0.03761544 -0.4452572   0.5922519 ] \t2\tfalse\n",
            "(1)\t 1259\t [ 0.82034117 -0.42418975 -0.16609545] \t0\tfalse\n",
            "(2)\t 1260\t [ 0.7844677 -1.2775265  0.5715644] \t0\tfalse\n",
            "(1)\t 1261\t [-0.914754    1.0736924  -0.08450273] \t1\ttrue\n",
            "(1)\t 1262\t [-0.11288394 -0.09392369  0.42062572] \t2\tfalse\n",
            "(1)\t 1263\t [ 2.335222  -1.0537326 -1.2033527] \t0\tfalse\n",
            "(2)\t 1264\t [ 0.6319751  -0.22160983 -0.01048845] \t0\tfalse\n",
            "(2)\t 1265\t [ 1.0653185  -1.3274935   0.47321138] \t0\tfalse\n",
            "(2)\t 1266\t [ 1.6809584  -1.4767765  -0.20376684] \t0\tfalse\n",
            "(1)\t 1267\t [-1.9187531   1.5352067   0.49902135] \t1\ttrue\n",
            "(1)\t 1268\t [ 1.5259199  -1.6627241   0.24785264] \t0\tfalse\n",
            "(1)\t 1269\t [ 1.5259199  -1.6627241   0.24785264] \t0\tfalse\n",
            "(2)\t 1270\t [ 2.072325  -1.6537735 -0.4116732] \t0\tfalse\n",
            "(0)\t 1271\t [-0.7595589  -0.41437462  1.5370092 ] \t2\tfalse\n",
            "(0)\t 1272\t [ 1.9749926 -1.2045227 -0.8220702] \t0\ttrue\n",
            "(0)\t 1273\t [ 2.35776   -1.624735  -0.7893067] \t0\ttrue\n",
            "(0)\t 1274\t [ 2.0683818  -1.7451165  -0.39267716] \t0\ttrue\n",
            "(0)\t 1275\t [ 2.0936294  -1.4581198  -0.56714714] \t0\ttrue\n",
            "(0)\t 1276\t [ 2.028091  -1.3591603 -0.6647679] \t0\ttrue\n",
            "(0)\t 1277\t [ 1.6535473  -1.1713284  -0.38935307] \t0\ttrue\n",
            "(1)\t 1278\t [-1.3980298   0.18441579  1.4779947 ] \t2\tfalse\n",
            "(2)\t 1279\t [ 2.321199  -1.485999  -0.9159087] \t0\tfalse\n",
            "(2)\t 1280\t [ 1.2917188 -0.6975689 -0.5440836] \t0\tfalse\n",
            "(1)\t 1281\t [ 1.2917188 -0.6975689 -0.5440836] \t0\tfalse\n",
            "(1)\t 1282\t [-2.0848103  1.5824367  0.5451716] \t1\ttrue\n",
            "(1)\t 1283\t [-2.026684   1.4689007  0.6909144] \t1\ttrue\n",
            "(1)\t 1284\t [ 0.8322414  -0.9027655   0.20010944] \t0\tfalse\n",
            "(1)\t 1285\t [ 0.8322414  -0.9027655   0.20010944] \t0\tfalse\n",
            "(1)\t 1286\t [ 1.3606145  -1.4606243   0.19191478] \t0\tfalse\n",
            "(2)\t 1287\t [ 1.1821035  -1.2655624   0.26815286] \t0\tfalse\n",
            "(0)\t 1288\t [ 2.2161293  -1.6885904  -0.58677495] \t0\ttrue\n",
            "(2)\t 1289\t [ 1.6259978  -1.5997573   0.02598692] \t0\tfalse\n",
            "(2)\t 1290\t [ 2.0157647  -1.6483654  -0.25063264] \t0\tfalse\n",
            "(1)\t 1291\t [ 0.4164804 -1.0769075  0.9057677] \t2\tfalse\n",
            "(1)\t 1292\t [ 0.4164804 -1.0769075  0.9057677] \t2\tfalse\n",
            "(0)\t 1293\t [ 1.4927932  -1.231968   -0.06046908] \t0\ttrue\n",
            "(0)\t 1294\t [ 1.8516377  -1.6872063  -0.13786905] \t0\ttrue\n",
            "(1)\t 1295\t [-0.4364907 -0.595751   1.3253641] \t2\tfalse\n",
            "(1)\t 1296\t [ 1.8516346  -1.7206935  -0.08390418] \t0\tfalse\n",
            "(1)\t 1297\t [ 1.8516346  -1.7206935  -0.08390418] \t0\tfalse\n",
            "(2)\t 1298\t [ 2.0265994  -1.6349964  -0.36958945] \t0\tfalse\n",
            "(1)\t 1299\t [ 0.37326422 -1.0812324   0.9013867 ] \t2\tfalse\n",
            "(1)\t 1300\t [ 0.37326422 -1.0812324   0.9013867 ] \t2\tfalse\n",
            "(2)\t 1301\t [ 1.5451139 -1.3062661 -0.3040531] \t0\tfalse\n",
            "(2)\t 1302\t [ 2.0997496  -1.4995883  -0.46652457] \t0\tfalse\n",
            "(0)\t 1303\t [ 1.7314063  -1.6194545   0.01610218] \t0\ttrue\n",
            "(2)\t 1304\t [ 1.7314063  -1.6194545   0.01610218] \t0\tfalse\n",
            "(0)\t 1305\t [ 1.6571923  -1.6242266  -0.00642281] \t0\ttrue\n",
            "(1)\t 1306\t [-0.7408885  -0.43002567  1.4008335 ] \t2\tfalse\n",
            "(1)\t 1307\t [-1.0838993   0.12470693  1.2757373 ] \t2\tfalse\n",
            "(2)\t 1308\t [ 1.9991828 -1.5811309 -0.5008478] \t0\tfalse\n",
            "(1)\t 1309\t [-0.10369637 -0.75751764  1.0251884 ] \t2\tfalse\n",
            "(2)\t 1310\t [-1.6445581  1.0360684  1.0319391] \t1\tfalse\n",
            "(2)\t 1311\t [ 0.18906946 -0.5875373   0.6552128 ] \t2\ttrue\n",
            "(0)\t 1312\t [ 1.5700759  -1.4073063  -0.10490945] \t0\ttrue\n",
            "(2)\t 1313\t [ 1.5021275  -1.7296513   0.30548295] \t0\tfalse\n",
            "(0)\t 1314\t [ 2.0364094 -1.4862903 -0.5772772] \t0\ttrue\n",
            "(1)\t 1315\t [ 0.78795874 -1.0605353   0.52629846] \t0\tfalse\n",
            "(2)\t 1316\t [-0.8035455  -0.01501373  1.2188312 ] \t2\ttrue\n",
            "(1)\t 1317\t [-1.026558  -0.3360332  1.7329863] \t2\tfalse\n",
            "(2)\t 1318\t [ 1.3061668  -1.3904771   0.11131744] \t0\tfalse\n",
            "(1)\t 1319\t [ 0.7231472  -1.2422072   0.68214387] \t0\tfalse\n",
            "(2)\t 1320\t [ 0.82990193 -1.3401033   0.6125694 ] \t0\tfalse\n",
            "(1)\t 1321\t [-1.6049023  1.1627743  0.7880702] \t1\ttrue\n",
            "(2)\t 1322\t [ 2.2707071 -1.6875418 -0.5083958] \t0\tfalse\n",
            "(2)\t 1323\t [ 1.7673045 -1.0730919 -0.5313646] \t0\tfalse\n",
            "(0)\t 1324\t [ 2.0983186 -1.6059469 -0.4654481] \t0\ttrue\n",
            "(1)\t 1325\t [ 1.3893296 -1.6246785  0.4209031] \t0\tfalse\n",
            "(2)\t 1326\t [ 1.9903241 -1.6183379 -0.2581171] \t0\tfalse\n",
            "(2)\t 1327\t [ 1.1501138  -1.1586983   0.10747084] \t0\tfalse\n",
            "(0)\t 1328\t [ 2.2654426 -1.6415431 -0.5768275] \t0\ttrue\n",
            "(0)\t 1329\t [ 1.8195221 -1.0585392 -0.8475119] \t0\ttrue\n",
            "(1)\t 1330\t [ 2.106772  -1.5758823 -0.5406932] \t0\tfalse\n",
            "(1)\t 1331\t [ 2.106772  -1.5758823 -0.5406932] \t0\tfalse\n",
            "(0)\t 1332\t [ 1.1880549  -1.2285522   0.04163159] \t0\ttrue\n",
            "(0)\t 1333\t [ 1.5670656  -1.4302627   0.06689975] \t0\ttrue\n",
            "(0)\t 1334\t [ 1.7534922  -1.5165333  -0.17888747] \t0\ttrue\n",
            "(0)\t 1335\t [ 1.4264631  -1.543837    0.14576094] \t0\ttrue\n",
            "(2)\t 1336\t [-2.0563395   1.5305395   0.45606717] \t1\tfalse\n",
            "(0)\t 1337\t [ 1.1223799  -1.4222926   0.36860076] \t0\ttrue\n",
            "(0)\t 1338\t [ 2.3331394  -1.4816161  -0.89604455] \t0\ttrue\n",
            "(0)\t 1339\t [ 1.6899135  -1.661394    0.09711726] \t0\ttrue\n",
            "(2)\t 1340\t [-1.709276    1.6030027  -0.04078035] \t1\tfalse\n",
            "(2)\t 1341\t [-1.6002      1.2737631   0.49067834] \t1\tfalse\n",
            "(1)\t 1342\t [ 0.24336001 -1.1324301   1.121013  ] \t2\tfalse\n",
            "(2)\t 1343\t [ 2.02246    -1.6906254  -0.26632056] \t0\tfalse\n",
            "(0)\t 1344\t [ 0.6855669 -1.3604048  0.8073672] \t2\tfalse\n",
            "(0)\t 1345\t [ 1.8076309  -1.692048   -0.05590281] \t0\ttrue\n",
            "(0)\t 1346\t [ 0.53092223 -1.156073    0.6342042 ] \t2\tfalse\n",
            "(0)\t 1347\t [ 0.9035304 -1.1916885  0.5762236] \t0\ttrue\n",
            "(0)\t 1348\t [ 2.0050204  -1.6956705  -0.16726021] \t0\ttrue\n",
            "(1)\t 1349\t [ 0.20857848 -1.0594877   1.0510195 ] \t2\tfalse\n",
            "(0)\t 1350\t [ 2.4274738  -1.5999721  -0.88255215] \t0\ttrue\n",
            "(2)\t 1351\t [-0.82792133  0.8139211   0.05210352] \t1\tfalse\n",
            "(0)\t 1352\t [ 1.8218392 -1.3737055 -0.4969358] \t0\ttrue\n",
            "(1)\t 1353\t [-0.15657677 -1.1878476   1.4876586 ] \t2\tfalse\n",
            "(1)\t 1354\t [ 0.73571706 -1.3150102   0.6713193 ] \t0\tfalse\n",
            "(2)\t 1355\t [ 1.2825421   0.21254598 -1.5101826 ] \t0\tfalse\n",
            "(1)\t 1356\t [ 1.8694528  -1.4853247  -0.38177425] \t0\tfalse\n",
            "(2)\t 1357\t [ 0.8957108 -1.425215   0.6220802] \t0\tfalse\n",
            "(0)\t 1358\t [ 2.130381  -1.4113452 -0.8116075] \t0\ttrue\n",
            "(0)\t 1359\t [-0.5131933   0.3040367   0.40260926] \t2\tfalse\n",
            "(2)\t 1360\t [ 2.2025442  -1.7179835  -0.44293818] \t0\tfalse\n",
            "(0)\t 1361\t [ 2.244256  -1.1386768 -1.1141158] \t0\ttrue\n",
            "(0)\t 1362\t [ 2.5772183 -1.099757  -1.4934553] \t0\ttrue\n",
            "(0)\t 1363\t [ 1.5061277 -1.696886   0.1575066] \t0\ttrue\n",
            "(0)\t 1364\t [ 2.329328  -1.3059646 -1.0681025] \t0\ttrue\n",
            "(2)\t 1365\t [ 2.3083777 -1.5391127 -0.7415124] \t0\tfalse\n",
            "(2)\t 1366\t [ 1.5143365  -1.6078516   0.13644905] \t0\tfalse\n",
            "(2)\t 1367\t [ 1.7125331  -1.6494553   0.01341618] \t0\tfalse\n",
            "(2)\t 1368\t [ 1.4590988  -1.4111276   0.03772334] \t0\tfalse\n",
            "(2)\t 1369\t [-1.7930248  1.3708581  0.5270382] \t1\tfalse\n",
            "(0)\t 1370\t [ 1.9309626  -1.6585451  -0.16189854] \t0\ttrue\n",
            "(0)\t 1371\t [ 1.672089   -1.4919026  -0.00557083] \t0\ttrue\n",
            "(2)\t 1372\t [-0.43757233 -0.5926702   1.2955911 ] \t2\ttrue\n",
            "(2)\t 1373\t [-0.7698757   0.18372068  0.97216946] \t2\ttrue\n",
            "(2)\t 1374\t [ 1.7722443  -1.4824104  -0.19779865] \t0\tfalse\n",
            "(1)\t 1375\t [ 0.5721206 -1.3121524  0.6387868] \t2\tfalse\n",
            "(1)\t 1376\t [ 0.5721206 -1.3121524  0.6387868] \t2\tfalse\n",
            "(0)\t 1377\t [ 2.3408809 -1.4719868 -0.937196 ] \t0\ttrue\n",
            "(2)\t 1378\t [ 1.8493713  -1.7422453  -0.06144882] \t0\tfalse\n",
            "(0)\t 1379\t [ 1.2510499 -1.3484684  0.2713184] \t0\ttrue\n",
            "(0)\t 1380\t [ 2.0183485  -1.5235711  -0.58991617] \t0\ttrue\n",
            "(2)\t 1381\t [ 0.9343442  -1.1665148   0.24606575] \t0\tfalse\n",
            "(0)\t 1382\t [ 0.92085344 -1.3707668   0.5320877 ] \t0\ttrue\n",
            "(2)\t 1383\t [ 1.0660994  -1.4732326   0.35491237] \t0\tfalse\n",
            "(2)\t 1384\t [ 0.93793595 -1.4309541   0.5990104 ] \t0\tfalse\n",
            "(2)\t 1385\t [-0.219297  -1.0813152  1.5550977] \t2\ttrue\n",
            "(0)\t 1386\t [ 1.4529352  -1.6942152   0.21972176] \t0\ttrue\n",
            "(2)\t 1387\t [ 1.433668   -1.5664974   0.21791057] \t0\tfalse\n",
            "(2)\t 1388\t [ 1.7124375  -1.4565004  -0.20718999] \t0\tfalse\n",
            "(2)\t 1389\t [-1.4154103   1.5870951  -0.35708568] \t1\tfalse\n",
            "(2)\t 1390\t [ 0.02601292 -0.7713931   0.9526725 ] \t2\ttrue\n",
            "(0)\t 1391\t [-1.3303541   1.5380336  -0.35256067] \t1\tfalse\n",
            "(2)\t 1392\t [ 1.6627141  -1.595903    0.00711246] \t0\tfalse\n",
            "(0)\t 1393\t [ 2.131415   -1.7228345  -0.38585457] \t0\ttrue\n",
            "(2)\t 1394\t [-1.408831    1.5276369  -0.30979416] \t1\tfalse\n",
            "(1)\t 1395\t [-0.98154783 -0.2015574   1.509703  ] \t2\tfalse\n",
            "(2)\t 1396\t [ 2.2199287 -1.5529264 -0.641305 ] \t0\tfalse\n",
            "(1)\t 1397\t [-1.3492719   1.2962204   0.04557377] \t1\ttrue\n",
            "(2)\t 1398\t [ 1.0169815 -1.3894926  0.5058595] \t0\tfalse\n",
            "(2)\t 1399\t [ 1.1555498 -1.4259831  0.3130764] \t0\tfalse\n",
            "(2)\t 1400\t [ 0.52318   -1.234881   0.8107178] \t2\ttrue\n",
            "Number of true predictions: 621\n",
            "Number of false predictions: 779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "outputId": "5706a0db-4254-47ca-f9a1-5dd4a56ae760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 44.3254817987152 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgrPoBuDDz_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb732eb9-3dc6-4bf7-a36d-80121be7e8df"
      },
      "source": [
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "scores = [3.0, 1.0, 0.2]\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(softmax(predictions[i][j]))"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8995174  0.0187859  0.08169675]\n",
            "[0.34850636 0.11178983 0.53970385]\n",
            "[0.22535679 0.04691707 0.7277261 ]\n",
            "[0.1525602 0.0547467 0.7926931]\n",
            "[0.6758458  0.05404187 0.2701124 ]\n",
            "[0.68047494 0.04797579 0.2715493 ]\n",
            "[0.75674266 0.03797506 0.2052823 ]\n",
            "[0.48234314 0.0526038  0.4650531 ]\n",
            "[0.8690333  0.02574507 0.10522162]\n",
            "[0.921767   0.02404652 0.0541865 ]\n",
            "[0.48767698 0.10906175 0.4032613 ]\n",
            "[0.76039726 0.02853839 0.21106437]\n",
            "[0.8759862  0.03374667 0.09026714]\n",
            "[0.86631083 0.02622804 0.10746115]\n",
            "[0.8033796  0.03073549 0.16588494]\n",
            "[0.7060805  0.2163394  0.07758006]\n",
            "[0.18756397 0.27776897 0.5346671 ]\n",
            "[0.7700243  0.10928261 0.12069302]\n",
            "[0.49891102 0.17570566 0.32538337]\n",
            "[0.50931656 0.06604114 0.4246423 ]\n",
            "[0.88510704 0.02337947 0.09151355]\n",
            "[0.5739738  0.11453787 0.31148842]\n",
            "[0.5103088 0.1399928 0.3496984]\n",
            "[0.84347093 0.02756718 0.12896189]\n",
            "[0.89252657 0.02241094 0.08506255]\n",
            "[0.4330252  0.06693632 0.5000385 ]\n",
            "[0.6099291  0.19562687 0.19444399]\n",
            "[0.41964093 0.10344156 0.47691745]\n",
            "[0.68539125 0.05368335 0.2609254 ]\n",
            "[0.8208388  0.02774537 0.15141582]\n",
            "[0.8440567  0.11099896 0.04494431]\n",
            "[0.5951741  0.05493775 0.34988827]\n",
            "[0.5641616  0.05529361 0.3805448 ]\n",
            "[0.8881421  0.02328528 0.08857264]\n",
            "[0.8407706  0.03004052 0.1291889 ]\n",
            "[0.02567885 0.79481417 0.17950694]\n",
            "[0.8247233  0.05826863 0.117008  ]\n",
            "[0.7738641  0.03341621 0.19271973]\n",
            "[0.8065459  0.06338816 0.1300659 ]\n",
            "[0.92975885 0.02335614 0.04688507]\n",
            "[0.92975885 0.02335614 0.04688507]\n",
            "[0.33689007 0.26614606 0.39696386]\n",
            "[0.39570668 0.09127352 0.5130198 ]\n",
            "[0.04606594 0.82793814 0.1259959 ]\n",
            "[0.7611772  0.05484227 0.18398058]\n",
            "[0.02726652 0.7946388  0.17809466]\n",
            "[0.7533379  0.0477858  0.19887629]\n",
            "[0.03724241 0.3389171  0.62384045]\n",
            "[0.848885   0.04116146 0.10995353]\n",
            "[0.04468428 0.26919055 0.68612516]\n",
            "[0.81937134 0.03083226 0.14979649]\n",
            "[0.81937134 0.03083226 0.14979649]\n",
            "[0.09401282 0.06973737 0.8362499 ]\n",
            "[0.9079436  0.02676721 0.06528916]\n",
            "[0.8571544  0.0279887  0.11485677]\n",
            "[0.68067205 0.13820255 0.18112539]\n",
            "[0.9210723  0.03240928 0.04651834]\n",
            "[0.85831565 0.02447599 0.11720842]\n",
            "[0.94431734 0.03335775 0.02232498]\n",
            "[0.04418071 0.49427757 0.46154168]\n",
            "[0.0318427  0.833106   0.13505132]\n",
            "[0.23380831 0.08136886 0.6848228 ]\n",
            "[0.7255546  0.04214027 0.23230521]\n",
            "[0.7255546  0.04214027 0.23230521]\n",
            "[0.62763846 0.05348406 0.31887746]\n",
            "[0.802468   0.0346294  0.16290258]\n",
            "[0.75266993 0.03600284 0.21132717]\n",
            "[0.9200558  0.02753866 0.05240548]\n",
            "[0.02208611 0.7434305  0.23448344]\n",
            "[0.86416465 0.02581639 0.11001887]\n",
            "[0.96181524 0.01849317 0.01969154]\n",
            "[0.89975333 0.0220606  0.07818606]\n",
            "[0.9234298  0.02131206 0.05525806]\n",
            "[0.9321707  0.01727677 0.05055244]\n",
            "[0.9557391  0.02504029 0.01922059]\n",
            "[0.8761794  0.02755835 0.09626225]\n",
            "[0.8166477  0.03761848 0.14573382]\n",
            "[0.740214   0.03582303 0.22396298]\n",
            "[0.6695284  0.08562456 0.24484703]\n",
            "[0.9538089  0.0169068  0.02928424]\n",
            "[0.8533966  0.02927291 0.11733051]\n",
            "[0.04507535 0.4535135  0.5014112 ]\n",
            "[0.04507535 0.4535135  0.5014112 ]\n",
            "[0.1621561  0.72345126 0.11439268]\n",
            "[0.84164023 0.0275556  0.1308042 ]\n",
            "[0.65604436 0.04075565 0.30320004]\n",
            "[0.2943025  0.14593825 0.55975926]\n",
            "[0.2943025  0.14593825 0.55975926]\n",
            "[0.93011945 0.01970054 0.05017996]\n",
            "[0.93011945 0.01970054 0.05017996]\n",
            "[0.9270888  0.01923476 0.05367653]\n",
            "[0.8406604  0.02458362 0.13475595]\n",
            "[0.8759075  0.0427124  0.08138011]\n",
            "[0.8759075  0.0427124  0.08138011]\n",
            "[0.86079437 0.03651582 0.10268979]\n",
            "[0.22200789 0.13318826 0.6448039 ]\n",
            "[0.74208647 0.03027523 0.22763821]\n",
            "[0.91264284 0.01972036 0.0676368 ]\n",
            "[0.66336083 0.04417847 0.29246068]\n",
            "[0.6114827  0.05809215 0.3304251 ]\n",
            "[0.03376388 0.34157193 0.6246642 ]\n",
            "[0.8895296  0.02906845 0.08140193]\n",
            "[0.8895296  0.02906845 0.08140193]\n",
            "[0.03536746 0.5802656  0.384367  ]\n",
            "[0.69740814 0.03937802 0.26321387]\n",
            "[0.84485257 0.02777697 0.1273704 ]\n",
            "[0.04608425 0.73365474 0.22026108]\n",
            "[0.8312956  0.02861414 0.14009032]\n",
            "[0.9182236  0.02107321 0.06070324]\n",
            "[0.85859    0.02195307 0.11945701]\n",
            "[0.03991871 0.7919181  0.16816315]\n",
            "[0.75052077 0.0405272  0.20895205]\n",
            "[0.9493928  0.03018574 0.02042145]\n",
            "[0.11377877 0.6050052  0.281216  ]\n",
            "[0.90894824 0.02082963 0.07022206]\n",
            "[0.75624675 0.03712616 0.20662706]\n",
            "[0.93676054 0.01724383 0.0459956 ]\n",
            "[0.28376362 0.09459963 0.62163675]\n",
            "[0.90378416 0.01968967 0.07652612]\n",
            "[0.90378416 0.01968967 0.07652612]\n",
            "[0.8124981  0.02976692 0.15773499]\n",
            "[0.7878237  0.05442916 0.15774712]\n",
            "[0.76606166 0.03325954 0.20067884]\n",
            "[0.07101018 0.14267138 0.7863184 ]\n",
            "[0.75729567 0.04138659 0.20131776]\n",
            "[0.75729567 0.04138659 0.20131776]\n",
            "[0.28839988 0.07731242 0.6342877 ]\n",
            "[0.9384868  0.03743673 0.02407649]\n",
            "[0.02322368 0.73278683 0.2439895 ]\n",
            "[0.80870545 0.0454891  0.14580548]\n",
            "[0.87661964 0.02498361 0.09839679]\n",
            "[0.8175271  0.03328802 0.14918482]\n",
            "[0.73825645 0.04394073 0.21780276]\n",
            "[0.02482095 0.7881975  0.18698156]\n",
            "[0.03658992 0.29721075 0.66619927]\n",
            "[0.04649269 0.7629861  0.19052121]\n",
            "[0.79627347 0.03292569 0.1708008 ]\n",
            "[0.48568    0.13672684 0.37759316]\n",
            "[0.6617451  0.26835108 0.06990386]\n",
            "[0.9012881  0.02058872 0.07812319]\n",
            "[0.9489134  0.02824429 0.02284226]\n",
            "[0.34438774 0.0696701  0.58594215]\n",
            "[0.06733038 0.08783467 0.844835  ]\n",
            "[0.9078932  0.02313526 0.06897145]\n",
            "[0.76643497 0.04778259 0.18578243]\n",
            "[0.5229021  0.1320588  0.34503913]\n",
            "[0.8496984  0.02342983 0.12687168]\n",
            "[0.8026921  0.06150731 0.13580051]\n",
            "[0.7655374  0.03558947 0.19887319]\n",
            "[0.88383806 0.02228746 0.09387444]\n",
            "[0.9464884  0.03406408 0.01944749]\n",
            "[0.8242574  0.02604211 0.14970055]\n",
            "[0.04253558 0.80338037 0.15408403]\n",
            "[0.8386168  0.02622739 0.13515589]\n",
            "[0.865808   0.02949581 0.10469613]\n",
            "[0.836686   0.02744722 0.13586676]\n",
            "[0.649226  0.0566521 0.2941219]\n",
            "[0.9528853  0.01978853 0.0273261 ]\n",
            "[0.91922337 0.0197318  0.0610448 ]\n",
            "[0.6337379  0.04654311 0.3197189 ]\n",
            "[0.898999   0.02501195 0.07598908]\n",
            "[0.6098162  0.03434521 0.3558386 ]\n",
            "[0.028759   0.83330774 0.13793336]\n",
            "[0.9109699  0.02879375 0.06023636]\n",
            "[0.6440801  0.06095847 0.29496148]\n",
            "[0.72217953 0.09378965 0.18403077]\n",
            "[0.82644266 0.04294035 0.13061692]\n",
            "[0.7754935  0.03441062 0.19009589]\n",
            "[0.9334143  0.01775855 0.0488272 ]\n",
            "[0.8082556  0.02656911 0.1651752 ]\n",
            "[0.84940946 0.0267917  0.12379882]\n",
            "[0.84940946 0.0267917  0.12379882]\n",
            "[0.6500562  0.053498   0.29644585]\n",
            "[0.88269454 0.02326303 0.09404242]\n",
            "[0.91163933 0.01885906 0.06950165]\n",
            "[0.74751866 0.19566618 0.05681514]\n",
            "[0.4841849  0.05136451 0.46445054]\n",
            "[0.7913054  0.05075187 0.15794276]\n",
            "[0.91422474 0.02803279 0.05774252]\n",
            "[0.7202052  0.0416606  0.23813418]\n",
            "[0.90058345 0.01952275 0.07989381]\n",
            "[0.87785155 0.02455585 0.09759263]\n",
            "[0.87785155 0.02455585 0.09759263]\n",
            "[0.89091754 0.02483354 0.08424892]\n",
            "[0.7557039  0.16554071 0.07875542]\n",
            "[0.85656893 0.02783209 0.11559904]\n",
            "[0.10214338 0.09018707 0.8076696 ]\n",
            "[0.9152124  0.01906904 0.06571853]\n",
            "[0.8905103  0.02133837 0.0881514 ]\n",
            "[0.57134145 0.05253814 0.3761204 ]\n",
            "[0.02763139 0.5104294  0.46193922]\n",
            "[0.7772413  0.03838704 0.1843717 ]\n",
            "[0.7772413  0.03838704 0.1843717 ]\n",
            "[0.8615092  0.02782914 0.11066175]\n",
            "[0.7077412  0.11423396 0.17802484]\n",
            "[0.7077412  0.11423396 0.17802484]\n",
            "[0.6685604  0.20102759 0.13041197]\n",
            "[0.92650145 0.02048079 0.05301778]\n",
            "[0.6864568  0.04149076 0.27205238]\n",
            "[0.7924742  0.03821015 0.16931565]\n",
            "[0.38551596 0.07139283 0.54309124]\n",
            "[0.74118507 0.04304041 0.21577452]\n",
            "[0.60667306 0.33183432 0.06149258]\n",
            "[0.8668335  0.02597214 0.10719442]\n",
            "[0.846724   0.03090563 0.12237039]\n",
            "[0.03295459 0.6513138  0.31573164]\n",
            "[0.7395991  0.05421309 0.20618778]\n",
            "[0.57840896 0.06604498 0.35554603]\n",
            "[0.42555338 0.05555276 0.5188939 ]\n",
            "[0.43097192 0.06661958 0.5024085 ]\n",
            "[0.914352   0.02131294 0.06433509]\n",
            "[0.90180904 0.01985564 0.0783353 ]\n",
            "[0.86689925 0.02593694 0.1071638 ]\n",
            "[0.69607234 0.04360585 0.2603218 ]\n",
            "[0.94393224 0.01906331 0.03700446]\n",
            "[0.71844846 0.03602387 0.24552764]\n",
            "[0.892418   0.02320665 0.08437542]\n",
            "[0.19137266 0.36432007 0.44430724]\n",
            "[0.64638203 0.05611504 0.29750293]\n",
            "[0.04255225 0.6848922  0.27255556]\n",
            "[0.85095334 0.02285499 0.12619169]\n",
            "[0.11219043 0.06414703 0.8236626 ]\n",
            "[0.9166772  0.01880247 0.06452046]\n",
            "[0.03482135 0.7641142  0.20106445]\n",
            "[0.7803551  0.13927177 0.08037317]\n",
            "[0.8583628  0.02807112 0.11356612]\n",
            "[0.03832315 0.13738272 0.82429415]\n",
            "[0.83149445 0.0239432  0.1445624 ]\n",
            "[0.40841872 0.08690932 0.504672  ]\n",
            "[0.88272846 0.02154847 0.09572315]\n",
            "[0.8803229  0.02516952 0.09450757]\n",
            "[0.48671404 0.05342552 0.4598605 ]\n",
            "[0.41145065 0.09251448 0.49603492]\n",
            "[0.7757061  0.0319398  0.19235402]\n",
            "[0.92386556 0.01782282 0.05831162]\n",
            "[0.5031953  0.15763184 0.3391729 ]\n",
            "[0.8007445  0.03933694 0.15991862]\n",
            "[0.7238309  0.05766121 0.2185079 ]\n",
            "[0.87655824 0.02159247 0.10184921]\n",
            "[0.9210472  0.01891132 0.06004153]\n",
            "[0.85503525 0.03368394 0.11128078]\n",
            "[0.86485773 0.02764895 0.10749336]\n",
            "[0.9331747  0.02106373 0.04576149]\n",
            "[0.94109905 0.02061761 0.03828324]\n",
            "[0.9108614  0.01830794 0.07083061]\n",
            "[0.8334023  0.02438242 0.14221531]\n",
            "[0.5398665  0.40417954 0.05595391]\n",
            "[0.90032876 0.02528155 0.07438973]\n",
            "[0.67371494 0.05757622 0.26870888]\n",
            "[0.23029143 0.68565273 0.08405585]\n",
            "[0.9248343  0.02083097 0.0543347 ]\n",
            "[0.9248343  0.02083097 0.0543347 ]\n",
            "[0.32233357 0.12020189 0.55746454]\n",
            "[0.896232   0.02310106 0.08066686]\n",
            "[0.9323305  0.02226042 0.04540919]\n",
            "[0.9323305  0.02226042 0.04540919]\n",
            "[0.9393148  0.03195301 0.02873224]\n",
            "[0.14021412 0.13621841 0.7235675 ]\n",
            "[0.03663657 0.14669277 0.8166707 ]\n",
            "[0.06002793 0.5582174  0.38175467]\n",
            "[0.92844045 0.01850652 0.05305305]\n",
            "[0.6792166  0.03901157 0.28177184]\n",
            "[0.77280736 0.03579064 0.19140206]\n",
            "[0.7448751  0.03759916 0.21752574]\n",
            "[0.49341735 0.06421913 0.4423635 ]\n",
            "[0.87591296 0.09248722 0.0315998 ]\n",
            "[0.92432886 0.01864987 0.05702127]\n",
            "[0.02376578 0.6763577  0.29987654]\n",
            "[0.41964093 0.10344156 0.47691745]\n",
            "[0.15725143 0.6387031  0.20404543]\n",
            "[0.02970785 0.60171735 0.36857486]\n",
            "[0.91214937 0.01923396 0.06861667]\n",
            "[0.60937434 0.05603766 0.334588  ]\n",
            "[0.7230006  0.03549674 0.2415026 ]\n",
            "[0.73754305 0.03662463 0.2258324 ]\n",
            "[0.75057244 0.0362636  0.21316393]\n",
            "[0.8403041  0.03452558 0.12517029]\n",
            "[0.8869857  0.03148246 0.08153182]\n",
            "[0.75054646 0.04702096 0.20243256]\n",
            "[0.17499958 0.2575053  0.5674951 ]\n",
            "[0.03038309 0.6337068  0.33591014]\n",
            "[0.76167536 0.0356647  0.20266   ]\n",
            "[0.9087132  0.02295198 0.06833481]\n",
            "[0.8683042  0.02518822 0.10650755]\n",
            "[0.771918   0.03731652 0.19076553]\n",
            "[0.05320395 0.1421699  0.8046261 ]\n",
            "[0.0576499  0.71486694 0.22748317]\n",
            "[0.53843254 0.07082738 0.39074007]\n",
            "[0.9106643  0.02252249 0.06681312]\n",
            "[0.08722819 0.69782436 0.21494742]\n",
            "[0.02664607 0.7648822  0.20847176]\n",
            "[0.90255064 0.02593162 0.07151774]\n",
            "[0.63166106 0.04800794 0.32033095]\n",
            "[0.81996036 0.14061491 0.03942477]\n",
            "[0.9131829  0.0452484  0.04156869]\n",
            "[0.82185125 0.03215213 0.14599662]\n",
            "[0.92357236 0.01714953 0.05927815]\n",
            "[0.8056704  0.03648537 0.15784423]\n",
            "[0.86198235 0.030769   0.10724861]\n",
            "[0.86198235 0.030769   0.10724861]\n",
            "[0.5341636  0.06801739 0.39781904]\n",
            "[0.42759946 0.05526683 0.5171337 ]\n",
            "[0.52600086 0.04985568 0.42414352]\n",
            "[0.88939357 0.02257979 0.08802667]\n",
            "[0.88939357 0.02257979 0.08802667]\n",
            "[0.6717695  0.06010076 0.26812977]\n",
            "[0.03746444 0.5314357  0.4310998 ]\n",
            "[0.85501397 0.02661617 0.11836987]\n",
            "[0.78148544 0.03953155 0.17898296]\n",
            "[0.34038198 0.08343863 0.5761794 ]\n",
            "[0.81292313 0.05867412 0.1284028 ]\n",
            "[0.18785095 0.06889876 0.74325025]\n",
            "[0.9368059  0.02122615 0.04196799]\n",
            "[0.9347758  0.02435143 0.04087279]\n",
            "[0.86119515 0.03456878 0.10423604]\n",
            "[0.93029886 0.01767713 0.05202403]\n",
            "[0.89789563 0.02134207 0.08076227]\n",
            "[0.14136754 0.05463239 0.8040001 ]\n",
            "[0.02297043 0.8117332  0.16529639]\n",
            "[0.698494   0.04079329 0.26071262]\n",
            "[0.8031589  0.03428679 0.16255437]\n",
            "[0.90301573 0.02171197 0.07527234]\n",
            "[0.9147171  0.02901637 0.05626655]\n",
            "[0.7675331  0.03987189 0.19259505]\n",
            "[0.7074545  0.05876049 0.23378493]\n",
            "[0.12029445 0.16582629 0.7138792 ]\n",
            "[0.8104907  0.03577278 0.15373643]\n",
            "[0.8104907  0.03577278 0.15373643]\n",
            "[0.56103444 0.06067918 0.37828642]\n",
            "[0.8814534  0.02143969 0.09710692]\n",
            "[0.9523874  0.01808461 0.02952803]\n",
            "[0.80969673 0.03011182 0.16019142]\n",
            "[0.6119865  0.04882293 0.33919054]\n",
            "[0.83058864 0.03587701 0.13353439]\n",
            "[0.04152798 0.5484222  0.41004983]\n",
            "[0.34469172 0.30191728 0.35339102]\n",
            "[0.34469172 0.30191728 0.35339102]\n",
            "[0.9148237  0.02190144 0.06327491]\n",
            "[0.02756223 0.6683942  0.3040436 ]\n",
            "[0.7662177  0.15573397 0.07804834]\n",
            "[0.5440018  0.35635248 0.09964573]\n",
            "[0.5440018  0.35635248 0.09964573]\n",
            "[0.86273986 0.02869957 0.10856058]\n",
            "[0.13198312 0.17085987 0.69715697]\n",
            "[0.77621746 0.03208724 0.19169526]\n",
            "[0.90723    0.0244571  0.06831286]\n",
            "[0.81387734 0.04546611 0.14065655]\n",
            "[0.3054616  0.6092932  0.08524527]\n",
            "[0.68678886 0.03721482 0.27599633]\n",
            "[0.64273745 0.05392389 0.30333868]\n",
            "[0.50017256 0.08749746 0.41232997]\n",
            "[0.11058759 0.10931087 0.78010154]\n",
            "[0.19129321 0.14536242 0.6633444 ]\n",
            "[0.934531   0.0184502  0.04701879]\n",
            "[0.94382924 0.02394143 0.03222934]\n",
            "[0.7436395  0.05581582 0.20054466]\n",
            "[0.04661284 0.14333539 0.8100518 ]\n",
            "[0.02612713 0.7101749  0.26369798]\n",
            "[0.5299534  0.06737178 0.40267476]\n",
            "[0.6248479  0.05809976 0.31705236]\n",
            "[0.6248479  0.05809976 0.31705236]\n",
            "[0.6203119  0.09174173 0.28794634]\n",
            "[0.8612761  0.02688134 0.1118426 ]\n",
            "[0.66171753 0.13183971 0.20644273]\n",
            "[0.4334181  0.05489146 0.5116905 ]\n",
            "[0.05022999 0.7648425  0.18492754]\n",
            "[0.6415831  0.05484623 0.30357072]\n",
            "[0.29426834 0.38004243 0.32568926]\n",
            "[0.07922607 0.7457311  0.17504284]\n",
            "[0.82451236 0.03463334 0.14085433]\n",
            "[0.9360104  0.02315803 0.04083157]\n",
            "[0.3783243  0.17296174 0.44871396]\n",
            "[0.91749007 0.01984915 0.06266076]\n",
            "[0.6501298  0.04920655 0.3006636 ]\n",
            "[0.3773502  0.12969188 0.49295783]\n",
            "[0.02932281 0.7703169  0.20036024]\n",
            "[0.0492462 0.5747871 0.3759667]\n",
            "[0.8262823  0.02936381 0.14435396]\n",
            "[0.7904245  0.03067067 0.17890479]\n",
            "[0.6998652  0.09803893 0.20209587]\n",
            "[0.47798046 0.08424307 0.43777642]\n",
            "[0.9143203  0.01938718 0.06629257]\n",
            "[0.4895118  0.08089876 0.42958942]\n",
            "[0.92610496 0.02731173 0.04658333]\n",
            "[0.14995384 0.05793992 0.7921063 ]\n",
            "[0.6394586  0.04726343 0.31327793]\n",
            "[0.21417059 0.05302446 0.73280495]\n",
            "[0.9504344  0.03102812 0.01853746]\n",
            "[0.9225665  0.03230203 0.04513158]\n",
            "[0.66873187 0.04958395 0.28168413]\n",
            "[0.9035564  0.02067625 0.07576735]\n",
            "[0.5232312  0.05882781 0.41794094]\n",
            "[0.8872703  0.02219199 0.09053769]\n",
            "[0.38378817 0.12395225 0.4922595 ]\n",
            "[0.8550769  0.04774082 0.09718233]\n",
            "[0.85196584 0.02740544 0.12062868]\n",
            "[0.8783873  0.02310495 0.09850778]\n",
            "[0.7650595  0.03273252 0.20220791]\n",
            "[0.863938   0.02557933 0.11048263]\n",
            "[0.86132014 0.02342173 0.11525809]\n",
            "[0.93300843 0.01780423 0.04918725]\n",
            "[0.02148036 0.7046738  0.27384588]\n",
            "[0.05261688 0.08424792 0.86313516]\n",
            "[0.12843321 0.48786837 0.38369837]\n",
            "[0.7550476  0.05310309 0.1918493 ]\n",
            "[0.80559134 0.03132119 0.16308747]\n",
            "[0.37572733 0.07214361 0.55212903]\n",
            "[0.30577132 0.08128803 0.61294067]\n",
            "[0.04667002 0.2218887  0.7314413 ]\n",
            "[0.833279   0.02984287 0.13687812]\n",
            "[0.8896044  0.02460421 0.08579143]\n",
            "[0.49884948 0.06593441 0.43521613]\n",
            "[0.7832262  0.03894206 0.17783174]\n",
            "[0.05787033 0.14607492 0.7960547 ]\n",
            "[0.03754823 0.8333749  0.1290768 ]\n",
            "[0.08541279 0.45878166 0.45580548]\n",
            "[0.02800816 0.81680447 0.1551874 ]\n",
            "[0.86234725 0.02502054 0.11263223]\n",
            "[0.05179405 0.1946307  0.7535752 ]\n",
            "[0.8222502  0.06974196 0.10800784]\n",
            "[0.9183645  0.01803242 0.06360307]\n",
            "[0.79189944 0.02952961 0.17857099]\n",
            "[0.9159784  0.02398568 0.06003591]\n",
            "[0.91822976 0.02027069 0.06149958]\n",
            "[0.87945044 0.02339552 0.09715407]\n",
            "[0.81833625 0.03897897 0.1426847 ]\n",
            "[0.9441459  0.01889474 0.03695941]\n",
            "[0.9028092  0.02121479 0.07597595]\n",
            "[0.7532483  0.05008428 0.19666737]\n",
            "[0.8775469  0.02677475 0.09567835]\n",
            "[0.929662   0.01587714 0.05446083]\n",
            "[0.91355824 0.01852033 0.06792146]\n",
            "[0.91355824 0.01852033 0.06792146]\n",
            "[0.05780447 0.61355394 0.3286416 ]\n",
            "[0.8301065  0.02982416 0.1400693 ]\n",
            "[0.87680596 0.0373503  0.08584375]\n",
            "[0.89267683 0.02383637 0.08348681]\n",
            "[0.71010476 0.036776   0.25311926]\n",
            "[0.92251235 0.02524504 0.05224255]\n",
            "[0.85917085 0.03066195 0.11016721]\n",
            "[0.89810383 0.02766287 0.07423331]\n",
            "[0.8242349  0.04501335 0.13075168]\n",
            "[0.8657311  0.04553846 0.08873049]\n",
            "[0.7530549  0.10276399 0.14418106]\n",
            "[0.5042512  0.3859004  0.10984837]\n",
            "[0.02845179 0.82867914 0.142869  ]\n",
            "[0.91921246 0.05626513 0.02452236]\n",
            "[0.91921246 0.05626513 0.02452236]\n",
            "[0.93646204 0.01717528 0.04636265]\n",
            "[0.5634393  0.06802835 0.36853236]\n",
            "[0.10601337 0.06464403 0.8293426 ]\n",
            "[0.9201347  0.01999912 0.05986616]\n",
            "[0.02050036 0.8289979  0.15050174]\n",
            "[0.8974702  0.02040154 0.08212829]\n",
            "[0.8245359  0.03496129 0.14050284]\n",
            "[0.6672533  0.05932515 0.27342153]\n",
            "[0.8255087  0.02536546 0.14912593]\n",
            "[0.88399607 0.03522219 0.08078174]\n",
            "[0.8599247  0.03401743 0.10605789]\n",
            "[0.895008   0.02210015 0.08289187]\n",
            "[0.77353233 0.03973737 0.18673025]\n",
            "[0.8612965  0.02801702 0.11068653]\n",
            "[0.50763357 0.4208943  0.07147211]\n",
            "[0.8222963  0.05321464 0.12448908]\n",
            "[0.90076005 0.03407075 0.06516926]\n",
            "[0.9423975  0.01747939 0.04012316]\n",
            "[0.9423975  0.01747939 0.04012316]\n",
            "[0.8729444  0.02534942 0.1017061 ]\n",
            "[0.7441924  0.06582292 0.18998466]\n",
            "[0.9573763  0.0207126  0.02191106]\n",
            "[0.5825546  0.05031673 0.36712867]\n",
            "[0.5155625  0.06485068 0.4195868 ]\n",
            "[0.9121412  0.02818218 0.05967651]\n",
            "[0.33853477 0.1312226  0.5302426 ]\n",
            "[0.22455303 0.06279291 0.71265405]\n",
            "[0.8840295  0.02211777 0.09385276]\n",
            "[0.18663937 0.0575464  0.7558142 ]\n",
            "[0.9385147  0.03673233 0.02475294]\n",
            "[0.9155207  0.02474096 0.05973829]\n",
            "[0.6462326  0.05164329 0.30212402]\n",
            "[0.42764303 0.09216969 0.4801873 ]\n",
            "[0.02661418 0.78299344 0.19039242]\n",
            "[0.8950042  0.02444861 0.08054706]\n",
            "[0.83449197 0.03512823 0.13037984]\n",
            "[0.7475906  0.04414366 0.20826569]\n",
            "[0.7475906  0.04414366 0.20826569]\n",
            "[0.03334906 0.8369867  0.1296643 ]\n",
            "[0.6543631  0.04731066 0.29832628]\n",
            "[0.02336873 0.7396901  0.23694117]\n",
            "[0.86826056 0.02526056 0.10647889]\n",
            "[0.9131357  0.05851836 0.02834601]\n",
            "[0.03832269 0.19321316 0.7684641 ]\n",
            "[0.21959937 0.05501591 0.7253847 ]\n",
            "[0.91960406 0.02019287 0.06020308]\n",
            "[0.895619   0.02173453 0.08264657]\n",
            "[0.16750547 0.14318097 0.68931353]\n",
            "[0.74499106 0.04823443 0.20677458]\n",
            "[0.83738834 0.0359632  0.12664853]\n",
            "[0.23623699 0.11189254 0.6518704 ]\n",
            "[0.47471794 0.06331225 0.4619698 ]\n",
            "[0.9241614  0.01752473 0.05831387]\n",
            "[0.7031826  0.04033772 0.25647974]\n",
            "[0.93200046 0.02143915 0.04656043]\n",
            "[0.26597655 0.09724373 0.63677967]\n",
            "[0.23753053 0.07576507 0.6867044 ]\n",
            "[0.13191551 0.7587859  0.10929858]\n",
            "[0.45818332 0.10493904 0.43687767]\n",
            "[0.7622458  0.03153827 0.20621593]\n",
            "[0.92260206 0.02237398 0.05502391]\n",
            "[0.84529555 0.04765378 0.10705076]\n",
            "[0.14109878 0.20093574 0.6579654 ]\n",
            "[0.7776571  0.05719568 0.16514726]\n",
            "[0.5882335  0.06974598 0.3420206 ]\n",
            "[0.02269298 0.7066118  0.27069515]\n",
            "[0.37852266 0.07795165 0.5435257 ]\n",
            "[0.44109538 0.0752236  0.48368105]\n",
            "[0.86476153 0.02686779 0.1083706 ]\n",
            "[0.6253321  0.05955813 0.3151098 ]\n",
            "[0.83955485 0.02446142 0.13598369]\n",
            "[0.68536216 0.04397203 0.2706658 ]\n",
            "[0.81639016 0.08555777 0.09805211]\n",
            "[0.6557054  0.04817593 0.29611874]\n",
            "[0.71067494 0.05403117 0.2352939 ]\n",
            "[0.33192146 0.52934456 0.13873404]\n",
            "[0.04345802 0.7440107  0.21253127]\n",
            "[0.9148017  0.01960061 0.06559771]\n",
            "[0.9465517  0.01905343 0.03439486]\n",
            "[0.81150836 0.03075529 0.15773639]\n",
            "[0.5259352  0.05722591 0.41683894]\n",
            "[0.90686125 0.02037734 0.07276137]\n",
            "[0.42356026 0.44086966 0.1355701 ]\n",
            "[0.68547344 0.05445122 0.2600753 ]\n",
            "[0.11268521 0.7703879  0.11692689]\n",
            "[0.7916434  0.05500955 0.15334705]\n",
            "[0.5356085  0.05924788 0.40514365]\n",
            "[0.03957312 0.7635722  0.1968546 ]\n",
            "[0.915125   0.02208013 0.06279486]\n",
            "[0.7198833  0.04237511 0.23774159]\n",
            "[0.8577489  0.02514743 0.11710354]\n",
            "[0.39138842 0.04179151 0.56682   ]\n",
            "[0.87426084 0.02361629 0.10212293]\n",
            "[0.6890097  0.11941436 0.1915759 ]\n",
            "[0.82724935 0.02825158 0.14449912]\n",
            "[0.06809474 0.18061258 0.7512927 ]\n",
            "[0.9502295  0.02625345 0.02351709]\n",
            "[0.8060892  0.03182641 0.16208437]\n",
            "[0.82839996 0.03478941 0.13681063]\n",
            "[0.95562714 0.02416721 0.02020564]\n",
            "[0.7995526  0.15272059 0.04772686]\n",
            "[0.02216982 0.6735774  0.30425277]\n",
            "[0.09231687 0.18406722 0.7236159 ]\n",
            "[0.8802123  0.02436442 0.09542334]\n",
            "[0.95329183 0.02677562 0.0199326 ]\n",
            "[0.02873178 0.77379715 0.19747098]\n",
            "[0.8400073  0.02827455 0.13171804]\n",
            "[0.73212194 0.03996282 0.22791523]\n",
            "[0.47487554 0.16499144 0.360133  ]\n",
            "[0.604748   0.07004032 0.32521167]\n",
            "[0.4123676  0.06564285 0.5219895 ]\n",
            "[0.22602397 0.25189856 0.5220775 ]\n",
            "[0.86874914 0.02758409 0.10366674]\n",
            "[0.68456393 0.21164979 0.10378629]\n",
            "[0.54764676 0.05597208 0.39638114]\n",
            "[0.7298976  0.04560972 0.2244927 ]\n",
            "[0.9422935  0.01705258 0.04065386]\n",
            "[0.0353957  0.5719429  0.39266133]\n",
            "[0.02445045 0.80344826 0.17210127]\n",
            "[0.3497055  0.06619776 0.5840968 ]\n",
            "[0.54261786 0.07745574 0.37992647]\n",
            "[0.54261786 0.07745574 0.37992647]\n",
            "[0.47716108 0.05323795 0.46960095]\n",
            "[0.9237256  0.02382834 0.052446  ]\n",
            "[0.9237256  0.02382834 0.052446  ]\n",
            "[0.07369031 0.0665312  0.8597785 ]\n",
            "[0.6997153  0.04390549 0.25637922]\n",
            "[0.7215965  0.03660556 0.24179803]\n",
            "[0.8148914  0.05081672 0.13429184]\n",
            "[0.9619264  0.01980433 0.01826935]\n",
            "[0.8389887  0.02854626 0.13246496]\n",
            "[0.84169763 0.03333325 0.12496913]\n",
            "[0.02140671 0.8118213  0.16677195]\n",
            "[0.7823509  0.05856474 0.15908441]\n",
            "[0.69129217 0.05048665 0.25822118]\n",
            "[0.88208187 0.02302626 0.09489194]\n",
            "[0.8141841  0.05177765 0.13403824]\n",
            "[0.9353275  0.03165208 0.03302047]\n",
            "[0.17834151 0.05797716 0.76368135]\n",
            "[0.78687584 0.03969564 0.1734286 ]\n",
            "[0.9015909  0.02131919 0.07708993]\n",
            "[0.92919314 0.01788912 0.05291778]\n",
            "[0.25368991 0.11444265 0.63186747]\n",
            "[0.39006636 0.04582839 0.5641053 ]\n",
            "[0.22477756 0.08078129 0.6944412 ]\n",
            "[0.7155935  0.03648206 0.24792448]\n",
            "[0.95742744 0.01883548 0.02373711]\n",
            "[0.7436541  0.03164253 0.22470337]\n",
            "[0.6543539  0.2089082  0.13673788]\n",
            "[0.7246498  0.04158108 0.23376912]\n",
            "[0.7246498  0.04158108 0.23376912]\n",
            "[0.85462415 0.02845589 0.11691994]\n",
            "[0.85398495 0.03040417 0.11561089]\n",
            "[0.02584407 0.8053101  0.16884585]\n",
            "[0.779345   0.0916933  0.12896177]\n",
            "[0.89111936 0.08016975 0.02871096]\n",
            "[0.56643945 0.06221522 0.37134528]\n",
            "[0.8887783  0.01957714 0.09164459]\n",
            "[0.5080077  0.07324407 0.41874817]\n",
            "[0.8567269  0.08313769 0.06013547]\n",
            "[0.8736952  0.02486951 0.10143524]\n",
            "[0.85659647 0.06227728 0.08112633]\n",
            "[0.88515234 0.05048604 0.06436171]\n",
            "[0.7184344  0.03880128 0.2427643 ]\n",
            "[0.02584734 0.72774947 0.24640316]\n",
            "[0.78256834 0.03010386 0.18732783]\n",
            "[0.91017085 0.0191687  0.07066041]\n",
            "[0.47047004 0.15330273 0.37622723]\n",
            "[0.7371341  0.10200841 0.16085747]\n",
            "[0.50782776 0.06912629 0.42304596]\n",
            "[0.01857009 0.7524882  0.22894175]\n",
            "[0.5628574  0.19867563 0.23846702]\n",
            "[0.10353139 0.79020286 0.10626569]\n",
            "[0.93627423 0.01817622 0.04554953]\n",
            "[0.68347234 0.25575796 0.06076973]\n",
            "[0.9133586  0.02540991 0.0612314 ]\n",
            "[0.02002771 0.76592356 0.21404876]\n",
            "[0.78193444 0.03288304 0.18518256]\n",
            "[0.6049038  0.07441404 0.32068214]\n",
            "[0.7280082  0.03599468 0.23599713]\n",
            "[0.90556604 0.02158636 0.07284756]\n",
            "[0.88628036 0.02695664 0.08676294]\n",
            "[0.7649965  0.16545819 0.06954527]\n",
            "[0.9064056  0.01937447 0.07421996]\n",
            "[0.8215498  0.08477522 0.09367504]\n",
            "[0.8458401  0.03864269 0.11551726]\n",
            "[0.8686475  0.02473102 0.10662145]\n",
            "[0.74867606 0.06498788 0.18633617]\n",
            "[0.93578917 0.01920309 0.04500782]\n",
            "[0.9150176  0.0186436  0.06633881]\n",
            "[0.9099201  0.01944762 0.0706322 ]\n",
            "[0.90872663 0.02330101 0.06797229]\n",
            "[0.7737831  0.08744237 0.13877448]\n",
            "[0.87792265 0.02527106 0.09680628]\n",
            "[0.66281575 0.07071818 0.26646608]\n",
            "[0.85027134 0.02692596 0.12280267]\n",
            "[0.06794703 0.80280715 0.12924583]\n",
            "[0.6150284  0.06190681 0.32306483]\n",
            "[0.6937668  0.15570106 0.15053216]\n",
            "[0.8651314  0.02468325 0.11018534]\n",
            "[0.9332941  0.04237105 0.02433492]\n",
            "[0.45407015 0.05866872 0.48726115]\n",
            "[0.5123563 0.0901291 0.3975146]\n",
            "[0.8826571  0.0266095  0.09073337]\n",
            "[0.94718784 0.01777571 0.03503642]\n",
            "[0.95153046 0.02839565 0.02007377]\n",
            "[0.8666373  0.04821273 0.08514996]\n",
            "[0.4231207  0.11262195 0.46425733]\n",
            "[0.17238928 0.1011515  0.7264592 ]\n",
            "[0.8014426  0.03824069 0.16031662]\n",
            "[0.90448403 0.0207159  0.07480009]\n",
            "[0.90448403 0.0207159  0.07480009]\n",
            "[0.78617215 0.03602747 0.17780031]\n",
            "[0.7291589  0.04144406 0.22939715]\n",
            "[0.03474302 0.5413134  0.42394358]\n",
            "[0.74591684 0.03469237 0.21939068]\n",
            "[0.38060248 0.25966474 0.3597328 ]\n",
            "[0.91217434 0.02316593 0.06465978]\n",
            "[0.91217434 0.02316593 0.06465978]\n",
            "[0.8636618  0.05631411 0.08002403]\n",
            "[0.91708523 0.02674662 0.05616822]\n",
            "[0.5586771  0.05903995 0.38228294]\n",
            "[0.91151094 0.03687209 0.05161689]\n",
            "[0.31489336 0.32052627 0.36458033]\n",
            "[0.84516615 0.03060091 0.12423296]\n",
            "[0.05996105 0.7600012  0.1800377 ]\n",
            "[0.75471085 0.04318525 0.2021039 ]\n",
            "[0.9053585  0.02252424 0.0721172 ]\n",
            "[0.47631618 0.11223171 0.41145214]\n",
            "[0.03602801 0.83510065 0.1288714 ]\n",
            "[0.8807821  0.03150776 0.08771013]\n",
            "[0.87646675 0.03544931 0.08808393]\n",
            "[0.9063974  0.01927372 0.07432891]\n",
            "[0.77795553 0.03718016 0.1848643 ]\n",
            "[0.14134236 0.05414191 0.8045157 ]\n",
            "[0.84881544 0.03294135 0.11824322]\n",
            "[0.76487863 0.03679937 0.19832201]\n",
            "[0.67474973 0.04541789 0.27983242]\n",
            "[0.32546026 0.13516778 0.53937197]\n",
            "[0.88858974 0.02626498 0.08514529]\n",
            "[0.4043743  0.06173587 0.5338899 ]\n",
            "[0.88688564 0.02498389 0.08813045]\n",
            "[0.84189886 0.04006713 0.11803403]\n",
            "[0.91010916 0.02797047 0.06192033]\n",
            "[0.8671086  0.03464829 0.09824308]\n",
            "[0.8671086  0.03464829 0.09824308]\n",
            "[0.02263504 0.6953778  0.2819871 ]\n",
            "[0.9341602  0.01792515 0.04791465]\n",
            "[0.46846652 0.15341194 0.37812153]\n",
            "[0.0187846  0.7603216  0.22089374]\n",
            "[0.10197861 0.06910603 0.8289154 ]\n",
            "[0.02350732 0.7897247  0.186768  ]\n",
            "[0.8034876  0.02937382 0.16713858]\n",
            "[0.1981401  0.11955357 0.6823063 ]\n",
            "[0.58714765 0.06233801 0.35051438]\n",
            "[0.63589    0.05532445 0.3087855 ]\n",
            "[0.8308602  0.03696506 0.13217476]\n",
            "[0.07887667 0.7938165  0.12730686]\n",
            "[0.07723337 0.06716169 0.855605  ]\n",
            "[0.8898127  0.02146401 0.08872329]\n",
            "[0.62135273 0.08479276 0.29385448]\n",
            "[0.81285906 0.05391443 0.1332265 ]\n",
            "[0.8596528  0.03373682 0.10661048]\n",
            "[0.86223364 0.02799713 0.10976928]\n",
            "[0.8773607  0.02185157 0.10078768]\n",
            "[0.8694642  0.0259563  0.10457949]\n",
            "[0.3411678  0.11110742 0.5477247 ]\n",
            "[0.94347477 0.03074576 0.02577949]\n",
            "[0.54295963 0.05772497 0.39931545]\n",
            "[0.198835   0.07906883 0.72209615]\n",
            "[0.88431156 0.02896556 0.08672287]\n",
            "[0.6875591  0.04637378 0.26606712]\n",
            "[0.03505407 0.32721892 0.63772696]\n",
            "[0.05555573 0.81800944 0.12643485]\n",
            "[0.8988753  0.02481784 0.07630692]\n",
            "[0.8471273  0.07205957 0.08081314]\n",
            "[0.79456663 0.1257774  0.07965599]\n",
            "[0.7323155  0.06760286 0.20008166]\n",
            "[0.9430306  0.02736359 0.02960581]\n",
            "[0.9335328  0.01766232 0.0488049 ]\n",
            "[0.25778767 0.14313638 0.5990759 ]\n",
            "[0.62507594 0.05547174 0.31945226]\n",
            "[0.9041027  0.03270408 0.06319325]\n",
            "[0.63776577 0.06996041 0.29227385]\n",
            "[0.48159197 0.06688844 0.45151958]\n",
            "[0.919835   0.02047962 0.05968542]\n",
            "[0.53809714 0.37280735 0.08909553]\n",
            "[0.5936326  0.06328207 0.3430854 ]\n",
            "[0.04211583 0.6129396  0.3449445 ]\n",
            "[0.03520247 0.5248152  0.43998235]\n",
            "[0.02722754 0.7850066  0.18776591]\n",
            "[0.80810994 0.06108264 0.13080743]\n",
            "[0.59071684 0.07904661 0.33023655]\n",
            "[0.41373488 0.07492896 0.5113361 ]\n",
            "[0.02077708 0.7453238  0.23389912]\n",
            "[0.03696128 0.7928842  0.17015463]\n",
            "[0.951151   0.027635   0.02121392]\n",
            "[0.5361801  0.39782527 0.06599463]\n",
            "[0.5115017  0.08654277 0.4019555 ]\n",
            "[0.77440125 0.03314079 0.19245794]\n",
            "[0.77440125 0.03314079 0.19245794]\n",
            "[0.02061713 0.8216725  0.15771042]\n",
            "[0.9264017  0.03350499 0.0400934 ]\n",
            "[0.77882737 0.05588695 0.16528568]\n",
            "[0.69658744 0.0422503  0.26116228]\n",
            "[0.91937345 0.05510088 0.02552565]\n",
            "[0.7362366  0.03752894 0.22623448]\n",
            "[0.41339603 0.37996307 0.20664091]\n",
            "[0.15364958 0.31127918 0.5350712 ]\n",
            "[0.8852319  0.02545954 0.0893085 ]\n",
            "[0.7582199  0.03677876 0.20500138]\n",
            "[0.8469572  0.02470309 0.12833974]\n",
            "[0.02708554 0.37133324 0.60158116]\n",
            "[0.9235257  0.02906412 0.04741031]\n",
            "[0.9235257  0.02906412 0.04741031]\n",
            "[0.03497768 0.36754042 0.5974819 ]\n",
            "[0.17003296 0.1079867  0.7219803 ]\n",
            "[0.03945259 0.1174437  0.8431037 ]\n",
            "[0.9220516  0.02056215 0.05738613]\n",
            "[0.86275184 0.02920235 0.10804576]\n",
            "[0.7757161  0.18139412 0.04288982]\n",
            "[0.855202   0.03038537 0.11441258]\n",
            "[0.250215  0.1265307 0.6232543]\n",
            "[0.6021076  0.05739899 0.34049344]\n",
            "[0.30127287 0.08274633 0.6159808 ]\n",
            "[0.02024209 0.79478014 0.18497771]\n",
            "[0.8871476  0.02377439 0.08907797]\n",
            "[0.65891415 0.14492261 0.19616319]\n",
            "[0.8292278  0.08277156 0.08800059]\n",
            "[0.84040666 0.05196906 0.10762423]\n",
            "[0.94428843 0.03374125 0.0219703 ]\n",
            "[0.23484296 0.48002863 0.28512844]\n",
            "[0.91140586 0.02186226 0.06673187]\n",
            "[0.87523186 0.02531333 0.09945484]\n",
            "[0.87733144 0.02459944 0.09806903]\n",
            "[0.8882182  0.02549818 0.08628363]\n",
            "[0.78208286 0.04778572 0.1701314 ]\n",
            "[0.07442939 0.13455081 0.79101974]\n",
            "[0.05547101 0.8127562  0.13177274]\n",
            "[0.07094697 0.13003822 0.7990148 ]\n",
            "[0.07512109 0.06574964 0.8591293 ]\n",
            "[0.74129    0.04161126 0.21709876]\n",
            "[0.878628   0.03037036 0.09100159]\n",
            "[0.27187267 0.13464837 0.593479  ]\n",
            "[0.690809   0.06022878 0.24896221]\n",
            "[0.8945289  0.02328964 0.08218145]\n",
            "[0.9261199  0.01790202 0.05597809]\n",
            "[0.8767021  0.02299154 0.1003065 ]\n",
            "[0.8767021  0.02299154 0.1003065 ]\n",
            "[0.8880024  0.02607807 0.0859195 ]\n",
            "[0.45717642 0.07559093 0.46723264]\n",
            "[0.5153337  0.05488323 0.42978305]\n",
            "[0.9257869  0.01829745 0.05591563]\n",
            "[0.06162118 0.23536487 0.7030139 ]\n",
            "[0.87516344 0.02472758 0.10010894]\n",
            "[0.885526   0.02284124 0.09163275]\n",
            "[0.49941465 0.14670563 0.3538797 ]\n",
            "[0.82585526 0.02972481 0.14441992]\n",
            "[0.1381441  0.05271752 0.8091384 ]\n",
            "[0.06024834 0.23716402 0.7025876 ]\n",
            "[0.80985355 0.03662495 0.15352151]\n",
            "[0.90476674 0.03301726 0.06221605]\n",
            "[0.91476744 0.02285673 0.06237574]\n",
            "[0.17894346 0.10936419 0.71169233]\n",
            "[0.17451023 0.18803768 0.63745207]\n",
            "[0.76271534 0.03873312 0.19855158]\n",
            "[0.8873294  0.0219783  0.09069225]\n",
            "[0.7864142  0.03569717 0.17788865]\n",
            "[0.62146795 0.05586138 0.32267067]\n",
            "[0.8991607  0.02415149 0.07668791]\n",
            "[0.94618493 0.02214185 0.03167323]\n",
            "[0.03350437 0.58863896 0.37785664]\n",
            "[0.03350437 0.58863896 0.37785664]\n",
            "[0.9043281  0.02402399 0.07164792]\n",
            "[0.945869   0.01697627 0.03715465]\n",
            "[0.8298297  0.02987008 0.14030023]\n",
            "[0.8298297  0.02987008 0.14030023]\n",
            "[0.8675479  0.02985375 0.10259828]\n",
            "[0.890696   0.02162703 0.08767697]\n",
            "[0.890696   0.02162703 0.08767697]\n",
            "[0.7975941  0.03636636 0.16603962]\n",
            "[0.89034563 0.02799449 0.08165985]\n",
            "[0.81842595 0.02835909 0.15321492]\n",
            "[0.7692612  0.0350773  0.19566147]\n",
            "[0.73856604 0.04120142 0.2202326 ]\n",
            "[0.02558958 0.72272503 0.2516854 ]\n",
            "[0.06467577 0.48038137 0.45494276]\n",
            "[0.8152504  0.03328362 0.15146597]\n",
            "[0.37891886 0.08241504 0.5386661 ]\n",
            "[0.04269053 0.21078286 0.7465266 ]\n",
            "[0.80377674 0.04759518 0.14862807]\n",
            "[0.8894441  0.07915458 0.03140128]\n",
            "[0.8044793  0.0615983  0.13392237]\n",
            "[0.38852757 0.08960156 0.52187085]\n",
            "[0.601067   0.05519758 0.34373543]\n",
            "[0.928534   0.01951313 0.05195293]\n",
            "[0.4490717  0.10086016 0.45006812]\n",
            "[0.7776805  0.02878495 0.19353454]\n",
            "[0.43551454 0.11917132 0.4453142 ]\n",
            "[0.06808407 0.07640019 0.8555157 ]\n",
            "[0.8153973  0.02659317 0.15800951]\n",
            "[0.80236614 0.05296205 0.1446718 ]\n",
            "[0.88406575 0.0232964  0.09263787]\n",
            "[0.02212025 0.78091156 0.19696821]\n",
            "[0.73330045 0.06191216 0.20478734]\n",
            "[0.8580417  0.02617602 0.11578221]\n",
            "[0.8709876  0.02253135 0.10648105]\n",
            "[0.66558117 0.07874906 0.2556697 ]\n",
            "[0.66558117 0.07874906 0.2556697 ]\n",
            "[0.9270779  0.02399023 0.04893181]\n",
            "[0.940055   0.02157445 0.03837059]\n",
            "[0.89716685 0.02231465 0.08051848]\n",
            "[0.40526226 0.23898733 0.35575044]\n",
            "[0.09890577 0.60380346 0.2972908 ]\n",
            "[0.030052   0.66479707 0.305151  ]\n",
            "[0.8681933  0.0455538  0.08625288]\n",
            "[0.6267368  0.04921345 0.32404974]\n",
            "[0.9147562  0.02737466 0.0578692 ]\n",
            "[0.05238248 0.4934688  0.45414877]\n",
            "[0.7868449  0.04235432 0.1708008 ]\n",
            "[0.82807446 0.04486407 0.12706138]\n",
            "[0.6394671  0.06966843 0.2908644 ]\n",
            "[0.47092226 0.09336015 0.43571764]\n",
            "[0.73061126 0.06067936 0.20870939]\n",
            "[0.17855562 0.61512965 0.20631473]\n",
            "[0.08216421 0.34843057 0.56940526]\n",
            "[0.63571066 0.05227294 0.31201634]\n",
            "[0.8597558  0.02588545 0.11435882]\n",
            "[0.8597558  0.02588545 0.11435882]\n",
            "[0.8781269  0.02206279 0.0998103 ]\n",
            "[0.920031   0.02438938 0.0555796 ]\n",
            "[0.14542829 0.06408026 0.7904914 ]\n",
            "[0.90391344 0.02139354 0.074693  ]\n",
            "[0.5579694  0.06453662 0.37749398]\n",
            "[0.5579694  0.06453662 0.37749398]\n",
            "[0.928595   0.01809768 0.05330728]\n",
            "[0.93153185 0.01759554 0.05087261]\n",
            "[0.82717675 0.03856665 0.13425657]\n",
            "[0.82717675 0.03856665 0.13425657]\n",
            "[0.92908376 0.02867085 0.04224538]\n",
            "[0.7062421  0.04029568 0.25346223]\n",
            "[0.86026144 0.02749864 0.11223988]\n",
            "[0.92068434 0.02151001 0.05780565]\n",
            "[0.64952606 0.04082998 0.30964398]\n",
            "[0.60506433 0.08681361 0.30812207]\n",
            "[0.80820704 0.03818661 0.15360631]\n",
            "[0.88353807 0.0218204  0.09464159]\n",
            "[0.20957719 0.06575185 0.72467095]\n",
            "[0.18083134 0.09991725 0.7192514 ]\n",
            "[0.63385564 0.04486807 0.32127625]\n",
            "[0.9083502  0.05965731 0.03199248]\n",
            "[0.92408204 0.02019209 0.05572589]\n",
            "[0.03305038 0.81356686 0.15338284]\n",
            "[0.9017547  0.0238941  0.07435123]\n",
            "[0.92869383 0.01862929 0.05267682]\n",
            "[0.70206565 0.04075121 0.2571832 ]\n",
            "[0.04882521 0.08906658 0.86210823]\n",
            "[0.90478194 0.06783443 0.0273836 ]\n",
            "[0.04560107 0.2729914  0.6814076 ]\n",
            "[0.20474438 0.05388489 0.7413707 ]\n",
            "[0.9411878  0.02221174 0.03660041]\n",
            "[0.9199414  0.0258834  0.05417521]\n",
            "[0.14896509 0.06393479 0.78710014]\n",
            "[0.7896997  0.04805299 0.16224734]\n",
            "[0.8201912  0.03822661 0.14158219]\n",
            "[0.9208793  0.02336669 0.05575404]\n",
            "[0.02352858 0.73255247 0.2439189 ]\n",
            "[0.8787229  0.02346535 0.09781171]\n",
            "[0.47547987 0.10390712 0.42061293]\n",
            "[0.8965526  0.02418209 0.0792653 ]\n",
            "[0.7596291  0.04788623 0.19248472]\n",
            "[0.7596291  0.04788623 0.19248472]\n",
            "[0.6760396  0.04799778 0.27596265]\n",
            "[0.49238172 0.43156236 0.07605595]\n",
            "[0.70571977 0.04055368 0.25372654]\n",
            "[0.09363901 0.6637791  0.24258184]\n",
            "[0.70528764 0.04718408 0.24752831]\n",
            "[0.8820887  0.02146368 0.09644766]\n",
            "[0.01880606 0.7379501  0.24324383]\n",
            "[0.9024134  0.02551059 0.07207597]\n",
            "[0.9024134  0.02551059 0.07207597]\n",
            "[0.87130904 0.02561727 0.1030736 ]\n",
            "[0.93130577 0.02034755 0.04834672]\n",
            "[0.69090164 0.03789014 0.27120817]\n",
            "[0.6882187  0.05777847 0.25400278]\n",
            "[0.79253125 0.03500684 0.17246188]\n",
            "[0.87794197 0.03020245 0.09185563]\n",
            "[0.17213988 0.1226596  0.7052005 ]\n",
            "[0.8021217  0.03039886 0.16747949]\n",
            "[0.8678162  0.02358203 0.1086018 ]\n",
            "[0.9358673  0.01802947 0.04610329]\n",
            "[0.8796058  0.02131626 0.09907792]\n",
            "[0.05600216 0.7431143  0.2008835 ]\n",
            "[0.09920398 0.13636898 0.764427  ]\n",
            "[0.6279432  0.05128824 0.3207685 ]\n",
            "[0.87890446 0.08726438 0.0338312 ]\n",
            "[0.87890446 0.08726438 0.0338312 ]\n",
            "[0.0185383  0.8079637  0.17349802]\n",
            "[0.6004912  0.05273568 0.34677312]\n",
            "[0.7170139  0.0491388  0.23384729]\n",
            "[0.118117   0.06677109 0.8151119 ]\n",
            "[0.7456667  0.08998534 0.16434796]\n",
            "[0.6758728  0.04451955 0.27960768]\n",
            "[0.79461765 0.06599092 0.1393914 ]\n",
            "[0.29626712 0.62375885 0.07997403]\n",
            "[0.62004346 0.11978279 0.26017374]\n",
            "[0.573225   0.05252143 0.37425348]\n",
            "[0.04356625 0.6078079  0.3486259 ]\n",
            "[0.6046493  0.05484007 0.34051067]\n",
            "[0.8480117  0.0285162  0.12347207]\n",
            "[0.8760062 0.0292761 0.0947177]\n",
            "[0.89379627 0.0292961  0.07690756]\n",
            "[0.89379627 0.0292961  0.07690756]\n",
            "[0.8872092  0.02186538 0.09092551]\n",
            "[0.50882834 0.08004268 0.41112894]\n",
            "[0.93527627 0.0198591  0.04486461]\n",
            "[0.490768   0.05100667 0.45822528]\n",
            "[0.90874785 0.02138631 0.06986587]\n",
            "[0.8093767  0.0455505  0.14507277]\n",
            "[0.8547095  0.04476506 0.10052542]\n",
            "[0.9367086  0.03944308 0.02384832]\n",
            "[0.07514631 0.19720492 0.72764874]\n",
            "[0.64437383 0.06055477 0.2950714 ]\n",
            "[0.43726218 0.21193519 0.35080263]\n",
            "[0.7655451  0.06460806 0.16984677]\n",
            "[0.52405167 0.07278914 0.4031592 ]\n",
            "[0.3300856  0.18657374 0.48334065]\n",
            "[0.9120374  0.02365003 0.06431258]\n",
            "[0.08780568 0.37597278 0.53622156]\n",
            "[0.39990363 0.07032383 0.5297725 ]\n",
            "[0.425128   0.08574623 0.48912573]\n",
            "[0.78211373 0.02866177 0.18922445]\n",
            "[0.03255753 0.62725765 0.34018484]\n",
            "[0.13549681 0.15046279 0.7140404 ]\n",
            "[0.610929   0.09985632 0.28921467]\n",
            "[0.92595285 0.02414311 0.04990399]\n",
            "[0.9526461  0.02281737 0.02453659]\n",
            "[0.0292383  0.47409922 0.49666253]\n",
            "[0.8999894  0.02328943 0.07672111]\n",
            "[0.7680861  0.03537563 0.19653834]\n",
            "[0.5673892  0.05690829 0.3757025 ]\n",
            "[0.54613656 0.13691022 0.31695324]\n",
            "[0.9097035  0.0209251  0.06937137]\n",
            "[0.8593681  0.0426178  0.09801416]\n",
            "[0.6981899  0.04467124 0.25713888]\n",
            "[0.84454036 0.02800478 0.1274549 ]\n",
            "[0.77665514 0.1820375  0.04130729]\n",
            "[0.0479009  0.15628049 0.7958186 ]\n",
            "[0.4454488  0.0790177  0.47553346]\n",
            "[0.57249993 0.07456261 0.35293743]\n",
            "[0.10747916 0.11239877 0.7801221 ]\n",
            "[0.8893242  0.03159602 0.07907974]\n",
            "[0.9127859  0.02594107 0.0612731 ]\n",
            "[0.8587332  0.02317379 0.11809302]\n",
            "[0.5780204  0.05074555 0.37123406]\n",
            "[0.5780204  0.05074555 0.37123406]\n",
            "[0.674168   0.03940435 0.28642762]\n",
            "[0.89863455 0.02168187 0.0796835 ]\n",
            "[0.8235503  0.02845583 0.14799398]\n",
            "[0.02058111 0.81429857 0.16512038]\n",
            "[0.05518202 0.5344234  0.41039452]\n",
            "[0.61920786 0.05676841 0.32402378]\n",
            "[0.11031533 0.23418815 0.65549654]\n",
            "[0.04982625 0.2912738  0.65889996]\n",
            "[0.87361944 0.02714497 0.09923553]\n",
            "[0.91587436 0.01919637 0.06492919]\n",
            "[0.88556385 0.02444727 0.08998889]\n",
            "[0.9133637  0.01856889 0.06806745]\n",
            "[0.9364165  0.01611864 0.04746486]\n",
            "[0.09576537 0.5026774  0.40155727]\n",
            "[0.66262203 0.04823674 0.28914118]\n",
            "[0.92390066 0.02018519 0.05591413]\n",
            "[0.19928361 0.7042911  0.0964252 ]\n",
            "[0.19928361 0.7042911  0.0964252 ]\n",
            "[0.78553736 0.04977734 0.16468532]\n",
            "[0.73519456 0.03924737 0.22555807]\n",
            "[0.21135843 0.06970096 0.7189406 ]\n",
            "[0.799241   0.05024238 0.15051661]\n",
            "[0.03151368 0.82465404 0.14383224]\n",
            "[0.04545628 0.3087561  0.6457876 ]\n",
            "[0.03589766 0.8282545  0.13584787]\n",
            "[0.6189819  0.05054366 0.33047447]\n",
            "[0.6189819  0.05054366 0.33047447]\n",
            "[0.93082553 0.01788605 0.05128841]\n",
            "[0.9442848  0.01898182 0.0367334 ]\n",
            "[0.66921186 0.06122725 0.26956093]\n",
            "[0.27069798 0.07975639 0.64954567]\n",
            "[0.87982297 0.02481872 0.0953583 ]\n",
            "[0.8888834  0.02546343 0.08565313]\n",
            "[0.6497348  0.07095827 0.27930692]\n",
            "[0.35917297 0.06554419 0.5752828 ]\n",
            "[0.35917297 0.06554419 0.5752828 ]\n",
            "[0.8709115  0.08054944 0.04853912]\n",
            "[0.32009816 0.08529835 0.5946035 ]\n",
            "[0.06246019 0.4263764  0.5111634 ]\n",
            "[0.9174842  0.0292376  0.05327814]\n",
            "[0.8541606  0.02984919 0.11599021]\n",
            "[0.38420653 0.07767249 0.538121  ]\n",
            "[0.9539898  0.02452671 0.02148341]\n",
            "[0.90127045 0.02209307 0.07663648]\n",
            "[0.93991363 0.03749646 0.02258994]\n",
            "[0.7542692  0.04100865 0.20472218]\n",
            "[0.07770914 0.78840494 0.13388598]\n",
            "[0.5000687  0.05674625 0.44318503]\n",
            "[0.4971817  0.05375706 0.44906127]\n",
            "[0.05979272 0.08428475 0.8559225 ]\n",
            "[0.80754876 0.03705277 0.1553985 ]\n",
            "[0.95446885 0.02584336 0.01968784]\n",
            "[0.85756236 0.02914177 0.11329588]\n",
            "[0.5409695  0.07802187 0.38100863]\n",
            "[0.16677172 0.11222614 0.7210021 ]\n",
            "[0.8968601  0.01970699 0.08343288]\n",
            "[0.90420985 0.02203625 0.07375393]\n",
            "[0.8853599  0.02810397 0.08653614]\n",
            "[0.9577332  0.01748143 0.02478529]\n",
            "[0.10752699 0.44879016 0.44368285]\n",
            "[0.54566246 0.08398716 0.3703504 ]\n",
            "[0.54566246 0.08398716 0.3703504 ]\n",
            "[0.3094753  0.55582505 0.13469966]\n",
            "[0.3094753  0.55582505 0.13469966]\n",
            "[0.7112746  0.08119299 0.2075324 ]\n",
            "[0.848678   0.02388309 0.127439  ]\n",
            "[0.83219874 0.03604087 0.13176045]\n",
            "[0.8008967  0.0322188  0.16688453]\n",
            "[0.72848433 0.03738205 0.23413365]\n",
            "[0.85377574 0.02777462 0.11844963]\n",
            "[0.90084285 0.02071811 0.07843902]\n",
            "[0.02173223 0.8105935  0.1676743 ]\n",
            "[0.8894308  0.02789702 0.0826721 ]\n",
            "[0.8602716  0.10721198 0.03251651]\n",
            "[0.9345678  0.01925648 0.04617568]\n",
            "[0.05781582 0.7633381  0.17884603]\n",
            "[0.7685379  0.03923919 0.19222288]\n",
            "[0.83177143 0.03803656 0.13019207]\n",
            "[0.84169763 0.03333325 0.12496913]\n",
            "[0.40354294 0.08269797 0.51375914]\n",
            "[0.7873037  0.08480384 0.12789248]\n",
            "[0.93106234 0.02295454 0.04598309]\n",
            "[0.7786657  0.03747264 0.1838616 ]\n",
            "[0.27950552 0.10660335 0.6138911 ]\n",
            "[0.9123818  0.0224337  0.06518442]\n",
            "[0.9056168 0.0302804 0.0641028]\n",
            "[0.07117287 0.06706829 0.8617588 ]\n",
            "[0.06328251 0.37738138 0.5593361 ]\n",
            "[0.67991287 0.15309203 0.16699511]\n",
            "[0.51821077 0.1826779  0.29911128]\n",
            "[0.8780346  0.02299045 0.098975  ]\n",
            "[0.75651145 0.06364116 0.17984734]\n",
            "[0.83285445 0.12697265 0.04017284]\n",
            "[0.55236864 0.08562593 0.36200544]\n",
            "[0.16478816 0.7433385  0.0918733 ]\n",
            "[0.80149794 0.06769592 0.13080621]\n",
            "[0.77492887 0.03660304 0.18846816]\n",
            "[0.05118383 0.54692817 0.40188798]\n",
            "[0.85745496 0.03409406 0.10845103]\n",
            "[0.05419332 0.7808135  0.16499317]\n",
            "[0.52666676 0.18453147 0.28880173]\n",
            "[0.8263447  0.07662699 0.09702826]\n",
            "[0.87831616 0.02582991 0.09585394]\n",
            "[0.93845123 0.03093882 0.03060996]\n",
            "[0.86934006 0.02199527 0.10866468]\n",
            "[0.05396606 0.20665056 0.7393834 ]\n",
            "[0.95332545 0.02537994 0.02129466]\n",
            "[0.80496716 0.02696801 0.16806488]\n",
            "[0.05097464 0.10848864 0.8405368 ]\n",
            "[0.09956972 0.23598133 0.6644489 ]\n",
            "[0.502131   0.06108854 0.43678048]\n",
            "[0.40114748 0.4247086  0.17414397]\n",
            "[0.69328344 0.04627464 0.26044193]\n",
            "[0.03656049 0.7992562  0.1641833 ]\n",
            "[0.93225276 0.01802292 0.04972433]\n",
            "[0.9173309  0.02189698 0.06077201]\n",
            "[0.70736265 0.04427166 0.24836569]\n",
            "[0.8690283  0.02569921 0.1052725 ]\n",
            "[0.94464093 0.01615722 0.03920194]\n",
            "[0.91716486 0.02002881 0.06280632]\n",
            "[0.58776915 0.05375494 0.3584759 ]\n",
            "[0.2666049  0.06971164 0.6636834 ]\n",
            "[0.02756953 0.5485273  0.42390317]\n",
            "[0.48436663 0.06792464 0.44770873]\n",
            "[0.16784146 0.0592195  0.772939  ]\n",
            "[0.7092624  0.04614501 0.2445926 ]\n",
            "[0.91048044 0.02623512 0.06328449]\n",
            "[0.8386927  0.04346777 0.11783951]\n",
            "[0.81047124 0.03713765 0.15239114]\n",
            "[0.82770175 0.02873992 0.1435584 ]\n",
            "[0.82770175 0.02873992 0.1435584 ]\n",
            "[0.03181219 0.6830876  0.28510025]\n",
            "[0.8936676  0.04407056 0.06226186]\n",
            "[0.8936676  0.04407056 0.06226186]\n",
            "[0.86966306 0.02659741 0.10373955]\n",
            "[0.9390181  0.02355735 0.03742456]\n",
            "[0.87500364 0.04075376 0.08424262]\n",
            "[0.77843934 0.03513662 0.186424  ]\n",
            "[0.8589997  0.03987077 0.10112953]\n",
            "[0.8808553  0.03058901 0.08855565]\n",
            "[0.8608294  0.03135863 0.10781197]\n",
            "[0.7238309  0.05766121 0.2185079 ]\n",
            "[0.17885643 0.28719527 0.5339483 ]\n",
            "[0.7922517  0.03469654 0.17305173]\n",
            "[0.59558064 0.05237639 0.35204294]\n",
            "[0.34480655 0.19772458 0.4574689 ]\n",
            "[0.04382094 0.76632816 0.18985088]\n",
            "[0.85659647 0.06227728 0.08112633]\n",
            "[0.23739992 0.11088916 0.6517109 ]\n",
            "[0.11374361 0.6634575  0.22279887]\n",
            "[0.95887154 0.02150234 0.01962617]\n",
            "[0.02111817 0.74929494 0.22958685]\n",
            "[0.04287481 0.14166954 0.8154557 ]\n",
            "[0.7132192  0.04190324 0.24487753]\n",
            "[0.89439046 0.02106621 0.0845433 ]\n",
            "[0.78691643 0.03501432 0.17806923]\n",
            "[0.7893217  0.03308221 0.17759608]\n",
            "[0.79335415 0.03373504 0.17291076]\n",
            "[0.8270762  0.03381721 0.13910657]\n",
            "[0.8648069  0.02927482 0.10591824]\n",
            "[0.8561747  0.02978643 0.11403886]\n",
            "[0.8844779  0.02658135 0.08894078]\n",
            "[0.9328143  0.01885433 0.04833139]\n",
            "[0.9328143  0.01885433 0.04833139]\n",
            "[0.9284653  0.017494   0.05404069]\n",
            "[0.92560685 0.01791701 0.05647618]\n",
            "[0.8847838  0.02263293 0.09258328]\n",
            "[0.7653785  0.04273812 0.19188336]\n",
            "[0.8593725  0.02698088 0.11364664]\n",
            "[0.29633427 0.06161149 0.6420542 ]\n",
            "[0.6547419  0.05373306 0.2915251 ]\n",
            "[0.82450175 0.03033527 0.14516293]\n",
            "[0.81310606 0.03690773 0.14998615]\n",
            "[0.20526946 0.09987019 0.69486034]\n",
            "[0.76843077 0.03221232 0.199357  ]\n",
            "[0.64733917 0.07164607 0.2810147 ]\n",
            "[0.9042814  0.01936    0.07635858]\n",
            "[0.07369587 0.5477573  0.37854686]\n",
            "[0.7853399  0.05522145 0.15943865]\n",
            "[0.32591864 0.08437428 0.5897071 ]\n",
            "[0.8105307  0.03670176 0.15276752]\n",
            "[0.9044692  0.02720482 0.06832607]\n",
            "[0.44267738 0.22423963 0.33308297]\n",
            "[0.78390425 0.03424628 0.18184938]\n",
            "[0.8552769  0.02616363 0.11855944]\n",
            "[0.75352377 0.04249074 0.2039855 ]\n",
            "[0.6642713  0.04814652 0.28758213]\n",
            "[0.03986491 0.52519655 0.43493855]\n",
            "[0.7685227  0.03867073 0.1928066 ]\n",
            "[0.02686743 0.7527762  0.22035637]\n",
            "[0.9254633  0.01875133 0.05578533]\n",
            "[0.9000248  0.01890355 0.08107168]\n",
            "[0.26964414 0.3323499  0.39800596]\n",
            "[0.02459562 0.67834747 0.29705697]\n",
            "[0.1349057  0.15148535 0.7136089 ]\n",
            "[0.4707573  0.08160994 0.4476328 ]\n",
            "[0.8827131  0.02425799 0.09302893]\n",
            "[0.39402083 0.09441539 0.5115638 ]\n",
            "[0.89508235 0.01849382 0.08642377]\n",
            "[0.8185682  0.03527288 0.14615886]\n",
            "[0.8185682  0.03527288 0.14615886]\n",
            "[0.85636586 0.02467153 0.11896265]\n",
            "[0.5683302  0.05979398 0.3718757 ]\n",
            "[0.8012591  0.15782669 0.04091426]\n",
            "[0.04139407 0.7742397  0.18436617]\n",
            "[0.8723173  0.03261622 0.09506644]\n",
            "[0.71715    0.04554233 0.23730771]\n",
            "[0.02247154 0.8340689  0.14345957]\n",
            "[0.8779775  0.03422962 0.08779285]\n",
            "[0.8738625  0.02724257 0.09889492]\n",
            "[0.8207975  0.0401402  0.13906226]\n",
            "[0.8666278  0.025781   0.10759117]\n",
            "[0.8666278  0.025781   0.10759117]\n",
            "[0.95890695 0.0210088  0.02008421]\n",
            "[0.95439804 0.01596103 0.02964094]\n",
            "[0.11286394 0.31151545 0.57562065]\n",
            "[0.92953354 0.04370143 0.02676504]\n",
            "[0.88703334 0.0225085  0.09045815]\n",
            "[0.94531596 0.01935871 0.03532534]\n",
            "[0.5235962  0.04670622 0.42969754]\n",
            "[0.8423746  0.02819496 0.1294304 ]\n",
            "[0.8423746  0.02819496 0.1294304 ]\n",
            "[0.86050665 0.02991498 0.1095783 ]\n",
            "[0.6886307  0.05214238 0.25922695]\n",
            "[0.6026531  0.04942809 0.34791878]\n",
            "[0.9597829  0.01964322 0.0205739 ]\n",
            "[0.9474003  0.01973997 0.03285982]\n",
            "[0.5440309  0.05124038 0.40472874]\n",
            "[0.7273131  0.03045333 0.24223365]\n",
            "[0.7273131  0.03045333 0.24223365]\n",
            "[0.18489887 0.08146287 0.7336383 ]\n",
            "[0.7259897  0.04593389 0.22807638]\n",
            "[0.7353967  0.05216836 0.21243498]\n",
            "[0.9008222  0.02275524 0.07642248]\n",
            "[0.86039335 0.02845653 0.11115005]\n",
            "[0.9027637  0.0401918  0.05704439]\n",
            "[0.9027637  0.0401918  0.05704439]\n",
            "[0.8030137  0.03147185 0.16551444]\n",
            "[0.8030137  0.03147185 0.16551444]\n",
            "[0.8268349  0.03000096 0.14316419]\n",
            "[0.8495514  0.02713684 0.12331169]\n",
            "[0.9358864  0.01722351 0.04689006]\n",
            "[0.01954667 0.7518207  0.22863264]\n",
            "[0.928148   0.02364872 0.04820337]\n",
            "[0.928148   0.02364872 0.04820337]\n",
            "[0.8326903  0.03860041 0.12870933]\n",
            "[0.8326903  0.03860041 0.12870933]\n",
            "[0.04438573 0.71058434 0.2450299 ]\n",
            "[0.02496689 0.80623955 0.16879351]\n",
            "[0.94683063 0.02097907 0.03219024]\n",
            "[0.77591515 0.03277719 0.1913077 ]\n",
            "[0.81172216 0.07472943 0.11354843]\n",
            "[0.8659184  0.03778155 0.0963    ]\n",
            "[0.28228024 0.18777767 0.5299421 ]\n",
            "[0.60205454 0.17343746 0.22450802]\n",
            "[0.5166799  0.06572171 0.4175984 ]\n",
            "[0.09435678 0.6891987  0.21644457]\n",
            "[0.2685251  0.27366498 0.4578099 ]\n",
            "[0.9409119  0.03175007 0.02733792]\n",
            "[0.5123265  0.21819237 0.26948115]\n",
            "[0.6080754  0.05556129 0.3363633 ]\n",
            "[0.8372456  0.03560131 0.12715314]\n",
            "[0.02280697 0.72127914 0.25591388]\n",
            "[0.7576889  0.0312378  0.21107335]\n",
            "[0.7576889  0.0312378  0.21107335]\n",
            "[0.90293807 0.02174873 0.07531322]\n",
            "[0.08095684 0.11433131 0.8047119 ]\n",
            "[0.90695155 0.03773446 0.05531401]\n",
            "[0.9419592  0.01755725 0.04048353]\n",
            "[0.9030035  0.01993004 0.07706641]\n",
            "[0.9102756  0.02610164 0.06362273]\n",
            "[0.9078623  0.03068707 0.06145061]\n",
            "[0.8410654  0.04988865 0.10904592]\n",
            "[0.04235415 0.20613101 0.7515148 ]\n",
            "[0.94207406 0.02092377 0.03700215]\n",
            "[0.7714393  0.10552736 0.12303337]\n",
            "[0.7714393  0.10552736 0.12303337]\n",
            "[0.01851252 0.7246538  0.25683373]\n",
            "[0.02036101 0.6712936  0.30834535]\n",
            "[0.58552927 0.10328677 0.31118396]\n",
            "[0.58552927 0.10328677 0.31118396]\n",
            "[0.72976565 0.04344451 0.22678982]\n",
            "[0.6722995  0.0581507  0.26954982]\n",
            "[0.92525744 0.01864079 0.0561018 ]\n",
            "[0.80539936 0.03199511 0.16260557]\n",
            "[0.8854953  0.0226921  0.09181262]\n",
            "[0.35016903 0.07865161 0.5711794 ]\n",
            "[0.35016903 0.07865161 0.5711794 ]\n",
            "[0.7830126  0.05133576 0.16565157]\n",
            "[0.8577729  0.02491563 0.11731149]\n",
            "[0.13027613 0.11109614 0.7586277 ]\n",
            "[0.8529244  0.02395897 0.12311663]\n",
            "[0.8529244  0.02395897 0.12311663]\n",
            "[0.8954509  0.02300546 0.08154366]\n",
            "[0.34138274 0.07971898 0.57889825]\n",
            "[0.34138274 0.07971898 0.57889825]\n",
            "[0.82295537 0.04753765 0.129507  ]\n",
            "[0.9056634  0.02476249 0.06957418]\n",
            "[0.82307017 0.02885204 0.14807773]\n",
            "[0.82307017 0.02885204 0.14807773]\n",
            "[0.81497747 0.0306227  0.1543998 ]\n",
            "[0.09192285 0.12543811 0.7826391 ]\n",
            "[0.06695271 0.22421214 0.7088351 ]\n",
            "[0.9009419  0.02510651 0.07395156]\n",
            "[0.2168134  0.11275484 0.67043173]\n",
            "[0.03319136 0.48440236 0.48240626]\n",
            "[0.3274609  0.15062027 0.52191883]\n",
            "[0.8075999  0.04112781 0.1512723 ]\n",
            "[0.7453266  0.02943086 0.22524257]\n",
            "[0.90679616 0.02676829 0.06643547]\n",
            "[0.51887494 0.08170938 0.39941567]\n",
            "[0.09296769 0.20454411 0.7024882 ]\n",
            "[0.05322714 0.10617548 0.8405974 ]\n",
            "[0.7298306  0.04921353 0.22095588]\n",
            "[0.47620586 0.0667194  0.45707473]\n",
            "[0.5211484  0.05950316 0.41934848]\n",
            "[0.03588399 0.5713305  0.39278552]\n",
            "[0.9249062  0.01766249 0.0574313 ]\n",
            "[0.8629655  0.05039935 0.08663515]\n",
            "[0.9077439  0.0223471  0.06990908]\n",
            "[0.69990045 0.03436127 0.26573822]\n",
            "[0.88288397 0.02391562 0.09320045]\n",
            "[0.68875223 0.06844766 0.24280013]\n",
            "[0.9273047  0.01863975 0.05405554]\n",
            "[0.88833493 0.04996324 0.06170189]\n",
            "[0.9124199  0.02295294 0.06462718]\n",
            "[0.9124199  0.02295294 0.06462718]\n",
            "[0.7107345  0.06341445 0.22585101]\n",
            "[0.78553766 0.03921425 0.17524809]\n",
            "[0.8454453  0.03213154 0.12242318]\n",
            "[0.75237185 0.03858758 0.2090406 ]\n",
            "[0.0202201  0.7303735  0.24940641]\n",
            "[0.645543   0.05067432 0.3037827 ]\n",
            "[0.9419452  0.0207634  0.03729142]\n",
            "[0.807497   0.0282935  0.16420953]\n",
            "[0.02962806 0.81321925 0.15715267]\n",
            "[0.03731472 0.6607338  0.30195147]\n",
            "[0.27338183 0.06906715 0.6575511 ]\n",
            "[0.88826346 0.02167551 0.09006106]\n",
            "[0.44271627 0.05722308 0.5000607 ]\n",
            "[0.8436465 0.0254841 0.1308694]\n",
            "[0.4359428  0.08068202 0.48337513]\n",
            "[0.54232705 0.06672971 0.39094326]\n",
            "[0.8782531  0.02169852 0.10004842]\n",
            "[0.27751538 0.07808596 0.6443986 ]\n",
            "[0.94846517 0.01690145 0.03463342]\n",
            "[0.11660944 0.60225    0.28114063]\n",
            "[0.8777025  0.03593684 0.0863607 ]\n",
            "[0.15305491 0.05457227 0.7923728 ]\n",
            "[0.4839631  0.06225765 0.45377928]\n",
            "[0.71211684 0.2442631  0.04362007]\n",
            "[0.8770491  0.03062404 0.09232692]\n",
            "[0.53798276 0.05282057 0.40919665]\n",
            "[0.92445016 0.02677511 0.04877465]\n",
            "[0.17352076 0.39288908 0.4335902 ]\n",
            "[0.9167569  0.01817986 0.06506326]\n",
            "[0.9356817  0.03176429 0.03255411]\n",
            "[0.95935655 0.02427113 0.01637227]\n",
            "[0.7690903  0.03125548 0.19965427]\n",
            "[0.9435427  0.02488709 0.03157012]\n",
            "[0.9357186  0.01996188 0.0443195 ]\n",
            "[0.771503   0.03399293 0.19450407]\n",
            "[0.8213477  0.02847305 0.15017925]\n",
            "[0.7703758  0.04366959 0.18595466]\n",
            "[0.02870379 0.67919743 0.29209876]\n",
            "[0.86885077 0.02399064 0.10715862]\n",
            "[0.8136251  0.03438114 0.15199381]\n",
            "[0.13306972 0.11395177 0.7529785 ]\n",
            "[0.10748027 0.27891406 0.6136057 ]\n",
            "[0.84886396 0.03276118 0.11837482]\n",
            "[0.45027235 0.06841414 0.4813135 ]\n",
            "[0.45027235 0.06841414 0.4813135 ]\n",
            "[0.9435873  0.0208389  0.03557389]\n",
            "[0.8506926  0.02343977 0.12586762]\n",
            "[0.6897878  0.05125769 0.25895455]\n",
            "[0.90693074 0.02626261 0.06680657]\n",
            "[0.61546355 0.07530276 0.3092337 ]\n",
            "[0.56211495 0.05683124 0.38105384]\n",
            "[0.6369508  0.05026758 0.31278166]\n",
            "[0.553673   0.05181524 0.3945117 ]\n",
            "[0.13663031 0.05770021 0.8056695 ]\n",
            "[0.7494403  0.03220675 0.21835305]\n",
            "[0.7427963  0.03697553 0.22022821]\n",
            "[0.8412483  0.03537301 0.1233788 ]\n",
            "[0.04163638 0.8383868  0.11997684]\n",
            "[0.25147408 0.11328808 0.6352379 ]\n",
            "[0.04702079 0.8279705  0.1250087 ]\n",
            "[0.8133926  0.03126805 0.15533938]\n",
            "[0.9075488  0.01923051 0.07322069]\n",
            "[0.04376286 0.8248928  0.13134433]\n",
            "[0.06554013 0.14297265 0.7914872 ]\n",
            "[0.9257678  0.02128001 0.05295221]\n",
            "[0.05228832 0.7367622  0.21094945]\n",
            "[0.59173185 0.05333429 0.3549339 ]\n",
            "[0.6638775 0.0502276 0.2858949]\n",
            "[0.39911878 0.0687996  0.5320816 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "outputId": "8b529b3b-d3b0-4f30-a14a-5df515542447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "                 \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "outputId": "edd83191-a14a-45f0-8fd6-e834e93942a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "outputId": "c2bfe25d-b1b7-409d-898c-19a916a52fa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-task2tamil'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task2tamil\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task2tamil/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task2tamil/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task2tamil/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    }
  ]
}
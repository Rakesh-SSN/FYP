{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT TASK 2 TAMIL",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "43bbc79fd8e34f819e055fe380bc2dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6be67a8459e8455b9eb500e156d7a2c9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_360d4b234d1c4e2090f49ec43d2e0a91",
              "IPY_MODEL_e7e448b29c994bbd8360de31f7c7edaa"
            ]
          }
        },
        "6be67a8459e8455b9eb500e156d7a2c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "360d4b234d1c4e2090f49ec43d2e0a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b27c28b90f1c4b2ba447f3d032c6dc86",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d4298ffb04fc4feb930f950e3870dd62"
          }
        },
        "e7e448b29c994bbd8360de31f7c7edaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c8f56db2a0704ff38e2fdac69787dc05",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 996k/996k [00:00&lt;00:00, 10.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47ab62d9a8564d5d85e7189aa04d1f43"
          }
        },
        "b27c28b90f1c4b2ba447f3d032c6dc86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d4298ffb04fc4feb930f950e3870dd62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8f56db2a0704ff38e2fdac69787dc05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47ab62d9a8564d5d85e7189aa04d1f43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a3686e37005461eb952bd0aee3d6de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0b4eb135dcd540209dec98fb71923597",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bd91461a4fb44382a1419cf0475e2234",
              "IPY_MODEL_b581de37b8214c518dcb46b48ad1353c"
            ]
          }
        },
        "0b4eb135dcd540209dec98fb71923597": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd91461a4fb44382a1419cf0475e2234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6815733fddc3431d98400d7e54423016",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 569,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 569,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_512af8706d7a489d923d0ea114bd8aaf"
          }
        },
        "b581de37b8214c518dcb46b48ad1353c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ad4efde4d3744936b72102905c221199",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 569/569 [00:00&lt;00:00, 15.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0110a99d9314e33981045ad8f4a2cd1"
          }
        },
        "6815733fddc3431d98400d7e54423016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "512af8706d7a489d923d0ea114bd8aaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad4efde4d3744936b72102905c221199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0110a99d9314e33981045ad8f4a2cd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ffef0beb8cc8438b8a82cb27c1febbe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f0144ac119054671ad6b50c356980ff8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4cf0c8c8f6a94da4b5bcabbede87b2ce",
              "IPY_MODEL_016946483af54b259125dc2825671302"
            ]
          }
        },
        "f0144ac119054671ad6b50c356980ff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4cf0c8c8f6a94da4b5bcabbede87b2ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_34df24829aeb47289cd80a50bfefa241",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e223f43338354426b0377d3907964d84"
          }
        },
        "016946483af54b259125dc2825671302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ecde303333344b4cb502cdcf525e1516",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 714M/714M [00:14&lt;00:00, 49.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23d003f486924cbc947f54f622e1da6b"
          }
        },
        "34df24829aeb47289cd80a50bfefa241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e223f43338354426b0377d3907964d84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ecde303333344b4cb502cdcf525e1516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23d003f486924cbc947f54f622e1da6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT%20TASK%202%20TAMIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "17eb2576-d6e6-43ed-d7c7-50ecf069653e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 67.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 59.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 65.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=c9427c12e27b957ae0f73668cd8cfa80465d0ca2a84a7a273e99ef8881f1ba6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "889eacfd-66e4-4cfc-eaf4-71a8092fe0f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "cda6ce02-4b4b-4e0b-eab5-bf6b7a954db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/tamil/task2tamil.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 3,495\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2179</th>\n",
              "      <td>TAM2185</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>அன்னை பராசக்தி, தன் சக்தியை ஒன்று திரட்டி அடக்...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2131</th>\n",
              "      <td>TAM2137</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>மகளிர் காங்கிரஸ் கூட்டம், குஷ்பு புறக்கணிப்பு....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>TAM0298</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>மேற்குவங்க தேர்தலில் மோதலால் நடிகை ரூபாகங்குலி...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1818</th>\n",
              "      <td>TAM1824</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>ப.சிதம்பரத்தை மஹாராஷ்டிராவில் திணிப்பதின் மூலம...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>TAM0798</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>தமிழகத்தில் வாக்கு எண்ணிக்கைக்கு தடை விதிக்க உ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080</th>\n",
              "      <td>TAM1081</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>காங்.,கில் இருந்து வாசன் நீக்கம்.புதிய கட்சி ஆ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2185</th>\n",
              "      <td>TAM2191</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>தனக்கென்று பொருள் சேர்ப்பதில், புகழ் சேர்ப்பதி...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>874</th>\n",
              "      <td>TAM0875</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>முதலமைச்சர் ஜெயலலிதா சென்னை ஆர்.கே.நகர் தொகுதி...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2776</th>\n",
              "      <td>TAM2782</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>2017-ம் ஆண்டுக்குள் சென்னையில் அனைத்து தடங்களி...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>TAM0402</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>ராமநாதபுரம் தொகுதியில் மனிதநேய மக்கள் கட்சி தல...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "2179         TAM2185  ...  அன்னை பராசக்தி, தன் சக்தியை ஒன்று திரட்டி அடக்...\n",
              "2131         TAM2137  ...  மகளிர் காங்கிரஸ் கூட்டம், குஷ்பு புறக்கணிப்பு....\n",
              "297          TAM0298  ...  மேற்குவங்க தேர்தலில் மோதலால் நடிகை ரூபாகங்குலி...\n",
              "1818         TAM1824  ...  ப.சிதம்பரத்தை மஹாராஷ்டிராவில் திணிப்பதின் மூலம...\n",
              "797          TAM0798  ...  தமிழகத்தில் வாக்கு எண்ணிக்கைக்கு தடை விதிக்க உ...\n",
              "1080         TAM1081  ...  காங்.,கில் இருந்து வாசன் நீக்கம்.புதிய கட்சி ஆ...\n",
              "2185         TAM2191  ...  தனக்கென்று பொருள் சேர்ப்பதில், புகழ் சேர்ப்பதி...\n",
              "874          TAM0875  ...  முதலமைச்சர் ஜெயலலிதா சென்னை ஆர்.கே.நகர் தொகுதி...\n",
              "2776         TAM2782  ...  2017-ம் ஆண்டுக்குள் சென்னையில் அனைத்து தடங்களி...\n",
              "401          TAM0402  ...  ராமநாதபுரம் தொகுதியில் மனிதநேய மக்கள் கட்சி தல...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "1226695f-c6db-47e6-c84f-d656e18436a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]\n",
        "print(df.label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       1\n",
            "1       1\n",
            "2       1\n",
            "3       1\n",
            "4       1\n",
            "       ..\n",
            "3490    0\n",
            "3491    0\n",
            "3492    0\n",
            "3493    0\n",
            "3494    0\n",
            "Name: label, Length: 3495, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "70a52e57-268a-45c8-cca0-ecae7ce39d23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "43bbc79fd8e34f819e055fe380bc2dfe",
            "6be67a8459e8455b9eb500e156d7a2c9",
            "360d4b234d1c4e2090f49ec43d2e0a91",
            "e7e448b29c994bbd8360de31f7c7edaa",
            "b27c28b90f1c4b2ba447f3d032c6dc86",
            "d4298ffb04fc4feb930f950e3870dd62",
            "c8f56db2a0704ff38e2fdac69787dc05",
            "47ab62d9a8564d5d85e7189aa04d1f43"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43bbc79fd8e34f819e055fe380bc2dfe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=995526, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "bb02983b-f00a-4bdd-c6da-36503df2e2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Tokenized:  ['ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'பி', '##ர', '##ச', '##ார', '##ம', 'ச', '##ெ', '##ய', '##தா', '##ர', '.', '<', 'eo', '##l', '>', 'தி', '.', 'மு', '.', 'க', '.', ',', 'வ', '##ே', '##ட', '##பா', '##ள', '##ர', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'சி', '##ன', '##ன', 'ச', '##ே', '##ல', '##ம', 'பகுதி', '##ய', '##ில', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'ஓ', '##ட', '##டு', 'ச', '##ே', '##க', '##ரி', '##த', '##தா', '##ர', '.']\n",
            "Token IDs:  [1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "79c3af9b-9f59-44a1-9325-8c2e7c1a8b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Token IDs: [101, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "0e437003-b2a8-41fc-80cc-076a1273e4bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "2eceefd8-842c-4ffb-c6cd-498240a52afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "f52a0baa-bf33-41b8-d7be-eb9c0bad0762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 20% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "# print(train_inputs)\n",
        "# print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "\n",
        "print(labels)\n",
        "#print(train_masks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "303a36c0-6667-42c4-b69c-a58e748c78f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9a3686e37005461eb952bd0aee3d6de6",
            "0b4eb135dcd540209dec98fb71923597",
            "bd91461a4fb44382a1419cf0475e2234",
            "b581de37b8214c518dcb46b48ad1353c",
            "6815733fddc3431d98400d7e54423016",
            "512af8706d7a489d923d0ea114bd8aaf",
            "ad4efde4d3744936b72102905c221199",
            "a0110a99d9314e33981045ad8f4a2cd1",
            "ffef0beb8cc8438b8a82cb27c1febbe5",
            "f0144ac119054671ad6b50c356980ff8",
            "4cf0c8c8f6a94da4b5bcabbede87b2ce",
            "016946483af54b259125dc2825671302",
            "34df24829aeb47289cd80a50bfefa241",
            "e223f43338354426b0377d3907964d84",
            "ecde303333344b4cb502cdcf525e1516",
            "23d003f486924cbc947f54f622e1da6b"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a3686e37005461eb952bd0aee3d6de6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=569, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffef0beb8cc8438b8a82cb27c1febbe5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=714314041, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "outputId": "719f746b-20cf-40d0-c199-5726b612a23a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (3, 768)\n",
            "classifier.bias                                                 (3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "outputId": "5ee5725a-133f-4f58-9e4c-ac66f4d5fd66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     88.    Elapsed: 0:00:16.\n",
            "  Batch    80  of     88.    Elapsed: 0:00:31.\n",
            "\n",
            "  Average training loss: 0.95\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     88.    Elapsed: 0:00:16.\n",
            "  Batch    80  of     88.    Elapsed: 0:00:33.\n",
            "\n",
            "  Average training loss: 0.81\n",
            "  Training epcoh took: 0:00:36\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     88.    Elapsed: 0:00:17.\n",
            "  Batch    80  of     88.    Elapsed: 0:00:34.\n",
            "\n",
            "  Average training loss: 0.74\n",
            "  Training epcoh took: 0:00:37\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     88.    Elapsed: 0:00:17.\n",
            "  Batch    80  of     88.    Elapsed: 0:00:34.\n",
            "\n",
            "  Average training loss: 0.67\n",
            "  Training epcoh took: 0:00:38\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "outputId": "60299d12-9424-44b4-9a37-7e989ccf88c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task2tamil-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "#print(len(labels))\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "#print(len(prediction_labels))\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 1,400\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "outputId": "254ceb2a-54b6-4eb6-f4f2-29729f8a1b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  #print(outputs)\n",
        "  # print(logits)\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,400 test sentences...\n",
            "    DONE.\n",
            "*************************\n",
            "44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/task2tamil-results.txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "outputId": "386b6524-ce93-42d7-d890-bfba0c0a36d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] and predictions[i][j][0]>predictions[i][j][2]):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j][0],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] and predictions[i][j][1]>predictions[i][j][2]):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j][1],\"\\t1\\t\",end=\"\")\n",
        "      x=2\n",
        "    elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "      #print(\" Semi Paraphrase     \",end=\"\")\n",
        "      #count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "      x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)\n",
        "  \n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(0)\t 1\t 1.4855146 \t0\ttrue\n",
            "(2)\t 2\t 0.83573675 \t2\tfalse\n",
            "(2)\t 3\t 1.3688653 \t0\tfalse\n",
            "(0)\t 4\t 1.0711408 \t0\ttrue\n",
            "(2)\t 5\t 0.9218596 \t2\tfalse\n",
            "(0)\t 6\t 0.5061002 \t2\tfalse\n",
            "(2)\t 7\t 0.98334426 \t0\tfalse\n",
            "(2)\t 8\t 1.1188661 \t0\tfalse\n",
            "(1)\t 9\t 1.5447568 \t0\tfalse\n",
            "(2)\t 10\t 1.894133 \t0\tfalse\n",
            "(1)\t 11\t 1.2951614 \t2\ttrue\n",
            "(0)\t 12\t 1.5055842 \t0\ttrue\n",
            "(0)\t 13\t 2.0048654 \t0\ttrue\n",
            "(0)\t 14\t 1.1153752 \t0\ttrue\n",
            "(2)\t 15\t 1.0670508 \t2\tfalse\n",
            "(0)\t 16\t 1.9035739 \t0\ttrue\n",
            "(0)\t 17\t 0.46005142 \t2\tfalse\n",
            "(0)\t 18\t 0.44973442 \t0\ttrue\n",
            "(0)\t 19\t 0.17902708 \t1\tfalse\n",
            "(2)\t 20\t 1.064911 \t0\tfalse\n",
            "(0)\t 21\t 1.7740957 \t0\ttrue\n",
            "(0)\t 22\t 0.7862855 \t0\ttrue\n",
            "(2)\t 23\t 1.190407 \t0\tfalse\n",
            "(2)\t 24\t 0.94443077 \t0\tfalse\n",
            "(1)\t 25\t 1.4665132 \t0\tfalse\n",
            "(2)\t 26\t 0.85383904 \t2\tfalse\n",
            "(1)\t 27\t 1.9157931 \t0\tfalse\n",
            "(2)\t 28\t 0.78901625 \t0\tfalse\n",
            "(2)\t 29\t 1.0188257 \t0\tfalse\n",
            "(2)\t 30\t 1.3512884 \t0\tfalse\n",
            "(0)\t 31\t 1.7676768 \t0\ttrue\n",
            "(2)\t 32\t 1.0887352 \t0\tfalse\n",
            "(2)\t 33\t 0.92503124 \t0\tfalse\n",
            "(0)\t 34\t 2.268354 \t0\ttrue\n",
            "(1)\t 35\t 1.026834 \t0\tfalse\n",
            "(1)\t 36\t 1.4764475 \t1\tfalse\n",
            "(0)\t 37\t 1.367438 \t0\ttrue\n",
            "(0)\t 38\t 1.1342564 \t0\ttrue\n",
            "(0)\t 39\t 0.4072734 \t2\tfalse\n",
            "(1)\t 40\t 2.157684 \t0\tfalse\n",
            "(1)\t 41\t 2.157684 \t0\tfalse\n",
            "(2)\t 42\t 0.49215323 \t1\ttrue\n",
            "(2)\t 43\t 1.4634411 \t2\tfalse\n",
            "(2)\t 44\t 1.6405227 \t1\ttrue\n",
            "(2)\t 45\t 1.091563 \t0\tfalse\n",
            "(1)\t 46\t 1.4281034 \t1\tfalse\n",
            "(2)\t 47\t 0.996763 \t2\tfalse\n",
            "(1)\t 48\t 1.4581606 \t2\ttrue\n",
            "(2)\t 49\t 1.3288779 \t0\tfalse\n",
            "(1)\t 50\t 1.1268398 \t2\ttrue\n",
            "(1)\t 51\t 1.7593474 \t0\tfalse\n",
            "(1)\t 52\t 1.7593474 \t0\tfalse\n",
            "(1)\t 53\t 1.4526366 \t2\ttrue\n",
            "(2)\t 54\t 2.2687047 \t0\tfalse\n",
            "(0)\t 55\t 1.4887599 \t0\ttrue\n",
            "(1)\t 56\t 2.2466352 \t0\tfalse\n",
            "(0)\t 57\t 2.050208 \t0\ttrue\n",
            "(2)\t 58\t 2.0978894 \t0\tfalse\n",
            "(0)\t 59\t 2.3365414 \t0\ttrue\n",
            "(1)\t 60\t 1.1085105 \t1\tfalse\n",
            "(1)\t 61\t 1.8051208 \t1\tfalse\n",
            "(1)\t 62\t 1.2173579 \t0\tfalse\n",
            "(1)\t 63\t 0.9974819 \t0\tfalse\n",
            "(1)\t 64\t 0.9974819 \t0\tfalse\n",
            "(0)\t 65\t 0.8974255 \t2\tfalse\n",
            "(0)\t 66\t 1.0270197 \t0\ttrue\n",
            "(2)\t 67\t 1.2563746 \t0\tfalse\n",
            "(2)\t 68\t 1.1710817 \t2\tfalse\n",
            "(2)\t 69\t 1.4589331 \t1\ttrue\n",
            "(2)\t 70\t 1.2472723 \t0\tfalse\n",
            "(0)\t 71\t 2.5887172 \t0\ttrue\n",
            "(0)\t 72\t 2.0710645 \t0\ttrue\n",
            "(2)\t 73\t 1.5198202 \t2\tfalse\n",
            "(0)\t 74\t 1.5918016 \t0\ttrue\n",
            "(2)\t 75\t 2.2927682 \t0\tfalse\n",
            "(2)\t 76\t 0.97559476 \t0\tfalse\n",
            "(1)\t 77\t 0.807122 \t0\tfalse\n",
            "(2)\t 78\t 0.9186037 \t0\tfalse\n",
            "(0)\t 79\t 1.918121 \t0\ttrue\n",
            "(2)\t 80\t 2.2931309 \t0\tfalse\n",
            "(0)\t 81\t 0.88967687 \t0\ttrue\n",
            "(1)\t 82\t 1.0596457 \t2\ttrue\n",
            "(1)\t 83\t 1.0596457 \t2\ttrue\n",
            "(2)\t 84\t 0.8335405 \t1\ttrue\n",
            "(0)\t 85\t 1.1684529 \t0\ttrue\n",
            "(2)\t 86\t 1.077525 \t0\tfalse\n",
            "(1)\t 87\t 0.9100852 \t2\ttrue\n",
            "(1)\t 88\t 0.9100852 \t2\ttrue\n",
            "(1)\t 89\t 1.349266 \t0\tfalse\n",
            "(1)\t 90\t 1.349266 \t0\tfalse\n",
            "(0)\t 91\t 1.0436549 \t0\ttrue\n",
            "(2)\t 92\t 1.4397386 \t0\tfalse\n",
            "(1)\t 93\t 2.146296 \t0\tfalse\n",
            "(1)\t 94\t 2.146296 \t0\tfalse\n",
            "(0)\t 95\t 2.3031623 \t0\ttrue\n",
            "(1)\t 96\t 1.0942415 \t2\ttrue\n",
            "(2)\t 97\t 1.2811338 \t0\tfalse\n",
            "(0)\t 98\t 1.0318018 \t0\ttrue\n",
            "(0)\t 99\t 1.682145 \t0\ttrue\n",
            "(0)\t 100\t 0.94839776 \t0\ttrue\n",
            "(1)\t 101\t 1.1390011 \t1\tfalse\n",
            "(1)\t 102\t 1.1100616 \t0\tfalse\n",
            "(1)\t 103\t 1.1100616 \t0\tfalse\n",
            "(2)\t 104\t 1.2364845 \t1\ttrue\n",
            "(2)\t 105\t 1.0467782 \t0\tfalse\n",
            "(2)\t 106\t 1.0834874 \t0\tfalse\n",
            "(1)\t 107\t 1.3896542 \t1\tfalse\n",
            "(2)\t 108\t 1.3491489 \t0\tfalse\n",
            "(0)\t 109\t 2.2751544 \t0\ttrue\n",
            "(0)\t 110\t 2.2753382 \t0\ttrue\n",
            "(1)\t 111\t 1.5845771 \t1\tfalse\n",
            "(1)\t 112\t 0.8586013 \t0\tfalse\n",
            "(0)\t 113\t 2.5524478 \t0\ttrue\n",
            "(2)\t 114\t 0.4235818 \t2\tfalse\n",
            "(2)\t 115\t 1.0561343 \t0\tfalse\n",
            "(2)\t 116\t 1.1120099 \t0\tfalse\n",
            "(1)\t 117\t 1.3199792 \t0\tfalse\n",
            "(1)\t 118\t 1.0646644 \t2\ttrue\n",
            "(1)\t 119\t 1.9454923 \t0\tfalse\n",
            "(1)\t 120\t 1.9454923 \t0\tfalse\n",
            "(1)\t 121\t 0.8865495 \t2\ttrue\n",
            "(2)\t 122\t 1.0086186 \t0\tfalse\n",
            "(2)\t 123\t 1.0766039 \t0\tfalse\n",
            "(0)\t 124\t 1.3114686 \t2\tfalse\n",
            "(1)\t 125\t 1.5220438 \t0\tfalse\n",
            "(1)\t 126\t 1.5220438 \t0\tfalse\n",
            "(0)\t 127\t 0.7858612 \t2\tfalse\n",
            "(0)\t 128\t 2.3767855 \t0\ttrue\n",
            "(2)\t 129\t 0.9516021 \t1\ttrue\n",
            "(0)\t 130\t 1.2789149 \t0\ttrue\n",
            "(0)\t 131\t 1.344605 \t0\ttrue\n",
            "(0)\t 132\t 1.2512918 \t0\ttrue\n",
            "(2)\t 133\t 1.0680362 \t0\tfalse\n",
            "(1)\t 134\t 1.9291695 \t1\tfalse\n",
            "(2)\t 135\t 1.2473708 \t2\tfalse\n",
            "(1)\t 136\t 1.5712677 \t1\tfalse\n",
            "(2)\t 137\t 1.625696 \t0\tfalse\n",
            "(0)\t 138\t 0.8319689 \t0\ttrue\n",
            "(0)\t 139\t 2.1411922 \t0\ttrue\n",
            "(0)\t 140\t 1.1717218 \t0\ttrue\n",
            "(0)\t 141\t 2.4807475 \t0\ttrue\n",
            "(2)\t 142\t 1.0662287 \t0\tfalse\n",
            "(1)\t 143\t 1.2374777 \t2\ttrue\n",
            "(2)\t 144\t 1.4304937 \t0\tfalse\n",
            "(1)\t 145\t 1.1199825 \t0\tfalse\n",
            "(0)\t 146\t 0.7638291 \t0\ttrue\n",
            "(2)\t 147\t 1.4254936 \t0\tfalse\n",
            "(2)\t 148\t 2.1525683 \t0\tfalse\n",
            "(0)\t 149\t 0.88973725 \t0\ttrue\n",
            "(2)\t 150\t 2.4481192 \t0\tfalse\n",
            "(0)\t 151\t 2.2664218 \t0\ttrue\n",
            "(0)\t 152\t 1.5178701 \t0\ttrue\n",
            "(1)\t 153\t 1.681462 \t1\tfalse\n",
            "(0)\t 154\t 2.0553577 \t0\ttrue\n",
            "(0)\t 155\t 1.2734231 \t0\ttrue\n",
            "(2)\t 156\t 1.7578806 \t0\tfalse\n",
            "(0)\t 157\t 1.3015769 \t0\ttrue\n",
            "(0)\t 158\t 2.548028 \t0\ttrue\n",
            "(2)\t 159\t 1.1580096 \t0\tfalse\n",
            "(0)\t 160\t 0.9089866 \t2\tfalse\n",
            "(0)\t 161\t 1.3729795 \t0\ttrue\n",
            "(2)\t 162\t 1.4695314 \t0\tfalse\n",
            "(1)\t 163\t 1.7115993 \t1\tfalse\n",
            "(0)\t 164\t 2.2110426 \t0\ttrue\n",
            "(2)\t 165\t 0.8107503 \t2\tfalse\n",
            "(0)\t 166\t 1.1265519 \t0\ttrue\n",
            "(0)\t 167\t 1.648962 \t0\ttrue\n",
            "(0)\t 168\t 1.4099137 \t0\ttrue\n",
            "(2)\t 169\t 2.3702202 \t0\tfalse\n",
            "(0)\t 170\t 1.2255154 \t0\ttrue\n",
            "(1)\t 171\t 0.6249066 \t2\ttrue\n",
            "(1)\t 172\t 0.6249066 \t2\ttrue\n",
            "(2)\t 173\t 1.0966389 \t0\tfalse\n",
            "(0)\t 174\t 2.301619 \t0\ttrue\n",
            "(0)\t 175\t 1.5607393 \t0\ttrue\n",
            "(0)\t 176\t 2.3093092 \t0\ttrue\n",
            "(2)\t 177\t 1.3540355 \t0\tfalse\n",
            "(2)\t 178\t 1.3395008 \t0\tfalse\n",
            "(2)\t 179\t 2.0897799 \t0\tfalse\n",
            "(0)\t 180\t 1.0114099 \t0\ttrue\n",
            "(0)\t 181\t 1.8247117 \t0\ttrue\n",
            "(1)\t 182\t 1.5647095 \t0\tfalse\n",
            "(1)\t 183\t 1.5647095 \t0\tfalse\n",
            "(0)\t 184\t 1.6050439 \t0\ttrue\n",
            "(0)\t 185\t 2.438133 \t0\ttrue\n",
            "(2)\t 186\t 1.0699843 \t0\tfalse\n",
            "(0)\t 187\t 0.8661466 \t2\tfalse\n",
            "(0)\t 188\t 2.0330396 \t0\ttrue\n",
            "(0)\t 189\t 1.4947181 \t0\ttrue\n",
            "(2)\t 190\t 0.9162535 \t0\tfalse\n",
            "(1)\t 191\t 0.9476429 \t2\ttrue\n",
            "(1)\t 192\t 1.2569817 \t0\tfalse\n",
            "(1)\t 193\t 1.2569817 \t0\tfalse\n",
            "(2)\t 194\t 1.2249478 \t0\tfalse\n",
            "(1)\t 195\t 1.2010201 \t0\tfalse\n",
            "(1)\t 196\t 1.2010201 \t0\tfalse\n",
            "(1)\t 197\t 2.0966842 \t0\tfalse\n",
            "(2)\t 198\t 1.5110114 \t0\tfalse\n",
            "(2)\t 199\t 1.4255712 \t0\tfalse\n",
            "(2)\t 200\t 0.8511645 \t0\tfalse\n",
            "(2)\t 201\t 0.96441126 \t2\tfalse\n",
            "(0)\t 202\t 0.98708117 \t0\ttrue\n",
            "(0)\t 203\t 2.1660872 \t0\ttrue\n",
            "(0)\t 204\t 1.2295203 \t0\ttrue\n",
            "(2)\t 205\t 1.0102953 \t0\tfalse\n",
            "(2)\t 206\t 1.3214709 \t1\ttrue\n",
            "(2)\t 207\t 1.1625531 \t0\tfalse\n",
            "(2)\t 208\t 0.8326609 \t2\tfalse\n",
            "(2)\t 209\t 0.879272 \t0\tfalse\n",
            "(2)\t 210\t 1.3580772 \t0\tfalse\n",
            "(2)\t 211\t 2.3967657 \t0\tfalse\n",
            "(0)\t 212\t 2.5558279 \t0\ttrue\n",
            "(0)\t 213\t 1.8983797 \t0\ttrue\n",
            "(0)\t 214\t 1.6422781 \t0\ttrue\n",
            "(0)\t 215\t 2.2647862 \t0\ttrue\n",
            "(0)\t 216\t 1.1148973 \t0\ttrue\n",
            "(0)\t 217\t 1.2652085 \t0\ttrue\n",
            "(1)\t 218\t 1.4940567 \t2\ttrue\n",
            "(2)\t 219\t 0.94488895 \t0\tfalse\n",
            "(2)\t 220\t 0.4710894 \t0\tfalse\n",
            "(0)\t 221\t 2.1176026 \t0\ttrue\n",
            "(2)\t 222\t 0.8978882 \t0\tfalse\n",
            "(0)\t 223\t 2.2604325 \t0\ttrue\n",
            "(1)\t 224\t 1.4138342 \t1\tfalse\n",
            "(0)\t 225\t 1.7685064 \t0\ttrue\n",
            "(0)\t 226\t 1.3445959 \t0\ttrue\n",
            "(2)\t 227\t 0.76034707 \t2\tfalse\n",
            "(0)\t 228\t 1.4742217 \t0\ttrue\n",
            "(2)\t 229\t 0.9087593 \t2\tfalse\n",
            "(0)\t 230\t 0.9287406 \t2\tfalse\n",
            "(2)\t 231\t 1.3458369 \t0\tfalse\n",
            "(0)\t 232\t 0.925647 \t0\ttrue\n",
            "(2)\t 233\t 1.3406487 \t2\tfalse\n",
            "(2)\t 234\t 2.4191017 \t0\tfalse\n",
            "(0)\t 235\t 1.5627674 \t0\ttrue\n",
            "(0)\t 236\t 0.25028268 \t2\tfalse\n",
            "(0)\t 237\t 1.5273913 \t0\ttrue\n",
            "(1)\t 238\t 1.3055243 \t0\tfalse\n",
            "(2)\t 239\t 1.442323 \t0\tfalse\n",
            "(0)\t 240\t 1.4270461 \t0\ttrue\n",
            "(0)\t 241\t 1.4600927 \t0\ttrue\n",
            "(2)\t 242\t 0.9420787 \t0\tfalse\n",
            "(0)\t 243\t 1.7761538 \t0\ttrue\n",
            "(0)\t 244\t 2.3492446 \t0\ttrue\n",
            "(2)\t 245\t 1.5800318 \t0\tfalse\n",
            "(2)\t 246\t 1.3977351 \t0\tfalse\n",
            "(0)\t 247\t 1.1587521 \t0\ttrue\n",
            "(0)\t 248\t 1.0313405 \t0\ttrue\n",
            "(2)\t 249\t 1.3383105 \t2\tfalse\n",
            "(0)\t 250\t 1.000822 \t1\tfalse\n",
            "(1)\t 251\t 2.3939254 \t0\tfalse\n",
            "(1)\t 252\t 2.3939254 \t0\tfalse\n",
            "(0)\t 253\t 1.0780193 \t0\ttrue\n",
            "(0)\t 254\t 1.0871271 \t2\tfalse\n",
            "(1)\t 255\t 2.4115772 \t0\tfalse\n",
            "(1)\t 256\t 2.4115772 \t0\tfalse\n",
            "(0)\t 257\t 0.93898684 \t0\ttrue\n",
            "(2)\t 258\t 1.4471625 \t2\tfalse\n",
            "(1)\t 259\t 1.5670508 \t2\ttrue\n",
            "(1)\t 260\t 1.4199798 \t1\tfalse\n",
            "(1)\t 261\t 0.7484319 \t2\ttrue\n",
            "(0)\t 262\t 0.9459644 \t0\ttrue\n",
            "(1)\t 263\t 1.0530908 \t0\tfalse\n",
            "(1)\t 264\t 1.1335348 \t2\ttrue\n",
            "(1)\t 265\t 0.8612626 \t2\ttrue\n",
            "(0)\t 266\t 2.379577 \t0\ttrue\n",
            "(0)\t 267\t 0.97370255 \t0\ttrue\n",
            "(1)\t 268\t 1.4354252 \t1\tfalse\n",
            "(0)\t 269\t 0.78901625 \t0\ttrue\n",
            "(1)\t 270\t 1.3432052 \t1\tfalse\n",
            "(2)\t 271\t 1.1985987 \t1\ttrue\n",
            "(0)\t 272\t 2.066463 \t0\ttrue\n",
            "(1)\t 273\t 0.82963747 \t2\ttrue\n",
            "(2)\t 274\t 1.0143579 \t0\tfalse\n",
            "(0)\t 275\t 1.3112111 \t0\ttrue\n",
            "(0)\t 276\t 1.3001955 \t0\ttrue\n",
            "(0)\t 277\t 1.2096123 \t0\ttrue\n",
            "(2)\t 278\t 1.8471544 \t0\tfalse\n",
            "(1)\t 279\t 1.0892103 \t0\tfalse\n",
            "(1)\t 280\t 1.0291705 \t1\tfalse\n",
            "(1)\t 281\t 1.1183712 \t1\tfalse\n",
            "(0)\t 282\t 1.346643 \t0\ttrue\n",
            "(2)\t 283\t 2.1078045 \t0\tfalse\n",
            "(2)\t 284\t 1.2862248 \t0\tfalse\n",
            "(2)\t 285\t 1.3359717 \t0\tfalse\n",
            "(1)\t 286\t 1.4955124 \t2\ttrue\n",
            "(1)\t 287\t 0.90374476 \t1\tfalse\n",
            "(2)\t 288\t 1.3720111 \t2\tfalse\n",
            "(0)\t 289\t 1.7833267 \t0\ttrue\n",
            "(2)\t 290\t 0.74199903 \t1\ttrue\n",
            "(1)\t 291\t 1.5935018 \t1\tfalse\n",
            "(2)\t 292\t 1.2519193 \t0\tfalse\n",
            "(2)\t 293\t 0.88790214 \t2\tfalse\n",
            "(1)\t 294\t 2.0077126 \t0\tfalse\n",
            "(1)\t 295\t 1.4587119 \t0\tfalse\n",
            "(0)\t 296\t 1.2719675 \t0\ttrue\n",
            "(0)\t 297\t 2.0652187 \t0\ttrue\n",
            "(0)\t 298\t 1.1035235 \t0\ttrue\n",
            "(1)\t 299\t 0.88431084 \t0\tfalse\n",
            "(1)\t 300\t 0.88431084 \t0\tfalse\n",
            "(1)\t 301\t 0.9769768 \t0\tfalse\n",
            "(2)\t 302\t 1.1096898 \t0\tfalse\n",
            "(2)\t 303\t 1.3206298 \t0\tfalse\n",
            "(1)\t 304\t 1.2032945 \t0\tfalse\n",
            "(1)\t 305\t 1.2032945 \t0\tfalse\n",
            "(0)\t 306\t 2.0295808 \t0\ttrue\n",
            "(2)\t 307\t 0.979296 \t1\ttrue\n",
            "(2)\t 308\t 1.1068817 \t0\tfalse\n",
            "(0)\t 309\t 2.171536 \t0\ttrue\n",
            "(2)\t 310\t 1.2217355 \t2\tfalse\n",
            "(2)\t 311\t 0.66876674 \t0\tfalse\n",
            "(1)\t 312\t 1.2940202 \t2\ttrue\n",
            "(2)\t 313\t 2.012427 \t0\tfalse\n",
            "(2)\t 314\t 2.5028617 \t0\tfalse\n",
            "(0)\t 315\t 1.4179417 \t0\ttrue\n",
            "(2)\t 316\t 1.9740611 \t0\tfalse\n",
            "(0)\t 317\t 1.6329379 \t0\ttrue\n",
            "(2)\t 318\t 0.93708926 \t0\tfalse\n",
            "(2)\t 319\t 2.001256 \t1\ttrue\n",
            "(2)\t 320\t 1.5517814 \t0\tfalse\n",
            "(0)\t 321\t 1.5477891 \t0\ttrue\n",
            "(0)\t 322\t 2.1381018 \t0\ttrue\n",
            "(0)\t 323\t 2.147844 \t0\ttrue\n",
            "(2)\t 324\t 0.965873 \t0\tfalse\n",
            "(0)\t 325\t 2.1767156 \t0\ttrue\n",
            "(1)\t 326\t 1.5054866 \t2\ttrue\n",
            "(1)\t 327\t 1.7878048 \t0\tfalse\n",
            "(1)\t 328\t 1.7878048 \t0\tfalse\n",
            "(2)\t 329\t 1.1131008 \t2\tfalse\n",
            "(2)\t 330\t 1.1369281 \t0\tfalse\n",
            "(0)\t 331\t 1.1329123 \t0\ttrue\n",
            "(2)\t 332\t 1.3146424 \t0\tfalse\n",
            "(1)\t 333\t 1.3324524 \t2\ttrue\n",
            "(0)\t 334\t 1.2467811 \t0\ttrue\n",
            "(2)\t 335\t 1.0423025 \t1\ttrue\n",
            "(1)\t 336\t 0.33988124 \t2\ttrue\n",
            "(1)\t 337\t 0.33988124 \t2\ttrue\n",
            "(2)\t 338\t 2.2861896 \t0\tfalse\n",
            "(1)\t 339\t 0.96563673 \t2\ttrue\n",
            "(2)\t 340\t 2.0869415 \t0\tfalse\n",
            "(1)\t 341\t 1.3325411 \t0\tfalse\n",
            "(1)\t 342\t 1.3325411 \t0\tfalse\n",
            "(2)\t 343\t 2.0807447 \t0\tfalse\n",
            "(0)\t 344\t 1.1334943 \t0\ttrue\n",
            "(2)\t 345\t 1.0527847 \t0\tfalse\n",
            "(0)\t 346\t 1.8815415 \t0\ttrue\n",
            "(0)\t 347\t 2.2604687 \t0\ttrue\n",
            "(2)\t 348\t 1.3016677 \t1\ttrue\n",
            "(2)\t 349\t 0.9256542 \t2\tfalse\n",
            "(2)\t 350\t 1.0088428 \t2\tfalse\n",
            "(0)\t 351\t 0.67811793 \t2\tfalse\n",
            "(1)\t 352\t 1.5642792 \t2\ttrue\n",
            "(2)\t 353\t 0.9089143 \t2\tfalse\n",
            "(0)\t 354\t 1.6067748 \t0\ttrue\n",
            "(0)\t 355\t 2.1807783 \t0\ttrue\n",
            "(2)\t 356\t 0.7520087 \t2\tfalse\n",
            "(1)\t 357\t 1.6476811 \t2\ttrue\n",
            "(1)\t 358\t 1.3610289 \t1\tfalse\n",
            "(1)\t 359\t 1.578103 \t2\ttrue\n",
            "(1)\t 360\t 1.1412007 \t0\tfalse\n",
            "(1)\t 361\t 1.1412007 \t0\tfalse\n",
            "(0)\t 362\t 0.6288376 \t2\tfalse\n",
            "(0)\t 363\t 0.97857237 \t0\ttrue\n",
            "(0)\t 364\t 0.79833245 \t0\ttrue\n",
            "(1)\t 365\t 0.9562344 \t2\ttrue\n",
            "(1)\t 366\t 1.4164333 \t1\tfalse\n",
            "(0)\t 367\t 1.3575512 \t0\ttrue\n",
            "(1)\t 368\t 0.9529275 \t2\ttrue\n",
            "(1)\t 369\t 1.3786924 \t1\tfalse\n",
            "(2)\t 370\t 1.3674456 \t0\tfalse\n",
            "(2)\t 371\t 1.5082223 \t0\tfalse\n",
            "(1)\t 372\t 0.67176354 \t0\tfalse\n",
            "(2)\t 373\t 1.1047698 \t0\tfalse\n",
            "(2)\t 374\t 0.8190825 \t2\tfalse\n",
            "(2)\t 375\t 0.8674277 \t2\tfalse\n",
            "(2)\t 376\t 1.038734 \t1\ttrue\n",
            "(1)\t 377\t 1.707939 \t1\tfalse\n",
            "(0)\t 378\t 1.2599661 \t0\ttrue\n",
            "(0)\t 379\t 1.9574727 \t0\ttrue\n",
            "(0)\t 380\t 1.0052998 \t0\ttrue\n",
            "(2)\t 381\t 1.1762259 \t2\tfalse\n",
            "(0)\t 382\t 2.1659706 \t0\ttrue\n",
            "(0)\t 383\t 1.5550479 \t2\tfalse\n",
            "(2)\t 384\t 0.44253796 \t2\tfalse\n",
            "(0)\t 385\t 0.9961248 \t2\tfalse\n",
            "(0)\t 386\t 0.9398095 \t0\ttrue\n",
            "(2)\t 387\t 1.0845034 \t0\tfalse\n",
            "(0)\t 388\t 2.4331415 \t0\ttrue\n",
            "(0)\t 389\t 2.5768979 \t0\ttrue\n",
            "(0)\t 390\t 2.218957 \t0\ttrue\n",
            "(0)\t 391\t 1.6855676 \t0\ttrue\n",
            "(0)\t 392\t 1.0040997 \t0\ttrue\n",
            "(0)\t 393\t 1.568087 \t0\ttrue\n",
            "(2)\t 394\t 1.0578505 \t2\tfalse\n",
            "(0)\t 395\t 1.3336496 \t0\ttrue\n",
            "(0)\t 396\t 1.1122894 \t0\ttrue\n",
            "(0)\t 397\t 1.6345413 \t0\ttrue\n",
            "(0)\t 398\t 1.0730181 \t0\ttrue\n",
            "(0)\t 399\t 1.4661813 \t0\ttrue\n",
            "(2)\t 400\t 1.79978 \t0\tfalse\n",
            "(2)\t 401\t 2.0283303 \t0\tfalse\n",
            "(2)\t 402\t 1.3046743 \t1\ttrue\n",
            "(1)\t 403\t 1.4453536 \t2\ttrue\n",
            "(2)\t 404\t 0.9958281 \t1\ttrue\n",
            "(2)\t 405\t 0.96291214 \t0\tfalse\n",
            "(2)\t 406\t 1.2707671 \t0\tfalse\n",
            "(1)\t 407\t 1.0988282 \t2\ttrue\n",
            "(1)\t 408\t 1.1887631 \t2\ttrue\n",
            "(1)\t 409\t 1.4176804 \t2\ttrue\n",
            "(2)\t 410\t 1.3467757 \t0\tfalse\n",
            "(2)\t 411\t 1.2510369 \t0\tfalse\n",
            "(1)\t 412\t 1.5412633 \t2\ttrue\n",
            "(1)\t 413\t 0.8816941 \t2\ttrue\n",
            "(1)\t 414\t 1.4781244 \t2\ttrue\n",
            "(1)\t 415\t 1.9487796 \t1\tfalse\n",
            "(2)\t 416\t 0.5820511 \t2\tfalse\n",
            "(1)\t 417\t 1.7045281 \t1\tfalse\n",
            "(2)\t 418\t 1.212278 \t0\tfalse\n",
            "(1)\t 419\t 1.2367884 \t2\ttrue\n",
            "(0)\t 420\t 0.66547424 \t0\ttrue\n",
            "(2)\t 421\t 1.6855137 \t0\tfalse\n",
            "(0)\t 422\t 1.598298 \t0\ttrue\n",
            "(0)\t 423\t 2.2373126 \t0\ttrue\n",
            "(2)\t 424\t 1.7889727 \t0\tfalse\n",
            "(0)\t 425\t 2.1530473 \t0\ttrue\n",
            "(1)\t 426\t 1.1238219 \t0\tfalse\n",
            "(0)\t 427\t 2.25361 \t0\ttrue\n",
            "(0)\t 428\t 1.6414418 \t0\ttrue\n",
            "(0)\t 429\t 0.8338152 \t0\ttrue\n",
            "(0)\t 430\t 1.2795364 \t0\ttrue\n",
            "(2)\t 431\t 1.252654 \t0\tfalse\n",
            "(1)\t 432\t 1.0669535 \t0\tfalse\n",
            "(1)\t 433\t 1.0669535 \t0\tfalse\n",
            "(2)\t 434\t 1.0741276 \t1\ttrue\n",
            "(0)\t 435\t 1.6391941 \t0\ttrue\n",
            "(1)\t 436\t 1.4323143 \t2\ttrue\n",
            "(0)\t 437\t 2.0386322 \t0\ttrue\n",
            "(2)\t 438\t 0.96652275 \t0\tfalse\n",
            "(2)\t 439\t 1.3594178 \t0\tfalse\n",
            "(0)\t 440\t 1.2888623 \t0\ttrue\n",
            "(2)\t 441\t 0.8621226 \t2\tfalse\n",
            "(2)\t 442\t 2.1060863 \t0\tfalse\n",
            "(0)\t 443\t 2.0656881 \t0\ttrue\n",
            "(0)\t 444\t 1.4892389 \t0\ttrue\n",
            "(0)\t 445\t 1.7689599 \t0\ttrue\n",
            "(2)\t 446\t 1.9523995 \t1\ttrue\n",
            "(1)\t 447\t 1.8608346 \t0\tfalse\n",
            "(1)\t 448\t 1.8608346 \t0\tfalse\n",
            "(2)\t 449\t 1.1848829 \t0\tfalse\n",
            "(0)\t 450\t 0.8405219 \t2\tfalse\n",
            "(2)\t 451\t 1.0758508 \t2\tfalse\n",
            "(0)\t 452\t 2.2222044 \t0\ttrue\n",
            "(1)\t 453\t 1.8444982 \t1\tfalse\n",
            "(0)\t 454\t 1.4911407 \t0\ttrue\n",
            "(2)\t 455\t 1.0637028 \t0\tfalse\n",
            "(2)\t 456\t 0.8675636 \t2\tfalse\n",
            "(0)\t 457\t 1.5853949 \t0\ttrue\n",
            "(2)\t 458\t 1.796327 \t0\tfalse\n",
            "(2)\t 459\t 0.76943743 \t2\tfalse\n",
            "(2)\t 460\t 1.0378335 \t0\tfalse\n",
            "(0)\t 461\t 1.3505268 \t0\ttrue\n",
            "(2)\t 462\t 1.1015882 \t0\tfalse\n",
            "(2)\t 463\t 0.85130864 \t1\ttrue\n",
            "(1)\t 464\t 0.9682916 \t0\tfalse\n",
            "(0)\t 465\t 2.2256153 \t0\ttrue\n",
            "(1)\t 466\t 1.7091984 \t0\tfalse\n",
            "(1)\t 467\t 1.7091984 \t0\tfalse\n",
            "(2)\t 468\t 1.0524305 \t0\tfalse\n",
            "(2)\t 469\t 1.7959383 \t0\tfalse\n",
            "(0)\t 470\t 2.4086611 \t0\ttrue\n",
            "(2)\t 471\t 1.3193554 \t2\tfalse\n",
            "(2)\t 472\t 0.84977895 \t2\tfalse\n",
            "(0)\t 473\t 1.8011861 \t0\ttrue\n",
            "(2)\t 474\t 1.3882421 \t0\tfalse\n",
            "(2)\t 475\t 1.9327022 \t0\tfalse\n",
            "(2)\t 476\t 1.574206 \t0\tfalse\n",
            "(2)\t 477\t 1.1132796 \t0\tfalse\n",
            "(0)\t 478\t 2.4918776 \t0\ttrue\n",
            "(0)\t 479\t 2.3250744 \t0\ttrue\n",
            "(2)\t 480\t 0.8894538 \t0\tfalse\n",
            "(2)\t 481\t 0.9305187 \t0\tfalse\n",
            "(1)\t 482\t 1.5618467 \t1\tfalse\n",
            "(0)\t 483\t 1.6103823 \t0\ttrue\n",
            "(2)\t 484\t 1.1375043 \t0\tfalse\n",
            "(1)\t 485\t 0.8270948 \t0\tfalse\n",
            "(1)\t 486\t 0.8270948 \t0\tfalse\n",
            "(2)\t 487\t 1.9741645 \t1\ttrue\n",
            "(0)\t 488\t 1.3954436 \t2\tfalse\n",
            "(1)\t 489\t 1.6102839 \t1\tfalse\n",
            "(0)\t 490\t 1.7252613 \t0\ttrue\n",
            "(0)\t 491\t 2.0998876 \t0\ttrue\n",
            "(1)\t 492\t 1.4859644 \t2\ttrue\n",
            "(1)\t 493\t 1.6520537 \t2\ttrue\n",
            "(2)\t 494\t 1.4421004 \t0\tfalse\n",
            "(0)\t 495\t 2.259509 \t0\ttrue\n",
            "(1)\t 496\t 1.3139242 \t2\ttrue\n",
            "(2)\t 497\t 0.6751287 \t2\tfalse\n",
            "(0)\t 498\t 1.5419942 \t0\ttrue\n",
            "(2)\t 499\t 1.0006657 \t2\tfalse\n",
            "(2)\t 500\t 0.8314036 \t2\tfalse\n",
            "(0)\t 501\t 1.8758049 \t0\ttrue\n",
            "(0)\t 502\t 1.879808 \t0\ttrue\n",
            "(0)\t 503\t 1.3705717 \t2\tfalse\n",
            "(2)\t 504\t 1.0660759 \t0\tfalse\n",
            "(1)\t 505\t 1.3472565 \t2\ttrue\n",
            "(1)\t 506\t 1.0654899 \t1\tfalse\n",
            "(2)\t 507\t 0.9715061 \t0\tfalse\n",
            "(0)\t 508\t 1.6367868 \t0\ttrue\n",
            "(0)\t 509\t 1.8789991 \t0\ttrue\n",
            "(2)\t 510\t 1.669401 \t0\tfalse\n",
            "(1)\t 511\t 1.3199207 \t2\ttrue\n",
            "(1)\t 512\t 0.84741765 \t2\ttrue\n",
            "(1)\t 513\t 1.113783 \t2\ttrue\n",
            "(1)\t 514\t 1.7198722 \t1\tfalse\n",
            "(2)\t 515\t 1.1760151 \t2\tfalse\n",
            "(2)\t 516\t 0.89036655 \t0\tfalse\n",
            "(0)\t 517\t 1.7179465 \t0\ttrue\n",
            "(2)\t 518\t 1.1797361 \t0\tfalse\n",
            "(0)\t 519\t 1.2674699 \t0\ttrue\n",
            "(2)\t 520\t 0.9196522 \t0\tfalse\n",
            "(2)\t 521\t 0.22538538 \t1\ttrue\n",
            "(2)\t 522\t 1.2469639 \t0\tfalse\n",
            "(1)\t 523\t 0.95637274 \t2\ttrue\n",
            "(2)\t 524\t 2.1475368 \t0\tfalse\n",
            "(1)\t 525\t 1.2092603 \t1\tfalse\n",
            "(0)\t 526\t 1.8841809 \t0\ttrue\n",
            "(0)\t 527\t 2.3354971 \t0\ttrue\n",
            "(2)\t 528\t 0.86957794 \t2\tfalse\n",
            "(2)\t 529\t 0.86347 \t2\tfalse\n",
            "(2)\t 530\t 1.5292119 \t0\tfalse\n",
            "(0)\t 531\t 0.69073224 \t0\ttrue\n",
            "(0)\t 532\t 0.87147945 \t0\ttrue\n",
            "(0)\t 533\t 1.0959928 \t1\tfalse\n",
            "(0)\t 534\t 0.8747771 \t0\ttrue\n",
            "(2)\t 535\t 0.9841388 \t2\tfalse\n",
            "(1)\t 536\t 0.9118145 \t1\tfalse\n",
            "(0)\t 537\t 1.2124854 \t0\ttrue\n",
            "(2)\t 538\t 1.021341 \t2\tfalse\n",
            "(0)\t 539\t 1.5823568 \t0\ttrue\n",
            "(2)\t 540\t 1.7588767 \t0\tfalse\n",
            "(1)\t 541\t 1.0180677 \t2\ttrue\n",
            "(0)\t 542\t 0.23272207 \t0\ttrue\n",
            "(0)\t 543\t 0.86053604 \t2\tfalse\n",
            "(0)\t 544\t 0.5824989 \t2\tfalse\n",
            "(0)\t 545\t 2.598023 \t0\ttrue\n",
            "(0)\t 546\t 2.13364 \t0\ttrue\n",
            "(2)\t 547\t 0.8807205 \t2\tfalse\n",
            "(1)\t 548\t 2.358966 \t0\tfalse\n",
            "(0)\t 549\t 2.4503295 \t0\ttrue\n",
            "(1)\t 550\t 1.4199591 \t1\tfalse\n",
            "(1)\t 551\t 1.0562167 \t2\ttrue\n",
            "(2)\t 552\t 1.4753249 \t0\tfalse\n",
            "(0)\t 553\t 2.5601037 \t0\ttrue\n",
            "(2)\t 554\t 1.3479367 \t1\ttrue\n",
            "(2)\t 555\t 2.0992372 \t0\tfalse\n",
            "(2)\t 556\t 1.4941235 \t0\tfalse\n",
            "(1)\t 557\t 1.1571542 \t2\ttrue\n",
            "(1)\t 558\t 1.5148317 \t2\ttrue\n",
            "(0)\t 559\t 1.0025673 \t0\ttrue\n",
            "(2)\t 560\t 0.87175584 \t2\tfalse\n",
            "(2)\t 561\t 1.2190038 \t0\tfalse\n",
            "(2)\t 562\t 2.0159605 \t0\tfalse\n",
            "(1)\t 563\t 0.80300874 \t0\tfalse\n",
            "(0)\t 564\t 1.2346563 \t2\tfalse\n",
            "(0)\t 565\t 1.5684621 \t0\ttrue\n",
            "(1)\t 566\t 1.3007137 \t1\tfalse\n",
            "(2)\t 567\t 1.662393 \t1\ttrue\n",
            "(2)\t 568\t 1.2049366 \t0\tfalse\n",
            "(1)\t 569\t 0.7634177 \t0\tfalse\n",
            "(1)\t 570\t 0.7634177 \t0\tfalse\n",
            "(2)\t 571\t 1.0939082 \t2\tfalse\n",
            "(1)\t 572\t 2.2220733 \t0\tfalse\n",
            "(1)\t 573\t 2.2220733 \t0\tfalse\n",
            "(2)\t 574\t 0.8809684 \t2\tfalse\n",
            "(2)\t 575\t 0.96521276 \t0\tfalse\n",
            "(0)\t 576\t 1.1600728 \t0\ttrue\n",
            "(0)\t 577\t 1.2782731 \t0\ttrue\n",
            "(2)\t 578\t 2.5497384 \t0\tfalse\n",
            "(2)\t 579\t 1.436711 \t0\tfalse\n",
            "(2)\t 580\t 1.5244743 \t0\tfalse\n",
            "(1)\t 581\t 1.9932959 \t1\tfalse\n",
            "(2)\t 582\t 1.0881443 \t0\tfalse\n",
            "(2)\t 583\t 0.93579596 \t0\tfalse\n",
            "(0)\t 584\t 1.502275 \t0\ttrue\n",
            "(0)\t 585\t 1.3432995 \t0\ttrue\n",
            "(0)\t 586\t 2.2215817 \t0\ttrue\n",
            "(0)\t 587\t 1.1259362 \t0\ttrue\n",
            "(0)\t 588\t 1.0518284 \t0\ttrue\n",
            "(0)\t 589\t 2.1512008 \t0\ttrue\n",
            "(1)\t 590\t 0.95214057 \t0\tfalse\n",
            "(2)\t 591\t 1.0888469 \t2\tfalse\n",
            "(0)\t 592\t 1.2997098 \t0\ttrue\n",
            "(1)\t 593\t 1.217954 \t2\ttrue\n",
            "(0)\t 594\t 1.2005965 \t0\ttrue\n",
            "(2)\t 595\t 2.0377862 \t0\tfalse\n",
            "(0)\t 596\t 1.3121786 \t0\ttrue\n",
            "(2)\t 597\t 0.86213595 \t1\ttrue\n",
            "(1)\t 598\t 1.0178436 \t0\tfalse\n",
            "(1)\t 599\t 1.0178436 \t0\tfalse\n",
            "(0)\t 600\t 1.0375305 \t0\ttrue\n",
            "(2)\t 601\t 1.7534865 \t0\tfalse\n",
            "(2)\t 602\t 1.927135 \t1\ttrue\n",
            "(0)\t 603\t 0.26210278 \t0\ttrue\n",
            "(0)\t 604\t 2.1490948 \t0\ttrue\n",
            "(0)\t 605\t 0.99152863 \t2\tfalse\n",
            "(0)\t 606\t 1.5592797 \t0\ttrue\n",
            "(2)\t 607\t 1.0223511 \t2\tfalse\n",
            "(0)\t 608\t 1.3097578 \t0\ttrue\n",
            "(0)\t 609\t 2.2477615 \t0\ttrue\n",
            "(2)\t 610\t 2.3093302 \t0\tfalse\n",
            "(0)\t 611\t 1.328849 \t0\ttrue\n",
            "(2)\t 612\t 0.8740963 \t2\tfalse\n",
            "(2)\t 613\t 1.5332007 \t1\ttrue\n",
            "(0)\t 614\t 1.6220719 \t0\ttrue\n",
            "(2)\t 615\t 1.5490772 \t0\tfalse\n",
            "(2)\t 616\t 0.84029037 \t2\tfalse\n",
            "(1)\t 617\t 1.3186597 \t2\ttrue\n",
            "(2)\t 618\t 0.9423098 \t0\tfalse\n",
            "(1)\t 619\t 1.800546 \t1\tfalse\n",
            "(0)\t 620\t 0.14108852 \t0\ttrue\n",
            "(0)\t 621\t 1.1748717 \t1\tfalse\n",
            "(0)\t 622\t 2.0712583 \t0\ttrue\n",
            "(0)\t 623\t 0.821078 \t0\ttrue\n",
            "(2)\t 624\t 1.3855697 \t0\tfalse\n",
            "(1)\t 625\t 1.4077897 \t1\tfalse\n",
            "(2)\t 626\t 0.83311564 \t2\tfalse\n",
            "(1)\t 627\t 0.56493837 \t2\ttrue\n",
            "(2)\t 628\t 1.3802032 \t0\tfalse\n",
            "(2)\t 629\t 2.0397947 \t0\tfalse\n",
            "(0)\t 630\t 0.5107045 \t0\ttrue\n",
            "(0)\t 631\t 0.8445438 \t0\ttrue\n",
            "(0)\t 632\t 1.2528616 \t0\ttrue\n",
            "(0)\t 633\t 1.863042 \t0\ttrue\n",
            "(0)\t 634\t 1.6061468 \t0\ttrue\n",
            "(0)\t 635\t 0.98894244 \t0\ttrue\n",
            "(2)\t 636\t 1.3856579 \t0\tfalse\n",
            "(2)\t 637\t 2.1540706 \t0\tfalse\n",
            "(0)\t 638\t 1.1697441 \t0\ttrue\n",
            "(2)\t 639\t 1.4461385 \t0\tfalse\n",
            "(2)\t 640\t 1.6045192 \t0\tfalse\n",
            "(0)\t 641\t 1.3321551 \t0\ttrue\n",
            "(2)\t 642\t 1.5524724 \t0\tfalse\n",
            "(0)\t 643\t 0.8605213 \t0\ttrue\n",
            "(2)\t 644\t 1.1633443 \t0\tfalse\n",
            "(1)\t 645\t 1.6412307 \t1\tfalse\n",
            "(1)\t 646\t 0.92836773 \t2\ttrue\n",
            "(1)\t 647\t 0.4651278 \t0\tfalse\n",
            "(2)\t 648\t 1.6208332 \t0\tfalse\n",
            "(2)\t 649\t 2.3257957 \t0\tfalse\n",
            "(1)\t 650\t 0.72022617 \t2\ttrue\n",
            "(1)\t 651\t 1.016834 \t2\ttrue\n",
            "(2)\t 652\t 1.4811234 \t0\tfalse\n",
            "(0)\t 653\t 2.309235 \t0\ttrue\n",
            "(0)\t 654\t 2.4055336 \t0\ttrue\n",
            "(0)\t 655\t 1.8449858 \t0\ttrue\n",
            "(0)\t 656\t 0.86012256 \t2\tfalse\n",
            "(1)\t 657\t 1.4549645 \t2\ttrue\n",
            "(0)\t 658\t 0.98285365 \t0\ttrue\n",
            "(1)\t 659\t 1.5463283 \t0\tfalse\n",
            "(1)\t 660\t 1.5463283 \t0\tfalse\n",
            "(1)\t 661\t 0.9221444 \t2\ttrue\n",
            "(0)\t 662\t 1.051144 \t0\ttrue\n",
            "(2)\t 663\t 0.57466197 \t1\ttrue\n",
            "(2)\t 664\t 1.4410844 \t0\tfalse\n",
            "(2)\t 665\t 0.24563587 \t1\ttrue\n",
            "(1)\t 666\t 1.9034785 \t0\tfalse\n",
            "(1)\t 667\t 1.9034785 \t0\tfalse\n",
            "(0)\t 668\t 1.3066324 \t0\ttrue\n",
            "(0)\t 669\t 1.3599095 \t0\ttrue\n",
            "(0)\t 670\t 0.85659045 \t0\ttrue\n",
            "(2)\t 671\t 1.6473682 \t0\tfalse\n",
            "(0)\t 672\t 0.6021045 \t1\tfalse\n",
            "(1)\t 673\t 0.97661924 \t0\tfalse\n",
            "(1)\t 674\t 0.56506777 \t1\tfalse\n",
            "(0)\t 675\t 1.089446 \t0\ttrue\n",
            "(2)\t 676\t 1.0172662 \t2\tfalse\n",
            "(2)\t 677\t 1.6353275 \t0\tfalse\n",
            "(0)\t 678\t 0.89143246 \t1\tfalse\n",
            "(0)\t 679\t 2.1676776 \t0\ttrue\n",
            "(2)\t 680\t 1.5952946 \t0\tfalse\n",
            "(0)\t 681\t 2.0561428 \t0\ttrue\n",
            "(0)\t 682\t 1.0876 \t0\ttrue\n",
            "(2)\t 683\t 1.2506465 \t0\tfalse\n",
            "(0)\t 684\t 1.6192397 \t0\ttrue\n",
            "(0)\t 685\t 1.1243266 \t0\ttrue\n",
            "(0)\t 686\t 1.6228132 \t0\ttrue\n",
            "(0)\t 687\t 0.5346843 \t2\tfalse\n",
            "(0)\t 688\t 2.0527081 \t0\ttrue\n",
            "(0)\t 689\t 0.9290272 \t0\ttrue\n",
            "(0)\t 690\t 2.2529113 \t0\ttrue\n",
            "(1)\t 691\t 0.8682182 \t0\tfalse\n",
            "(2)\t 692\t 2.413727 \t0\tfalse\n",
            "(1)\t 693\t 1.8398756 \t0\tfalse\n",
            "(1)\t 694\t 1.8398756 \t0\tfalse\n",
            "(1)\t 695\t 1.4181212 \t1\tfalse\n",
            "(0)\t 696\t 1.8705299 \t0\ttrue\n",
            "(0)\t 697\t 1.5479723 \t0\ttrue\n",
            "(1)\t 698\t 1.7781348 \t1\tfalse\n",
            "(1)\t 699\t 0.9426534 \t0\tfalse\n",
            "(1)\t 700\t 2.04875 \t1\tfalse\n",
            "(0)\t 701\t 1.1904306 \t0\ttrue\n",
            "(1)\t 702\t 0.7781553 \t2\ttrue\n",
            "(2)\t 703\t 0.996984 \t0\tfalse\n",
            "(2)\t 704\t 0.93219733 \t0\tfalse\n",
            "(2)\t 705\t 1.7665783 \t0\tfalse\n",
            "(2)\t 706\t 0.9775618 \t1\ttrue\n",
            "(1)\t 707\t 1.2258943 \t2\ttrue\n",
            "(0)\t 708\t 1.7714916 \t0\ttrue\n",
            "(2)\t 709\t 0.7500576 \t2\tfalse\n",
            "(1)\t 710\t 0.83004636 \t2\ttrue\n",
            "(2)\t 711\t 2.252815 \t0\tfalse\n",
            "(1)\t 712\t 1.3591838 \t2\ttrue\n",
            "(2)\t 713\t 1.4413143 \t0\tfalse\n",
            "(0)\t 714\t 1.5109301 \t0\ttrue\n",
            "(1)\t 715\t 1.4451529 \t2\ttrue\n",
            "(0)\t 716\t 2.4903662 \t0\ttrue\n",
            "(1)\t 717\t 0.5219796 \t0\tfalse\n",
            "(1)\t 718\t 1.2732759 \t2\ttrue\n",
            "(0)\t 719\t 1.3136095 \t0\ttrue\n",
            "(2)\t 720\t 1.6261436 \t0\tfalse\n",
            "(1)\t 721\t 1.5289667 \t2\ttrue\n",
            "(1)\t 722\t 1.7160234 \t1\tfalse\n",
            "(0)\t 723\t 2.2138512 \t0\ttrue\n",
            "(2)\t 724\t 1.238173 \t0\tfalse\n",
            "(0)\t 725\t 1.2406689 \t0\ttrue\n",
            "(2)\t 726\t 1.7430325 \t0\tfalse\n",
            "(0)\t 727\t 2.45888 \t0\ttrue\n",
            "(0)\t 728\t 1.440252 \t0\ttrue\n",
            "(2)\t 729\t 0.49895975 \t2\tfalse\n",
            "(1)\t 730\t 1.0116303 \t0\tfalse\n",
            "(0)\t 731\t 1.7129658 \t0\ttrue\n",
            "(1)\t 732\t 1.0828576 \t2\ttrue\n",
            "(2)\t 733\t 1.0831949 \t2\tfalse\n",
            "(2)\t 734\t 1.2236165 \t0\tfalse\n",
            "(2)\t 735\t 1.3302981 \t0\tfalse\n",
            "(2)\t 736\t 0.89274186 \t0\tfalse\n",
            "(1)\t 737\t 1.0324931 \t1\tfalse\n",
            "(1)\t 738\t 1.0627338 \t2\ttrue\n",
            "(2)\t 739\t 1.7860138 \t1\ttrue\n",
            "(0)\t 740\t 0.5043179 \t0\ttrue\n",
            "(1)\t 741\t 0.99484277 \t0\tfalse\n",
            "(2)\t 742\t 1.0854465 \t2\tfalse\n",
            "(1)\t 743\t 1.3586333 \t1\tfalse\n",
            "(1)\t 744\t 1.838309 \t1\tfalse\n",
            "(2)\t 745\t 2.3963492 \t0\tfalse\n",
            "(2)\t 746\t 0.9834172 \t0\tfalse\n",
            "(1)\t 747\t 0.8378134 \t2\ttrue\n",
            "(1)\t 748\t 1.0228282 \t0\tfalse\n",
            "(1)\t 749\t 1.0228282 \t0\tfalse\n",
            "(1)\t 750\t 1.7809998 \t1\tfalse\n",
            "(0)\t 751\t 2.1793168 \t0\ttrue\n",
            "(1)\t 752\t 0.91699046 \t0\tfalse\n",
            "(2)\t 753\t 1.2549042 \t2\tfalse\n",
            "(0)\t 754\t 2.2517102 \t0\ttrue\n",
            "(2)\t 755\t 1.0516698 \t0\tfalse\n",
            "(1)\t 756\t 0.51521087 \t0\tfalse\n",
            "(1)\t 757\t 1.4825534 \t2\ttrue\n",
            "(2)\t 758\t 1.0827595 \t0\tfalse\n",
            "(2)\t 759\t 0.9401552 \t0\tfalse\n",
            "(0)\t 760\t 1.653139 \t0\ttrue\n",
            "(1)\t 761\t 1.2085713 \t2\ttrue\n",
            "(1)\t 762\t 2.1498606 \t0\tfalse\n",
            "(1)\t 763\t 2.1498606 \t0\tfalse\n",
            "(1)\t 764\t 1.4301344 \t2\ttrue\n",
            "(2)\t 765\t 1.2681749 \t2\tfalse\n",
            "(2)\t 766\t 1.5351275 \t2\tfalse\n",
            "(0)\t 767\t 1.1349453 \t0\ttrue\n",
            "(2)\t 768\t 2.06172 \t0\tfalse\n",
            "(2)\t 769\t 1.102146 \t0\tfalse\n",
            "(0)\t 770\t 1.6734027 \t0\ttrue\n",
            "(2)\t 771\t 0.7723808 \t2\tfalse\n",
            "(2)\t 772\t 0.8398857 \t0\tfalse\n",
            "(1)\t 773\t 1.1519295 \t2\ttrue\n",
            "(1)\t 774\t 1.9201354 \t1\tfalse\n",
            "(2)\t 775\t 1.42198 \t0\tfalse\n",
            "(2)\t 776\t 0.26651126 \t1\ttrue\n",
            "(0)\t 777\t 1.9586879 \t0\ttrue\n",
            "(0)\t 778\t 1.4858812 \t0\ttrue\n",
            "(0)\t 779\t 2.5688293 \t0\ttrue\n",
            "(2)\t 780\t 0.7674065 \t1\ttrue\n",
            "(1)\t 781\t 1.3548626 \t0\tfalse\n",
            "(0)\t 782\t 1.2544174 \t0\ttrue\n",
            "(0)\t 783\t 1.7014357 \t0\ttrue\n",
            "(2)\t 784\t 0.79679465 \t2\tfalse\n",
            "(2)\t 785\t 1.2722647 \t0\tfalse\n",
            "(2)\t 786\t 0.33943027 \t2\tfalse\n",
            "(2)\t 787\t 1.6540087 \t1\ttrue\n",
            "(0)\t 788\t 0.89646405 \t2\tfalse\n",
            "(2)\t 789\t 1.2757019 \t2\tfalse\n",
            "(0)\t 790\t 1.7316736 \t0\ttrue\n",
            "(2)\t 791\t 1.0111202 \t0\tfalse\n",
            "(2)\t 792\t 0.78784287 \t0\tfalse\n",
            "(2)\t 793\t 0.7674991 \t2\tfalse\n",
            "(0)\t 794\t 1.4813132 \t0\ttrue\n",
            "(2)\t 795\t 1.0960033 \t0\tfalse\n",
            "(1)\t 796\t 1.4376808 \t0\tfalse\n",
            "(1)\t 797\t 1.4376808 \t0\tfalse\n",
            "(2)\t 798\t 1.4403992 \t0\tfalse\n",
            "(1)\t 799\t 1.1974785 \t2\ttrue\n",
            "(2)\t 800\t 0.9807472 \t2\tfalse\n",
            "(2)\t 801\t 1.901304 \t0\tfalse\n",
            "(1)\t 802\t 1.4508039 \t2\ttrue\n",
            "(0)\t 803\t 2.4074352 \t0\ttrue\n",
            "(2)\t 804\t 1.0014594 \t0\tfalse\n",
            "(1)\t 805\t 0.40235397 \t2\ttrue\n",
            "(2)\t 806\t 0.80704415 \t2\tfalse\n",
            "(2)\t 807\t 1.0209626 \t2\tfalse\n",
            "(1)\t 808\t 1.4636462 \t2\ttrue\n",
            "(0)\t 809\t 0.7263807 \t0\ttrue\n",
            "(2)\t 810\t 0.124889806 \t0\tfalse\n",
            "(1)\t 811\t 1.0230796 \t2\ttrue\n",
            "(1)\t 812\t 1.4765016 \t2\ttrue\n",
            "(0)\t 813\t 1.1026953 \t2\tfalse\n",
            "(0)\t 814\t 1.1147993 \t0\ttrue\n",
            "(0)\t 815\t 1.6506772 \t0\ttrue\n",
            "(2)\t 816\t 1.3115693 \t0\tfalse\n",
            "(2)\t 817\t 0.87107646 \t2\tfalse\n",
            "(0)\t 818\t 2.2583275 \t0\ttrue\n",
            "(0)\t 819\t 2.4011457 \t0\ttrue\n",
            "(1)\t 820\t 1.2825291 \t1\tfalse\n",
            "(1)\t 821\t 1.2825291 \t1\tfalse\n",
            "(0)\t 822\t 2.2143288 \t0\ttrue\n",
            "(0)\t 823\t 1.9850683 \t0\ttrue\n",
            "(1)\t 824\t 1.0930113 \t0\tfalse\n",
            "(1)\t 825\t 1.0930113 \t0\tfalse\n",
            "(0)\t 826\t 2.265342 \t0\ttrue\n",
            "(1)\t 827\t 1.6197076 \t0\tfalse\n",
            "(1)\t 828\t 1.6197076 \t0\tfalse\n",
            "(2)\t 829\t 0.84192014 \t0\tfalse\n",
            "(2)\t 830\t 2.2690551 \t0\tfalse\n",
            "(0)\t 831\t 1.1047083 \t2\tfalse\n",
            "(0)\t 832\t 1.253869 \t0\ttrue\n",
            "(2)\t 833\t 1.6245698 \t0\tfalse\n",
            "(1)\t 834\t 1.5009912 \t1\tfalse\n",
            "(1)\t 835\t 1.1439297 \t2\ttrue\n",
            "(2)\t 836\t 1.2500956 \t2\tfalse\n",
            "(0)\t 837\t 0.8998083 \t2\tfalse\n",
            "(1)\t 838\t 1.1457249 \t2\ttrue\n",
            "(0)\t 839\t 1.487986 \t0\ttrue\n",
            "(0)\t 840\t 1.7438587 \t0\ttrue\n",
            "(0)\t 841\t 0.5784806 \t0\ttrue\n",
            "(2)\t 842\t 0.7521624 \t2\tfalse\n",
            "(2)\t 843\t 1.0598555 \t0\tfalse\n",
            "(0)\t 844\t 2.446037 \t0\ttrue\n",
            "(1)\t 845\t 1.0960629 \t0\tfalse\n",
            "(2)\t 846\t 1.2030009 \t0\tfalse\n",
            "(2)\t 847\t 0.9109044 \t2\tfalse\n",
            "(2)\t 848\t 1.1958189 \t2\tfalse\n",
            "(0)\t 849\t 1.6813009 \t0\ttrue\n",
            "(0)\t 850\t 1.016199 \t0\ttrue\n",
            "(2)\t 851\t 2.048236 \t0\tfalse\n",
            "(2)\t 852\t 1.3597848 \t1\ttrue\n",
            "(1)\t 853\t 0.8951819 \t2\ttrue\n",
            "(2)\t 854\t 1.0655903 \t0\tfalse\n",
            "(2)\t 855\t 1.0154037 \t0\tfalse\n",
            "(1)\t 856\t 0.76620096 \t2\ttrue\n",
            "(1)\t 857\t 0.76620096 \t2\ttrue\n",
            "(0)\t 858\t 2.1937978 \t0\ttrue\n",
            "(0)\t 859\t 2.2573938 \t0\ttrue\n",
            "(2)\t 860\t 1.1710027 \t0\tfalse\n",
            "(2)\t 861\t 0.61180645 \t1\ttrue\n",
            "(1)\t 862\t 1.0439192 \t2\ttrue\n",
            "(1)\t 863\t 1.134438 \t2\ttrue\n",
            "(2)\t 864\t 0.66659725 \t0\tfalse\n",
            "(0)\t 865\t 0.923066 \t0\ttrue\n",
            "(2)\t 866\t 2.2169907 \t0\tfalse\n",
            "(1)\t 867\t 1.0454823 \t2\ttrue\n",
            "(0)\t 868\t 0.8128336 \t0\ttrue\n",
            "(2)\t 869\t 1.0521321 \t0\tfalse\n",
            "(2)\t 870\t 1.3791722 \t0\tfalse\n",
            "(2)\t 871\t 0.9498713 \t2\tfalse\n",
            "(2)\t 872\t 0.8032915 \t2\tfalse\n",
            "(1)\t 873\t 1.1803594 \t1\tfalse\n",
            "(0)\t 874\t 1.1805974 \t2\tfalse\n",
            "(2)\t 875\t 0.87190175 \t2\tfalse\n",
            "(1)\t 876\t 1.1642903 \t2\ttrue\n",
            "(1)\t 877\t 1.1642903 \t2\ttrue\n",
            "(2)\t 878\t 1.1598694 \t0\tfalse\n",
            "(0)\t 879\t 2.1900463 \t0\ttrue\n",
            "(1)\t 880\t 0.8656842 \t0\tfalse\n",
            "(2)\t 881\t 2.0050745 \t0\tfalse\n",
            "(1)\t 882\t 1.0013742 \t0\tfalse\n",
            "(1)\t 883\t 1.0013742 \t0\tfalse\n",
            "(2)\t 884\t 1.9358771 \t0\tfalse\n",
            "(0)\t 885\t 1.1744225 \t0\ttrue\n",
            "(1)\t 886\t 2.2809255 \t0\tfalse\n",
            "(1)\t 887\t 2.2809255 \t0\tfalse\n",
            "(0)\t 888\t 2.1339302 \t0\ttrue\n",
            "(2)\t 889\t 1.3091246 \t0\tfalse\n",
            "(0)\t 890\t 2.2232707 \t0\ttrue\n",
            "(0)\t 891\t 1.961987 \t0\ttrue\n",
            "(1)\t 892\t 1.3052033 \t2\ttrue\n",
            "(1)\t 893\t 1.5974927 \t0\tfalse\n",
            "(0)\t 894\t 1.6835507 \t0\ttrue\n",
            "(1)\t 895\t 0.9503237 \t2\ttrue\n",
            "(0)\t 896\t 2.0250661 \t0\ttrue\n",
            "(1)\t 897\t 1.4723141 \t2\ttrue\n",
            "(1)\t 898\t 1.1107101 \t2\ttrue\n",
            "(1)\t 899\t 2.4277716 \t0\tfalse\n",
            "(2)\t 900\t 1.0703171 \t0\tfalse\n",
            "(2)\t 901\t 1.2443491 \t1\ttrue\n",
            "(0)\t 902\t 1.8316239 \t0\ttrue\n",
            "(1)\t 903\t 1.145242 \t0\tfalse\n",
            "(2)\t 904\t 1.4030493 \t2\tfalse\n",
            "(1)\t 905\t 1.6391679 \t2\ttrue\n",
            "(0)\t 906\t 2.062554 \t0\ttrue\n",
            "(2)\t 907\t 1.218201 \t2\tfalse\n",
            "(2)\t 908\t 1.0247254 \t2\tfalse\n",
            "(2)\t 909\t 2.2020047 \t0\tfalse\n",
            "(0)\t 910\t 2.1017673 \t0\ttrue\n",
            "(0)\t 911\t 1.1462122 \t0\ttrue\n",
            "(0)\t 912\t 2.1508732 \t0\ttrue\n",
            "(0)\t 913\t 2.0039337 \t0\ttrue\n",
            "(2)\t 914\t 2.3092241 \t0\tfalse\n",
            "(1)\t 915\t 1.1660331 \t1\tfalse\n",
            "(2)\t 916\t 1.6822156 \t0\tfalse\n",
            "(1)\t 917\t 0.9801604 \t2\ttrue\n",
            "(0)\t 918\t 1.402347 \t0\ttrue\n",
            "(1)\t 919\t 1.1122723 \t0\tfalse\n",
            "(1)\t 920\t 1.1122723 \t0\tfalse\n",
            "(1)\t 921\t 0.91391313 \t2\ttrue\n",
            "(2)\t 922\t 2.0041711 \t0\tfalse\n",
            "(0)\t 923\t 2.3890781 \t0\ttrue\n",
            "(0)\t 924\t 1.3790737 \t1\tfalse\n",
            "(0)\t 925\t 1.7907114 \t0\ttrue\n",
            "(0)\t 926\t 0.9440409 \t0\ttrue\n",
            "(2)\t 927\t 1.4939721 \t1\ttrue\n",
            "(1)\t 928\t 2.3767328 \t0\tfalse\n",
            "(1)\t 929\t 2.3767328 \t0\tfalse\n",
            "(0)\t 930\t 1.0155798 \t0\ttrue\n",
            "(0)\t 931\t 1.1717484 \t0\ttrue\n",
            "(2)\t 932\t 0.9306822 \t2\tfalse\n",
            "(1)\t 933\t 1.4514427 \t2\ttrue\n",
            "(2)\t 934\t 1.3638747 \t0\tfalse\n",
            "(2)\t 935\t 1.6639847 \t0\tfalse\n",
            "(1)\t 936\t 1.5414189 \t2\ttrue\n",
            "(0)\t 937\t 1.4770383 \t0\ttrue\n",
            "(0)\t 938\t 1.5868423 \t0\ttrue\n",
            "(0)\t 939\t 2.3761597 \t0\ttrue\n",
            "(0)\t 940\t 1.3453535 \t0\ttrue\n",
            "(2)\t 941\t 0.85254693 \t1\ttrue\n",
            "(0)\t 942\t 1.0410721 \t2\tfalse\n",
            "(2)\t 943\t 0.8498657 \t2\tfalse\n",
            "(1)\t 944\t 1.693795 \t0\tfalse\n",
            "(1)\t 945\t 1.693795 \t0\tfalse\n",
            "(1)\t 946\t 1.9193789 \t1\tfalse\n",
            "(2)\t 947\t 0.821225 \t0\tfalse\n",
            "(2)\t 948\t 0.8845884 \t0\tfalse\n",
            "(2)\t 949\t 1.3098226 \t0\tfalse\n",
            "(1)\t 950\t 1.761428 \t0\tfalse\n",
            "(2)\t 951\t 0.6588941 \t0\tfalse\n",
            "(2)\t 952\t 0.6227728 \t2\tfalse\n",
            "(0)\t 953\t 0.946788 \t1\tfalse\n",
            "(1)\t 954\t 0.7213286 \t2\ttrue\n",
            "(0)\t 955\t 1.7223129 \t0\ttrue\n",
            "(2)\t 956\t 1.0296499 \t1\ttrue\n",
            "(0)\t 957\t 1.5671705 \t0\ttrue\n",
            "(2)\t 958\t 1.5038122 \t0\tfalse\n",
            "(2)\t 959\t 1.3276258 \t0\tfalse\n",
            "(1)\t 960\t 0.81706774 \t0\tfalse\n",
            "(1)\t 961\t 0.81706774 \t0\tfalse\n",
            "(2)\t 962\t 1.654831 \t0\tfalse\n",
            "(2)\t 963\t 0.94529617 \t0\tfalse\n",
            "(0)\t 964\t 2.2359784 \t0\ttrue\n",
            "(0)\t 965\t 0.99867076 \t0\ttrue\n",
            "(0)\t 966\t 0.9846021 \t2\tfalse\n",
            "(0)\t 967\t 1.1765453 \t2\tfalse\n",
            "(0)\t 968\t 1.2226334 \t0\ttrue\n",
            "(0)\t 969\t 2.2111719 \t0\ttrue\n",
            "(2)\t 970\t 1.326094 \t2\tfalse\n",
            "(0)\t 971\t 1.0633864 \t0\ttrue\n",
            "(2)\t 972\t 1.4523137 \t0\tfalse\n",
            "(0)\t 973\t 0.7289484 \t2\tfalse\n",
            "(2)\t 974\t 1.4560448 \t0\tfalse\n",
            "(0)\t 975\t 0.89432544 \t0\ttrue\n",
            "(2)\t 976\t 1.0744379 \t0\tfalse\n",
            "(2)\t 977\t 1.2599999 \t1\ttrue\n",
            "(2)\t 978\t 1.0397373 \t2\tfalse\n",
            "(1)\t 979\t 0.6880771 \t0\tfalse\n",
            "(2)\t 980\t 1.439266 \t0\tfalse\n",
            "(1)\t 981\t 1.597713 \t1\tfalse\n",
            "(1)\t 982\t 0.5778728 \t2\ttrue\n",
            "(0)\t 983\t 1.4188287 \t0\ttrue\n",
            "(1)\t 984\t 0.8758883 \t2\ttrue\n",
            "(0)\t 985\t 2.6159217 \t0\ttrue\n",
            "(2)\t 986\t 1.120495 \t2\tfalse\n",
            "(0)\t 987\t 0.80812526 \t2\tfalse\n",
            "(2)\t 988\t 0.907916 \t0\tfalse\n",
            "(2)\t 989\t 1.122407 \t2\tfalse\n",
            "(2)\t 990\t 0.89717615 \t0\tfalse\n",
            "(0)\t 991\t 1.5026808 \t0\ttrue\n",
            "(0)\t 992\t 1.6916186 \t0\ttrue\n",
            "(0)\t 993\t 1.0278624 \t0\ttrue\n",
            "(0)\t 994\t 1.1055921 \t0\ttrue\n",
            "(0)\t 995\t 0.7872144 \t0\ttrue\n",
            "(1)\t 996\t 1.5331993 \t2\ttrue\n",
            "(1)\t 997\t 0.70149016 \t0\tfalse\n",
            "(2)\t 998\t 0.9877325 \t2\tfalse\n",
            "(1)\t 999\t 1.3721917 \t2\ttrue\n",
            "(0)\t 1000\t 2.257647 \t0\ttrue\n",
            "(0)\t 1001\t 1.8805827 \t0\ttrue\n",
            "(0)\t 1002\t 1.4916986 \t0\ttrue\n",
            "(1)\t 1003\t 0.9104343 \t0\tfalse\n",
            "(1)\t 1004\t 0.9104343 \t0\tfalse\n",
            "(2)\t 1005\t 1.403558 \t0\tfalse\n",
            "(0)\t 1006\t 1.4172618 \t0\ttrue\n",
            "(2)\t 1007\t 1.2839324 \t0\tfalse\n",
            "(2)\t 1008\t 1.6371053 \t1\ttrue\n",
            "(2)\t 1009\t 0.8791116 \t1\ttrue\n",
            "(2)\t 1010\t 0.8972424 \t2\tfalse\n",
            "(1)\t 1011\t 1.2427312 \t2\ttrue\n",
            "(1)\t 1012\t 1.1193019 \t2\ttrue\n",
            "(1)\t 1013\t 1.1283247 \t0\tfalse\n",
            "(0)\t 1014\t 1.4468342 \t0\ttrue\n",
            "(0)\t 1015\t 1.6391217 \t0\ttrue\n",
            "(2)\t 1016\t 1.3087442 \t0\tfalse\n",
            "(0)\t 1017\t 1.9430101 \t0\ttrue\n",
            "(2)\t 1018\t 1.1605024 \t1\ttrue\n",
            "(0)\t 1019\t 1.347845 \t0\ttrue\n",
            "(0)\t 1020\t 1.8458501 \t0\ttrue\n",
            "(1)\t 1021\t 1.0210384 \t1\tfalse\n",
            "(1)\t 1022\t 1.0210384 \t1\tfalse\n",
            "(0)\t 1023\t 1.2906313 \t0\ttrue\n",
            "(2)\t 1024\t 1.6265182 \t0\tfalse\n",
            "(1)\t 1025\t 0.8486908 \t2\ttrue\n",
            "(2)\t 1026\t 0.8290324 \t0\tfalse\n",
            "(2)\t 1027\t 1.6932386 \t1\ttrue\n",
            "(1)\t 1028\t 1.1820858 \t2\ttrue\n",
            "(1)\t 1029\t 1.8179612 \t1\tfalse\n",
            "(1)\t 1030\t 0.76710945 \t2\ttrue\n",
            "(1)\t 1031\t 0.76710945 \t2\ttrue\n",
            "(0)\t 1032\t 1.5534818 \t0\ttrue\n",
            "(2)\t 1033\t 2.4312227 \t0\tfalse\n",
            "(2)\t 1034\t 0.9873495 \t0\tfalse\n",
            "(0)\t 1035\t 0.82301927 \t2\tfalse\n",
            "(0)\t 1036\t 1.405084 \t0\ttrue\n",
            "(0)\t 1037\t 1.917199 \t0\ttrue\n",
            "(0)\t 1038\t 1.139071 \t0\ttrue\n",
            "(1)\t 1039\t 1.0818732 \t0\tfalse\n",
            "(1)\t 1040\t 1.0818732 \t0\tfalse\n",
            "(0)\t 1041\t 1.8582844 \t0\ttrue\n",
            "(0)\t 1042\t 0.93397725 \t2\tfalse\n",
            "(1)\t 1043\t 0.99606764 \t1\tfalse\n",
            "(0)\t 1044\t 1.9101917 \t0\ttrue\n",
            "(2)\t 1045\t 1.270369 \t0\tfalse\n",
            "(1)\t 1046\t 1.3871199 \t2\ttrue\n",
            "(0)\t 1047\t 2.275355 \t0\ttrue\n",
            "(0)\t 1048\t 1.7863364 \t0\ttrue\n",
            "(0)\t 1049\t 2.4070945 \t0\ttrue\n",
            "(1)\t 1050\t 1.1752558 \t0\tfalse\n",
            "(2)\t 1051\t 1.2964659 \t1\ttrue\n",
            "(2)\t 1052\t 1.1260438 \t2\tfalse\n",
            "(1)\t 1053\t 0.9191787 \t0\tfalse\n",
            "(2)\t 1054\t 1.3444306 \t2\tfalse\n",
            "(0)\t 1055\t 1.0091285 \t0\ttrue\n",
            "(0)\t 1056\t 2.5069423 \t0\ttrue\n",
            "(0)\t 1057\t 1.8615944 \t0\ttrue\n",
            "(2)\t 1058\t 0.9349283 \t2\tfalse\n",
            "(2)\t 1059\t 0.79142296 \t2\tfalse\n",
            "(2)\t 1060\t 1.8002139 \t0\tfalse\n",
            "(0)\t 1061\t 2.292539 \t0\ttrue\n",
            "(0)\t 1062\t 2.4276948 \t0\ttrue\n",
            "(0)\t 1063\t 2.5103846 \t0\ttrue\n",
            "(1)\t 1064\t 1.1950755 \t0\tfalse\n",
            "(1)\t 1065\t 0.9925988 \t0\tfalse\n",
            "(1)\t 1066\t 0.9925988 \t0\tfalse\n",
            "(1)\t 1067\t 0.79102 \t1\tfalse\n",
            "(1)\t 1068\t 0.79102 \t1\tfalse\n",
            "(1)\t 1069\t 1.0478092 \t0\tfalse\n",
            "(0)\t 1070\t 1.6690655 \t0\ttrue\n",
            "(2)\t 1071\t 1.3012059 \t0\tfalse\n",
            "(0)\t 1072\t 1.3536513 \t0\ttrue\n",
            "(2)\t 1073\t 1.6330181 \t0\tfalse\n",
            "(0)\t 1074\t 2.058822 \t0\ttrue\n",
            "(0)\t 1075\t 2.136303 \t0\ttrue\n",
            "(1)\t 1076\t 1.8544312 \t1\tfalse\n",
            "(2)\t 1077\t 1.6475943 \t0\tfalse\n",
            "(0)\t 1078\t 2.5043302 \t0\ttrue\n",
            "(0)\t 1079\t 2.3833933 \t0\ttrue\n",
            "(2)\t 1080\t 1.1071589 \t1\ttrue\n",
            "(0)\t 1081\t 1.2207944 \t0\ttrue\n",
            "(2)\t 1082\t 1.4592414 \t0\tfalse\n",
            "(0)\t 1083\t 1.5244743 \t0\ttrue\n",
            "(2)\t 1084\t 1.1233011 \t0\tfalse\n",
            "(0)\t 1085\t 1.6894865 \t0\ttrue\n",
            "(0)\t 1086\t 2.4475849 \t0\ttrue\n",
            "(0)\t 1087\t 0.7322556 \t0\ttrue\n",
            "(0)\t 1088\t 1.0586768 \t2\tfalse\n",
            "(0)\t 1089\t 1.6212335 \t0\ttrue\n",
            "(0)\t 1090\t 1.8746647 \t0\ttrue\n",
            "(2)\t 1091\t 1.387285 \t2\tfalse\n",
            "(2)\t 1092\t 0.71028304 \t2\tfalse\n",
            "(2)\t 1093\t 0.69131017 \t0\tfalse\n",
            "(2)\t 1094\t 0.41146183 \t0\tfalse\n",
            "(2)\t 1095\t 1.1867894 \t0\tfalse\n",
            "(0)\t 1096\t 1.4793663 \t0\ttrue\n",
            "(0)\t 1097\t 1.5841656 \t0\ttrue\n",
            "(0)\t 1098\t 1.0169697 \t0\ttrue\n",
            "(0)\t 1099\t 0.8098988 \t1\tfalse\n",
            "(0)\t 1100\t 2.2496877 \t0\ttrue\n",
            "(2)\t 1101\t 1.2661593 \t0\tfalse\n",
            "(1)\t 1102\t 1.0039661 \t1\tfalse\n",
            "(2)\t 1103\t 0.98545104 \t0\tfalse\n",
            "(1)\t 1104\t 1.0593625 \t1\tfalse\n",
            "(1)\t 1105\t 1.2917904 \t2\ttrue\n",
            "(2)\t 1106\t 1.0238726 \t0\tfalse\n",
            "(1)\t 1107\t 1.0247359 \t0\tfalse\n",
            "(2)\t 1108\t 2.1801383 \t0\tfalse\n",
            "(0)\t 1109\t 1.2994236 \t0\ttrue\n",
            "(1)\t 1110\t 1.4496404 \t2\ttrue\n",
            "(0)\t 1111\t 2.5542736 \t0\ttrue\n",
            "(2)\t 1112\t 1.8158084 \t0\tfalse\n",
            "(1)\t 1113\t 1.3992614 \t2\ttrue\n",
            "(1)\t 1114\t 0.3844197 \t0\tfalse\n",
            "(2)\t 1115\t 0.86966634 \t2\tfalse\n",
            "(2)\t 1116\t 1.3223475 \t0\tfalse\n",
            "(1)\t 1117\t 1.287248 \t2\ttrue\n",
            "(2)\t 1118\t 1.5679868 \t1\ttrue\n",
            "(2)\t 1119\t 1.3227562 \t0\tfalse\n",
            "(0)\t 1120\t 0.75951797 \t2\tfalse\n",
            "(0)\t 1121\t 1.4063525 \t0\ttrue\n",
            "(0)\t 1122\t 2.3240223 \t0\ttrue\n",
            "(0)\t 1123\t 2.1717198 \t0\ttrue\n",
            "(2)\t 1124\t 2.2244737 \t0\tfalse\n",
            "(2)\t 1125\t 0.8769922 \t2\tfalse\n",
            "(2)\t 1126\t 1.0807436 \t0\tfalse\n",
            "(2)\t 1127\t 0.9291744 \t1\ttrue\n",
            "(0)\t 1128\t 0.8430519 \t2\tfalse\n",
            "(2)\t 1129\t 1.0759457 \t0\tfalse\n",
            "(2)\t 1130\t 0.91040176 \t2\tfalse\n",
            "(0)\t 1131\t 2.3754575 \t0\ttrue\n",
            "(1)\t 1132\t 1.379896 \t0\tfalse\n",
            "(2)\t 1133\t 1.6313866 \t0\tfalse\n",
            "(1)\t 1134\t 1.1967782 \t0\tfalse\n",
            "(1)\t 1135\t 1.1967782 \t0\tfalse\n",
            "(1)\t 1136\t 1.3154793 \t1\tfalse\n",
            "(1)\t 1137\t 1.6443931 \t0\tfalse\n",
            "(1)\t 1138\t 1.6443931 \t0\tfalse\n",
            "(0)\t 1139\t 1.8507059 \t0\ttrue\n",
            "(0)\t 1140\t 2.221052 \t0\ttrue\n",
            "(0)\t 1141\t 1.7267483 \t0\ttrue\n",
            "(2)\t 1142\t 1.1890508 \t0\tfalse\n",
            "(2)\t 1143\t 0.76567435 \t2\tfalse\n",
            "(2)\t 1144\t 1.2435448 \t0\tfalse\n",
            "(0)\t 1145\t 1.0364088 \t2\tfalse\n",
            "(2)\t 1146\t 1.3055243 \t0\tfalse\n",
            "(1)\t 1147\t 0.70550007 \t1\tfalse\n",
            "(0)\t 1148\t 1.0145141 \t0\ttrue\n",
            "(2)\t 1149\t 0.88440365 \t0\tfalse\n",
            "(2)\t 1150\t 2.1593757 \t0\tfalse\n",
            "(1)\t 1151\t 0.66305685 \t1\tfalse\n",
            "(0)\t 1152\t 2.3093302 \t0\ttrue\n",
            "(2)\t 1153\t 1.3979986 \t2\tfalse\n",
            "(1)\t 1154\t 1.3481708 \t1\tfalse\n",
            "(0)\t 1155\t 2.4622326 \t0\ttrue\n",
            "(1)\t 1156\t 1.6536189 \t1\tfalse\n",
            "(1)\t 1157\t 1.2263802 \t2\ttrue\n",
            "(2)\t 1158\t 1.3323029 \t0\tfalse\n",
            "(0)\t 1159\t 1.5277808 \t0\ttrue\n",
            "(1)\t 1160\t 1.5911624 \t2\ttrue\n",
            "(0)\t 1161\t 1.9592503 \t0\ttrue\n",
            "(0)\t 1162\t 0.886714 \t2\tfalse\n",
            "(2)\t 1163\t 0.9175756 \t0\tfalse\n",
            "(2)\t 1164\t 1.2430358 \t0\tfalse\n",
            "(0)\t 1165\t 1.481445 \t0\ttrue\n",
            "(0)\t 1166\t 1.1578737 \t0\ttrue\n",
            "(1)\t 1167\t 1.8688687 \t0\tfalse\n",
            "(1)\t 1168\t 1.8688687 \t0\tfalse\n",
            "(0)\t 1169\t 1.6921058 \t0\ttrue\n",
            "(0)\t 1170\t 1.5918115 \t0\ttrue\n",
            "(2)\t 1171\t 1.5509597 \t0\tfalse\n",
            "(2)\t 1172\t 1.1450611 \t0\tfalse\n",
            "(2)\t 1173\t 1.8118937 \t0\tfalse\n",
            "(0)\t 1174\t 1.4559253 \t0\ttrue\n",
            "(2)\t 1175\t 1.2026843 \t0\tfalse\n",
            "(0)\t 1176\t 1.7981313 \t0\ttrue\n",
            "(0)\t 1177\t 0.7667826 \t2\tfalse\n",
            "(0)\t 1178\t 1.2308372 \t2\tfalse\n",
            "(0)\t 1179\t 0.9271598 \t2\tfalse\n",
            "(0)\t 1180\t 0.6707316 \t2\tfalse\n",
            "(0)\t 1181\t 1.5925988 \t0\ttrue\n",
            "(1)\t 1182\t 0.70595413 \t1\tfalse\n",
            "(1)\t 1183\t 1.417446 \t0\tfalse\n",
            "(2)\t 1184\t 1.3975483 \t2\tfalse\n",
            "(2)\t 1185\t 1.5777321 \t0\tfalse\n",
            "(0)\t 1186\t 2.1309893 \t0\ttrue\n",
            "(1)\t 1187\t 1.241866 \t2\ttrue\n",
            "(2)\t 1188\t 1.1048772 \t0\tfalse\n",
            "(0)\t 1189\t 1.3394884 \t0\ttrue\n",
            "(2)\t 1190\t 1.1630422 \t0\tfalse\n",
            "(0)\t 1191\t 1.0733279 \t0\ttrue\n",
            "(1)\t 1192\t 1.2132883 \t1\tfalse\n",
            "(2)\t 1193\t 0.8263005 \t2\tfalse\n",
            "(1)\t 1194\t 1.6231034 \t1\tfalse\n",
            "(0)\t 1195\t 1.3646921 \t0\ttrue\n",
            "(0)\t 1196\t 2.119685 \t0\ttrue\n",
            "(0)\t 1197\t 0.61911094 \t2\tfalse\n",
            "(1)\t 1198\t 1.2895896 \t1\tfalse\n",
            "(2)\t 1199\t 0.4059811 \t2\tfalse\n",
            "(0)\t 1200\t 0.86497545 \t2\tfalse\n",
            "(0)\t 1201\t 1.0645301 \t0\ttrue\n",
            "(1)\t 1202\t 1.1793784 \t2\ttrue\n",
            "(2)\t 1203\t 1.8311487 \t0\tfalse\n",
            "(1)\t 1204\t 1.1860616 \t0\tfalse\n",
            "(1)\t 1205\t 1.1860616 \t0\tfalse\n",
            "(0)\t 1206\t 2.024255 \t0\ttrue\n",
            "(2)\t 1207\t 0.92520344 \t2\tfalse\n",
            "(0)\t 1208\t 1.2371217 \t0\ttrue\n",
            "(1)\t 1209\t 1.6543843 \t1\tfalse\n",
            "(2)\t 1210\t 1.4698372 \t0\tfalse\n",
            "(1)\t 1211\t 0.9344317 \t0\tfalse\n",
            "(1)\t 1212\t 1.9731998 \t1\tfalse\n",
            "(0)\t 1213\t 1.3554568 \t0\ttrue\n",
            "(2)\t 1214\t 1.5337621 \t0\tfalse\n",
            "(0)\t 1215\t 1.8952929 \t0\ttrue\n",
            "(1)\t 1216\t 2.425963 \t0\tfalse\n",
            "(1)\t 1217\t 2.425963 \t0\tfalse\n",
            "(0)\t 1218\t 2.5844738 \t0\ttrue\n",
            "(2)\t 1219\t 2.0322526 \t0\tfalse\n",
            "(1)\t 1220\t 1.0214177 \t1\tfalse\n",
            "(0)\t 1221\t 2.2070487 \t0\ttrue\n",
            "(2)\t 1222\t 1.351018 \t0\tfalse\n",
            "(0)\t 1223\t 1.1203707 \t0\ttrue\n",
            "(2)\t 1224\t 2.166822 \t0\tfalse\n",
            "(1)\t 1225\t 1.6291555 \t0\tfalse\n",
            "(1)\t 1226\t 1.6291555 \t0\tfalse\n",
            "(2)\t 1227\t 1.5803398 \t0\tfalse\n",
            "(1)\t 1228\t 0.85558486 \t2\ttrue\n",
            "(2)\t 1229\t 1.0297511 \t0\tfalse\n",
            "(0)\t 1230\t 2.6123862 \t0\ttrue\n",
            "(0)\t 1231\t 2.4250267 \t0\ttrue\n",
            "(2)\t 1232\t 1.2366838 \t0\tfalse\n",
            "(1)\t 1233\t 1.3151183 \t0\tfalse\n",
            "(1)\t 1234\t 1.3151183 \t0\tfalse\n",
            "(2)\t 1235\t 0.8435839 \t2\tfalse\n",
            "(0)\t 1236\t 1.3654568 \t0\ttrue\n",
            "(0)\t 1237\t 1.142201 \t0\ttrue\n",
            "(0)\t 1238\t 2.217382 \t0\ttrue\n",
            "(0)\t 1239\t 1.6120305 \t0\ttrue\n",
            "(1)\t 1240\t 1.503827 \t0\tfalse\n",
            "(1)\t 1241\t 1.503827 \t0\tfalse\n",
            "(1)\t 1242\t 0.8064524 \t2\ttrue\n",
            "(1)\t 1243\t 0.8064524 \t2\ttrue\n",
            "(0)\t 1244\t 1.445928 \t0\ttrue\n",
            "(2)\t 1245\t 1.2844015 \t0\tfalse\n",
            "(0)\t 1246\t 2.2966707 \t0\ttrue\n",
            "(1)\t 1247\t 1.579545 \t1\tfalse\n",
            "(1)\t 1248\t 1.8714137 \t0\tfalse\n",
            "(1)\t 1249\t 1.8714137 \t0\tfalse\n",
            "(1)\t 1250\t 0.8631727 \t0\tfalse\n",
            "(1)\t 1251\t 0.8631727 \t0\tfalse\n",
            "(1)\t 1252\t 1.1363553 \t1\tfalse\n",
            "(1)\t 1253\t 1.4274533 \t1\tfalse\n",
            "(2)\t 1254\t 1.7395136 \t0\tfalse\n",
            "(1)\t 1255\t 0.9148257 \t0\tfalse\n",
            "(2)\t 1256\t 0.98313934 \t2\tfalse\n",
            "(2)\t 1257\t 1.1673855 \t0\tfalse\n",
            "(1)\t 1258\t 1.1219757 \t0\tfalse\n",
            "(1)\t 1259\t 0.9943702 \t2\ttrue\n",
            "(2)\t 1260\t 0.92860734 \t2\tfalse\n",
            "(1)\t 1261\t 0.85952985 \t1\tfalse\n",
            "(1)\t 1262\t 0.4924227 \t2\ttrue\n",
            "(1)\t 1263\t 2.3069618 \t0\tfalse\n",
            "(2)\t 1264\t 1.312369 \t0\tfalse\n",
            "(2)\t 1265\t 0.9071277 \t2\tfalse\n",
            "(2)\t 1266\t 1.8649936 \t0\tfalse\n",
            "(1)\t 1267\t 1.5932989 \t1\tfalse\n",
            "(1)\t 1268\t 1.5919399 \t0\tfalse\n",
            "(1)\t 1269\t 1.5919399 \t0\tfalse\n",
            "(2)\t 1270\t 1.0311011 \t0\tfalse\n",
            "(0)\t 1271\t 0.94355005 \t2\tfalse\n",
            "(0)\t 1272\t 2.1733203 \t0\ttrue\n",
            "(0)\t 1273\t 2.4113472 \t0\ttrue\n",
            "(0)\t 1274\t 2.33053 \t0\ttrue\n",
            "(0)\t 1275\t 1.6021196 \t0\ttrue\n",
            "(0)\t 1276\t 2.2643485 \t0\ttrue\n",
            "(0)\t 1277\t 1.7884088 \t0\ttrue\n",
            "(1)\t 1278\t 1.4582282 \t2\ttrue\n",
            "(2)\t 1279\t 1.2205837 \t0\tfalse\n",
            "(2)\t 1280\t 1.0111578 \t0\tfalse\n",
            "(1)\t 1281\t 1.0111578 \t0\tfalse\n",
            "(1)\t 1282\t 1.6031874 \t1\tfalse\n",
            "(1)\t 1283\t 1.6235718 \t1\tfalse\n",
            "(1)\t 1284\t 1.2760192 \t2\ttrue\n",
            "(1)\t 1285\t 1.2760192 \t2\ttrue\n",
            "(1)\t 1286\t 0.9311815 \t0\tfalse\n",
            "(2)\t 1287\t 1.2374706 \t0\tfalse\n",
            "(0)\t 1288\t 2.0247707 \t0\ttrue\n",
            "(2)\t 1289\t 1.1432825 \t0\tfalse\n",
            "(2)\t 1290\t 1.774566 \t0\tfalse\n",
            "(1)\t 1291\t 0.97617555 \t0\tfalse\n",
            "(1)\t 1292\t 0.97617555 \t0\tfalse\n",
            "(0)\t 1293\t 1.6003335 \t0\ttrue\n",
            "(0)\t 1294\t 0.8832352 \t2\tfalse\n",
            "(1)\t 1295\t 1.3208371 \t2\ttrue\n",
            "(1)\t 1296\t 1.6367046 \t0\tfalse\n",
            "(1)\t 1297\t 1.6367046 \t0\tfalse\n",
            "(2)\t 1298\t 1.5346776 \t0\tfalse\n",
            "(1)\t 1299\t 0.82562387 \t2\ttrue\n",
            "(1)\t 1300\t 0.82562387 \t2\ttrue\n",
            "(2)\t 1301\t 1.560404 \t0\tfalse\n",
            "(2)\t 1302\t 2.2470007 \t0\tfalse\n",
            "(0)\t 1303\t 1.1453079 \t0\ttrue\n",
            "(2)\t 1304\t 1.1453079 \t0\tfalse\n",
            "(0)\t 1305\t 1.430434 \t0\ttrue\n",
            "(1)\t 1306\t 1.3307682 \t1\tfalse\n",
            "(1)\t 1307\t 1.0992013 \t2\ttrue\n",
            "(2)\t 1308\t 1.007565 \t0\tfalse\n",
            "(1)\t 1309\t 1.2933214 \t2\ttrue\n",
            "(2)\t 1310\t 0.97177434 \t1\ttrue\n",
            "(2)\t 1311\t 0.5891144 \t2\tfalse\n",
            "(0)\t 1312\t 1.3420135 \t0\ttrue\n",
            "(2)\t 1313\t 1.5576326 \t0\tfalse\n",
            "(0)\t 1314\t 1.6743927 \t0\ttrue\n",
            "(1)\t 1315\t 1.4263804 \t2\ttrue\n",
            "(2)\t 1316\t 1.3550167 \t2\tfalse\n",
            "(1)\t 1317\t 1.3175606 \t2\ttrue\n",
            "(2)\t 1318\t 0.83686686 \t2\tfalse\n",
            "(1)\t 1319\t 0.94117796 \t0\tfalse\n",
            "(2)\t 1320\t 0.82941324 \t0\tfalse\n",
            "(1)\t 1321\t 1.5656357 \t1\tfalse\n",
            "(2)\t 1322\t 2.1320035 \t0\tfalse\n",
            "(2)\t 1323\t 1.7771869 \t0\tfalse\n",
            "(0)\t 1324\t 1.5968007 \t0\ttrue\n",
            "(1)\t 1325\t 1.1206831 \t0\tfalse\n",
            "(2)\t 1326\t 1.1929911 \t2\tfalse\n",
            "(2)\t 1327\t 0.78528535 \t0\tfalse\n",
            "(0)\t 1328\t 1.4080222 \t0\ttrue\n",
            "(0)\t 1329\t 2.2968597 \t0\ttrue\n",
            "(1)\t 1330\t 1.9861977 \t0\tfalse\n",
            "(1)\t 1331\t 1.9861977 \t0\tfalse\n",
            "(0)\t 1332\t 1.8756806 \t0\ttrue\n",
            "(0)\t 1333\t 1.352412 \t0\ttrue\n",
            "(0)\t 1334\t 0.9439858 \t0\ttrue\n",
            "(0)\t 1335\t 1.0271761 \t0\ttrue\n",
            "(2)\t 1336\t 1.3350841 \t1\ttrue\n",
            "(0)\t 1337\t 1.0333127 \t2\tfalse\n",
            "(0)\t 1338\t 2.10089 \t0\ttrue\n",
            "(0)\t 1339\t 0.9770695 \t0\ttrue\n",
            "(2)\t 1340\t 1.6107292 \t1\ttrue\n",
            "(2)\t 1341\t 1.0367842 \t1\ttrue\n",
            "(1)\t 1342\t 1.3720391 \t2\ttrue\n",
            "(2)\t 1343\t 0.90012133 \t0\tfalse\n",
            "(0)\t 1344\t 0.8042102 \t2\tfalse\n",
            "(0)\t 1345\t 1.0881284 \t0\ttrue\n",
            "(0)\t 1346\t 1.0059398 \t2\tfalse\n",
            "(0)\t 1347\t 1.6045772 \t0\ttrue\n",
            "(0)\t 1348\t 1.3675569 \t0\ttrue\n",
            "(1)\t 1349\t 1.0627272 \t2\ttrue\n",
            "(0)\t 1350\t 2.3205497 \t0\ttrue\n",
            "(2)\t 1351\t 0.9002396 \t1\ttrue\n",
            "(0)\t 1352\t 1.1185765 \t0\ttrue\n",
            "(1)\t 1353\t 1.3449571 \t2\ttrue\n",
            "(1)\t 1354\t 0.921943 \t2\ttrue\n",
            "(2)\t 1355\t 1.6609288 \t0\tfalse\n",
            "(1)\t 1356\t 0.8804168 \t2\ttrue\n",
            "(2)\t 1357\t 0.8887948 \t2\tfalse\n",
            "(0)\t 1358\t 1.9593626 \t0\ttrue\n",
            "(0)\t 1359\t 0.9750586 \t1\tfalse\n",
            "(2)\t 1360\t 1.6351103 \t0\tfalse\n",
            "(0)\t 1361\t 2.2400148 \t0\ttrue\n",
            "(0)\t 1362\t 2.511633 \t0\ttrue\n",
            "(0)\t 1363\t 2.1660862 \t0\ttrue\n",
            "(0)\t 1364\t 2.4581208 \t0\ttrue\n",
            "(2)\t 1365\t 2.2714725 \t0\tfalse\n",
            "(2)\t 1366\t 0.9893043 \t0\tfalse\n",
            "(2)\t 1367\t 1.2829515 \t0\tfalse\n",
            "(2)\t 1368\t 0.9278672 \t0\tfalse\n",
            "(2)\t 1369\t 1.2422003 \t1\ttrue\n",
            "(0)\t 1370\t 1.6447829 \t0\ttrue\n",
            "(0)\t 1371\t 1.4261595 \t0\ttrue\n",
            "(2)\t 1372\t 1.1128695 \t2\tfalse\n",
            "(2)\t 1373\t 1.0711768 \t2\tfalse\n",
            "(2)\t 1374\t 1.7945331 \t0\tfalse\n",
            "(1)\t 1375\t 2.1663227 \t0\tfalse\n",
            "(1)\t 1376\t 2.1663227 \t0\tfalse\n",
            "(0)\t 1377\t 2.1543043 \t0\ttrue\n",
            "(2)\t 1378\t 1.4242376 \t0\tfalse\n",
            "(0)\t 1379\t 0.90338814 \t2\tfalse\n",
            "(0)\t 1380\t 1.2028984 \t0\ttrue\n",
            "(2)\t 1381\t 0.55285656 \t0\tfalse\n",
            "(0)\t 1382\t 1.0519917 \t2\tfalse\n",
            "(2)\t 1383\t 2.2200413 \t0\tfalse\n",
            "(2)\t 1384\t 1.0452623 \t0\tfalse\n",
            "(2)\t 1385\t 0.88330907 \t0\tfalse\n",
            "(0)\t 1386\t 1.9048817 \t0\ttrue\n",
            "(2)\t 1387\t 1.0737058 \t0\tfalse\n",
            "(2)\t 1388\t 1.3605225 \t0\tfalse\n",
            "(2)\t 1389\t 1.3012228 \t1\ttrue\n",
            "(2)\t 1390\t 1.0384028 \t2\tfalse\n",
            "(0)\t 1391\t 1.2121073 \t1\tfalse\n",
            "(2)\t 1392\t 0.9311082 \t0\tfalse\n",
            "(0)\t 1393\t 2.136672 \t0\ttrue\n",
            "(2)\t 1394\t 1.5683296 \t1\ttrue\n",
            "(1)\t 1395\t 1.451232 \t2\ttrue\n",
            "(2)\t 1396\t 2.3420525 \t0\tfalse\n",
            "(1)\t 1397\t 0.88386357 \t1\tfalse\n",
            "(2)\t 1398\t 0.7556493 \t2\tfalse\n",
            "(2)\t 1399\t 0.9614743 \t2\tfalse\n",
            "(2)\t 1400\t 1.0070131 \t2\tfalse\n",
            "Number of true predictions: 633\n",
            "Number of false predictions: 767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "outputId": "ddea6616-34dc-4829-ee3a-34c7dc79f877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 45.182012847965744 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "outputId": "d4d2d5d7-61fc-4ea4-92da-4000f2ffcacb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "                 \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "outputId": "92147c3f-f7fe-4010-8888-14bec04c3ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "outputId": "c00a928e-927e-4d83-d2b7-f24e017ff985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-task2tamil'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task2tamil\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task2tamil/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task2tamil/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task2tamil/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT TASK 1 MALAYALAM",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5cbdd004c28947198dee88d27ca04654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_879bb268593e4cc391160aeef15b9c01",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5ced5bc4c1fa451d8c8aacb3b48ff189",
              "IPY_MODEL_b91f8e3941ab45928e595f6878137ea4"
            ]
          }
        },
        "879bb268593e4cc391160aeef15b9c01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ced5bc4c1fa451d8c8aacb3b48ff189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4c96591998504f22a18239da1dcd8a63",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2a8403089bf4815acb9b66bea39dd95"
          }
        },
        "b91f8e3941ab45928e595f6878137ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ef510181bf0044b7b65c76300c3fd093",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 996k/996k [00:00&lt;00:00, 2.67MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0249816da50f4eba8ab3e21def45764a"
          }
        },
        "4c96591998504f22a18239da1dcd8a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2a8403089bf4815acb9b66bea39dd95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef510181bf0044b7b65c76300c3fd093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0249816da50f4eba8ab3e21def45764a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "707b8cdca310450ebbcdda421ab12f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_be21d3feb6014a2383d77dc94dfc49c2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_28e34a8714aa4fd2bb80494847954cab",
              "IPY_MODEL_e6b0d379cb4741e0a53b9412ef6ae4de"
            ]
          }
        },
        "be21d3feb6014a2383d77dc94dfc49c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28e34a8714aa4fd2bb80494847954cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2eb6d8f2d3d248c0906db3b642e09c2b",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 569,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 569,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7632bc701fa14acba58add1421bad721"
          }
        },
        "e6b0d379cb4741e0a53b9412ef6ae4de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4a293061d9f1412d907ef8b4eba1fdc7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 569/569 [00:00&lt;00:00, 21.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e07ed8c1ab62423d949993f8ab41e77c"
          }
        },
        "2eb6d8f2d3d248c0906db3b642e09c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7632bc701fa14acba58add1421bad721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a293061d9f1412d907ef8b4eba1fdc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e07ed8c1ab62423d949993f8ab41e77c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a204ed04181f4e7e8aa75156215693d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_03a238527e524b62bd531c5ba588711d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_175d31d4ce94484babfdd7a1c278f90c",
              "IPY_MODEL_5739c1c9b7714e509c92d9ea79b7621a"
            ]
          }
        },
        "03a238527e524b62bd531c5ba588711d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "175d31d4ce94484babfdd7a1c278f90c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_838ab3beaad34843b4655a5afd1f6bf8",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc5f90abe7fd496cafd55c02790633c0"
          }
        },
        "5739c1c9b7714e509c92d9ea79b7621a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e9b29d03cbd1484abb18102fd0bdbaa9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 714M/714M [00:17&lt;00:00, 41.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e13b2baa1d4541a2a2065cc968a42c2f"
          }
        },
        "838ab3beaad34843b4655a5afd1f6bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc5f90abe7fd496cafd55c02790633c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e9b29d03cbd1484abb18102fd0bdbaa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e13b2baa1d4541a2a2065cc968a42c2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT_TASK_1_MALAYALAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "aa1cb21a-b186-4dc6-8b9c-4935648bebc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 4.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=e105310743133dfd998953896fcf296dd4fc4ea33d758be2055a9876fe394a88\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "36591880-6eb1-4934-e1d3-baf2f9f18ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "150d76da-6080-4bdf-9276-267faa708ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/malayalam/task1malayalam.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1722</th>\n",
              "      <td>MAL1723</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>സിനിമയില്‍ യുക്തിക്കു ഒരു പ്രാധാന്യവും ഇല്ലെന്...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1424</th>\n",
              "      <td>MAL1425</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ഒറ്റയാന്‍മാരുടെ ജീവിതമാണ് സിദ്ദിഖ് എന്ന സംവിധാ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>645</th>\n",
              "      <td>MAL0646</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>റിയോ ഒളിമ്പിക്‌സില്‍ ഇന്ത്യയുടെ ഗുഡ്‌വില്‍ അം...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1792</th>\n",
              "      <td>MAL1793</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>അടുത്തയാഴ്ച മുഖ്യമന്ത്രിക്കും മന്ത്രിമാര്‍ക്കു...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2067</th>\n",
              "      <td>MAL2068</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ജർമനിയിലെ നോർത്തേൺ വെസ്റ്റ് ഫാളിയ ശാസ്ത്ര കലാ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>MAL0487</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>സ്ഥാനമൊഴിയും മുമ്പ് താജ്മഹൽ കാണാൻ ഒബാമയ്ക്കു മ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1122</th>\n",
              "      <td>MAL1123</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>നോവലിസ്റ്റും കവിയുമായ ജിം ഹാരിസൺ അരിസോണയിലെ വസ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1770</th>\n",
              "      <td>MAL1771</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ഉത്തര്‍പ്രദേശില്‍ ദേശീയ അന്വേഷണ ഏജന്‍സി ഉദ്യോ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2346</th>\n",
              "      <td>MAL2347</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>സൗദി സര്‍ക്കാരിനെ സമ്മര്‍ദ്ദത്തിലാക്കാനുള്ള ഇറ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>616</th>\n",
              "      <td>MAL0617</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>വീട്ടുകാര്‍ ഉംറക്ക് പോയ അവസരം നോക്കി  നാല്പ്പത...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "1722         MAL1723  ...  സിനിമയില്‍ യുക്തിക്കു ഒരു പ്രാധാന്യവും ഇല്ലെന്...\n",
              "1424         MAL1425  ...  ഒറ്റയാന്‍മാരുടെ ജീവിതമാണ് സിദ്ദിഖ് എന്ന സംവിധാ...\n",
              "645          MAL0646  ...  റിയോ ഒളിമ്പിക്‌സില്‍ ഇന്ത്യയുടെ ഗുഡ്‌വില്‍ അം...\n",
              "1792         MAL1793  ...  അടുത്തയാഴ്ച മുഖ്യമന്ത്രിക്കും മന്ത്രിമാര്‍ക്കു...\n",
              "2067         MAL2068  ...  ജർമനിയിലെ നോർത്തേൺ വെസ്റ്റ് ഫാളിയ ശാസ്ത്ര കലാ ...\n",
              "486          MAL0487  ...  സ്ഥാനമൊഴിയും മുമ്പ് താജ്മഹൽ കാണാൻ ഒബാമയ്ക്കു മ...\n",
              "1122         MAL1123  ...  നോവലിസ്റ്റും കവിയുമായ ജിം ഹാരിസൺ അരിസോണയിലെ വസ...\n",
              "1770         MAL1771  ...  ഉത്തര്‍പ്രദേശില്‍ ദേശീയ അന്വേഷണ ഏജന്‍സി ഉദ്യോ...\n",
              "2346         MAL2347  ...  സൗദി സര്‍ക്കാരിനെ സമ്മര്‍ദ്ദത്തിലാക്കാനുള്ള ഇറ...\n",
              "616          MAL0617  ...  വീട്ടുകാര്‍ ഉംറക്ക് പോയ അവസരം നോക്കി  നാല്പ്പത...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "b9e324d9-b096-478a-e6a7-2280381a1167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1884</th>\n",
              "      <td>പല ഒത്തുതീര്‍പ്പ് ഫോര്‍മുലകളും ഉയര്‍ന്നെങ്കിലു...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1647</th>\n",
              "      <td>പാകിസ്താനിലെ പഞ്ചാബ്, സിന്ധ് പ്രവിശ്യകളില്‍ നി...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2452</th>\n",
              "      <td>കാസ്ട്രോ എന്ന അതികായന്‍ ആഗോള ഭീമന്മാരെ വിരട്...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2459</th>\n",
              "      <td>മൊബൈല്‍ ഫോണുകളും ലാപ്‌ടോപ്പുകളും പൊട്ടിത്തെറിച...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1452</th>\n",
              "      <td>സര്‍വ്വകലാശാല തീരുമാനത്തിനെതിരെ ശക്തമായി പ്രതി...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1884  പല ഒത്തുതീര്‍പ്പ് ഫോര്‍മുലകളും ഉയര്‍ന്നെങ്കിലു...      0\n",
              "1647  പാകിസ്താനിലെ പഞ്ചാബ്, സിന്ധ് പ്രവിശ്യകളില്‍ നി...      0\n",
              "2452  കാസ്ട്രോ എന്ന അതികായന്‍ ആഗോള ഭീമന്മാരെ വിരട്...      0\n",
              "2459  മൊബൈല്‍ ഫോണുകളും ലാപ്‌ടോപ്പുകളും പൊട്ടിത്തെറിച...      0\n",
              "1452  സര്‍വ്വകലാശാല തീരുമാനത്തിനെതിരെ ശക്തമായി പ്രതി...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "cf1ed3f9-3d47-4f36-dfbb-5107fa875f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "5cbdd004c28947198dee88d27ca04654",
            "879bb268593e4cc391160aeef15b9c01",
            "5ced5bc4c1fa451d8c8aacb3b48ff189",
            "b91f8e3941ab45928e595f6878137ea4",
            "4c96591998504f22a18239da1dcd8a63",
            "f2a8403089bf4815acb9b66bea39dd95",
            "ef510181bf0044b7b65c76300c3fd093",
            "0249816da50f4eba8ab3e21def45764a"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cbdd004c28947198dee88d27ca04654",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=995526, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "180d592d-0130-4645-acbb-2a5dbc7166a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  റോയൽ ചലഞ്ചേഴ്സിനെ ആറു വിക്കറ്റിന് തകർത്ത് മുംബൈ വീണ്ടും വിജയവഴിയിൽ.<eol>ബാംഗ്ലൂര്‍ റോയൽ ചലഞ്ചേഴ്സിനെ മുംബൈ ആറ് വിക്കറ്റിന് തോല്‍പിച്ചു.\n",
            "Tokenized:  ['റ', '##േ', '##ായ', '##ൽ', 'ച', '##ല', '##ഞ', '##ച', '##േ', '##ഴ', '##സി', '##നെ', 'ആ', '##റ', 'വി', '##ക', '##ക', '##റ', '##റി', '##ന', 'ത', '##കർ', '##ത', '##ത', 'മ', '##ം', '##ബ', '##ൈ', 'വ', '##ീ', '##ണ', '##ടം', 'വി', '##ജ', '##യ', '##വ', '##ഴി', '##യിൽ', '.', '<', 'eo', '##l', '>', 'ബ', '##ാം', '##ഗ', '##ല', '##ര', 'റ', '##േ', '##ായ', '##ൽ', 'ച', '##ല', '##ഞ', '##ച', '##േ', '##ഴ', '##സി', '##നെ', 'മ', '##ം', '##ബ', '##ൈ', 'ആ', '##റ', 'വി', '##ക', '##ക', '##റ', '##റി', '##ന', 'ത', '##േ', '##ാല', '##പ', '##ി', '##ച', '##ച', '.']\n",
            "Token IDs:  [1360, 29400, 40542, 15080, 1339, 38847, 111389, 111386, 29400, 111399, 108628, 47639, 1322, 75301, 80054, 17896, 17896, 75301, 37054, 25344, 1348, 94330, 20854, 20854, 1357, 14885, 111397, 59009, 1364, 60434, 36089, 53144, 80054, 111388, 18395, 36877, 90865, 17878, 119, 133, 13934, 10161, 135, 1355, 25406, 111383, 38847, 23290, 1360, 29400, 40542, 15080, 1339, 38847, 111389, 111386, 29400, 111399, 108628, 47639, 1357, 14885, 111397, 59009, 1322, 75301, 80054, 17896, 17896, 75301, 37054, 25344, 1348, 29400, 79591, 111395, 15035, 111386, 111386, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "a142c77e-375c-4bec-f27a-ffff5e972b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  റോയൽ ചലഞ്ചേഴ്സിനെ ആറു വിക്കറ്റിന് തകർത്ത് മുംബൈ വീണ്ടും വിജയവഴിയിൽ.<eol>ബാംഗ്ലൂര്‍ റോയൽ ചലഞ്ചേഴ്സിനെ മുംബൈ ആറ് വിക്കറ്റിന് തോല്‍പിച്ചു.\n",
            "Token IDs: [101, 1360, 29400, 40542, 15080, 1339, 38847, 111389, 111386, 29400, 111399, 108628, 47639, 1322, 75301, 80054, 17896, 17896, 75301, 37054, 25344, 1348, 94330, 20854, 20854, 1357, 14885, 111397, 59009, 1364, 60434, 36089, 53144, 80054, 111388, 18395, 36877, 90865, 17878, 119, 133, 13934, 10161, 135, 1355, 25406, 111383, 38847, 23290, 1360, 29400, 40542, 15080, 1339, 38847, 111389, 111386, 29400, 111399, 108628, 47639, 1357, 14885, 111397, 59009, 1322, 75301, 80054, 17896, 17896, 75301, 37054, 25344, 1348, 29400, 79591, 111395, 15035, 111386, 111386, 119, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "0617841d-50d2-45b2-effb-c52632a03f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "bc6b42b5-43e5-4303-e0be-6f37c9cf15c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "be880fea-e36a-43df-b372-c7b498d8be7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "print(train_inputs)\n",
        "print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "#print(train_masks)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101   1357 111399 ...  23290  27215  21403]\n",
            " [   101   1357  25344 ...  75301  17896  71430]\n",
            " [   101   1325  20854 ...  20854  20854  23290]\n",
            " ...\n",
            " [   101  80054  36877 ... 111393  18395  47357]\n",
            " [   101   1321 111395 ...  60955  47025  93330]\n",
            " [   101   1321  36877 ...  42870  30585   1353]]\n",
            "[1 0 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "d0404824-dafe-4c06-b5e6-31ec3d11da44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "707b8cdca310450ebbcdda421ab12f14",
            "be21d3feb6014a2383d77dc94dfc49c2",
            "28e34a8714aa4fd2bb80494847954cab",
            "e6b0d379cb4741e0a53b9412ef6ae4de",
            "2eb6d8f2d3d248c0906db3b642e09c2b",
            "7632bc701fa14acba58add1421bad721",
            "4a293061d9f1412d907ef8b4eba1fdc7",
            "e07ed8c1ab62423d949993f8ab41e77c",
            "a204ed04181f4e7e8aa75156215693d3",
            "03a238527e524b62bd531c5ba588711d",
            "175d31d4ce94484babfdd7a1c278f90c",
            "5739c1c9b7714e509c92d9ea79b7621a",
            "838ab3beaad34843b4655a5afd1f6bf8",
            "bc5f90abe7fd496cafd55c02790633c0",
            "e9b29d03cbd1484abb18102fd0bdbaa9",
            "e13b2baa1d4541a2a2065cc968a42c2f"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "707b8cdca310450ebbcdda421ab12f14",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=569, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a204ed04181f4e7e8aa75156215693d3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=714314041, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "outputId": "f820d16b-176a-4b7b-b02a-0bce74be7dbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "outputId": "37350a30-2c95-4f7d-c9b5-01b54034e24b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.62\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.74\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.78\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.78\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifs2QgT9HeUe",
        "colab_type": "code",
        "outputId": "b078a15b-a800-4da3-9237-0094f13f0524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "outputId": "af383470-4990-46dc-fb05-23ef74fe482c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task1malayalam-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "outputId": "8168556c-490e-4418-fd5c-85f160a5a483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "#print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 900 test sentences...\n",
            "    DONE.\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/task1malayalam-results.txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "outputId": "8d502318-9e0a-4c2e-be48-e0f64bbff539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] ):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] ):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    # elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "    #   #print(\" Semi Paraphrase     \",end=\"\")\n",
        "    #   #count_NP+=1      \n",
        "    #   print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "    #   x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(1)\t 1\t [0.16321082 0.24623224] \t1\ttrue\n",
            "(1)\t 2\t [ 1.493887  -1.2624252] \t0\tfalse\n",
            "(1)\t 3\t [-0.89619684  1.1375929 ] \t1\ttrue\n",
            "(0)\t 4\t [-0.56694406  0.72688025] \t1\tfalse\n",
            "(0)\t 5\t [ 1.6242793 -1.3457456] \t0\ttrue\n",
            "(0)\t 6\t [-0.60103685  0.5904726 ] \t1\tfalse\n",
            "(1)\t 7\t [-0.24860007  0.5863644 ] \t1\ttrue\n",
            "(0)\t 8\t [ 1.4973912 -1.3049142] \t0\ttrue\n",
            "(0)\t 9\t [ 1.5932965 -1.3237065] \t0\ttrue\n",
            "(1)\t 10\t [-1.665229   1.6251606] \t1\ttrue\n",
            "(0)\t 11\t [0.07114942 0.24919803] \t1\tfalse\n",
            "(1)\t 12\t [ 1.4799099 -1.2228427] \t0\tfalse\n",
            "(0)\t 13\t [0.05678448 0.07627615] \t1\tfalse\n",
            "(0)\t 14\t [ 1.5225875 -1.334876 ] \t0\ttrue\n",
            "(0)\t 15\t [ 1.5135739 -1.2722533] \t0\ttrue\n",
            "(1)\t 16\t [ 1.3900368 -0.9873585] \t0\tfalse\n",
            "(0)\t 17\t [-0.47355312  0.65885913] \t1\tfalse\n",
            "(0)\t 18\t [ 1.3953091 -1.2481467] \t0\ttrue\n",
            "(1)\t 19\t [-0.25348592  0.45665157] \t1\ttrue\n",
            "(0)\t 20\t [ 1.507575  -1.4275995] \t0\ttrue\n",
            "(0)\t 21\t [ 1.6461138 -1.3552827] \t0\ttrue\n",
            "(0)\t 22\t [ 1.6584682 -1.4386401] \t0\ttrue\n",
            "(0)\t 23\t [ 0.33495545 -0.13675186] \t0\ttrue\n",
            "(0)\t 24\t [ 1.6428235 -1.3696709] \t0\ttrue\n",
            "(0)\t 25\t [ 1.6389439 -1.3952487] \t0\ttrue\n",
            "(0)\t 26\t [ 1.5800073 -1.313026 ] \t0\ttrue\n",
            "(0)\t 27\t [ 1.548149  -1.3547997] \t0\ttrue\n",
            "(1)\t 28\t [ 1.1728563 -0.7355546] \t0\tfalse\n",
            "(1)\t 29\t [-0.8282451  1.2017365] \t1\ttrue\n",
            "(0)\t 30\t [ 1.6461966 -1.2693706] \t0\ttrue\n",
            "(1)\t 31\t [ 1.4632972 -1.2498347] \t0\tfalse\n",
            "(1)\t 32\t [ 1.345063  -1.1625938] \t0\tfalse\n",
            "(1)\t 33\t [ 0.32933283 -0.06769254] \t0\tfalse\n",
            "(1)\t 34\t [ 1.6266699 -1.313664 ] \t0\tfalse\n",
            "(0)\t 35\t [ 1.2280166  -0.94575703] \t0\ttrue\n",
            "(1)\t 36\t [ 1.089489  -0.8862593] \t0\tfalse\n",
            "(1)\t 37\t [ 1.6221769 -1.3636343] \t0\tfalse\n",
            "(1)\t 38\t [-1.4798471  1.5878311] \t1\ttrue\n",
            "(0)\t 39\t [-0.70971483  0.9790322 ] \t1\tfalse\n",
            "(1)\t 40\t [ 0.29647836 -0.00625339] \t0\tfalse\n",
            "(0)\t 41\t [ 1.6218476 -1.361787 ] \t0\ttrue\n",
            "(1)\t 42\t [-1.1570942  1.4330422] \t1\ttrue\n",
            "(1)\t 43\t [-0.44784778  0.7114748 ] \t1\ttrue\n",
            "(0)\t 44\t [ 1.5802103 -1.4326681] \t0\ttrue\n",
            "(1)\t 45\t [ 0.95839155 -0.6119298 ] \t0\tfalse\n",
            "(0)\t 46\t [ 1.4890553 -1.257966 ] \t0\ttrue\n",
            "(1)\t 47\t [ 0.8925107 -0.5131163] \t0\tfalse\n",
            "(1)\t 48\t [-1.1231544  1.4634044] \t1\ttrue\n",
            "(0)\t 49\t [ 1.5877161 -1.3448653] \t0\ttrue\n",
            "(1)\t 50\t [-0.10072347  0.25019604] \t1\ttrue\n",
            "(0)\t 51\t [ 1.2926059 -1.1556585] \t0\ttrue\n",
            "(1)\t 52\t [ 0.8410606 -0.7754767] \t0\tfalse\n",
            "(0)\t 53\t [ 1.3306257 -1.1397288] \t0\ttrue\n",
            "(1)\t 54\t [-0.6820905  1.1253839] \t1\ttrue\n",
            "(1)\t 55\t [ 0.36428764 -0.08635858] \t0\tfalse\n",
            "(0)\t 56\t [ 1.5598917 -1.2742918] \t0\ttrue\n",
            "(0)\t 57\t [ 1.6337923 -1.3386389] \t0\ttrue\n",
            "(1)\t 58\t [0.09845104 0.35098067] \t1\ttrue\n",
            "(1)\t 59\t [ 1.6174786 -1.3149735] \t0\tfalse\n",
            "(0)\t 60\t [ 1.6969631 -1.3163269] \t0\ttrue\n",
            "(0)\t 61\t [ 1.2519705 -1.092494 ] \t0\ttrue\n",
            "(0)\t 62\t [-0.3316865  0.9081485] \t1\tfalse\n",
            "(0)\t 63\t [ 1.5718391 -1.3592205] \t0\ttrue\n",
            "(0)\t 64\t [ 1.5373354 -1.350117 ] \t0\ttrue\n",
            "(0)\t 65\t [ 1.3613362 -1.241092 ] \t0\ttrue\n",
            "(0)\t 66\t [-0.67853296  0.86116356] \t1\tfalse\n",
            "(0)\t 67\t [ 1.5529683 -1.2512382] \t0\ttrue\n",
            "(0)\t 68\t [-0.07986612  0.20267563] \t1\tfalse\n",
            "(0)\t 69\t [ 1.5702136 -1.2666306] \t0\ttrue\n",
            "(0)\t 70\t [ 0.50507617 -0.12195559] \t0\ttrue\n",
            "(1)\t 71\t [ 1.3862942 -1.3099242] \t0\tfalse\n",
            "(1)\t 72\t [-1.458055   1.4438084] \t1\ttrue\n",
            "(0)\t 73\t [ 1.5669873 -1.3264483] \t0\ttrue\n",
            "(0)\t 74\t [ 1.6406195 -1.4338859] \t0\ttrue\n",
            "(0)\t 75\t [ 1.5667077 -1.3020453] \t0\ttrue\n",
            "(0)\t 76\t [ 1.3379904 -1.2464693] \t0\ttrue\n",
            "(0)\t 77\t [ 1.5495602 -1.2715352] \t0\ttrue\n",
            "(1)\t 78\t [ 1.52441   -1.3455507] \t0\tfalse\n",
            "(0)\t 79\t [ 1.492255  -1.2545066] \t0\ttrue\n",
            "(1)\t 80\t [ 0.48063773 -0.19004512] \t0\tfalse\n",
            "(0)\t 81\t [ 1.5441461 -1.2415237] \t0\ttrue\n",
            "(0)\t 82\t [ 0.27464545 -0.16322057] \t0\ttrue\n",
            "(0)\t 83\t [ 1.5725181 -1.3068506] \t0\ttrue\n",
            "(1)\t 84\t [-1.3903388  1.6424594] \t1\ttrue\n",
            "(0)\t 85\t [ 0.66496676 -0.462219  ] \t0\ttrue\n",
            "(0)\t 86\t [0.16010243 0.10410103] \t0\ttrue\n",
            "(0)\t 87\t [ 1.5411669 -1.2344011] \t0\ttrue\n",
            "(0)\t 88\t [ 1.3128164 -0.9954523] \t0\ttrue\n",
            "(0)\t 89\t [ 0.8360441 -0.5959852] \t0\ttrue\n",
            "(0)\t 90\t [ 1.5893011 -1.3732077] \t0\ttrue\n",
            "(0)\t 91\t [ 1.6457549 -1.3573208] \t0\ttrue\n",
            "(0)\t 92\t [ 1.5498326 -1.2979254] \t0\ttrue\n",
            "(0)\t 93\t [ 1.5948973 -1.3679993] \t0\ttrue\n",
            "(0)\t 94\t [0.5447017  0.16395958] \t0\ttrue\n",
            "(1)\t 95\t [-1.381763   1.4514441] \t1\ttrue\n",
            "(0)\t 96\t [-0.18475887  0.24589819] \t1\tfalse\n",
            "(1)\t 97\t [-1.1267526  1.3954065] \t1\ttrue\n",
            "(0)\t 98\t [0.05341605 0.15757579] \t1\tfalse\n",
            "(0)\t 99\t [ 0.50381476 -0.60934365] \t0\ttrue\n",
            "(0)\t 100\t [ 1.4938705 -1.2649059] \t0\ttrue\n",
            "(1)\t 101\t [ 1.4469274 -1.2918321] \t0\tfalse\n",
            "(1)\t 102\t [-0.85827494  1.0598071 ] \t1\ttrue\n",
            "(0)\t 103\t [ 0.84060115 -0.2980795 ] \t0\ttrue\n",
            "(0)\t 104\t [0.28897735 0.02169633] \t0\ttrue\n",
            "(0)\t 105\t [ 0.86205333 -0.5850111 ] \t0\ttrue\n",
            "(0)\t 106\t [-0.6657791   0.90245646] \t1\tfalse\n",
            "(0)\t 107\t [-1.2554872  1.3826942] \t1\tfalse\n",
            "(1)\t 108\t [-0.04559032  0.4654235 ] \t1\ttrue\n",
            "(0)\t 109\t [ 1.2744298 -1.0146213] \t0\ttrue\n",
            "(1)\t 110\t [ 1.2689692 -1.1128831] \t0\tfalse\n",
            "(1)\t 111\t [ 1.2029519  -0.86698794] \t0\tfalse\n",
            "(0)\t 112\t [ 1.1708374 -1.0708599] \t0\ttrue\n",
            "(0)\t 113\t [ 1.5765909 -1.3251921] \t0\ttrue\n",
            "(0)\t 114\t [-0.7441394  0.8829912] \t1\tfalse\n",
            "(1)\t 115\t [-0.03915153  0.3784111 ] \t1\ttrue\n",
            "(1)\t 116\t [ 1.351795  -1.1173458] \t0\tfalse\n",
            "(1)\t 117\t [ 1.6089735 -1.3567312] \t0\tfalse\n",
            "(0)\t 118\t [ 0.44042414 -0.14463319] \t0\ttrue\n",
            "(0)\t 119\t [ 1.2555256 -1.3098099] \t0\ttrue\n",
            "(0)\t 120\t [ 1.6575565 -1.3781579] \t0\ttrue\n",
            "(0)\t 121\t [ 0.8715476 -0.7039036] \t0\ttrue\n",
            "(0)\t 122\t [ 0.84594524 -0.5498699 ] \t0\ttrue\n",
            "(0)\t 123\t [ 1.4952905 -1.2354897] \t0\ttrue\n",
            "(1)\t 124\t [0.12851806 0.067618  ] \t0\tfalse\n",
            "(1)\t 125\t [ 1.6745522 -1.327657 ] \t0\tfalse\n",
            "(1)\t 126\t [ 1.2763302 -0.9121109] \t0\tfalse\n",
            "(0)\t 127\t [ 1.6056428 -1.3329824] \t0\ttrue\n",
            "(0)\t 128\t [-0.44425446  0.5296935 ] \t1\tfalse\n",
            "(0)\t 129\t [ 1.4299105 -1.2199829] \t0\ttrue\n",
            "(0)\t 130\t [ 1.5822699 -1.338707 ] \t0\ttrue\n",
            "(0)\t 131\t [ 1.519709  -1.3376114] \t0\ttrue\n",
            "(0)\t 132\t [-0.3663884  0.1422993] \t1\tfalse\n",
            "(0)\t 133\t [ 0.72016644 -0.5753486 ] \t0\ttrue\n",
            "(0)\t 134\t [ 1.4229876 -1.1783421] \t0\ttrue\n",
            "(0)\t 135\t [0.18444817 0.00316126] \t0\ttrue\n",
            "(0)\t 136\t [ 1.5631807 -1.3579202] \t0\ttrue\n",
            "(1)\t 137\t [ 1.1909628 -1.0008435] \t0\tfalse\n",
            "(0)\t 138\t [ 1.5076354 -1.1758114] \t0\ttrue\n",
            "(1)\t 139\t [-1.0603769  1.232246 ] \t1\ttrue\n",
            "(0)\t 140\t [-0.5417807   0.89346707] \t1\tfalse\n",
            "(0)\t 141\t [ 1.5789807 -1.4098121] \t0\ttrue\n",
            "(0)\t 142\t [ 1.6000057 -1.2457578] \t0\ttrue\n",
            "(0)\t 143\t [ 0.50060844 -0.2843638 ] \t0\ttrue\n",
            "(0)\t 144\t [ 1.5796094 -1.3353994] \t0\ttrue\n",
            "(1)\t 145\t [-0.63889325  0.67670935] \t1\ttrue\n",
            "(1)\t 146\t [-0.02030871  0.11581672] \t1\ttrue\n",
            "(0)\t 147\t [ 1.6387488 -1.3646035] \t0\ttrue\n",
            "(1)\t 148\t [ 1.5185945 -1.2055109] \t0\tfalse\n",
            "(0)\t 149\t [ 0.7286388 -0.7463138] \t0\ttrue\n",
            "(0)\t 150\t [0.05580302 0.17314883] \t1\tfalse\n",
            "(0)\t 151\t [-0.3133347   0.62554294] \t1\tfalse\n",
            "(1)\t 152\t [-1.1584296  1.1711807] \t1\ttrue\n",
            "(0)\t 153\t [ 1.5054367 -1.0899359] \t0\ttrue\n",
            "(0)\t 154\t [ 1.4556189 -1.1390774] \t0\ttrue\n",
            "(1)\t 155\t [0.00765974 0.11208683] \t1\ttrue\n",
            "(0)\t 156\t [-0.65655744  0.8253961 ] \t1\tfalse\n",
            "(1)\t 157\t [-0.6633955   0.98796594] \t1\ttrue\n",
            "(0)\t 158\t [ 0.93359876 -0.59764576] \t0\ttrue\n",
            "(1)\t 159\t [ 1.4777207 -1.3345361] \t0\tfalse\n",
            "(1)\t 160\t [-1.4525003  1.5691495] \t1\ttrue\n",
            "(0)\t 161\t [ 1.0035008  -0.84240055] \t0\ttrue\n",
            "(1)\t 162\t [-0.88899    1.1576941] \t1\ttrue\n",
            "(0)\t 163\t [ 1.5285497 -1.3768158] \t0\ttrue\n",
            "(0)\t 164\t [ 0.803618   -0.42478058] \t0\ttrue\n",
            "(1)\t 165\t [ 1.3507586 -1.0326602] \t0\tfalse\n",
            "(0)\t 166\t [ 1.6774607 -1.3624251] \t0\ttrue\n",
            "(1)\t 167\t [ 0.9776102 -1.1438375] \t0\tfalse\n",
            "(0)\t 168\t [ 1.4888208 -1.3496299] \t0\ttrue\n",
            "(0)\t 169\t [ 1.7092457 -1.4085574] \t0\ttrue\n",
            "(0)\t 170\t [ 1.4394679 -1.1384935] \t0\ttrue\n",
            "(0)\t 171\t [ 0.32070172 -0.09078695] \t0\ttrue\n",
            "(1)\t 172\t [-0.8722379  1.1998138] \t1\ttrue\n",
            "(1)\t 173\t [-0.39192182  0.49087012] \t1\ttrue\n",
            "(1)\t 174\t [ 1.5017165 -1.0914233] \t0\tfalse\n",
            "(1)\t 175\t [ 1.6204821 -1.4395084] \t0\tfalse\n",
            "(1)\t 176\t [-1.599479   1.7145933] \t1\ttrue\n",
            "(0)\t 177\t [-0.6044106  0.7745925] \t1\tfalse\n",
            "(1)\t 178\t [-0.3915468  0.6480529] \t1\ttrue\n",
            "(1)\t 179\t [ 1.4947975 -1.2077048] \t0\tfalse\n",
            "(1)\t 180\t [-0.43625277  0.5098121 ] \t1\ttrue\n",
            "(1)\t 181\t [-1.7407516  1.4944699] \t1\ttrue\n",
            "(0)\t 182\t [ 0.6323301  -0.35469788] \t0\ttrue\n",
            "(0)\t 183\t [ 0.4645854  -0.38702306] \t0\ttrue\n",
            "(0)\t 184\t [-0.11272942  0.21778709] \t1\tfalse\n",
            "(0)\t 185\t [ 1.454597  -1.2084903] \t0\ttrue\n",
            "(1)\t 186\t [ 1.1654822 -1.1455649] \t0\tfalse\n",
            "(0)\t 187\t [ 0.942032   -0.63649267] \t0\ttrue\n",
            "(0)\t 188\t [0.03274557 0.10801475] \t1\tfalse\n",
            "(0)\t 189\t [ 0.6834836  -0.33151844] \t0\ttrue\n",
            "(0)\t 190\t [ 1.5143604 -1.3594549] \t0\ttrue\n",
            "(1)\t 191\t [ 1.5489426 -1.3336439] \t0\tfalse\n",
            "(0)\t 192\t [ 1.4576712 -1.1422987] \t0\ttrue\n",
            "(0)\t 193\t [ 1.5522437 -1.4058769] \t0\ttrue\n",
            "(0)\t 194\t [-0.7747555  0.9239914] \t1\tfalse\n",
            "(1)\t 195\t [ 1.2430092  -0.99768686] \t0\tfalse\n",
            "(0)\t 196\t [ 1.6632042 -1.3718982] \t0\ttrue\n",
            "(0)\t 197\t [ 1.4846361 -1.384972 ] \t0\ttrue\n",
            "(0)\t 198\t [-0.6137985  0.7518196] \t1\tfalse\n",
            "(0)\t 199\t [ 0.8086089 -0.5060905] \t0\ttrue\n",
            "(1)\t 200\t [-0.87395    0.9520966] \t1\ttrue\n",
            "(1)\t 201\t [ 1.0926961 -1.0226634] \t0\tfalse\n",
            "(1)\t 202\t [ 1.5991924 -1.2969795] \t0\tfalse\n",
            "(0)\t 203\t [ 1.6955514 -1.2990294] \t0\ttrue\n",
            "(0)\t 204\t [-0.1330902  0.3711318] \t1\tfalse\n",
            "(1)\t 205\t [-0.15875143  0.6224848 ] \t1\ttrue\n",
            "(0)\t 206\t [ 1.6782417 -1.3856177] \t0\ttrue\n",
            "(0)\t 207\t [ 1.1559099 -1.055309 ] \t0\ttrue\n",
            "(0)\t 208\t [ 1.6120927 -1.3880891] \t0\ttrue\n",
            "(1)\t 209\t [-0.04743866  0.36044496] \t1\ttrue\n",
            "(0)\t 210\t [ 1.6528349 -1.3197178] \t0\ttrue\n",
            "(0)\t 211\t [ 1.54359   -1.3670454] \t0\ttrue\n",
            "(0)\t 212\t [ 1.6315966 -1.319294 ] \t0\ttrue\n",
            "(1)\t 213\t [-1.58846    1.5591474] \t1\ttrue\n",
            "(1)\t 214\t [0.10572375 0.13682336] \t1\ttrue\n",
            "(1)\t 215\t [ 1.4334326 -1.3768744] \t0\tfalse\n",
            "(1)\t 216\t [-0.82010776  1.0783069 ] \t1\ttrue\n",
            "(0)\t 217\t [ 1.4282948 -1.1019754] \t0\ttrue\n",
            "(0)\t 218\t [ 1.0506685 -0.7181277] \t0\ttrue\n",
            "(1)\t 219\t [-1.296263   1.5027676] \t1\ttrue\n",
            "(1)\t 220\t [ 1.6092346 -1.4085746] \t0\tfalse\n",
            "(1)\t 221\t [ 1.4459414 -1.0774192] \t0\tfalse\n",
            "(1)\t 222\t [ 0.6002041  -0.27625954] \t0\tfalse\n",
            "(1)\t 223\t [-0.15761715  0.50209075] \t1\ttrue\n",
            "(0)\t 224\t [ 1.6477492 -1.3631365] \t0\ttrue\n",
            "(0)\t 225\t [ 1.6211984 -1.3650768] \t0\ttrue\n",
            "(1)\t 226\t [-1.4571711  1.6771433] \t1\ttrue\n",
            "(0)\t 227\t [ 1.3662004 -1.2412126] \t0\ttrue\n",
            "(0)\t 228\t [ 1.5998774 -1.3629987] \t0\ttrue\n",
            "(0)\t 229\t [ 1.3098097 -0.9965906] \t0\ttrue\n",
            "(1)\t 230\t [-1.7409995  1.5470128] \t1\ttrue\n",
            "(1)\t 231\t [ 1.2939086 -1.1409901] \t0\tfalse\n",
            "(1)\t 232\t [-0.84891427  0.80124503] \t1\ttrue\n",
            "(0)\t 233\t [ 1.147634  -1.0464706] \t0\ttrue\n",
            "(0)\t 234\t [ 1.4436321 -1.2044064] \t0\ttrue\n",
            "(1)\t 235\t [-0.20907247  0.9532347 ] \t1\ttrue\n",
            "(0)\t 236\t [ 1.6551201 -1.3761668] \t0\ttrue\n",
            "(1)\t 237\t [-0.88863665  1.1001135 ] \t1\ttrue\n",
            "(1)\t 238\t [-1.4151685  1.5056304] \t1\ttrue\n",
            "(1)\t 239\t [ 0.80213654 -0.41661656] \t0\tfalse\n",
            "(0)\t 240\t [ 0.9860529 -0.5247367] \t0\ttrue\n",
            "(0)\t 241\t [ 1.1128858  -0.97915477] \t0\ttrue\n",
            "(0)\t 242\t [ 1.2288065 -1.0496526] \t0\ttrue\n",
            "(1)\t 243\t [ 1.3775893 -1.146165 ] \t0\tfalse\n",
            "(0)\t 244\t [ 1.594604  -1.3011057] \t0\ttrue\n",
            "(0)\t 245\t [ 1.2945849 -1.2957458] \t0\ttrue\n",
            "(0)\t 246\t [ 1.4348133 -1.1670706] \t0\ttrue\n",
            "(0)\t 247\t [-1.4953392  1.3396804] \t1\tfalse\n",
            "(1)\t 248\t [ 1.4785717 -1.2890046] \t0\tfalse\n",
            "(1)\t 249\t [ 1.5452961 -1.2337788] \t0\tfalse\n",
            "(1)\t 250\t [ 0.6991571  -0.25464582] \t0\tfalse\n",
            "(1)\t 251\t [-1.6155696  1.5188124] \t1\ttrue\n",
            "(0)\t 252\t [-0.8857539  1.2630798] \t1\tfalse\n",
            "(1)\t 253\t [-1.5447271  1.5794159] \t1\ttrue\n",
            "(1)\t 254\t [-0.7670245  1.1855175] \t1\ttrue\n",
            "(1)\t 255\t [-0.09782223  0.2554854 ] \t1\ttrue\n",
            "(0)\t 256\t [ 1.3575776 -1.1532743] \t0\ttrue\n",
            "(0)\t 257\t [-0.7334884  1.0399132] \t1\tfalse\n",
            "(0)\t 258\t [ 1.1007233 -0.9385209] \t0\ttrue\n",
            "(1)\t 259\t [-0.04263109  0.38598204] \t1\ttrue\n",
            "(1)\t 260\t [ 0.26621795 -0.03018754] \t0\tfalse\n",
            "(1)\t 261\t [-1.020921   1.1948187] \t1\ttrue\n",
            "(0)\t 262\t [-0.6204275   0.61180794] \t1\tfalse\n",
            "(0)\t 263\t [ 1.4640105 -1.194474 ] \t0\ttrue\n",
            "(0)\t 264\t [-0.13845387  0.37206885] \t1\tfalse\n",
            "(1)\t 265\t [-1.2091175  1.5289217] \t1\ttrue\n",
            "(0)\t 266\t [ 1.4428337 -1.1336765] \t0\ttrue\n",
            "(0)\t 267\t [ 1.2738018 -1.0273949] \t0\ttrue\n",
            "(0)\t 268\t [ 0.6237987  -0.06034545] \t0\ttrue\n",
            "(0)\t 269\t [ 0.4740438  -0.26855016] \t0\ttrue\n",
            "(0)\t 270\t [-0.7752304  0.8821478] \t1\tfalse\n",
            "(0)\t 271\t [ 1.4311656 -1.1939178] \t0\ttrue\n",
            "(0)\t 272\t [ 0.68017054 -0.36166257] \t0\ttrue\n",
            "(1)\t 273\t [-0.1811684   0.30516765] \t1\ttrue\n",
            "(0)\t 274\t [-0.26693344  0.8222541 ] \t1\tfalse\n",
            "(0)\t 275\t [-0.49148363  0.68405163] \t1\tfalse\n",
            "(0)\t 276\t [ 1.3852893 -1.239979 ] \t0\ttrue\n",
            "(1)\t 277\t [-0.99630785  1.0220069 ] \t1\ttrue\n",
            "(1)\t 278\t [ 0.5816085  -0.32078838] \t0\tfalse\n",
            "(1)\t 279\t [-1.7025362  1.4353597] \t1\ttrue\n",
            "(1)\t 280\t [-1.1840984  1.5188293] \t1\ttrue\n",
            "(0)\t 281\t [ 1.4526687 -1.0954527] \t0\ttrue\n",
            "(0)\t 282\t [ 1.4938707 -1.2914672] \t0\ttrue\n",
            "(0)\t 283\t [ 0.65377784 -0.3526165 ] \t0\ttrue\n",
            "(1)\t 284\t [-1.0986913  1.4653357] \t1\ttrue\n",
            "(1)\t 285\t [-1.6897925  1.6066777] \t1\ttrue\n",
            "(1)\t 286\t [-1.424067   1.6680602] \t1\ttrue\n",
            "(0)\t 287\t [-1.4550014  1.4760307] \t1\tfalse\n",
            "(1)\t 288\t [-1.7808604  1.6419021] \t1\ttrue\n",
            "(1)\t 289\t [-1.3699789  1.584377 ] \t1\ttrue\n",
            "(0)\t 290\t [ 0.36395058 -0.13237368] \t0\ttrue\n",
            "(1)\t 291\t [ 1.5573428 -1.1706727] \t0\tfalse\n",
            "(1)\t 292\t [ 1.6260467 -1.1576164] \t0\tfalse\n",
            "(1)\t 293\t [0.02787823 0.5999666 ] \t1\ttrue\n",
            "(0)\t 294\t [ 1.5804064 -1.2920952] \t0\ttrue\n",
            "(1)\t 295\t [ 0.42770085 -0.02861395] \t0\tfalse\n",
            "(0)\t 296\t [ 1.4749892 -1.2543375] \t0\ttrue\n",
            "(0)\t 297\t [ 0.5110511 -0.2622692] \t0\ttrue\n",
            "(0)\t 298\t [-0.27660555  0.3470934 ] \t1\tfalse\n",
            "(0)\t 299\t [ 1.5250497 -1.2518368] \t0\ttrue\n",
            "(0)\t 300\t [ 0.78241694 -0.52265143] \t0\ttrue\n",
            "(1)\t 301\t [ 1.0837095  -0.88047874] \t0\tfalse\n",
            "(0)\t 302\t [ 0.91215485 -0.6074113 ] \t0\ttrue\n",
            "(0)\t 303\t [ 1.4397382 -1.4271257] \t0\ttrue\n",
            "(0)\t 304\t [ 1.4118044 -1.1276356] \t0\ttrue\n",
            "(1)\t 305\t [ 1.6198413 -1.4347634] \t0\tfalse\n",
            "(1)\t 306\t [-0.65944433  0.87890685] \t1\ttrue\n",
            "(1)\t 307\t [ 1.3870928 -1.2630086] \t0\tfalse\n",
            "(1)\t 308\t [-0.60315734  0.73078716] \t1\ttrue\n",
            "(1)\t 309\t [-0.3912456  0.5463324] \t1\ttrue\n",
            "(1)\t 310\t [ 1.4283801 -1.2567556] \t0\tfalse\n",
            "(1)\t 311\t [ 1.6647573 -1.3904384] \t0\tfalse\n",
            "(1)\t 312\t [ 0.58638275 -0.26684648] \t0\tfalse\n",
            "(1)\t 313\t [ 1.4355634 -1.3327441] \t0\tfalse\n",
            "(0)\t 314\t [ 1.3610984 -1.1668754] \t0\ttrue\n",
            "(0)\t 315\t [ 1.2101697 -1.0998069] \t0\ttrue\n",
            "(0)\t 316\t [ 1.4383254 -1.2259605] \t0\ttrue\n",
            "(0)\t 317\t [ 1.5548117 -1.3347455] \t0\ttrue\n",
            "(0)\t 318\t [-0.00662285  0.3322304 ] \t1\tfalse\n",
            "(0)\t 319\t [ 1.5916114 -1.4279516] \t0\ttrue\n",
            "(1)\t 320\t [ 1.432787  -1.2720556] \t0\tfalse\n",
            "(0)\t 321\t [ 1.1501248 -1.010264 ] \t0\ttrue\n",
            "(0)\t 322\t [-0.02913642  0.16801858] \t1\tfalse\n",
            "(0)\t 323\t [-0.7846772  0.9048575] \t1\tfalse\n",
            "(1)\t 324\t [ 1.5156851 -1.2110604] \t0\tfalse\n",
            "(1)\t 325\t [-1.7654773  1.6279697] \t1\ttrue\n",
            "(0)\t 326\t [ 1.5707902 -1.3412235] \t0\ttrue\n",
            "(0)\t 327\t [ 1.5687463 -1.4127439] \t0\ttrue\n",
            "(0)\t 328\t [ 1.6195246 -1.3343794] \t0\ttrue\n",
            "(0)\t 329\t [ 1.5727642 -1.3222382] \t0\ttrue\n",
            "(0)\t 330\t [ 0.4435038  -0.15486616] \t0\ttrue\n",
            "(0)\t 331\t [ 0.9320152 -0.7200358] \t0\ttrue\n",
            "(0)\t 332\t [ 1.5806448 -1.4786098] \t0\ttrue\n",
            "(1)\t 333\t [ 1.5511727 -1.3109473] \t0\tfalse\n",
            "(1)\t 334\t [-1.4527639  1.5816071] \t1\ttrue\n",
            "(0)\t 335\t [-0.20549347  0.26619214] \t1\tfalse\n",
            "(1)\t 336\t [-1.5682268  1.5451641] \t1\ttrue\n",
            "(0)\t 337\t [ 1.5193089 -1.3704436] \t0\ttrue\n",
            "(0)\t 338\t [ 0.85812134 -0.7280298 ] \t0\ttrue\n",
            "(1)\t 339\t [ 1.3090084 -1.0084987] \t0\tfalse\n",
            "(0)\t 340\t [-0.2720906   0.51750946] \t1\tfalse\n",
            "(1)\t 341\t [ 1.564795 -1.205718] \t0\tfalse\n",
            "(1)\t 342\t [ 1.2571758 -0.879473 ] \t0\tfalse\n",
            "(1)\t 343\t [-0.53073317  0.8850031 ] \t1\ttrue\n",
            "(1)\t 344\t [ 0.8927957  -0.48392275] \t0\tfalse\n",
            "(1)\t 345\t [-0.56510794  0.9816156 ] \t1\ttrue\n",
            "(1)\t 346\t [-0.72039056  0.8128314 ] \t1\ttrue\n",
            "(1)\t 347\t [-1.7494862  1.570031 ] \t1\ttrue\n",
            "(0)\t 348\t [-1.0962155  1.3404226] \t1\tfalse\n",
            "(0)\t 349\t [-0.10603575  0.42215332] \t1\tfalse\n",
            "(0)\t 350\t [ 1.5675614 -1.248833 ] \t0\ttrue\n",
            "(0)\t 351\t [ 0.5752659  -0.02784419] \t0\ttrue\n",
            "(1)\t 352\t [-0.31442457  0.5317268 ] \t1\ttrue\n",
            "(1)\t 353\t [-1.7195408  1.622499 ] \t1\ttrue\n",
            "(0)\t 354\t [-0.9091909  1.1930076] \t1\tfalse\n",
            "(0)\t 355\t [ 1.5754387 -1.3182293] \t0\ttrue\n",
            "(1)\t 356\t [-0.67217195  1.0616663 ] \t1\ttrue\n",
            "(1)\t 357\t [-1.0075493  1.4511399] \t1\ttrue\n",
            "(0)\t 358\t [-1.0645525  1.208497 ] \t1\tfalse\n",
            "(1)\t 359\t [ 0.60122436 -0.18744978] \t0\tfalse\n",
            "(1)\t 360\t [ 1.4737532 -1.28631  ] \t0\tfalse\n",
            "(1)\t 361\t [ 1.216626  -0.9307077] \t0\tfalse\n",
            "(0)\t 362\t [ 0.46320117 -0.00354595] \t0\ttrue\n",
            "(1)\t 363\t [ 1.3926756 -1.2352799] \t0\tfalse\n",
            "(1)\t 364\t [ 1.3175951 -1.1671128] \t0\tfalse\n",
            "(1)\t 365\t [-1.5392808  1.5181516] \t1\ttrue\n",
            "(1)\t 366\t [ 0.3554807  -0.09182513] \t0\tfalse\n",
            "(1)\t 367\t [-1.335205   1.6654875] \t1\ttrue\n",
            "(1)\t 368\t [-1.4175947  1.6365799] \t1\ttrue\n",
            "(0)\t 369\t [ 1.4433773 -1.27329  ] \t0\ttrue\n",
            "(1)\t 370\t [-1.2901273  1.6336858] \t1\ttrue\n",
            "(1)\t 371\t [ 1.2571375 -0.957659 ] \t0\tfalse\n",
            "(0)\t 372\t [ 1.7046729 -1.3723493] \t0\ttrue\n",
            "(0)\t 373\t [ 1.2351843 -1.0843229] \t0\ttrue\n",
            "(1)\t 374\t [ 1.2792532 -1.1440457] \t0\tfalse\n",
            "(1)\t 375\t [ 1.4643984 -1.2430875] \t0\tfalse\n",
            "(1)\t 376\t [ 1.3122473 -1.0648847] \t0\tfalse\n",
            "(0)\t 377\t [-0.09206375  0.42667267] \t1\tfalse\n",
            "(0)\t 378\t [0.09672428 0.02429616] \t0\ttrue\n",
            "(1)\t 379\t [-0.90157944  1.3136446 ] \t1\ttrue\n",
            "(0)\t 380\t [-0.04706598  0.2991999 ] \t1\tfalse\n",
            "(0)\t 381\t [-0.75800765  0.7063312 ] \t1\tfalse\n",
            "(1)\t 382\t [-0.8608332  1.336324 ] \t1\ttrue\n",
            "(0)\t 383\t [ 1.3835702 -1.2495868] \t0\ttrue\n",
            "(0)\t 384\t [-1.3289132  1.4449613] \t1\tfalse\n",
            "(1)\t 385\t [ 1.2716945 -1.0589184] \t0\tfalse\n",
            "(0)\t 386\t [ 1.4954028 -1.3382431] \t0\ttrue\n",
            "(0)\t 387\t [ 1.6653589 -1.4279866] \t0\ttrue\n",
            "(1)\t 388\t [-0.19981357  0.44349742] \t1\ttrue\n",
            "(0)\t 389\t [-0.34754497  0.59983635] \t1\tfalse\n",
            "(0)\t 390\t [ 1.562347  -1.2503679] \t0\ttrue\n",
            "(1)\t 391\t [-0.93275225  1.2670851 ] \t1\ttrue\n",
            "(1)\t 392\t [ 1.3954788 -1.1454885] \t0\tfalse\n",
            "(1)\t 393\t [-1.1954947  1.1247891] \t1\ttrue\n",
            "(1)\t 394\t [-1.2176677  1.3642004] \t1\ttrue\n",
            "(1)\t 395\t [-0.40378392  0.5824941 ] \t1\ttrue\n",
            "(0)\t 396\t [ 1.562344  -1.4017843] \t0\ttrue\n",
            "(1)\t 397\t [-1.4677724  1.5464054] \t1\ttrue\n",
            "(1)\t 398\t [ 1.41884   -1.2253588] \t0\tfalse\n",
            "(1)\t 399\t [-0.46891326  0.7841613 ] \t1\ttrue\n",
            "(1)\t 400\t [-0.02150129  0.25442293] \t1\ttrue\n",
            "(1)\t 401\t [ 0.69787586 -0.5142046 ] \t0\tfalse\n",
            "(0)\t 402\t [ 1.2065219 -1.1022865] \t0\ttrue\n",
            "(0)\t 403\t [ 1.4681377 -1.2200024] \t0\ttrue\n",
            "(0)\t 404\t [-0.2091293   0.36631727] \t1\tfalse\n",
            "(1)\t 405\t [ 0.5115356 -0.2251081] \t0\tfalse\n",
            "(1)\t 406\t [-0.36251444  0.68069303] \t1\ttrue\n",
            "(0)\t 407\t [ 1.5694654 -1.3531184] \t0\ttrue\n",
            "(1)\t 408\t [ 0.8266071  -0.68951774] \t0\tfalse\n",
            "(0)\t 409\t [-0.8078602   0.90792096] \t1\tfalse\n",
            "(1)\t 410\t [-1.2149637  1.3262444] \t1\ttrue\n",
            "(1)\t 411\t [-1.7707211  1.5301521] \t1\ttrue\n",
            "(1)\t 412\t [ 1.1514587 -1.0046982] \t0\tfalse\n",
            "(1)\t 413\t [-1.4016098  1.4638358] \t1\ttrue\n",
            "(1)\t 414\t [ 1.4191861 -1.2171236] \t0\tfalse\n",
            "(0)\t 415\t [ 1.4905906 -1.033236 ] \t0\ttrue\n",
            "(1)\t 416\t [-0.96940583  0.9670554 ] \t1\ttrue\n",
            "(0)\t 417\t [ 1.4053326 -1.0914607] \t0\ttrue\n",
            "(1)\t 418\t [-1.378042   1.3879164] \t1\ttrue\n",
            "(1)\t 419\t [-0.26269823  0.25899062] \t1\ttrue\n",
            "(1)\t 420\t [-0.42474362  0.5436208 ] \t1\ttrue\n",
            "(1)\t 421\t [ 1.394999  -1.1306939] \t0\tfalse\n",
            "(1)\t 422\t [ 1.5565648 -1.1115947] \t0\tfalse\n",
            "(1)\t 423\t [ 1.1817567 -0.8993821] \t0\tfalse\n",
            "(1)\t 424\t [-0.15353149  0.3350505 ] \t1\ttrue\n",
            "(1)\t 425\t [-0.6063816   0.74341595] \t1\ttrue\n",
            "(0)\t 426\t [-0.42238182  0.6073909 ] \t1\tfalse\n",
            "(0)\t 427\t [ 1.3021824 -1.2254721] \t0\ttrue\n",
            "(1)\t 428\t [ 0.3385009  -0.16880931] \t0\tfalse\n",
            "(1)\t 429\t [ 1.584172  -1.2084305] \t0\tfalse\n",
            "(0)\t 430\t [ 1.7158755 -1.3044239] \t0\ttrue\n",
            "(1)\t 431\t [-0.82197076  0.9632163 ] \t1\ttrue\n",
            "(1)\t 432\t [-0.786173   0.8759025] \t1\ttrue\n",
            "(0)\t 433\t [ 0.8499948  -0.68691236] \t0\ttrue\n",
            "(1)\t 434\t [ 1.4108121 -1.0957661] \t0\tfalse\n",
            "(1)\t 435\t [ 0.9104675  -0.90739167] \t0\tfalse\n",
            "(0)\t 436\t [ 1.6334219 -1.3401496] \t0\ttrue\n",
            "(1)\t 437\t [ 1.1366942  -0.77769953] \t0\tfalse\n",
            "(1)\t 438\t [-0.78856516  0.82188594] \t1\ttrue\n",
            "(1)\t 439\t [-0.67259884  1.1904366 ] \t1\ttrue\n",
            "(1)\t 440\t [0.4771878  0.02584661] \t0\tfalse\n",
            "(1)\t 441\t [-0.7573075  0.9264655] \t1\ttrue\n",
            "(0)\t 442\t [ 1.4048792 -1.3401748] \t0\ttrue\n",
            "(1)\t 443\t [-1.6456249  1.6480281] \t1\ttrue\n",
            "(0)\t 444\t [ 1.600249 -1.293354] \t0\ttrue\n",
            "(1)\t 445\t [ 1.3900447 -1.2145407] \t0\tfalse\n",
            "(1)\t 446\t [ 1.6334895 -1.3455646] \t0\tfalse\n",
            "(0)\t 447\t [-0.9287615  1.1308511] \t1\tfalse\n",
            "(1)\t 448\t [ 0.92647445 -0.81440246] \t0\tfalse\n",
            "(0)\t 449\t [ 1.3880887 -1.2979313] \t0\ttrue\n",
            "(1)\t 450\t [0.08132922 0.20537542] \t1\ttrue\n",
            "(1)\t 451\t [ 0.6832578 -0.5243911] \t0\tfalse\n",
            "(1)\t 452\t [ 0.63049567 -0.31025398] \t0\tfalse\n",
            "(0)\t 453\t [-0.64588904  0.7616191 ] \t1\tfalse\n",
            "(1)\t 454\t [ 1.4096849 -1.0392435] \t0\tfalse\n",
            "(0)\t 455\t [ 1.2065854 -1.0409051] \t0\ttrue\n",
            "(1)\t 456\t [-1.5568881  1.5468051] \t1\ttrue\n",
            "(0)\t 457\t [ 1.4305124 -1.2771449] \t0\ttrue\n",
            "(1)\t 458\t [ 1.3753715 -1.0119078] \t0\tfalse\n",
            "(1)\t 459\t [-0.53611034  0.76482475] \t1\ttrue\n",
            "(1)\t 460\t [-0.21810743  0.5560457 ] \t1\ttrue\n",
            "(1)\t 461\t [-1.4798888  1.3812357] \t1\ttrue\n",
            "(1)\t 462\t [-1.4803042  1.4114254] \t1\ttrue\n",
            "(1)\t 463\t [-0.73999166  0.8338866 ] \t1\ttrue\n",
            "(1)\t 464\t [-0.5204817  0.6631688] \t1\ttrue\n",
            "(1)\t 465\t [-0.76879644  1.0045277 ] \t1\ttrue\n",
            "(1)\t 466\t [ 1.0825701  -0.80530596] \t0\tfalse\n",
            "(1)\t 467\t [-0.55985546  0.7716431 ] \t1\ttrue\n",
            "(1)\t 468\t [-0.02389031  0.19695878] \t1\ttrue\n",
            "(0)\t 469\t [ 1.6353129 -1.3458138] \t0\ttrue\n",
            "(1)\t 470\t [ 1.5791519 -1.3531432] \t0\tfalse\n",
            "(0)\t 471\t [0.1915631 0.2361595] \t1\tfalse\n",
            "(0)\t 472\t [ 1.1677394 -0.9029109] \t0\ttrue\n",
            "(0)\t 473\t [ 0.4356051  -0.31234646] \t0\ttrue\n",
            "(0)\t 474\t [-0.43095544  0.80009574] \t1\tfalse\n",
            "(0)\t 475\t [ 1.4332024 -1.1846914] \t0\ttrue\n",
            "(0)\t 476\t [0.09381527 0.08858623] \t0\ttrue\n",
            "(0)\t 477\t [ 1.6328111 -1.3820341] \t0\ttrue\n",
            "(1)\t 478\t [-1.6091055  1.4265113] \t1\ttrue\n",
            "(0)\t 479\t [ 1.5845561 -1.3061528] \t0\ttrue\n",
            "(0)\t 480\t [ 1.3075129 -1.2632895] \t0\ttrue\n",
            "(0)\t 481\t [ 0.98525774 -0.99638927] \t0\ttrue\n",
            "(1)\t 482\t [-1.446248   1.4050198] \t1\ttrue\n",
            "(0)\t 483\t [ 1.6290472 -1.2955431] \t0\ttrue\n",
            "(0)\t 484\t [-0.5449634  0.9281689] \t1\tfalse\n",
            "(1)\t 485\t [-1.4151597  1.4260616] \t1\ttrue\n",
            "(1)\t 486\t [ 1.631283  -1.3429371] \t0\tfalse\n",
            "(1)\t 487\t [-0.5509551  0.8546299] \t1\ttrue\n",
            "(0)\t 488\t [ 1.4746541 -1.3734393] \t0\ttrue\n",
            "(0)\t 489\t [ 1.0621368 -0.7340344] \t0\ttrue\n",
            "(1)\t 490\t [ 1.0385236 -0.5870837] \t0\tfalse\n",
            "(0)\t 491\t [ 1.3372117 -0.9808664] \t0\ttrue\n",
            "(0)\t 492\t [ 0.46688458 -0.23607972] \t0\ttrue\n",
            "(0)\t 493\t [0.14343524 0.5130624 ] \t1\tfalse\n",
            "(1)\t 494\t [ 0.6141072  -0.48328316] \t0\tfalse\n",
            "(1)\t 495\t [ 0.5019449  -0.05495271] \t0\tfalse\n",
            "(1)\t 496\t [ 1.1408665 -1.0944961] \t0\tfalse\n",
            "(1)\t 497\t [-0.7540051   0.98097646] \t1\ttrue\n",
            "(0)\t 498\t [ 1.5079303 -1.2548542] \t0\ttrue\n",
            "(1)\t 499\t [ 1.579145  -1.3378828] \t0\tfalse\n",
            "(1)\t 500\t [ 1.2600977 -0.9072207] \t0\tfalse\n",
            "(1)\t 501\t [-0.5925262  1.1384982] \t1\ttrue\n",
            "(0)\t 502\t [-0.3299073   0.83620685] \t1\tfalse\n",
            "(0)\t 503\t [ 1.3602365 -1.1411846] \t0\ttrue\n",
            "(1)\t 504\t [0.26049846 0.26423004] \t1\ttrue\n",
            "(0)\t 505\t [ 1.5434332 -1.3837152] \t0\ttrue\n",
            "(1)\t 506\t [-0.84525144  1.2715639 ] \t1\ttrue\n",
            "(0)\t 507\t [ 1.4547281 -1.2483447] \t0\ttrue\n",
            "(0)\t 508\t [ 1.3074715 -1.1623316] \t0\ttrue\n",
            "(0)\t 509\t [ 1.2474668 -1.0626631] \t0\ttrue\n",
            "(1)\t 510\t [-0.86334103  1.017593  ] \t1\ttrue\n",
            "(1)\t 511\t [ 1.3509824 -1.1779296] \t0\tfalse\n",
            "(0)\t 512\t [ 1.554574 -1.272183] \t0\ttrue\n",
            "(1)\t 513\t [-1.705665  1.562773] \t1\ttrue\n",
            "(1)\t 514\t [-0.14066537  0.44483334] \t1\ttrue\n",
            "(1)\t 515\t [-0.2708781  0.3946004] \t1\ttrue\n",
            "(1)\t 516\t [-0.7948957  1.2733178] \t1\ttrue\n",
            "(0)\t 517\t [ 1.5217365 -1.1856391] \t0\ttrue\n",
            "(0)\t 518\t [ 1.5257545 -1.2944777] \t0\ttrue\n",
            "(1)\t 519\t [-0.13848197  0.60483414] \t1\ttrue\n",
            "(1)\t 520\t [0.17465921 0.11051229] \t0\tfalse\n",
            "(1)\t 521\t [ 0.37255573 -0.07754491] \t0\tfalse\n",
            "(1)\t 522\t [-1.6302893  1.5190687] \t1\ttrue\n",
            "(1)\t 523\t [-1.804251   1.6039886] \t1\ttrue\n",
            "(0)\t 524\t [ 1.5321383 -1.2625957] \t0\ttrue\n",
            "(0)\t 525\t [ 1.0273678  -0.89489746] \t0\ttrue\n",
            "(1)\t 526\t [0.08927345 0.2744758 ] \t1\ttrue\n",
            "(1)\t 527\t [ 1.1027474  -0.99086595] \t0\tfalse\n",
            "(1)\t 528\t [ 1.5336714 -1.2216983] \t0\tfalse\n",
            "(1)\t 529\t [-1.7240099  1.5359101] \t1\ttrue\n",
            "(0)\t 530\t [-0.61090326  1.0500488 ] \t1\tfalse\n",
            "(0)\t 531\t [-0.49704343  0.6907552 ] \t1\tfalse\n",
            "(0)\t 532\t [ 0.7153937  -0.42943013] \t0\ttrue\n",
            "(1)\t 533\t [-0.90893483  1.4535049 ] \t1\ttrue\n",
            "(1)\t 534\t [ 1.5475667 -1.2595682] \t0\tfalse\n",
            "(0)\t 535\t [ 1.3891413 -1.3341968] \t0\ttrue\n",
            "(1)\t 536\t [ 0.9086877  -0.56464773] \t0\tfalse\n",
            "(0)\t 537\t [ 1.5362332 -1.4210962] \t0\ttrue\n",
            "(0)\t 538\t [ 1.5195124 -1.3878813] \t0\ttrue\n",
            "(0)\t 539\t [ 1.5553954 -1.3612719] \t0\ttrue\n",
            "(0)\t 540\t [ 1.4486823 -1.2804971] \t0\ttrue\n",
            "(0)\t 541\t [ 1.5160766 -1.4008667] \t0\ttrue\n",
            "(0)\t 542\t [ 1.3203291 -1.0391366] \t0\ttrue\n",
            "(0)\t 543\t [ 1.2618198 -1.1858674] \t0\ttrue\n",
            "(0)\t 544\t [ 0.39760733 -0.20242564] \t0\ttrue\n",
            "(0)\t 545\t [ 1.5898429 -1.2809582] \t0\ttrue\n",
            "(1)\t 546\t [ 1.389944  -1.1918476] \t0\tfalse\n",
            "(0)\t 547\t [ 0.7322489 -0.6026776] \t0\ttrue\n",
            "(0)\t 548\t [ 1.5647317 -1.3164985] \t0\ttrue\n",
            "(1)\t 549\t [ 0.73894334 -0.3749615 ] \t0\tfalse\n",
            "(1)\t 550\t [ 0.9489363  -0.52194273] \t0\tfalse\n",
            "(1)\t 551\t [ 0.5489505  -0.28229332] \t0\tfalse\n",
            "(1)\t 552\t [ 1.2963672 -1.0126307] \t0\tfalse\n",
            "(1)\t 553\t [ 0.83514404 -0.44782045] \t0\tfalse\n",
            "(1)\t 554\t [-0.28588822  0.57635987] \t1\ttrue\n",
            "(0)\t 555\t [ 1.4775126 -1.4059298] \t0\ttrue\n",
            "(0)\t 556\t [ 1.4165409 -1.3068135] \t0\ttrue\n",
            "(0)\t 557\t [ 1.5494679 -1.1704507] \t0\ttrue\n",
            "(1)\t 558\t [0.17973956 0.11422379] \t0\tfalse\n",
            "(1)\t 559\t [-0.5820679  0.8618878] \t1\ttrue\n",
            "(0)\t 560\t [-0.9171852  0.9125539] \t1\tfalse\n",
            "(1)\t 561\t [ 1.4287674 -1.1360185] \t0\tfalse\n",
            "(0)\t 562\t [ 1.415785  -1.2611537] \t0\ttrue\n",
            "(0)\t 563\t [ 0.7195707 -0.5716731] \t0\ttrue\n",
            "(0)\t 564\t [ 1.396306  -1.2265806] \t0\ttrue\n",
            "(1)\t 565\t [-0.4555826   0.63179845] \t1\ttrue\n",
            "(1)\t 566\t [ 1.5536697 -1.3801341] \t0\tfalse\n",
            "(1)\t 567\t [-0.03884858  0.231493  ] \t1\ttrue\n",
            "(0)\t 568\t [ 1.174901   -0.84156287] \t0\ttrue\n",
            "(0)\t 569\t [ 1.2119465 -1.2759451] \t0\ttrue\n",
            "(0)\t 570\t [ 1.1850228 -1.0046203] \t0\ttrue\n",
            "(0)\t 571\t [ 1.5432444 -1.3223357] \t0\ttrue\n",
            "(1)\t 572\t [ 1.3757491 -1.0920186] \t0\tfalse\n",
            "(0)\t 573\t [ 1.5134661 -1.2485493] \t0\ttrue\n",
            "(0)\t 574\t [-0.8790786  1.1873572] \t1\tfalse\n",
            "(0)\t 575\t [-0.33153662  0.82621443] \t1\tfalse\n",
            "(0)\t 576\t [0.08976866 0.26217532] \t1\tfalse\n",
            "(0)\t 577\t [-0.6996696  1.0938039] \t1\tfalse\n",
            "(0)\t 578\t [ 1.5781971 -1.2808834] \t0\ttrue\n",
            "(0)\t 579\t [ 1.0817797 -0.7881501] \t0\ttrue\n",
            "(0)\t 580\t [ 1.4302473 -1.1119075] \t0\ttrue\n",
            "(0)\t 581\t [-0.41992655  0.74150425] \t1\tfalse\n",
            "(0)\t 582\t [ 1.5539737 -1.3180609] \t0\ttrue\n",
            "(0)\t 583\t [ 0.9190121  -0.48441473] \t0\ttrue\n",
            "(0)\t 584\t [ 0.9149741 -0.7074529] \t0\ttrue\n",
            "(0)\t 585\t [ 1.4042296 -1.1427507] \t0\ttrue\n",
            "(0)\t 586\t [ 1.5004861 -1.254025 ] \t0\ttrue\n",
            "(0)\t 587\t [ 1.5014477 -1.1804345] \t0\ttrue\n",
            "(1)\t 588\t [ 0.54415554 -0.26132816] \t0\tfalse\n",
            "(1)\t 589\t [ 0.66126287 -0.3912118 ] \t0\tfalse\n",
            "(1)\t 590\t [ 1.5686448 -1.2118728] \t0\tfalse\n",
            "(1)\t 591\t [-1.4591062  1.532251 ] \t1\ttrue\n",
            "(0)\t 592\t [ 1.5204276 -1.3205569] \t0\ttrue\n",
            "(0)\t 593\t [ 1.5425743 -1.1630225] \t0\ttrue\n",
            "(0)\t 594\t [ 1.5271958 -1.2437582] \t0\ttrue\n",
            "(1)\t 595\t [-1.6137278  1.1610312] \t1\ttrue\n",
            "(1)\t 596\t [-1.1654463  1.2659442] \t1\ttrue\n",
            "(1)\t 597\t [ 0.9109343  -0.55785066] \t0\tfalse\n",
            "(1)\t 598\t [ 1.5301797 -1.3033749] \t0\tfalse\n",
            "(0)\t 599\t [ 1.3500152 -1.1628938] \t0\ttrue\n",
            "(1)\t 600\t [-1.4630235  1.5747668] \t1\ttrue\n",
            "(0)\t 601\t [ 1.5615205 -1.2855122] \t0\ttrue\n",
            "(0)\t 602\t [-1.0097625  1.0572969] \t1\tfalse\n",
            "(1)\t 603\t [ 1.2453742 -1.167455 ] \t0\tfalse\n",
            "(0)\t 604\t [ 1.3305261 -1.0943159] \t0\ttrue\n",
            "(0)\t 605\t [ 0.8049946  -0.49583426] \t0\ttrue\n",
            "(1)\t 606\t [0.01030505 0.1354152 ] \t1\ttrue\n",
            "(0)\t 607\t [ 1.5960764 -1.417605 ] \t0\ttrue\n",
            "(1)\t 608\t [ 1.2364671 -1.0318133] \t0\tfalse\n",
            "(0)\t 609\t [ 1.6224238 -1.4422466] \t0\ttrue\n",
            "(1)\t 610\t [-1.642255   1.5222456] \t1\ttrue\n",
            "(1)\t 611\t [ 0.48906687 -0.06126923] \t0\tfalse\n",
            "(1)\t 612\t [-0.48018834  0.69762737] \t1\ttrue\n",
            "(1)\t 613\t [-1.5677631  1.4148966] \t1\ttrue\n",
            "(1)\t 614\t [ 1.6544861 -1.3509719] \t0\tfalse\n",
            "(0)\t 615\t [ 0.52632135 -0.25151303] \t0\ttrue\n",
            "(1)\t 616\t [ 0.74138606 -0.24039145] \t0\tfalse\n",
            "(0)\t 617\t [-0.94070566  1.1551311 ] \t1\tfalse\n",
            "(0)\t 618\t [ 0.69636065 -0.45290887] \t0\ttrue\n",
            "(0)\t 619\t [-0.8852065  1.0706265] \t1\tfalse\n",
            "(0)\t 620\t [-1.1178914  1.1430297] \t1\tfalse\n",
            "(0)\t 621\t [-0.38584295  0.52946573] \t1\tfalse\n",
            "(1)\t 622\t [ 1.5733291 -1.3194413] \t0\tfalse\n",
            "(0)\t 623\t [0.13716653 0.08218445] \t0\ttrue\n",
            "(0)\t 624\t [ 1.1653147 -0.8645527] \t0\ttrue\n",
            "(0)\t 625\t [-1.1773937  1.5174878] \t1\tfalse\n",
            "(1)\t 626\t [ 1.1212308 -0.9068201] \t0\tfalse\n",
            "(1)\t 627\t [-0.9083865  1.0826817] \t1\ttrue\n",
            "(0)\t 628\t [ 1.5532862 -1.2219858] \t0\ttrue\n",
            "(1)\t 629\t [-0.4275403  0.9625106] \t1\ttrue\n",
            "(1)\t 630\t [-0.54785514  0.9574295 ] \t1\ttrue\n",
            "(0)\t 631\t [ 1.4391954 -1.3436531] \t0\ttrue\n",
            "(1)\t 632\t [ 1.5774708 -1.4065163] \t0\tfalse\n",
            "(1)\t 633\t [-1.5906792  1.6516662] \t1\ttrue\n",
            "(0)\t 634\t [ 1.3415115 -1.2979527] \t0\ttrue\n",
            "(0)\t 635\t [-0.78232723  1.1073984 ] \t1\tfalse\n",
            "(0)\t 636\t [ 1.1427279 -0.8937751] \t0\ttrue\n",
            "(0)\t 637\t [-0.05701867  0.39803907] \t1\tfalse\n",
            "(1)\t 638\t [ 1.2590973 -1.03492  ] \t0\tfalse\n",
            "(0)\t 639\t [ 0.8025746 -0.5320859] \t0\ttrue\n",
            "(0)\t 640\t [ 1.183456   -0.88365966] \t0\ttrue\n",
            "(0)\t 641\t [ 1.1497614 -1.0504633] \t0\ttrue\n",
            "(1)\t 642\t [ 1.423943 -1.166132] \t0\tfalse\n",
            "(0)\t 643\t [ 1.1084511 -1.0515418] \t0\ttrue\n",
            "(0)\t 644\t [ 1.3066928 -1.1830525] \t0\ttrue\n",
            "(1)\t 645\t [ 0.9993487 -0.6335951] \t0\tfalse\n",
            "(0)\t 646\t [ 1.5613109 -1.4226865] \t0\ttrue\n",
            "(0)\t 647\t [ 1.5211382 -1.3123271] \t0\ttrue\n",
            "(1)\t 648\t [-0.9004801  1.3152903] \t1\ttrue\n",
            "(0)\t 649\t [ 1.5504345 -1.2722006] \t0\ttrue\n",
            "(0)\t 650\t [-0.232101   0.4277566] \t1\tfalse\n",
            "(0)\t 651\t [-0.71533346  0.9474688 ] \t1\tfalse\n",
            "(0)\t 652\t [-0.7083483  1.2852075] \t1\tfalse\n",
            "(0)\t 653\t [ 0.783197  -0.7237929] \t0\ttrue\n",
            "(1)\t 654\t [-0.08948758  0.378269  ] \t1\ttrue\n",
            "(1)\t 655\t [ 0.68636966 -0.20346537] \t0\tfalse\n",
            "(1)\t 656\t [ 1.4450738 -1.3618288] \t0\tfalse\n",
            "(1)\t 657\t [-0.07575411  0.2545141 ] \t1\ttrue\n",
            "(0)\t 658\t [ 1.4154074 -1.3914254] \t0\ttrue\n",
            "(1)\t 659\t [ 1.5567219 -1.3621811] \t0\tfalse\n",
            "(1)\t 660\t [-0.70354456  0.81650984] \t1\ttrue\n",
            "(1)\t 661\t [-1.2804286  1.5036962] \t1\ttrue\n",
            "(0)\t 662\t [ 1.7334375 -1.4322488] \t0\ttrue\n",
            "(0)\t 663\t [ 1.0323626  -0.80875623] \t0\ttrue\n",
            "(0)\t 664\t [ 1.69461   -1.3853812] \t0\ttrue\n",
            "(0)\t 665\t [ 1.6268811 -1.3862932] \t0\ttrue\n",
            "(0)\t 666\t [ 1.3633668 -1.2674332] \t0\ttrue\n",
            "(0)\t 667\t [ 1.5699568 -1.3727392] \t0\ttrue\n",
            "(0)\t 668\t [-0.7516199   0.93542373] \t1\tfalse\n",
            "(0)\t 669\t [ 1.205442  -1.0483363] \t0\ttrue\n",
            "(0)\t 670\t [ 1.5416596 -1.2834618] \t0\ttrue\n",
            "(1)\t 671\t [ 1.5996834 -1.2918718] \t0\tfalse\n",
            "(0)\t 672\t [ 1.6351268 -1.4028425] \t0\ttrue\n",
            "(0)\t 673\t [ 1.4104632 -1.1195811] \t0\ttrue\n",
            "(0)\t 674\t [ 1.2733889 -1.2448993] \t0\ttrue\n",
            "(1)\t 675\t [-0.61161834  0.7687888 ] \t1\ttrue\n",
            "(0)\t 676\t [-1.424438   1.2567934] \t1\tfalse\n",
            "(0)\t 677\t [-1.3403966  1.5763359] \t1\tfalse\n",
            "(0)\t 678\t [-0.33325198  0.5724813 ] \t1\tfalse\n",
            "(0)\t 679\t [ 0.30750453 -0.0271049 ] \t0\ttrue\n",
            "(0)\t 680\t [ 1.2409927 -1.0078769] \t0\ttrue\n",
            "(0)\t 681\t [ 1.186691  -1.0007955] \t0\ttrue\n",
            "(0)\t 682\t [ 1.3656297 -1.0593473] \t0\ttrue\n",
            "(0)\t 683\t [ 1.6408662 -1.3967983] \t0\ttrue\n",
            "(1)\t 684\t [-1.2332797  1.3796308] \t1\ttrue\n",
            "(1)\t 685\t [-0.4268588  0.6090732] \t1\ttrue\n",
            "(0)\t 686\t [-0.43341774  0.5233639 ] \t1\tfalse\n",
            "(1)\t 687\t [ 1.4246833 -1.1708605] \t0\tfalse\n",
            "(0)\t 688\t [-0.71306396  0.9983958 ] \t1\tfalse\n",
            "(0)\t 689\t [-1.0507213  1.1909739] \t1\tfalse\n",
            "(1)\t 690\t [ 0.7547905 -0.3665641] \t0\tfalse\n",
            "(1)\t 691\t [-0.6172744   0.82186925] \t1\ttrue\n",
            "(1)\t 692\t [-0.96971536  1.4103031 ] \t1\ttrue\n",
            "(0)\t 693\t [ 1.6038594 -1.2886169] \t0\ttrue\n",
            "(1)\t 694\t [-0.57901263  0.79944134] \t1\ttrue\n",
            "(1)\t 695\t [-0.17547448  0.39369977] \t1\ttrue\n",
            "(1)\t 696\t [0.02505598 0.22478545] \t1\ttrue\n",
            "(1)\t 697\t [ 1.3284022 -1.1024795] \t0\tfalse\n",
            "(0)\t 698\t [-0.49431697  0.5712626 ] \t1\tfalse\n",
            "(0)\t 699\t [ 1.2715154 -0.9681611] \t0\ttrue\n",
            "(0)\t 700\t [ 1.6311197 -1.4419153] \t0\ttrue\n",
            "(0)\t 701\t [-0.01700156  0.32353675] \t1\tfalse\n",
            "(1)\t 702\t [ 1.3961738 -1.2211356] \t0\tfalse\n",
            "(0)\t 703\t [ 1.504168  -1.3184831] \t0\ttrue\n",
            "(1)\t 704\t [-1.653414   1.5295172] \t1\ttrue\n",
            "(0)\t 705\t [ 1.5384598 -1.3355781] \t0\ttrue\n",
            "(1)\t 706\t [ 1.6345048 -1.3919507] \t0\tfalse\n",
            "(0)\t 707\t [ 1.5293055 -1.2482779] \t0\ttrue\n",
            "(0)\t 708\t [ 1.5820138 -1.3476334] \t0\ttrue\n",
            "(0)\t 709\t [0.13987675 0.08785612] \t0\ttrue\n",
            "(1)\t 710\t [-0.8734518  1.0295533] \t1\ttrue\n",
            "(0)\t 711\t [ 1.4055948 -1.2702662] \t0\ttrue\n",
            "(0)\t 712\t [ 1.6281615 -1.3434402] \t0\ttrue\n",
            "(0)\t 713\t [ 1.1039221 -1.0206584] \t0\ttrue\n",
            "(1)\t 714\t [-1.6241214  1.5078185] \t1\ttrue\n",
            "(1)\t 715\t [ 1.529544  -1.2333238] \t0\tfalse\n",
            "(0)\t 716\t [ 1.3675508 -1.1264045] \t0\ttrue\n",
            "(1)\t 717\t [-1.6538904  1.5906696] \t1\ttrue\n",
            "(0)\t 718\t [-0.13497117  0.2403828 ] \t1\tfalse\n",
            "(0)\t 719\t [ 1.5063812 -1.4045236] \t0\ttrue\n",
            "(1)\t 720\t [-1.1902115  1.53022  ] \t1\ttrue\n",
            "(1)\t 721\t [-0.9359014  1.3408599] \t1\ttrue\n",
            "(0)\t 722\t [ 0.6714397 -0.4224131] \t0\ttrue\n",
            "(1)\t 723\t [ 0.749872   -0.41162956] \t0\tfalse\n",
            "(1)\t 724\t [ 1.1325073  -0.86751777] \t0\tfalse\n",
            "(1)\t 725\t [0.24317753 0.20390302] \t0\tfalse\n",
            "(1)\t 726\t [ 0.36638996 -0.06457102] \t0\tfalse\n",
            "(1)\t 727\t [ 1.4722557 -1.1954525] \t0\tfalse\n",
            "(0)\t 728\t [ 1.5249482 -1.128011 ] \t0\ttrue\n",
            "(0)\t 729\t [ 1.6646558 -1.4105184] \t0\ttrue\n",
            "(1)\t 730\t [-0.6009058  0.7044625] \t1\ttrue\n",
            "(0)\t 731\t [ 1.5029447 -1.1632979] \t0\ttrue\n",
            "(0)\t 732\t [ 1.4187863 -1.1781238] \t0\ttrue\n",
            "(0)\t 733\t [ 1.0804102 -1.0891275] \t0\ttrue\n",
            "(0)\t 734\t [ 1.3791828 -0.9712267] \t0\ttrue\n",
            "(0)\t 735\t [-0.13031366  0.37511882] \t1\tfalse\n",
            "(0)\t 736\t [ 1.4720337 -1.1155235] \t0\ttrue\n",
            "(0)\t 737\t [ 0.9316345  -0.71921706] \t0\ttrue\n",
            "(0)\t 738\t [ 1.5976157 -1.3522568] \t0\ttrue\n",
            "(0)\t 739\t [ 0.26962355 -0.04546702] \t0\ttrue\n",
            "(0)\t 740\t [ 1.4775848 -1.3831689] \t0\ttrue\n",
            "(1)\t 741\t [ 0.8132159  -0.72321385] \t0\tfalse\n",
            "(1)\t 742\t [ 1.173424  -1.0577712] \t0\tfalse\n",
            "(1)\t 743\t [-1.6260145  1.5300035] \t1\ttrue\n",
            "(0)\t 744\t [ 1.6013157 -1.2613286] \t0\ttrue\n",
            "(0)\t 745\t [ 1.2907805  -0.97506726] \t0\ttrue\n",
            "(1)\t 746\t [-0.19630143  0.5451063 ] \t1\ttrue\n",
            "(1)\t 747\t [ 0.88069737 -0.69974643] \t0\tfalse\n",
            "(1)\t 748\t [-1.6422104  1.7203401] \t1\ttrue\n",
            "(0)\t 749\t [ 1.2525282 -0.9164538] \t0\ttrue\n",
            "(1)\t 750\t [ 0.49164447 -0.2399208 ] \t0\tfalse\n",
            "(1)\t 751\t [ 1.3386109 -1.1032515] \t0\tfalse\n",
            "(1)\t 752\t [-1.2826604  1.3693546] \t1\ttrue\n",
            "(0)\t 753\t [ 1.6052129 -1.3017371] \t0\ttrue\n",
            "(0)\t 754\t [ 1.5231513 -1.3307873] \t0\ttrue\n",
            "(0)\t 755\t [ 1.5185878 -1.2834144] \t0\ttrue\n",
            "(0)\t 756\t [ 1.5707991 -1.2835847] \t0\ttrue\n",
            "(0)\t 757\t [ 1.4877797 -1.4274325] \t0\ttrue\n",
            "(0)\t 758\t [ 1.5432556 -1.2719414] \t0\ttrue\n",
            "(0)\t 759\t [ 1.4753804 -1.263657 ] \t0\ttrue\n",
            "(0)\t 760\t [ 1.0516585  -0.65904456] \t0\ttrue\n",
            "(0)\t 761\t [ 1.4664181 -1.1761634] \t0\ttrue\n",
            "(0)\t 762\t [ 1.5847306 -1.3051608] \t0\ttrue\n",
            "(1)\t 763\t [-1.4548341  1.4462153] \t1\ttrue\n",
            "(0)\t 764\t [-0.15664232  0.21040568] \t1\tfalse\n",
            "(0)\t 765\t [ 1.4514257 -1.2562621] \t0\ttrue\n",
            "(1)\t 766\t [ 0.7916659  -0.45535415] \t0\tfalse\n",
            "(0)\t 767\t [ 0.49672055 -0.47024602] \t0\ttrue\n",
            "(0)\t 768\t [ 0.4694668  -0.02229456] \t0\ttrue\n",
            "(0)\t 769\t [-0.62412107  0.88501835] \t1\tfalse\n",
            "(1)\t 770\t [-1.049908   1.5253402] \t1\ttrue\n",
            "(0)\t 771\t [ 1.6383957 -1.3799516] \t0\ttrue\n",
            "(0)\t 772\t [ 1.5611302 -1.3314737] \t0\ttrue\n",
            "(0)\t 773\t [ 1.410461  -1.3037598] \t0\ttrue\n",
            "(1)\t 774\t [-0.7319275  0.8650102] \t1\ttrue\n",
            "(1)\t 775\t [-1.3796479  1.4398148] \t1\ttrue\n",
            "(0)\t 776\t [-1.216866   1.2497361] \t1\tfalse\n",
            "(1)\t 777\t [-1.2865885  1.487987 ] \t1\ttrue\n",
            "(1)\t 778\t [-1.1128976  1.4443467] \t1\ttrue\n",
            "(0)\t 779\t [ 1.416275 -1.07932 ] \t0\ttrue\n",
            "(1)\t 780\t [-0.4879651  0.6703053] \t1\ttrue\n",
            "(1)\t 781\t [ 1.1056585  -0.78160334] \t0\tfalse\n",
            "(0)\t 782\t [ 1.4846162 -1.3114896] \t0\ttrue\n",
            "(0)\t 783\t [ 1.1516753 -1.0261306] \t0\ttrue\n",
            "(0)\t 784\t [ 1.0443125 -1.0229888] \t0\ttrue\n",
            "(0)\t 785\t [ 1.4376969 -1.1937175] \t0\ttrue\n",
            "(0)\t 786\t [ 1.567121  -1.2418635] \t0\ttrue\n",
            "(1)\t 787\t [ 1.0983249 -0.5751493] \t0\tfalse\n",
            "(0)\t 788\t [ 1.6082412 -1.3250859] \t0\ttrue\n",
            "(1)\t 789\t [ 1.2156839  -0.92179817] \t0\tfalse\n",
            "(0)\t 790\t [ 1.5229669 -1.3340528] \t0\ttrue\n",
            "(0)\t 791\t [ 1.5270123 -1.2821766] \t0\ttrue\n",
            "(0)\t 792\t [ 1.6713527 -1.2488165] \t0\ttrue\n",
            "(1)\t 793\t [-1.3489316  1.4773228] \t1\ttrue\n",
            "(1)\t 794\t [ 0.91673136 -0.6769541 ] \t0\tfalse\n",
            "(0)\t 795\t [ 1.6156809 -1.3659761] \t0\ttrue\n",
            "(0)\t 796\t [ 1.5763874 -1.2541375] \t0\ttrue\n",
            "(1)\t 797\t [ 1.2107158  -0.91188633] \t0\tfalse\n",
            "(1)\t 798\t [ 1.6646281 -1.3728962] \t0\tfalse\n",
            "(1)\t 799\t [ 0.83052635 -0.52935314] \t0\tfalse\n",
            "(0)\t 800\t [ 1.0562856 -0.7328565] \t0\ttrue\n",
            "(0)\t 801\t [ 1.5326765 -1.384706 ] \t0\ttrue\n",
            "(0)\t 802\t [-1.3135874  1.3294096] \t1\tfalse\n",
            "(0)\t 803\t [ 1.4499424 -1.2598568] \t0\ttrue\n",
            "(1)\t 804\t [ 1.3169127  -0.91215974] \t0\tfalse\n",
            "(0)\t 805\t [-0.5621967  0.8415031] \t1\tfalse\n",
            "(0)\t 806\t [ 1.5688045 -1.125015 ] \t0\ttrue\n",
            "(1)\t 807\t [ 1.3528969 -1.1862551] \t0\tfalse\n",
            "(0)\t 808\t [ 1.4726427 -1.2611728] \t0\ttrue\n",
            "(0)\t 809\t [-0.50224704  0.66269755] \t1\tfalse\n",
            "(1)\t 810\t [ 0.585016   -0.38176778] \t0\tfalse\n",
            "(1)\t 811\t [ 1.5152926 -1.0835744] \t0\tfalse\n",
            "(1)\t 812\t [-1.7585362  1.4764097] \t1\ttrue\n",
            "(1)\t 813\t [-1.5585841  1.5661107] \t1\ttrue\n",
            "(1)\t 814\t [ 1.5099465 -1.20577  ] \t0\tfalse\n",
            "(0)\t 815\t [ 1.001903   -0.62796336] \t0\ttrue\n",
            "(0)\t 816\t [-0.12166462  0.20771356] \t1\tfalse\n",
            "(0)\t 817\t [-1.6732324  1.4302523] \t1\tfalse\n",
            "(0)\t 818\t [-1.5937614  1.4113396] \t1\tfalse\n",
            "(0)\t 819\t [ 1.3540716 -1.1308331] \t0\ttrue\n",
            "(0)\t 820\t [-1.0007604  1.0989084] \t1\tfalse\n",
            "(1)\t 821\t [ 0.98221856 -0.83741   ] \t0\tfalse\n",
            "(1)\t 822\t [-1.720401   1.6022874] \t1\ttrue\n",
            "(0)\t 823\t [0.00581498 0.2935507 ] \t1\tfalse\n",
            "(0)\t 824\t [ 1.2817807 -1.1212204] \t0\ttrue\n",
            "(0)\t 825\t [ 1.1303883 -1.0992725] \t0\ttrue\n",
            "(0)\t 826\t [ 1.6066914 -1.3266239] \t0\ttrue\n",
            "(1)\t 827\t [-1.1750221  1.3910462] \t1\ttrue\n",
            "(1)\t 828\t [ 1.1462379 -1.0216789] \t0\tfalse\n",
            "(1)\t 829\t [-1.5466748  1.3295996] \t1\ttrue\n",
            "(0)\t 830\t [ 1.6522348 -1.4365039] \t0\ttrue\n",
            "(0)\t 831\t [-1.5965316  1.3509767] \t1\tfalse\n",
            "(0)\t 832\t [-0.35718045  0.7933425 ] \t1\tfalse\n",
            "(0)\t 833\t [ 1.4259875 -1.1856164] \t0\ttrue\n",
            "(0)\t 834\t [ 0.19906592 -0.02447338] \t0\ttrue\n",
            "(0)\t 835\t [-0.06516515  0.38980454] \t1\tfalse\n",
            "(0)\t 836\t [ 1.3563317 -1.0999305] \t0\ttrue\n",
            "(1)\t 837\t [-0.258643    0.53419584] \t1\ttrue\n",
            "(1)\t 838\t [-0.03485515  0.4541601 ] \t1\ttrue\n",
            "(1)\t 839\t [ 0.9523078 -0.7238702] \t0\tfalse\n",
            "(0)\t 840\t [ 1.4961412 -1.2258915] \t0\ttrue\n",
            "(1)\t 841\t [-1.2839088  1.5952623] \t1\ttrue\n",
            "(0)\t 842\t [ 1.5075932 -1.2936095] \t0\ttrue\n",
            "(0)\t 843\t [-0.6188194  1.0893312] \t1\tfalse\n",
            "(0)\t 844\t [ 1.6341157 -1.3483846] \t0\ttrue\n",
            "(0)\t 845\t [ 1.5501833 -1.3772566] \t0\ttrue\n",
            "(0)\t 846\t [ 1.2967002 -1.0343571] \t0\ttrue\n",
            "(0)\t 847\t [ 1.6170537 -1.3258924] \t0\ttrue\n",
            "(0)\t 848\t [ 1.4948483 -1.3858776] \t0\ttrue\n",
            "(0)\t 849\t [ 1.4855705 -1.2615688] \t0\ttrue\n",
            "(0)\t 850\t [ 1.2288846 -0.8487016] \t0\ttrue\n",
            "(0)\t 851\t [ 0.47843236 -0.1426822 ] \t0\ttrue\n",
            "(0)\t 852\t [ 1.630001  -1.3894608] \t0\ttrue\n",
            "(0)\t 853\t [ 1.2992792 -0.9362893] \t0\ttrue\n",
            "(1)\t 854\t [-1.1794281  1.3354044] \t1\ttrue\n",
            "(0)\t 855\t [0.21776319 0.22122225] \t1\tfalse\n",
            "(0)\t 856\t [ 1.2785145 -0.6751522] \t0\ttrue\n",
            "(0)\t 857\t [-0.5888718  0.8317023] \t1\tfalse\n",
            "(0)\t 858\t [-0.9233091  1.2006664] \t1\tfalse\n",
            "(0)\t 859\t [ 1.5603787 -1.2646265] \t0\ttrue\n",
            "(1)\t 860\t [ 1.4983531 -1.2679337] \t0\tfalse\n",
            "(0)\t 861\t [ 1.1231178 -0.9159481] \t0\ttrue\n",
            "(1)\t 862\t [ 1.5338787 -1.2319642] \t0\tfalse\n",
            "(0)\t 863\t [ 1.5852488 -1.3541901] \t0\ttrue\n",
            "(1)\t 864\t [-1.680828   1.3056183] \t1\ttrue\n",
            "(1)\t 865\t [0.28451115 0.15842767] \t0\tfalse\n",
            "(1)\t 866\t [ 0.8911675  -0.68111694] \t0\tfalse\n",
            "(1)\t 867\t [-1.1646955  1.5402713] \t1\ttrue\n",
            "(0)\t 868\t [ 1.6142498 -1.3561127] \t0\ttrue\n",
            "(0)\t 869\t [ 1.22826   -0.9297665] \t0\ttrue\n",
            "(1)\t 870\t [-0.716624   0.8984702] \t1\ttrue\n",
            "(0)\t 871\t [ 1.5232015 -1.2835927] \t0\ttrue\n",
            "(1)\t 872\t [-0.6008438  0.7975805] \t1\ttrue\n",
            "(1)\t 873\t [ 0.50110155 -0.27876395] \t0\tfalse\n",
            "(0)\t 874\t [ 1.3259722 -1.0858707] \t0\ttrue\n",
            "(1)\t 875\t [-0.286148   0.5946562] \t1\ttrue\n",
            "(1)\t 876\t [ 0.97252655 -0.82314694] \t0\tfalse\n",
            "(0)\t 877\t [ 1.4968984 -1.27385  ] \t0\ttrue\n",
            "(1)\t 878\t [ 1.0009935  -0.85512817] \t0\tfalse\n",
            "(0)\t 879\t [-0.01182379  0.24087115] \t1\tfalse\n",
            "(0)\t 880\t [ 1.5477769 -1.3653839] \t0\ttrue\n",
            "(0)\t 881\t [ 1.3853958 -1.1615906] \t0\ttrue\n",
            "(1)\t 882\t [-0.82102895  1.039249  ] \t1\ttrue\n",
            "(1)\t 883\t [ 1.236024  -1.3067535] \t0\tfalse\n",
            "(0)\t 884\t [ 0.8656612 -0.5102833] \t0\ttrue\n",
            "(0)\t 885\t [ 1.6267929 -1.3906758] \t0\ttrue\n",
            "(1)\t 886\t [ 1.4482926 -1.1994082] \t0\tfalse\n",
            "(1)\t 887\t [ 1.2752466 -0.9834382] \t0\tfalse\n",
            "(0)\t 888\t [ 1.4294075 -1.251773 ] \t0\ttrue\n",
            "(0)\t 889\t [ 1.5437276 -1.228724 ] \t0\ttrue\n",
            "(0)\t 890\t [-0.34332317  0.6226766 ] \t1\tfalse\n",
            "(1)\t 891\t [ 1.5493377 -1.2968749] \t0\tfalse\n",
            "(1)\t 892\t [-0.81057525  1.2555485 ] \t1\ttrue\n",
            "(1)\t 893\t [ 0.9915552  -0.48325768] \t0\tfalse\n",
            "(1)\t 894\t [ 1.4805343 -1.126094 ] \t0\tfalse\n",
            "(0)\t 895\t [ 1.5283201 -1.3189032] \t0\ttrue\n",
            "(0)\t 896\t [ 1.4473017 -1.181247 ] \t0\ttrue\n",
            "(0)\t 897\t [ 0.8204601  -0.46996748] \t0\ttrue\n",
            "(1)\t 898\t [-0.01561138  0.56051993] \t1\ttrue\n",
            "(1)\t 899\t [-0.838987   0.8190622] \t1\ttrue\n",
            "(1)\t 900\t [ 1.636425  -1.3328824] \t0\tfalse\n",
            "Number of true predictions: 595\n",
            "Number of false predictions: 305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIgsWWCTKdY",
        "colab_type": "code",
        "outputId": "23e8d7f3-f73f-417d-e275-21d9fcdc88ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 66.0377358490566 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "outputId": "e0d25aa6-8023-40c2-8c78-68c0888c6067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI2K1bqaR5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "outputId": "26f97c82-b746-4c93-9d3f-bd644ab10314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QowNXrYZSRMR",
        "colab_type": "code",
        "outputId": "b5c04690-6059-4c0d-e09c-59a4478c7d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "print(flat_predictions)\n",
        "print(\"************\")\n",
        "print(flat_true_labels)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 1 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 0 0\n",
            " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0\n",
            " 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0\n",
            " 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0\n",
            " 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0\n",
            " 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1\n",
            " 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0\n",
            " 0 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0\n",
            " 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 0\n",
            " 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1\n",
            " 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0\n",
            " 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 1\n",
            " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0\n",
            " 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 1 1 0]\n",
            "************\n",
            "[1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "outputId": "9e3941d3-a90e-4a6c-cac8-3513c3d8bafa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-task1malayalam'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task1malayalam\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task1malayalam/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1malayalam/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1malayalam/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}
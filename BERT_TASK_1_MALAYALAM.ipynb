{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT TASK 1 MALAYALAM",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "506e8783a979489e958f6e737226dfba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c089885c41f145728b01782c7fc1ce11",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e6f0abcd1c514c23947197cf7a496012",
              "IPY_MODEL_c5547e6f017b4446833a1fe3b0486da1"
            ]
          }
        },
        "c089885c41f145728b01782c7fc1ce11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6f0abcd1c514c23947197cf7a496012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0114ee9a008649b5949c10539fb935b3",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c4486a9e4f94a3b827d28251d05450e"
          }
        },
        "c5547e6f017b4446833a1fe3b0486da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f02a7dae31374a68b236575f38d109fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 996k/996k [00:00&lt;00:00, 1.97MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22f12453726c4b2fa3a2df87f222e7b6"
          }
        },
        "0114ee9a008649b5949c10539fb935b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c4486a9e4f94a3b827d28251d05450e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f02a7dae31374a68b236575f38d109fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22f12453726c4b2fa3a2df87f222e7b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77de817a97f4441ca103429af28312f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9996491b359f4b35828da9f6eea9a4d1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_076c647ccd944cccb8ca98c62978ad92",
              "IPY_MODEL_db5a9c12c8b94c4b9a5997a08ca6dffc"
            ]
          }
        },
        "9996491b359f4b35828da9f6eea9a4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "076c647ccd944cccb8ca98c62978ad92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f29dfaf4a1974838a7a87d1006478245",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 569,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 569,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bb3f4ef745584ac2a72090d6ee13271a"
          }
        },
        "db5a9c12c8b94c4b9a5997a08ca6dffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5344de9c779642d2b4ce27e67cf0c6c5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 569/569 [00:00&lt;00:00, 24.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_441a814f7b214c2c933d435279171878"
          }
        },
        "f29dfaf4a1974838a7a87d1006478245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bb3f4ef745584ac2a72090d6ee13271a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5344de9c779642d2b4ce27e67cf0c6c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "441a814f7b214c2c933d435279171878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f4dcd560b346406d967e6b32d0e81d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1e98a3572b9a4e39a3a7b685166c61ff",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_628cabc486f24ea29c527196b3dcfbff",
              "IPY_MODEL_d97de6fac02f4cbfa5fbd20deab91c80"
            ]
          }
        },
        "1e98a3572b9a4e39a3a7b685166c61ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "628cabc486f24ea29c527196b3dcfbff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a87f61c60ae9495996a3949fd77614d1",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99fb84bab4b04d7888c6d6315e73466f"
          }
        },
        "d97de6fac02f4cbfa5fbd20deab91c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_461ec5f25664428a9430e99577759652",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 714M/714M [00:27&lt;00:00, 25.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47a8c9198c124bc6af3ad54aec32822e"
          }
        },
        "a87f61c60ae9495996a3949fd77614d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99fb84bab4b04d7888c6d6315e73466f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "461ec5f25664428a9430e99577759652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47a8c9198c124bc6af3ad54aec32822e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT_TASK_1_MALAYALAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "898bd0a5-7ddf-4568-ee41-e313b8f9552b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 46.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=e73ab84ca4d6ac39afc43bef34082b84ddc0d4053c19529acd7e07e83c2ee5f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "0a30fa87-5fb2-474a-e698-673568b936a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "2fff8539-372a-42ef-9773-f13e11829690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/malayalam/task1malayalam.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>MAL1084</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>കൂനൂര്‍- മേട്ടുപ്പാളയം ദേശീയ പാതയില്‍ കാട്ടേരി...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>536</th>\n",
              "      <td>MAL0537</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>വിമാനത്തിന്റെ സീറ്റിനടിയിൽ ഒളിപ്പിച്ച അറുപത് ല...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1767</th>\n",
              "      <td>MAL1768</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>സിറിയന്‍ ഉപപ്രധാനമന്ത്രി ഫഹ്ദ് ജസീം അല്‍ ഫ്രജി...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2277</th>\n",
              "      <td>MAL2278</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>തെരഞ്ഞെടുപ്പ് പരാജയത്തെ തുടര്‍ന്ന് മുഖ്യമന്ത്ര...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1068</th>\n",
              "      <td>MAL1069</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>രണ്ടായിരത്തി പതിനാലിൽ യൂറോപ്പ്യന്‍ രാജ്യങ്ങളില...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1944</th>\n",
              "      <td>MAL1945</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>മത്സരശേഷം ഫൈനലിലെ താരത്തിനുള്ള പുരസ്കാരം സ്വീക...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>MAL0570</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>ഓസ്‌ട്രേലിയക്കാരനായ ഫുട്‌ബോള്‍ താരം സ്റ്റീഫന്‍...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>MAL0518</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>ടോംസിന്റെ ഭൗതിക ശരീരം ഇന്ന് പൊതുദർശനത്തിനു വയ്...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>MAL1130</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ഭീകരബന്ധമുള്ളവർക്കായി വ്യാപക തിരച്ചിൽ നടക്കുന്...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1109</th>\n",
              "      <td>MAL1110</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ലോകത്തിലെ ഏറ്റവും മികച്ച ടീമുകളിലൊന്നായ ഓസ്ട്ര...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "1083         MAL1084  ...  കൂനൂര്‍- മേട്ടുപ്പാളയം ദേശീയ പാതയില്‍ കാട്ടേരി...\n",
              "536          MAL0537  ...  വിമാനത്തിന്റെ സീറ്റിനടിയിൽ ഒളിപ്പിച്ച അറുപത് ല...\n",
              "1767         MAL1768  ...  സിറിയന്‍ ഉപപ്രധാനമന്ത്രി ഫഹ്ദ് ജസീം അല്‍ ഫ്രജി...\n",
              "2277         MAL2278  ...  തെരഞ്ഞെടുപ്പ് പരാജയത്തെ തുടര്‍ന്ന് മുഖ്യമന്ത്ര...\n",
              "1068         MAL1069  ...  രണ്ടായിരത്തി പതിനാലിൽ യൂറോപ്പ്യന്‍ രാജ്യങ്ങളില...\n",
              "1944         MAL1945  ...  മത്സരശേഷം ഫൈനലിലെ താരത്തിനുള്ള പുരസ്കാരം സ്വീക...\n",
              "569          MAL0570  ...  ഓസ്‌ട്രേലിയക്കാരനായ ഫുട്‌ബോള്‍ താരം സ്റ്റീഫന്‍...\n",
              "517          MAL0518  ...  ടോംസിന്റെ ഭൗതിക ശരീരം ഇന്ന് പൊതുദർശനത്തിനു വയ്...\n",
              "1129         MAL1130  ...  ഭീകരബന്ധമുള്ളവർക്കായി വ്യാപക തിരച്ചിൽ നടക്കുന്...\n",
              "1109         MAL1110  ...  ലോകത്തിലെ ഏറ്റവും മികച്ച ടീമുകളിലൊന്നായ ഓസ്ട്ര...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "150836b1-a0fe-4d52-a3f5-c977e300f67d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2426</th>\n",
              "      <td>ഞങ്ങള്‍ ആവശ്യത്തിന് ആത്മപരിശോധന നടത്തിയെന്നും ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2137</th>\n",
              "      <td>പൂര്‍ണമായും തദ്ദേശീയമായി വികസിപ്പിച്ചെടുത്ത തേ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1106</th>\n",
              "      <td>സച്ചിന്‍ ടെന്‍ഡുല്‍ക്കറും ബ്രയാന്‍ ലാറയും ആന്‍...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2365</th>\n",
              "      <td>ഐ.പി.എല്ലിലെ നിര്‍ണായക മത്സരത്തില്‍ കൊല്‍ക്കത...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2228</th>\n",
              "      <td>അസ്സാമിൽ അധികാരത്തിലേറുകയും കേരളത്തിൽ അക്കൗണ്ട...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "2426  ഞങ്ങള്‍ ആവശ്യത്തിന് ആത്മപരിശോധന നടത്തിയെന്നും ...      0\n",
              "2137  പൂര്‍ണമായും തദ്ദേശീയമായി വികസിപ്പിച്ചെടുത്ത തേ...      0\n",
              "1106  സച്ചിന്‍ ടെന്‍ഡുല്‍ക്കറും ബ്രയാന്‍ ലാറയും ആന്‍...      0\n",
              "2365  ഐ.പി.എല്ലിലെ നിര്‍ണായക മത്സരത്തില്‍ കൊല്‍ക്കത...      0\n",
              "2228  അസ്സാമിൽ അധികാരത്തിലേറുകയും കേരളത്തിൽ അക്കൗണ്ട...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "6f6033e6-f2c4-40ae-9c57-cc109eea1fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "506e8783a979489e958f6e737226dfba",
            "c089885c41f145728b01782c7fc1ce11",
            "e6f0abcd1c514c23947197cf7a496012",
            "c5547e6f017b4446833a1fe3b0486da1",
            "0114ee9a008649b5949c10539fb935b3",
            "1c4486a9e4f94a3b827d28251d05450e",
            "f02a7dae31374a68b236575f38d109fb",
            "22f12453726c4b2fa3a2df87f222e7b6"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "506e8783a979489e958f6e737226dfba",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=995526, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "919e267a-0bf6-4d35-a064-0dbe5303ee10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  റോയൽ ചലഞ്ചേഴ്സിനെ ആറു വിക്കറ്റിന് തകർത്ത് മുംബൈ വീണ്ടും വിജയവഴിയിൽ.<eol>ബാംഗ്ലൂര്‍ റോയൽ ചലഞ്ചേഴ്സിനെ മുംബൈ ആറ് വിക്കറ്റിന് തോല്‍പിച്ചു.\n",
            "Tokenized:  ['റ', '##േ', '##ായ', '##ൽ', 'ച', '##ല', '##ഞ', '##ച', '##േ', '##ഴ', '##സി', '##നെ', 'ആ', '##റ', 'വി', '##ക', '##ക', '##റ', '##റി', '##ന', 'ത', '##കർ', '##ത', '##ത', 'മ', '##ം', '##ബ', '##ൈ', 'വ', '##ീ', '##ണ', '##ടം', 'വി', '##ജ', '##യ', '##വ', '##ഴി', '##യിൽ', '.', '<', 'eo', '##l', '>', 'ബ', '##ാം', '##ഗ', '##ല', '##ര', 'റ', '##േ', '##ായ', '##ൽ', 'ച', '##ല', '##ഞ', '##ച', '##േ', '##ഴ', '##സി', '##നെ', 'മ', '##ം', '##ബ', '##ൈ', 'ആ', '##റ', 'വി', '##ക', '##ക', '##റ', '##റി', '##ന', 'ത', '##േ', '##ാല', '##പ', '##ി', '##ച', '##ച', '.']\n",
            "Token IDs:  [1360, 29400, 40542, 15080, 1339, 38847, 111389, 111386, 29400, 111399, 108628, 47639, 1322, 75301, 80054, 17896, 17896, 75301, 37054, 25344, 1348, 94330, 20854, 20854, 1357, 14885, 111397, 59009, 1364, 60434, 36089, 53144, 80054, 111388, 18395, 36877, 90865, 17878, 119, 133, 13934, 10161, 135, 1355, 25406, 111383, 38847, 23290, 1360, 29400, 40542, 15080, 1339, 38847, 111389, 111386, 29400, 111399, 108628, 47639, 1357, 14885, 111397, 59009, 1322, 75301, 80054, 17896, 17896, 75301, 37054, 25344, 1348, 29400, 79591, 111395, 15035, 111386, 111386, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "0ec3eca6-64a6-4096-c89a-61fa16899d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  റോയൽ ചലഞ്ചേഴ്സിനെ ആറു വിക്കറ്റിന് തകർത്ത് മുംബൈ വീണ്ടും വിജയവഴിയിൽ.<eol>ബാംഗ്ലൂര്‍ റോയൽ ചലഞ്ചേഴ്സിനെ മുംബൈ ആറ് വിക്കറ്റിന് തോല്‍പിച്ചു.\n",
            "Token IDs: [101, 1360, 29400, 40542, 15080, 1339, 38847, 111389, 111386, 29400, 111399, 108628, 47639, 1322, 75301, 80054, 17896, 17896, 75301, 37054, 25344, 1348, 94330, 20854, 20854, 1357, 14885, 111397, 59009, 1364, 60434, 36089, 53144, 80054, 111388, 18395, 36877, 90865, 17878, 119, 133, 13934, 10161, 135, 1355, 25406, 111383, 38847, 23290, 1360, 29400, 40542, 15080, 1339, 38847, 111389, 111386, 29400, 111399, 108628, 47639, 1357, 14885, 111397, 59009, 1322, 75301, 80054, 17896, 17896, 75301, 37054, 25344, 1348, 29400, 79591, 111395, 15035, 111386, 111386, 119, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "08a8c27a-918d-42fe-b621-c26df077d24c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "e442dcb8-8937-4dd1-e125-2bdff77fc4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "256510d9-59c7-47b9-c3c2-f971ce5b2834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "print(train_inputs)\n",
        "print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "#print(train_masks)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101   1357 111399 ...  23290  27215  21403]\n",
            " [   101   1357  25344 ...  75301  17896  71430]\n",
            " [   101   1325  20854 ...  20854  20854  23290]\n",
            " ...\n",
            " [   101  80054  36877 ... 111393  18395  47357]\n",
            " [   101   1321 111395 ...  60955  47025  93330]\n",
            " [   101   1321  36877 ...  42870  30585   1353]]\n",
            "[1 0 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "b84b2b8e-7321-477f-f1a2-afc57eb863ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "77de817a97f4441ca103429af28312f3",
            "9996491b359f4b35828da9f6eea9a4d1",
            "076c647ccd944cccb8ca98c62978ad92",
            "db5a9c12c8b94c4b9a5997a08ca6dffc",
            "f29dfaf4a1974838a7a87d1006478245",
            "bb3f4ef745584ac2a72090d6ee13271a",
            "5344de9c779642d2b4ce27e67cf0c6c5",
            "441a814f7b214c2c933d435279171878",
            "f4dcd560b346406d967e6b32d0e81d16",
            "1e98a3572b9a4e39a3a7b685166c61ff",
            "628cabc486f24ea29c527196b3dcfbff",
            "d97de6fac02f4cbfa5fbd20deab91c80",
            "a87f61c60ae9495996a3949fd77614d1",
            "99fb84bab4b04d7888c6d6315e73466f",
            "461ec5f25664428a9430e99577759652",
            "47a8c9198c124bc6af3ad54aec32822e"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77de817a97f4441ca103429af28312f3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=569, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4dcd560b346406d967e6b32d0e81d16",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=714314041, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "outputId": "e9791046-02cb-4bf6-a6d1-15caf401e6bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "outputId": "d37ab66a-a0dc-4532-80b8-86b9c3a58b24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:10.\n",
            "\n",
            "  Average training loss: 0.66\n",
            "  Training epcoh took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.71\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epcoh took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epcoh took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.74\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epcoh took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.76\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "outputId": "2cac73f0-0b65-46b3-85db-6ada34a43cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task1malayalam-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "outputId": "edff176c-ef64-4467-f0cb-4d29235724c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "#print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 900 test sentences...\n",
            "    DONE.\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "outputId": "a1ff8869-d426-4a2e-95bc-7f23d0133804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] ):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] ):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    # elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "    #   #print(\" Semi Paraphrase     \",end=\"\")\n",
        "    #   #count_NP+=1      \n",
        "    #   print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "    #   x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(1)\t 1\t [0.3218278 0.4398323] \t1\ttrue\n",
            "(1)\t 2\t [ 0.9472605 -0.5090898] \t0\tfalse\n",
            "(1)\t 3\t [-1.0949655  0.6698108] \t1\ttrue\n",
            "(0)\t 4\t [-1.1190522  0.8447282] \t1\tfalse\n",
            "(0)\t 5\t [ 1.4777226 -1.8842671] \t0\ttrue\n",
            "(0)\t 6\t [-0.98325324  0.8131057 ] \t1\tfalse\n",
            "(1)\t 7\t [-0.9275268  1.0548472] \t1\ttrue\n",
            "(0)\t 8\t [ 1.4421917 -1.6062727] \t0\ttrue\n",
            "(0)\t 9\t [ 1.2412438 -1.9537463] \t0\ttrue\n",
            "(1)\t 10\t [-1.4360948  1.7577025] \t1\ttrue\n",
            "(0)\t 11\t [-0.53378934  0.42386213] \t1\tfalse\n",
            "(1)\t 12\t [ 1.1989329 -1.1229162] \t0\tfalse\n",
            "(0)\t 13\t [ 0.66106296 -0.80629665] \t0\ttrue\n",
            "(0)\t 14\t [ 1.2332919 -1.1866548] \t0\ttrue\n",
            "(0)\t 15\t [ 1.2803702 -1.9249794] \t0\ttrue\n",
            "(1)\t 16\t [ 1.08219   -1.5493871] \t0\tfalse\n",
            "(0)\t 17\t [-0.8250478  0.6357602] \t1\tfalse\n",
            "(0)\t 18\t [ 0.55180776 -0.98922   ] \t0\ttrue\n",
            "(1)\t 19\t [-0.00428103  0.05766505] \t1\ttrue\n",
            "(0)\t 20\t [ 1.1809686 -1.7740326] \t0\ttrue\n",
            "(0)\t 21\t [ 1.3606104 -1.774033 ] \t0\ttrue\n",
            "(0)\t 22\t [ 1.2987665 -2.0150194] \t0\ttrue\n",
            "(0)\t 23\t [ 0.04494196 -0.18529975] \t0\ttrue\n",
            "(0)\t 24\t [ 1.1811275 -1.3767409] \t0\ttrue\n",
            "(0)\t 25\t [ 1.3436592 -1.9280031] \t0\ttrue\n",
            "(0)\t 26\t [ 1.362785  -1.8925959] \t0\ttrue\n",
            "(0)\t 27\t [ 1.3689156 -1.9704325] \t0\ttrue\n",
            "(1)\t 28\t [0.5970457  0.00931406] \t0\tfalse\n",
            "(1)\t 29\t [-0.20161109  0.47889918] \t1\ttrue\n",
            "(0)\t 30\t [ 1.4329205 -1.733088 ] \t0\ttrue\n",
            "(1)\t 31\t [ 0.8710382 -1.4349692] \t0\tfalse\n",
            "(1)\t 32\t [ 1.1583151 -1.1621002] \t0\tfalse\n",
            "(1)\t 33\t [ 0.42009318 -0.9347233 ] \t0\tfalse\n",
            "(1)\t 34\t [ 1.1303914 -1.727536 ] \t0\tfalse\n",
            "(0)\t 35\t [ 1.1302905 -1.1123419] \t0\ttrue\n",
            "(1)\t 36\t [ 1.025564  -1.2093168] \t0\tfalse\n",
            "(1)\t 37\t [ 1.2193606 -1.8432264] \t0\tfalse\n",
            "(1)\t 38\t [-1.3500687  1.6920907] \t1\ttrue\n",
            "(0)\t 39\t [-0.0106354 -0.3118989] \t0\ttrue\n",
            "(1)\t 40\t [ 0.19598943 -0.2728597 ] \t0\tfalse\n",
            "(0)\t 41\t [ 1.2473284 -1.8606598] \t0\ttrue\n",
            "(1)\t 42\t [-1.1725246  1.2543566] \t1\ttrue\n",
            "(1)\t 43\t [-0.56510603  0.36413744] \t1\ttrue\n",
            "(0)\t 44\t [ 1.150681  -1.9271443] \t0\ttrue\n",
            "(1)\t 45\t [ 0.91983163 -0.63260293] \t0\tfalse\n",
            "(0)\t 46\t [ 1.2022808 -1.9324342] \t0\ttrue\n",
            "(1)\t 47\t [ 1.0806972 -1.4967072] \t0\tfalse\n",
            "(1)\t 48\t [ 0.8263165 -0.6051962] \t0\tfalse\n",
            "(0)\t 49\t [ 1.4042372 -1.763442 ] \t0\ttrue\n",
            "(1)\t 50\t [0.25525504 0.36849484] \t1\ttrue\n",
            "(0)\t 51\t [ 1.0850604 -1.4758614] \t0\ttrue\n",
            "(1)\t 52\t [ 0.73435605 -0.7486044 ] \t0\tfalse\n",
            "(0)\t 53\t [ 0.8986857 -0.5690715] \t0\ttrue\n",
            "(1)\t 54\t [-0.19978061  0.7585574 ] \t1\ttrue\n",
            "(1)\t 55\t [ 0.03000018 -0.20297278] \t0\tfalse\n",
            "(0)\t 56\t [ 0.68805206 -0.9211904 ] \t0\ttrue\n",
            "(0)\t 57\t [ 1.416765  -1.8266805] \t0\ttrue\n",
            "(1)\t 58\t [ 0.8993544 -1.5899774] \t0\tfalse\n",
            "(1)\t 59\t [ 1.371614  -1.3796942] \t0\tfalse\n",
            "(0)\t 60\t [ 1.347532  -1.6621029] \t0\ttrue\n",
            "(0)\t 61\t [-0.49540016  0.29539126] \t1\tfalse\n",
            "(0)\t 62\t [ 0.6728598 -1.109817 ] \t0\ttrue\n",
            "(0)\t 63\t [ 1.321789  -1.9116526] \t0\ttrue\n",
            "(0)\t 64\t [ 1.0502516 -0.9874294] \t0\ttrue\n",
            "(0)\t 65\t [ 1.1609874 -1.6991888] \t0\ttrue\n",
            "(0)\t 66\t [ 0.7874439 -0.6183184] \t0\ttrue\n",
            "(0)\t 67\t [ 1.2692552 -1.8726168] \t0\ttrue\n",
            "(0)\t 68\t [ 0.00227807 -0.12693606] \t0\ttrue\n",
            "(0)\t 69\t [ 1.3162816 -1.8295081] \t0\ttrue\n",
            "(0)\t 70\t [-0.11648074 -0.11672822] \t0\ttrue\n",
            "(1)\t 71\t [ 1.3053026 -1.72475  ] \t0\tfalse\n",
            "(1)\t 72\t [-1.277406   1.1153162] \t1\ttrue\n",
            "(0)\t 73\t [ 1.370029  -1.8713276] \t0\ttrue\n",
            "(0)\t 74\t [ 1.31184   -1.9605435] \t0\ttrue\n",
            "(0)\t 75\t [ 1.2613175 -1.8462448] \t0\ttrue\n",
            "(0)\t 76\t [ 0.96357703 -1.4879333 ] \t0\ttrue\n",
            "(0)\t 77\t [ 1.2773764 -1.8197522] \t0\ttrue\n",
            "(1)\t 78\t [ 1.2714803 -1.5145752] \t0\tfalse\n",
            "(0)\t 79\t [ 0.9760633  -0.61090904] \t0\ttrue\n",
            "(1)\t 80\t [ 0.067678   -0.24970281] \t0\tfalse\n",
            "(0)\t 81\t [ 1.133177  -1.1528729] \t0\ttrue\n",
            "(0)\t 82\t [ 0.7984759 -1.3899635] \t0\ttrue\n",
            "(0)\t 83\t [ 1.2908592 -1.9377646] \t0\ttrue\n",
            "(1)\t 84\t [ 1.0329481 -1.2952667] \t0\tfalse\n",
            "(0)\t 85\t [ 0.35199222 -0.34353414] \t0\ttrue\n",
            "(0)\t 86\t [ 0.28200394 -0.467446  ] \t0\ttrue\n",
            "(0)\t 87\t [ 1.4206624 -1.9112673] \t0\ttrue\n",
            "(0)\t 88\t [ 0.42388633 -0.5977304 ] \t0\ttrue\n",
            "(0)\t 89\t [ 1.2122942 -1.8433515] \t0\ttrue\n",
            "(0)\t 90\t [ 1.3153504 -1.8418472] \t0\ttrue\n",
            "(0)\t 91\t [ 1.1403368 -1.7860985] \t0\ttrue\n",
            "(0)\t 92\t [ 1.3912518 -1.2395028] \t0\ttrue\n",
            "(0)\t 93\t [ 1.4046342 -1.9697835] \t0\ttrue\n",
            "(0)\t 94\t [ 0.62098855 -0.4604857 ] \t0\ttrue\n",
            "(1)\t 95\t [-0.8155998  1.3024755] \t1\ttrue\n",
            "(0)\t 96\t [ 0.1112613 -0.6126101] \t0\ttrue\n",
            "(1)\t 97\t [-1.4805708  1.5544955] \t1\ttrue\n",
            "(0)\t 98\t [ 0.09379586 -0.45641768] \t0\ttrue\n",
            "(0)\t 99\t [ 1.0058843 -1.6323519] \t0\ttrue\n",
            "(0)\t 100\t [ 1.0624201 -1.0717938] \t0\ttrue\n",
            "(1)\t 101\t [ 1.2152543 -1.5832517] \t0\tfalse\n",
            "(1)\t 102\t [-0.7053293   0.24188405] \t1\ttrue\n",
            "(0)\t 103\t [ 0.6533298  -0.30255875] \t0\ttrue\n",
            "(0)\t 104\t [ 0.25110662 -0.612397  ] \t0\ttrue\n",
            "(0)\t 105\t [ 0.6046009  -0.49014705] \t0\ttrue\n",
            "(0)\t 106\t [-0.12084201 -0.1837916 ] \t0\ttrue\n",
            "(0)\t 107\t [-0.78353167  0.4510058 ] \t1\tfalse\n",
            "(1)\t 108\t [-0.65224004  0.97109616] \t1\ttrue\n",
            "(0)\t 109\t [ 1.300176  -1.8301972] \t0\ttrue\n",
            "(1)\t 110\t [ 1.3648363 -1.2088438] \t0\tfalse\n",
            "(1)\t 111\t [ 1.286591  -1.3962632] \t0\tfalse\n",
            "(0)\t 112\t [ 0.46534535 -0.18385127] \t0\ttrue\n",
            "(0)\t 113\t [ 1.3662537 -1.9610741] \t0\ttrue\n",
            "(0)\t 114\t [-0.58457303  0.20283061] \t1\tfalse\n",
            "(1)\t 115\t [-1.1150067  1.1908942] \t1\ttrue\n",
            "(1)\t 116\t [ 0.19150937 -0.42692137] \t0\tfalse\n",
            "(1)\t 117\t [ 1.358187  -1.9325576] \t0\tfalse\n",
            "(0)\t 118\t [-1.2239112  1.2959862] \t1\tfalse\n",
            "(0)\t 119\t [ 1.1887151 -1.5878346] \t0\ttrue\n",
            "(0)\t 120\t [ 1.3675542 -1.9119041] \t0\ttrue\n",
            "(0)\t 121\t [ 0.5518168 -0.7058895] \t0\ttrue\n",
            "(0)\t 122\t [ 0.7929671 -0.9137936] \t0\ttrue\n",
            "(0)\t 123\t [ 0.8531704 -0.527887 ] \t0\ttrue\n",
            "(1)\t 124\t [ 0.24688905 -0.5191614 ] \t0\tfalse\n",
            "(1)\t 125\t [ 1.2516587 -1.8277383] \t0\tfalse\n",
            "(1)\t 126\t [ 0.85919845 -0.5622005 ] \t0\tfalse\n",
            "(0)\t 127\t [ 1.4095047 -1.9215968] \t0\ttrue\n",
            "(0)\t 128\t [-0.24472868  0.00859371] \t1\tfalse\n",
            "(0)\t 129\t [ 0.7603457 -1.2097235] \t0\ttrue\n",
            "(0)\t 130\t [ 1.3719614 -1.9542361] \t0\ttrue\n",
            "(0)\t 131\t [ 1.3918253 -1.9463603] \t0\ttrue\n",
            "(0)\t 132\t [ 1.0261815 -1.5012109] \t0\ttrue\n",
            "(0)\t 133\t [ 1.139734  -1.8255259] \t0\ttrue\n",
            "(0)\t 134\t [ 1.1593945  -0.74721456] \t0\ttrue\n",
            "(0)\t 135\t [ 0.7908374  -0.92523444] \t0\ttrue\n",
            "(0)\t 136\t [ 1.3230121 -1.7842374] \t0\ttrue\n",
            "(1)\t 137\t [ 0.4671782  -0.54193825] \t0\tfalse\n",
            "(0)\t 138\t [ 1.2631553 -1.1513724] \t0\ttrue\n",
            "(1)\t 139\t [-1.1264539  1.1640115] \t1\ttrue\n",
            "(0)\t 140\t [ 0.18825233 -0.60642445] \t0\ttrue\n",
            "(0)\t 141\t [ 1.4011569 -1.7563167] \t0\ttrue\n",
            "(0)\t 142\t [ 1.1923689 -1.6696746] \t0\ttrue\n",
            "(0)\t 143\t [ 0.63348114 -1.1990894 ] \t0\ttrue\n",
            "(0)\t 144\t [ 1.2881458 -1.7331336] \t0\ttrue\n",
            "(1)\t 145\t [-0.01452329  0.03927349] \t1\ttrue\n",
            "(1)\t 146\t [ 0.06730071 -0.37319323] \t0\tfalse\n",
            "(0)\t 147\t [ 1.4000072 -1.9174908] \t0\ttrue\n",
            "(1)\t 148\t [ 1.0524771 -1.5731133] \t0\tfalse\n",
            "(0)\t 149\t [ 0.73415637 -1.2614201 ] \t0\ttrue\n",
            "(0)\t 150\t [ 0.52308464 -0.6376928 ] \t0\ttrue\n",
            "(0)\t 151\t [ 0.02088306 -0.28768227] \t0\ttrue\n",
            "(1)\t 152\t [-1.0955397   0.84400207] \t1\ttrue\n",
            "(0)\t 153\t [ 0.95362824 -1.0519722 ] \t0\ttrue\n",
            "(0)\t 154\t [ 0.9753227 -0.6211529] \t0\ttrue\n",
            "(1)\t 155\t [ 0.13439262 -0.17684689] \t0\tfalse\n",
            "(0)\t 156\t [-0.59092915  0.3801552 ] \t1\tfalse\n",
            "(1)\t 157\t [-0.45705917  0.542963  ] \t1\ttrue\n",
            "(0)\t 158\t [ 1.076273  -1.7529287] \t0\ttrue\n",
            "(1)\t 159\t [ 0.7490982 -1.0124798] \t0\tfalse\n",
            "(1)\t 160\t [-1.3699131  1.525433 ] \t1\ttrue\n",
            "(0)\t 161\t [0.27968976 0.29743487] \t1\tfalse\n",
            "(1)\t 162\t [-0.4548308   0.12040932] \t1\ttrue\n",
            "(0)\t 163\t [ 1.3356109 -1.7021114] \t0\ttrue\n",
            "(0)\t 164\t [ 0.1275167  -0.39508793] \t0\ttrue\n",
            "(1)\t 165\t [ 0.7183526 -0.922209 ] \t0\tfalse\n",
            "(0)\t 166\t [ 1.4367802 -1.9818215] \t0\ttrue\n",
            "(1)\t 167\t [ 0.8679017 -1.2923923] \t0\tfalse\n",
            "(0)\t 168\t [ 1.281104  -1.8056041] \t0\ttrue\n",
            "(0)\t 169\t [ 1.5374475 -1.9439359] \t0\ttrue\n",
            "(0)\t 170\t [ 1.2074804 -1.1308415] \t0\ttrue\n",
            "(0)\t 171\t [-0.30571467  0.30545232] \t1\tfalse\n",
            "(1)\t 172\t [-1.168712   1.1533918] \t1\ttrue\n",
            "(1)\t 173\t [ 0.11317682 -0.32067904] \t0\tfalse\n",
            "(1)\t 174\t [ 0.5451374 -0.9822499] \t0\tfalse\n",
            "(1)\t 175\t [ 1.2651055 -1.9156691] \t0\tfalse\n",
            "(1)\t 176\t [-1.4790071  1.6276519] \t1\ttrue\n",
            "(0)\t 177\t [ 0.28166285 -0.4157423 ] \t0\ttrue\n",
            "(1)\t 178\t [ 0.0465405  -0.34191847] \t0\tfalse\n",
            "(1)\t 179\t [ 1.0669386 -0.9975297] \t0\tfalse\n",
            "(1)\t 180\t [-0.757993    0.54602265] \t1\ttrue\n",
            "(1)\t 181\t [-1.501415  1.745929] \t1\ttrue\n",
            "(0)\t 182\t [ 0.95835984 -1.6338074 ] \t0\ttrue\n",
            "(0)\t 183\t [ 0.77977705 -0.9633151 ] \t0\ttrue\n",
            "(0)\t 184\t [ 0.3591436  -0.84338075] \t0\ttrue\n",
            "(0)\t 185\t [ 0.58168185 -0.69242215] \t0\ttrue\n",
            "(1)\t 186\t [ 1.3139541 -1.7378279] \t0\tfalse\n",
            "(0)\t 187\t [ 1.0036585 -1.0092214] \t0\ttrue\n",
            "(0)\t 188\t [ 0.8068688  -0.96136993] \t0\ttrue\n",
            "(0)\t 189\t [ 0.96286625 -1.4415345 ] \t0\ttrue\n",
            "(0)\t 190\t [ 1.3725154 -1.942427 ] \t0\ttrue\n",
            "(1)\t 191\t [ 1.0050473 -0.952175 ] \t0\tfalse\n",
            "(0)\t 192\t [ 1.3148906 -1.7551677] \t0\ttrue\n",
            "(0)\t 193\t [ 1.184161  -1.8176045] \t0\ttrue\n",
            "(0)\t 194\t [-0.88273335  0.65028524] \t1\tfalse\n",
            "(1)\t 195\t [ 1.0030754 -1.3976278] \t0\tfalse\n",
            "(0)\t 196\t [ 1.1447545 -1.9371412] \t0\ttrue\n",
            "(0)\t 197\t [ 1.2330705 -1.9096643] \t0\ttrue\n",
            "(0)\t 198\t [-0.17606091 -0.04587267] \t1\tfalse\n",
            "(0)\t 199\t [ 1.1804159 -1.1932033] \t0\ttrue\n",
            "(1)\t 200\t [-0.9644112   0.71159315] \t1\ttrue\n",
            "(1)\t 201\t [ 0.7551743 -0.647161 ] \t0\tfalse\n",
            "(1)\t 202\t [ 1.2618248 -1.7015866] \t0\tfalse\n",
            "(0)\t 203\t [ 1.2510006 -1.7695538] \t0\ttrue\n",
            "(0)\t 204\t [-0.12021673  0.01257839] \t1\tfalse\n",
            "(1)\t 205\t [ 1.214252  -1.2611535] \t0\tfalse\n",
            "(0)\t 206\t [ 1.3280189 -1.8693732] \t0\ttrue\n",
            "(0)\t 207\t [ 0.02135977 -0.5939414 ] \t0\ttrue\n",
            "(0)\t 208\t [ 1.3459055 -1.8753688] \t0\ttrue\n",
            "(1)\t 209\t [ 0.9547335 -1.6557704] \t0\tfalse\n",
            "(0)\t 210\t [ 1.3026295 -1.9356421] \t0\ttrue\n",
            "(0)\t 211\t [ 1.2452292 -1.8732411] \t0\ttrue\n",
            "(0)\t 212\t [ 1.3788552 -1.9401289] \t0\ttrue\n",
            "(1)\t 213\t [-1.417689   1.5770404] \t1\ttrue\n",
            "(1)\t 214\t [ 0.1657832  -0.55334646] \t0\tfalse\n",
            "(1)\t 215\t [ 1.2977462 -1.8559713] \t0\tfalse\n",
            "(1)\t 216\t [ 0.22393373 -0.6259664 ] \t0\tfalse\n",
            "(0)\t 217\t [ 0.9777907  -0.74066645] \t0\ttrue\n",
            "(0)\t 218\t [ 0.9404475 -1.5258567] \t0\ttrue\n",
            "(1)\t 219\t [-1.2101429  1.5522037] \t1\ttrue\n",
            "(1)\t 220\t [ 1.2682143 -1.8144081] \t0\tfalse\n",
            "(1)\t 221\t [ 1.2794125  -0.77479905] \t0\tfalse\n",
            "(1)\t 222\t [ 0.43918103 -0.4941123 ] \t0\tfalse\n",
            "(1)\t 223\t [ 0.7465898 -1.291431 ] \t0\tfalse\n",
            "(0)\t 224\t [ 1.2895005 -1.6711658] \t0\ttrue\n",
            "(0)\t 225\t [ 1.290373  -1.1713011] \t0\ttrue\n",
            "(1)\t 226\t [-0.4752161  0.9366632] \t1\ttrue\n",
            "(0)\t 227\t [ 0.77486825 -0.74860007] \t0\ttrue\n",
            "(0)\t 228\t [ 1.1551106 -0.9125408] \t0\ttrue\n",
            "(0)\t 229\t [ 0.3483573 -0.57682  ] \t0\ttrue\n",
            "(1)\t 230\t [-1.5304315  1.6075367] \t1\ttrue\n",
            "(1)\t 231\t [ 1.1360168 -1.7045714] \t0\tfalse\n",
            "(1)\t 232\t [-0.579679    0.39677134] \t1\ttrue\n",
            "(0)\t 233\t [ 0.7822933 -1.3153224] \t0\ttrue\n",
            "(0)\t 234\t [ 0.5918762  -0.40875354] \t0\ttrue\n",
            "(1)\t 235\t [0.15034577 0.34523845] \t1\ttrue\n",
            "(0)\t 236\t [ 1.1925404 -1.7281373] \t0\ttrue\n",
            "(1)\t 237\t [-0.7915345  0.7446412] \t1\ttrue\n",
            "(1)\t 238\t [-1.4252436  1.4807613] \t1\ttrue\n",
            "(1)\t 239\t [ 0.69315267 -0.79783225] \t0\tfalse\n",
            "(0)\t 240\t [ 0.70910823 -1.0046508 ] \t0\ttrue\n",
            "(0)\t 241\t [-0.14961837 -0.20083341] \t0\ttrue\n",
            "(0)\t 242\t [ 0.35806042 -0.4792513 ] \t0\ttrue\n",
            "(1)\t 243\t [ 0.21434538 -0.23031728] \t0\tfalse\n",
            "(0)\t 244\t [ 1.1325966 -1.6885998] \t0\ttrue\n",
            "(0)\t 245\t [ 1.1945903 -1.9289217] \t0\ttrue\n",
            "(0)\t 246\t [-0.514047    0.44151476] \t1\tfalse\n",
            "(0)\t 247\t [ 0.31421924 -0.04506074] \t0\ttrue\n",
            "(1)\t 248\t [ 1.293078  -1.3617114] \t0\tfalse\n",
            "(1)\t 249\t [ 1.0936434 -1.1287975] \t0\tfalse\n",
            "(1)\t 250\t [ 0.4950072  -0.19587402] \t0\tfalse\n",
            "(1)\t 251\t [-1.11003    1.5148709] \t1\ttrue\n",
            "(0)\t 252\t [ 1.1758645 -1.1059568] \t0\ttrue\n",
            "(1)\t 253\t [-1.2211357  1.399389 ] \t1\ttrue\n",
            "(1)\t 254\t [-0.0153816  0.3838065] \t1\ttrue\n",
            "(1)\t 255\t [-0.56392205  0.518689  ] \t1\ttrue\n",
            "(0)\t 256\t [ 1.1809702 -1.7697778] \t0\ttrue\n",
            "(0)\t 257\t [ 0.3084141  -0.49235237] \t0\ttrue\n",
            "(0)\t 258\t [ 0.66394377 -1.3187749 ] \t0\ttrue\n",
            "(1)\t 259\t [-0.1673002   0.12456883] \t1\ttrue\n",
            "(1)\t 260\t [ 0.31197867 -0.51824075] \t0\tfalse\n",
            "(1)\t 261\t [-1.0043913  0.7447128] \t1\ttrue\n",
            "(0)\t 262\t [ 1.0991952 -1.3117743] \t0\ttrue\n",
            "(0)\t 263\t [ 1.2431295 -1.6877272] \t0\ttrue\n",
            "(0)\t 264\t [ 0.04996084 -0.26247492] \t0\ttrue\n",
            "(1)\t 265\t [-1.0445721  0.9867107] \t1\ttrue\n",
            "(0)\t 266\t [ 0.722227 -1.312916] \t0\ttrue\n",
            "(0)\t 267\t [ 0.8153791 -1.0186466] \t0\ttrue\n",
            "(0)\t 268\t [-0.8013272  0.6404878] \t1\tfalse\n",
            "(0)\t 269\t [-0.04157411 -0.34945765] \t0\ttrue\n",
            "(0)\t 270\t [-1.0265934  0.7794908] \t1\tfalse\n",
            "(0)\t 271\t [ 0.7715781 -0.9740502] \t0\ttrue\n",
            "(0)\t 272\t [-0.07404113  0.06680135] \t1\tfalse\n",
            "(1)\t 273\t [ 0.3469027 -0.6182293] \t0\tfalse\n",
            "(0)\t 274\t [-0.08060043 -0.17714159] \t0\ttrue\n",
            "(0)\t 275\t [-1.1364353  0.8840045] \t1\tfalse\n",
            "(0)\t 276\t [ 1.1184688 -0.8757978] \t0\ttrue\n",
            "(1)\t 277\t [-0.9447219  0.6931256] \t1\ttrue\n",
            "(1)\t 278\t [ 0.40033814 -0.6156874 ] \t0\tfalse\n",
            "(1)\t 279\t [-1.5060984  1.6985623] \t1\ttrue\n",
            "(1)\t 280\t [-0.16426894 -0.24481586] \t0\tfalse\n",
            "(0)\t 281\t [ 0.87142646 -0.7815836 ] \t0\ttrue\n",
            "(0)\t 282\t [ 0.96203864 -1.4799949 ] \t0\ttrue\n",
            "(0)\t 283\t [ 0.34477085 -0.623678  ] \t0\ttrue\n",
            "(1)\t 284\t [-0.84482527  1.1900785 ] \t1\ttrue\n",
            "(1)\t 285\t [-1.452832   1.7905891] \t1\ttrue\n",
            "(1)\t 286\t [-1.3850052  1.7504551] \t1\ttrue\n",
            "(0)\t 287\t [-1.3060753  1.5319456] \t1\tfalse\n",
            "(1)\t 288\t [-1.4160174  1.5797507] \t1\ttrue\n",
            "(1)\t 289\t [-1.0258012  1.075212 ] \t1\ttrue\n",
            "(0)\t 290\t [ 0.8888663 -1.4498504] \t0\ttrue\n",
            "(1)\t 291\t [ 0.67552495 -0.2134018 ] \t0\tfalse\n",
            "(1)\t 292\t [ 1.0464141  -0.80863446] \t0\tfalse\n",
            "(1)\t 293\t [-0.8544402  1.0530128] \t1\ttrue\n",
            "(0)\t 294\t [ 1.250179  -1.5818245] \t0\ttrue\n",
            "(1)\t 295\t [-0.05980117  0.25303054] \t1\ttrue\n",
            "(0)\t 296\t [ 0.7657459  -0.38314793] \t0\ttrue\n",
            "(0)\t 297\t [ 0.22754225 -0.3460577 ] \t0\ttrue\n",
            "(0)\t 298\t [ 0.05252442 -0.45727587] \t0\ttrue\n",
            "(0)\t 299\t [ 0.6188124 -0.9559826] \t0\ttrue\n",
            "(0)\t 300\t [-0.5444298   0.26709482] \t1\tfalse\n",
            "(1)\t 301\t [ 0.08953102 -0.35750923] \t0\tfalse\n",
            "(0)\t 302\t [ 1.2356099 -1.8221433] \t0\ttrue\n",
            "(0)\t 303\t [ 1.3345547 -1.8395107] \t0\ttrue\n",
            "(0)\t 304\t [ 0.66143656 -0.8808775 ] \t0\ttrue\n",
            "(1)\t 305\t [ 1.3894854 -1.5862314] \t0\tfalse\n",
            "(1)\t 306\t [-0.5984272  0.5475721] \t1\ttrue\n",
            "(1)\t 307\t [ 1.3437405 -1.8415167] \t0\tfalse\n",
            "(1)\t 308\t [-0.66032314  0.50887907] \t1\ttrue\n",
            "(1)\t 309\t [-0.3369431   0.21173021] \t1\ttrue\n",
            "(1)\t 310\t [ 0.92840576 -1.4645419 ] \t0\tfalse\n",
            "(1)\t 311\t [ 1.3223758 -1.9932309] \t0\tfalse\n",
            "(1)\t 312\t [0.09642667 0.27818847] \t1\ttrue\n",
            "(1)\t 313\t [ 0.42315957 -0.8801445 ] \t0\tfalse\n",
            "(0)\t 314\t [ 1.0891899 -1.3540628] \t0\ttrue\n",
            "(0)\t 315\t [ 0.8414253 -1.4161428] \t0\ttrue\n",
            "(0)\t 316\t [ 0.9921004  -0.98727995] \t0\ttrue\n",
            "(0)\t 317\t [ 1.3429136 -1.8707902] \t0\ttrue\n",
            "(0)\t 318\t [ 0.42569238 -0.4900707 ] \t0\ttrue\n",
            "(0)\t 319\t [ 1.3964789 -1.8428761] \t0\ttrue\n",
            "(1)\t 320\t [ 0.8984097 -0.7497916] \t0\tfalse\n",
            "(0)\t 321\t [ 0.8489928 -1.0400747] \t0\ttrue\n",
            "(0)\t 322\t [ 0.80896056 -1.165617  ] \t0\ttrue\n",
            "(0)\t 323\t [-0.06030057 -0.25582877] \t0\ttrue\n",
            "(1)\t 324\t [ 0.8483007 -1.2189722] \t0\tfalse\n",
            "(1)\t 325\t [-1.5297289  1.8334043] \t1\ttrue\n",
            "(0)\t 326\t [ 1.2726052 -1.5986602] \t0\ttrue\n",
            "(0)\t 327\t [ 1.2389338 -1.6911232] \t0\ttrue\n",
            "(0)\t 328\t [ 1.3474975 -1.4367201] \t0\ttrue\n",
            "(0)\t 329\t [ 1.3329751 -1.2405665] \t0\ttrue\n",
            "(0)\t 330\t [ 0.77608854 -1.1025193 ] \t0\ttrue\n",
            "(0)\t 331\t [ 0.3819807  -0.65563834] \t0\ttrue\n",
            "(0)\t 332\t [ 1.475009 -1.691044] \t0\ttrue\n",
            "(1)\t 333\t [ 1.2118404 -1.393908 ] \t0\tfalse\n",
            "(1)\t 334\t [-1.5450805  1.7319891] \t1\ttrue\n",
            "(0)\t 335\t [ 0.74837226 -0.8667634 ] \t0\ttrue\n",
            "(1)\t 336\t [-1.4169251  1.7814128] \t1\ttrue\n",
            "(0)\t 337\t [ 1.0770934 -1.5828941] \t0\ttrue\n",
            "(0)\t 338\t [ 0.11137459 -0.28465   ] \t0\ttrue\n",
            "(1)\t 339\t [ 0.69278765 -0.33435827] \t0\tfalse\n",
            "(0)\t 340\t [-0.30543235 -0.04783535] \t1\tfalse\n",
            "(1)\t 341\t [ 0.9844587 -1.3953559] \t0\tfalse\n",
            "(1)\t 342\t [ 0.3713604  -0.08927312] \t0\tfalse\n",
            "(1)\t 343\t [-1.3597937  1.6143608] \t1\ttrue\n",
            "(1)\t 344\t [-0.09118871  0.47815686] \t1\ttrue\n",
            "(1)\t 345\t [-0.48759988  0.54870903] \t1\ttrue\n",
            "(1)\t 346\t [-1.1348225   0.84076405] \t1\ttrue\n",
            "(1)\t 347\t [-1.5687375  1.6993842] \t1\ttrue\n",
            "(0)\t 348\t [-1.1567584  0.9902688] \t1\tfalse\n",
            "(0)\t 349\t [ 0.7354795 -0.3137112] \t0\ttrue\n",
            "(0)\t 350\t [ 1.3291469 -1.0678262] \t0\ttrue\n",
            "(0)\t 351\t [ 1.2495694 -1.6509652] \t0\ttrue\n",
            "(1)\t 352\t [ 0.2070557  -0.43944702] \t0\tfalse\n",
            "(1)\t 353\t [-1.2783964  1.713325 ] \t1\ttrue\n",
            "(0)\t 354\t [-0.16218175  0.00164922] \t1\tfalse\n",
            "(0)\t 355\t [ 1.4381014 -1.8736043] \t0\ttrue\n",
            "(1)\t 356\t [-0.4028313  1.0636533] \t1\ttrue\n",
            "(1)\t 357\t [-1.1270097  1.301756 ] \t1\ttrue\n",
            "(0)\t 358\t [-1.0290437  0.9784024] \t1\tfalse\n",
            "(1)\t 359\t [ 0.5657073  -0.43178153] \t0\tfalse\n",
            "(1)\t 360\t [ 0.65634096 -1.1893741 ] \t0\tfalse\n",
            "(1)\t 361\t [ 0.75236464 -1.0878218 ] \t0\tfalse\n",
            "(0)\t 362\t [-1.0760727   0.79771256] \t1\tfalse\n",
            "(1)\t 363\t [ 1.1125227 -1.5734622] \t0\tfalse\n",
            "(1)\t 364\t [ 1.0421729  -0.70639527] \t0\tfalse\n",
            "(1)\t 365\t [-0.20060535  0.5469703 ] \t1\ttrue\n",
            "(1)\t 366\t [ 0.01545573 -0.24341404] \t0\tfalse\n",
            "(1)\t 367\t [-1.53618   1.676291] \t1\ttrue\n",
            "(1)\t 368\t [-1.5016387  1.6900676] \t1\ttrue\n",
            "(0)\t 369\t [ 1.2653795 -1.6372132] \t0\ttrue\n",
            "(1)\t 370\t [-1.5040927  1.7331767] \t1\ttrue\n",
            "(1)\t 371\t [ 0.82758933 -0.8444866 ] \t0\tfalse\n",
            "(0)\t 372\t [ 1.437829  -1.8528118] \t0\ttrue\n",
            "(0)\t 373\t [ 1.23228   -1.8215876] \t0\ttrue\n",
            "(1)\t 374\t [ 0.50701827 -0.46043375] \t0\tfalse\n",
            "(1)\t 375\t [ 1.0882397 -0.9152713] \t0\tfalse\n",
            "(1)\t 376\t [ 0.99171185 -0.8831953 ] \t0\tfalse\n",
            "(0)\t 377\t [-0.8485514  0.7916883] \t1\tfalse\n",
            "(0)\t 378\t [ 0.02058552 -0.1307976 ] \t0\ttrue\n",
            "(1)\t 379\t [-0.63220227  0.8274634 ] \t1\ttrue\n",
            "(0)\t 380\t [-0.94679636  0.7063035 ] \t1\tfalse\n",
            "(0)\t 381\t [ 0.23732992 -0.27901384] \t0\ttrue\n",
            "(1)\t 382\t [ 0.7200674  -0.66035485] \t0\tfalse\n",
            "(0)\t 383\t [ 1.286375  -1.7157075] \t0\ttrue\n",
            "(0)\t 384\t [-1.4529814  1.6782072] \t1\tfalse\n",
            "(1)\t 385\t [ 0.4950436 -0.7100002] \t0\tfalse\n",
            "(0)\t 386\t [ 1.3903278 -1.8792839] \t0\ttrue\n",
            "(0)\t 387\t [ 1.4089254 -1.9401869] \t0\ttrue\n",
            "(1)\t 388\t [ 0.20293802 -0.5518544 ] \t0\tfalse\n",
            "(0)\t 389\t [ 0.18669966 -0.5644623 ] \t0\ttrue\n",
            "(0)\t 390\t [ 1.277755 -1.779188] \t0\ttrue\n",
            "(1)\t 391\t [-1.3985729  1.7119522] \t1\ttrue\n",
            "(1)\t 392\t [ 1.1501985 -1.2219672] \t0\tfalse\n",
            "(1)\t 393\t [-0.2355546   0.27976385] \t1\ttrue\n",
            "(1)\t 394\t [-1.3485922  1.4573522] \t1\ttrue\n",
            "(1)\t 395\t [-0.4189112   0.09933388] \t1\ttrue\n",
            "(0)\t 396\t [ 1.4109019 -1.9744674] \t0\ttrue\n",
            "(1)\t 397\t [-1.3979771  1.7019341] \t1\ttrue\n",
            "(1)\t 398\t [ 0.67779315 -0.9406978 ] \t0\tfalse\n",
            "(1)\t 399\t [-0.6762799   0.49299714] \t1\ttrue\n",
            "(1)\t 400\t [-0.67501885  0.44890282] \t1\ttrue\n",
            "(1)\t 401\t [ 0.537964  -0.5296195] \t0\tfalse\n",
            "(0)\t 402\t [ 1.0554111 -1.5247335] \t0\ttrue\n",
            "(0)\t 403\t [ 1.2364537 -1.7546582] \t0\ttrue\n",
            "(0)\t 404\t [ 0.24409845 -0.7865866 ] \t0\ttrue\n",
            "(1)\t 405\t [ 0.01838013 -0.22156775] \t0\tfalse\n",
            "(1)\t 406\t [0.18978626 0.27017382] \t1\ttrue\n",
            "(0)\t 407\t [ 1.2809091 -1.9007385] \t0\ttrue\n",
            "(1)\t 408\t [ 0.9496467 -1.480428 ] \t0\tfalse\n",
            "(0)\t 409\t [-0.13490564 -0.02454374] \t1\tfalse\n",
            "(1)\t 410\t [-0.7244391   0.53827584] \t1\ttrue\n",
            "(1)\t 411\t [-1.467263   1.7984216] \t1\ttrue\n",
            "(1)\t 412\t [ 0.93963814 -0.9858892 ] \t0\tfalse\n",
            "(1)\t 413\t [-1.4287208  1.6617427] \t1\ttrue\n",
            "(1)\t 414\t [ 1.3384953 -1.6880985] \t0\tfalse\n",
            "(0)\t 415\t [ 1.2829795 -1.3525673] \t0\ttrue\n",
            "(1)\t 416\t [-0.15844515 -0.02211781] \t1\ttrue\n",
            "(0)\t 417\t [ 0.9812207 -1.0250798] \t0\ttrue\n",
            "(1)\t 418\t [ 0.09074735 -0.3366836 ] \t0\tfalse\n",
            "(1)\t 419\t [-0.20732161 -0.06494524] \t1\ttrue\n",
            "(1)\t 420\t [-0.46307397  0.2070795 ] \t1\ttrue\n",
            "(1)\t 421\t [ 0.8887748  -0.69359976] \t0\tfalse\n",
            "(1)\t 422\t [ 0.83856523 -1.4042861 ] \t0\tfalse\n",
            "(1)\t 423\t [ 0.8842056 -1.1813614] \t0\tfalse\n",
            "(1)\t 424\t [-0.32195762  0.2123403 ] \t1\ttrue\n",
            "(1)\t 425\t [-0.7569957   0.61128294] \t1\ttrue\n",
            "(0)\t 426\t [ 0.4623089 -0.9936603] \t0\ttrue\n",
            "(0)\t 427\t [ 1.0720613 -1.1457157] \t0\ttrue\n",
            "(1)\t 428\t [-0.01061316 -0.2398723 ] \t0\tfalse\n",
            "(1)\t 429\t [0.16723032 0.1382169 ] \t0\tfalse\n",
            "(0)\t 430\t [ 1.0872787 -1.2363987] \t0\ttrue\n",
            "(1)\t 431\t [-1.3157015  1.0033978] \t1\ttrue\n",
            "(1)\t 432\t [-0.8613242   0.70729446] \t1\ttrue\n",
            "(0)\t 433\t [ 0.06237206 -0.14782415] \t0\ttrue\n",
            "(1)\t 434\t [-0.3581508   0.44637427] \t1\ttrue\n",
            "(1)\t 435\t [ 0.03267094 -0.4018834 ] \t0\tfalse\n",
            "(0)\t 436\t [ 1.4902987 -1.5874352] \t0\ttrue\n",
            "(1)\t 437\t [ 0.8712271 -1.2062678] \t0\tfalse\n",
            "(1)\t 438\t [-0.15233943 -0.07337339] \t1\ttrue\n",
            "(1)\t 439\t [-0.40504602  0.5247235 ] \t1\ttrue\n",
            "(1)\t 440\t [ 0.06267427 -0.5761868 ] \t0\tfalse\n",
            "(1)\t 441\t [-1.0533297   0.82471585] \t1\ttrue\n",
            "(0)\t 442\t [ 1.2860109 -1.3635892] \t0\ttrue\n",
            "(1)\t 443\t [-1.3913591  1.8200909] \t1\ttrue\n",
            "(0)\t 444\t [ 1.2781048 -1.1569589] \t0\ttrue\n",
            "(1)\t 445\t [ 0.9142955 -1.265541 ] \t0\tfalse\n",
            "(1)\t 446\t [ 1.4286295 -1.8133879] \t0\tfalse\n",
            "(0)\t 447\t [-0.68369865  0.36421397] \t1\tfalse\n",
            "(1)\t 448\t [ 0.7566586 -1.0772324] \t0\tfalse\n",
            "(0)\t 449\t [ 1.2514523 -1.5588078] \t0\ttrue\n",
            "(1)\t 450\t [-0.6861002  1.2307681] \t1\ttrue\n",
            "(1)\t 451\t [-0.06339628 -0.28359464] \t0\tfalse\n",
            "(1)\t 452\t [ 0.76162064 -0.61221945] \t0\tfalse\n",
            "(0)\t 453\t [-0.06458637 -0.0997244 ] \t0\ttrue\n",
            "(1)\t 454\t [ 1.0311284 -1.632812 ] \t0\tfalse\n",
            "(0)\t 455\t [ 0.9220565 -1.1343892] \t0\ttrue\n",
            "(1)\t 456\t [-1.4290892  1.7000525] \t1\ttrue\n",
            "(0)\t 457\t [ 1.3646743 -1.9318849] \t0\ttrue\n",
            "(1)\t 458\t [ 0.6102485  -0.36412537] \t0\tfalse\n",
            "(1)\t 459\t [-0.719602    0.48831838] \t1\ttrue\n",
            "(1)\t 460\t [ 0.07347294 -0.2675793 ] \t0\tfalse\n",
            "(1)\t 461\t [-1.4416642  1.6476021] \t1\ttrue\n",
            "(1)\t 462\t [-0.4206477   0.30330646] \t1\ttrue\n",
            "(1)\t 463\t [-0.80878896  0.51163006] \t1\ttrue\n",
            "(1)\t 464\t [ 0.6159334  -0.49952763] \t0\tfalse\n",
            "(1)\t 465\t [-1.2493472  1.400281 ] \t1\ttrue\n",
            "(1)\t 466\t [-0.1793095  -0.05987237] \t1\ttrue\n",
            "(1)\t 467\t [-0.65031815  0.4109538 ] \t1\ttrue\n",
            "(1)\t 468\t [ 0.19567427 -0.20585239] \t0\tfalse\n",
            "(0)\t 469\t [ 1.2858212 -1.9574605] \t0\ttrue\n",
            "(1)\t 470\t [ 1.4181421 -1.9016519] \t0\tfalse\n",
            "(0)\t 471\t [-0.32695347  0.05773244] \t1\tfalse\n",
            "(0)\t 472\t [ 0.27290237 -0.34804812] \t0\ttrue\n",
            "(0)\t 473\t [ 0.73171186 -0.854561  ] \t0\ttrue\n",
            "(0)\t 474\t [ 0.33613098 -0.2526351 ] \t0\ttrue\n",
            "(0)\t 475\t [ 0.87707686 -0.9361242 ] \t0\ttrue\n",
            "(0)\t 476\t [ 0.6760293  -0.70165163] \t0\ttrue\n",
            "(0)\t 477\t [ 1.3481929 -1.9223553] \t0\ttrue\n",
            "(1)\t 478\t [-1.434669  1.733691] \t1\ttrue\n",
            "(0)\t 479\t [ 1.2575731 -1.3590562] \t0\ttrue\n",
            "(0)\t 480\t [ 1.094271  -1.1539731] \t0\ttrue\n",
            "(0)\t 481\t [ 0.94000375 -1.1916112 ] \t0\ttrue\n",
            "(1)\t 482\t [-1.4695444  1.6511637] \t1\ttrue\n",
            "(0)\t 483\t [ 1.3653404 -1.5656188] \t0\ttrue\n",
            "(0)\t 484\t [-0.27823088 -0.09453573] \t1\tfalse\n",
            "(1)\t 485\t [-0.7803275  0.7642833] \t1\ttrue\n",
            "(1)\t 486\t [ 1.4038184 -1.8879809] \t0\tfalse\n",
            "(1)\t 487\t [-0.7978445  1.028092 ] \t1\ttrue\n",
            "(0)\t 488\t [ 1.2786947 -1.81251  ] \t0\ttrue\n",
            "(0)\t 489\t [ 0.9329097 -0.8315866] \t0\ttrue\n",
            "(1)\t 490\t [-0.2021933  -0.10471984] \t1\ttrue\n",
            "(0)\t 491\t [ 0.26778528 -0.2656128 ] \t0\ttrue\n",
            "(0)\t 492\t [ 0.02871175 -0.5863855 ] \t0\ttrue\n",
            "(0)\t 493\t [ 0.6107258  -0.18030779] \t0\ttrue\n",
            "(1)\t 494\t [ 0.09312155 -0.33738753] \t0\tfalse\n",
            "(1)\t 495\t [ 0.5673711 -1.1931417] \t0\tfalse\n",
            "(1)\t 496\t [ 0.8610387 -0.9536824] \t0\tfalse\n",
            "(1)\t 497\t [-0.28723165 -0.18806452] \t1\ttrue\n",
            "(0)\t 498\t [ 1.2864215 -1.8627499] \t0\ttrue\n",
            "(1)\t 499\t [ 1.2984841 -1.1059272] \t0\tfalse\n",
            "(1)\t 500\t [ 1.0311159 -1.4737375] \t0\tfalse\n",
            "(1)\t 501\t [-0.22273037  0.32222518] \t1\ttrue\n",
            "(0)\t 502\t [ 0.4875759 -0.5471558] \t0\ttrue\n",
            "(0)\t 503\t [ 1.1817884 -1.7598267] \t0\ttrue\n",
            "(1)\t 504\t [-1.0391555  1.0367122] \t1\ttrue\n",
            "(0)\t 505\t [ 1.2932601 -1.8783282] \t0\ttrue\n",
            "(1)\t 506\t [ 0.7380054 -0.5767014] \t0\tfalse\n",
            "(0)\t 507\t [ 1.2949097 -1.8224745] \t0\ttrue\n",
            "(0)\t 508\t [ 1.0490137 -1.0106907] \t0\ttrue\n",
            "(0)\t 509\t [ 0.88768226 -0.9625045 ] \t0\ttrue\n",
            "(1)\t 510\t [-0.79619575  0.61409277] \t1\ttrue\n",
            "(1)\t 511\t [ 1.1255107 -1.6089966] \t0\tfalse\n",
            "(0)\t 512\t [ 1.0917935 -1.7160614] \t0\ttrue\n",
            "(1)\t 513\t [-1.0381485  1.4678668] \t1\ttrue\n",
            "(1)\t 514\t [-0.3601189   0.21376826] \t1\ttrue\n",
            "(1)\t 515\t [-0.48297867  0.16818981] \t1\ttrue\n",
            "(1)\t 516\t [ 1.0521277 -1.3455336] \t0\tfalse\n",
            "(0)\t 517\t [ 1.3301013 -1.9107238] \t0\ttrue\n",
            "(0)\t 518\t [ 1.2353047 -1.0928078] \t0\ttrue\n",
            "(1)\t 519\t [-0.16638166  0.78909606] \t1\ttrue\n",
            "(1)\t 520\t [ 0.5881828 -0.9319105] \t0\tfalse\n",
            "(1)\t 521\t [-0.4894069  0.3211316] \t1\ttrue\n",
            "(1)\t 522\t [-1.3310995  1.5078194] \t1\ttrue\n",
            "(1)\t 523\t [-1.4436266  1.5253012] \t1\ttrue\n",
            "(0)\t 524\t [ 1.3917203 -1.6638182] \t0\ttrue\n",
            "(0)\t 525\t [ 0.37454566 -0.5503657 ] \t0\ttrue\n",
            "(1)\t 526\t [ 0.74628973 -0.953429  ] \t0\tfalse\n",
            "(1)\t 527\t [ 1.028828  -1.3112173] \t0\tfalse\n",
            "(1)\t 528\t [ 1.3475685 -1.9235945] \t0\tfalse\n",
            "(1)\t 529\t [-1.541992   1.7863879] \t1\ttrue\n",
            "(0)\t 530\t [0.33251986 0.00518617] \t0\ttrue\n",
            "(0)\t 531\t [ 0.02342272 -0.04861231] \t0\ttrue\n",
            "(0)\t 532\t [-1.2233355  1.5499369] \t1\tfalse\n",
            "(1)\t 533\t [-0.1300632   0.02809492] \t1\ttrue\n",
            "(1)\t 534\t [ 1.3476887 -1.9744906] \t0\tfalse\n",
            "(0)\t 535\t [ 1.1643703 -1.7453659] \t0\ttrue\n",
            "(1)\t 536\t [0.62266016 0.01752209] \t0\tfalse\n",
            "(0)\t 537\t [ 1.239506  -1.8357987] \t0\ttrue\n",
            "(0)\t 538\t [ 1.494663  -1.9366621] \t0\ttrue\n",
            "(0)\t 539\t [ 1.3631289 -1.8703988] \t0\ttrue\n",
            "(0)\t 540\t [ 1.4813094 -1.8002077] \t0\ttrue\n",
            "(0)\t 541\t [ 1.2368653 -1.9202219] \t0\ttrue\n",
            "(0)\t 542\t [ 1.0556176 -1.5375972] \t0\ttrue\n",
            "(0)\t 543\t [ 0.85540515 -1.5292339 ] \t0\ttrue\n",
            "(0)\t 544\t [ 0.82777864 -0.2821426 ] \t0\ttrue\n",
            "(0)\t 545\t [ 1.2700975 -1.2889776] \t0\ttrue\n",
            "(1)\t 546\t [ 0.8455752 -1.381377 ] \t0\tfalse\n",
            "(0)\t 547\t [ 0.9700885 -1.5319787] \t0\ttrue\n",
            "(0)\t 548\t [ 1.0768617 -1.6627231] \t0\ttrue\n",
            "(1)\t 549\t [ 0.796126  -0.5720376] \t0\tfalse\n",
            "(1)\t 550\t [ 0.8157158 -1.4734817] \t0\tfalse\n",
            "(1)\t 551\t [-0.19477996 -0.02459029] \t1\ttrue\n",
            "(1)\t 552\t [ 0.9458468 -1.3736202] \t0\tfalse\n",
            "(1)\t 553\t [ 0.6614604 -1.2177014] \t0\tfalse\n",
            "(1)\t 554\t [-0.5784186   0.31236264] \t1\ttrue\n",
            "(0)\t 555\t [ 1.1183984 -1.7951546] \t0\ttrue\n",
            "(0)\t 556\t [ 1.3047614 -1.8094758] \t0\ttrue\n",
            "(0)\t 557\t [ 1.2963369 -1.9936786] \t0\ttrue\n",
            "(1)\t 558\t [-0.22291672  0.13413559] \t1\ttrue\n",
            "(1)\t 559\t [-1.1589959  1.22308  ] \t1\ttrue\n",
            "(0)\t 560\t [-0.8420687  0.5938339] \t1\tfalse\n",
            "(1)\t 561\t [ 0.7129806 -0.7397267] \t0\tfalse\n",
            "(0)\t 562\t [ 0.53904   -0.9453585] \t0\ttrue\n",
            "(0)\t 563\t [ 0.08862282 -0.43256322] \t0\ttrue\n",
            "(0)\t 564\t [ 1.1077294 -1.8361416] \t0\ttrue\n",
            "(1)\t 565\t [ 0.7179452 -1.112175 ] \t0\tfalse\n",
            "(1)\t 566\t [ 1.1793472 -1.6765501] \t0\tfalse\n",
            "(1)\t 567\t [ 0.7908635 -0.6407314] \t0\tfalse\n",
            "(0)\t 568\t [ 1.0288994 -1.5351865] \t0\ttrue\n",
            "(0)\t 569\t [ 0.9833065 -1.6979486] \t0\ttrue\n",
            "(0)\t 570\t [ 0.4840009 -0.8574693] \t0\ttrue\n",
            "(0)\t 571\t [ 1.1270584 -1.7223287] \t0\ttrue\n",
            "(1)\t 572\t [ 0.7057203  -0.83906156] \t0\tfalse\n",
            "(0)\t 573\t [ 1.2617444 -1.6522169] \t0\ttrue\n",
            "(0)\t 574\t [0.04341624 0.07522583] \t1\tfalse\n",
            "(0)\t 575\t [-0.5549158  0.5344566] \t1\tfalse\n",
            "(0)\t 576\t [-0.33442476  0.20912519] \t1\tfalse\n",
            "(0)\t 577\t [-0.64568794  1.2949822 ] \t1\tfalse\n",
            "(0)\t 578\t [ 1.2903566 -1.7721128] \t0\ttrue\n",
            "(0)\t 579\t [ 0.25015822 -0.63909894] \t0\ttrue\n",
            "(0)\t 580\t [ 0.6489098 -1.0224832] \t0\ttrue\n",
            "(0)\t 581\t [ 0.43191966 -0.4989552 ] \t0\ttrue\n",
            "(0)\t 582\t [ 1.1028137 -1.5917499] \t0\ttrue\n",
            "(0)\t 583\t [ 0.08893786 -0.06173137] \t0\ttrue\n",
            "(0)\t 584\t [ 0.6709273 -1.0786866] \t0\ttrue\n",
            "(0)\t 585\t [ 1.1756923 -1.1810142] \t0\ttrue\n",
            "(0)\t 586\t [ 1.3349893 -1.8276179] \t0\ttrue\n",
            "(0)\t 587\t [ 0.856848  -1.2395977] \t0\ttrue\n",
            "(1)\t 588\t [ 0.512279  -0.7647371] \t0\tfalse\n",
            "(1)\t 589\t [-0.4986476   0.50337386] \t1\ttrue\n",
            "(1)\t 590\t [ 1.0988828 -1.5611925] \t0\tfalse\n",
            "(1)\t 591\t [-0.16660774  0.53816783] \t1\ttrue\n",
            "(0)\t 592\t [ 1.3662498 -1.36167  ] \t0\ttrue\n",
            "(0)\t 593\t [ 1.2673881  -0.88274753] \t0\ttrue\n",
            "(0)\t 594\t [ 1.1712446 -1.2907878] \t0\ttrue\n",
            "(1)\t 595\t [-1.4502425  1.7713518] \t1\ttrue\n",
            "(1)\t 596\t [-0.7837225  0.6408259] \t1\ttrue\n",
            "(1)\t 597\t [ 0.6971324 -0.9210912] \t0\tfalse\n",
            "(1)\t 598\t [ 0.9388161 -0.746223 ] \t0\tfalse\n",
            "(0)\t 599\t [ 0.9148947 -1.5066345] \t0\ttrue\n",
            "(1)\t 600\t [-1.1696457  1.5329465] \t1\ttrue\n",
            "(0)\t 601\t [ 0.9947292 -1.6192759] \t0\ttrue\n",
            "(0)\t 602\t [-0.7170905  0.545374 ] \t1\tfalse\n",
            "(1)\t 603\t [ 1.1263094 -0.9619819] \t0\tfalse\n",
            "(0)\t 604\t [ 0.38344988 -0.43389302] \t0\ttrue\n",
            "(0)\t 605\t [ 0.31222922 -0.38737878] \t0\ttrue\n",
            "(1)\t 606\t [ 0.33015978 -0.7544263 ] \t0\tfalse\n",
            "(0)\t 607\t [ 1.3334693 -1.9614462] \t0\ttrue\n",
            "(1)\t 608\t [ 0.7012994 -0.671154 ] \t0\tfalse\n",
            "(0)\t 609\t [ 1.3130212 -1.9192519] \t0\ttrue\n",
            "(1)\t 610\t [-1.3634262  1.7247345] \t1\ttrue\n",
            "(1)\t 611\t [ 0.03495107 -0.25793156] \t0\tfalse\n",
            "(1)\t 612\t [ 0.17345908 -0.32953033] \t0\tfalse\n",
            "(1)\t 613\t [-0.04486537  0.3907522 ] \t1\ttrue\n",
            "(1)\t 614\t [ 1.2400252 -1.9094958] \t0\tfalse\n",
            "(0)\t 615\t [-0.1231553  0.2559479] \t1\tfalse\n",
            "(1)\t 616\t [-0.5977752   0.29458228] \t1\ttrue\n",
            "(0)\t 617\t [0.41157696 0.49404773] \t1\tfalse\n",
            "(0)\t 618\t [ 0.28294918 -0.6048995 ] \t0\ttrue\n",
            "(0)\t 619\t [-0.58895886  0.4332227 ] \t1\tfalse\n",
            "(0)\t 620\t [-0.49003536  0.17403646] \t1\tfalse\n",
            "(0)\t 621\t [-0.47114393  0.2524694 ] \t1\tfalse\n",
            "(1)\t 622\t [ 1.2682679 -1.7908022] \t0\tfalse\n",
            "(0)\t 623\t [ 0.33203748 -0.58726525] \t0\ttrue\n",
            "(0)\t 624\t [ 0.3538236 -0.6077892] \t0\ttrue\n",
            "(0)\t 625\t [-0.77900296  0.9104512 ] \t1\tfalse\n",
            "(1)\t 626\t [ 0.8518648 -1.6291203] \t0\tfalse\n",
            "(1)\t 627\t [-0.51683366  0.38513684] \t1\ttrue\n",
            "(0)\t 628\t [ 0.9184989 -1.3372651] \t0\ttrue\n",
            "(1)\t 629\t [-0.5054742  0.9834938] \t1\ttrue\n",
            "(1)\t 630\t [0.01906933 0.42766383] \t1\ttrue\n",
            "(0)\t 631\t [ 1.2062073 -1.3364125] \t0\ttrue\n",
            "(1)\t 632\t [ 1.1713811 -1.7767866] \t0\tfalse\n",
            "(1)\t 633\t [-1.1865666  1.6650027] \t1\ttrue\n",
            "(0)\t 634\t [ 0.9032938 -1.4167017] \t0\ttrue\n",
            "(0)\t 635\t [0.18090343 0.04887518] \t0\ttrue\n",
            "(0)\t 636\t [ 0.67950886 -0.79906607] \t0\ttrue\n",
            "(0)\t 637\t [ 1.2302    -1.7053404] \t0\ttrue\n",
            "(1)\t 638\t [ 0.7790603 -0.6124026] \t0\tfalse\n",
            "(0)\t 639\t [ 1.2228471 -1.7972023] \t0\ttrue\n",
            "(0)\t 640\t [ 0.61229265 -1.102936  ] \t0\ttrue\n",
            "(0)\t 641\t [ 0.9696878 -1.01792  ] \t0\ttrue\n",
            "(1)\t 642\t [ 0.2082215  -0.33072275] \t0\tfalse\n",
            "(0)\t 643\t [ 0.48254183 -0.7996527 ] \t0\ttrue\n",
            "(0)\t 644\t [ 0.72773635 -1.526582  ] \t0\ttrue\n",
            "(1)\t 645\t [-0.62049305  0.71866095] \t1\ttrue\n",
            "(0)\t 646\t [ 1.3618584 -1.5539482] \t0\ttrue\n",
            "(0)\t 647\t [ 1.2541693 -1.0728896] \t0\ttrue\n",
            "(1)\t 648\t [-1.371856   1.6357181] \t1\ttrue\n",
            "(0)\t 649\t [ 1.4536657 -1.4050286] \t0\ttrue\n",
            "(0)\t 650\t [-0.5617719   0.39387035] \t1\tfalse\n",
            "(0)\t 651\t [-0.2550294   0.02262048] \t1\tfalse\n",
            "(0)\t 652\t [-1.0374603  1.2857668] \t1\tfalse\n",
            "(0)\t 653\t [ 0.78172684 -1.3287714 ] \t0\ttrue\n",
            "(1)\t 654\t [ 0.7708229 -0.897047 ] \t0\tfalse\n",
            "(1)\t 655\t [-0.61217177  1.1389223 ] \t1\ttrue\n",
            "(1)\t 656\t [ 1.4049042 -1.4606475] \t0\tfalse\n",
            "(1)\t 657\t [-0.25217876  0.00351873] \t1\ttrue\n",
            "(0)\t 658\t [ 1.1356213 -1.6793419] \t0\ttrue\n",
            "(1)\t 659\t [ 1.0950185 -1.7231266] \t0\tfalse\n",
            "(1)\t 660\t [ 0.59985876 -0.7158053 ] \t0\tfalse\n",
            "(1)\t 661\t [-0.8781587  1.0795033] \t1\ttrue\n",
            "(0)\t 662\t [ 1.5684562 -1.8024442] \t0\ttrue\n",
            "(0)\t 663\t [ 0.6560515 -0.979772 ] \t0\ttrue\n",
            "(0)\t 664\t [ 1.3991972 -1.8721515] \t0\ttrue\n",
            "(0)\t 665\t [ 1.1194264 -1.8795233] \t0\ttrue\n",
            "(0)\t 666\t [ 1.05967   -1.6503488] \t0\ttrue\n",
            "(0)\t 667\t [ 1.3260591 -1.8933756] \t0\ttrue\n",
            "(0)\t 668\t [-0.64873946  0.369805  ] \t1\tfalse\n",
            "(0)\t 669\t [ 1.0951806 -1.0672822] \t0\ttrue\n",
            "(0)\t 670\t [ 1.3662628 -1.1251416] \t0\ttrue\n",
            "(1)\t 671\t [ 1.4770548 -1.4809098] \t0\tfalse\n",
            "(0)\t 672\t [ 1.5251052 -1.8077477] \t0\ttrue\n",
            "(0)\t 673\t [ 1.2220815 -1.7960857] \t0\ttrue\n",
            "(0)\t 674\t [ 1.1951857 -1.8376718] \t0\ttrue\n",
            "(1)\t 675\t [-0.6943639  0.6358353] \t1\ttrue\n",
            "(0)\t 676\t [-1.3150213  1.7178644] \t1\tfalse\n",
            "(0)\t 677\t [-1.3523114  1.4091451] \t1\tfalse\n",
            "(0)\t 678\t [ 0.58542836 -1.07967   ] \t0\ttrue\n",
            "(0)\t 679\t [-0.00265956  0.33508328] \t1\tfalse\n",
            "(0)\t 680\t [ 1.0540817  -0.78999674] \t0\ttrue\n",
            "(0)\t 681\t [ 0.8952946 -0.9409714] \t0\ttrue\n",
            "(0)\t 682\t [-0.27686825  0.14167146] \t1\tfalse\n",
            "(0)\t 683\t [ 1.3454676 -1.9318745] \t0\ttrue\n",
            "(1)\t 684\t [-1.093475   1.0802505] \t1\ttrue\n",
            "(1)\t 685\t [-0.883284    0.61120653] \t1\ttrue\n",
            "(0)\t 686\t [-0.9715538  0.7858652] \t1\tfalse\n",
            "(1)\t 687\t [ 0.9706383 -1.5584028] \t0\tfalse\n",
            "(0)\t 688\t [-0.6838292  0.4121298] \t1\tfalse\n",
            "(0)\t 689\t [-0.38697574  0.15794596] \t1\tfalse\n",
            "(1)\t 690\t [ 0.55402505 -0.5814328 ] \t0\tfalse\n",
            "(1)\t 691\t [-0.7326169   0.55772805] \t1\ttrue\n",
            "(1)\t 692\t [-0.980513   1.0278118] \t1\ttrue\n",
            "(0)\t 693\t [ 1.3143729 -1.4266505] \t0\ttrue\n",
            "(1)\t 694\t [-0.667525    0.42942703] \t1\ttrue\n",
            "(1)\t 695\t [-0.23759958  0.08985345] \t1\ttrue\n",
            "(1)\t 696\t [ 0.44405428 -0.6246576 ] \t0\tfalse\n",
            "(1)\t 697\t [ 0.89391696 -0.98424107] \t0\tfalse\n",
            "(0)\t 698\t [0.16572344 0.06465963] \t0\ttrue\n",
            "(0)\t 699\t [ 1.1252563 -1.7071931] \t0\ttrue\n",
            "(0)\t 700\t [ 1.2775853 -1.9887143] \t0\ttrue\n",
            "(0)\t 701\t [ 0.19106969 -0.29818714] \t0\ttrue\n",
            "(1)\t 702\t [ 0.49366456 -1.1431141 ] \t0\tfalse\n",
            "(0)\t 703\t [ 0.73482376 -0.78712696] \t0\ttrue\n",
            "(1)\t 704\t [-1.4524262  1.5498196] \t1\ttrue\n",
            "(0)\t 705\t [ 1.4569234 -1.5199823] \t0\ttrue\n",
            "(1)\t 706\t [ 1.3511989 -1.8775258] \t0\tfalse\n",
            "(0)\t 707\t [ 0.82750076 -0.5017849 ] \t0\ttrue\n",
            "(0)\t 708\t [ 1.2714319 -1.2503501] \t0\ttrue\n",
            "(0)\t 709\t [-0.04615399 -0.11732763] \t0\ttrue\n",
            "(1)\t 710\t [-1.4421765  1.3611478] \t1\ttrue\n",
            "(0)\t 711\t [ 1.2603375  -0.91151875] \t0\ttrue\n",
            "(0)\t 712\t [ 1.2364583 -1.630201 ] \t0\ttrue\n",
            "(0)\t 713\t [ 0.14916223 -0.49283737] \t0\ttrue\n",
            "(1)\t 714\t [-1.2019966  1.2814608] \t1\ttrue\n",
            "(1)\t 715\t [ 1.0078032 -1.0824578] \t0\tfalse\n",
            "(0)\t 716\t [ 0.5121248 -0.4545005] \t0\ttrue\n",
            "(1)\t 717\t [-1.2948701  1.6513894] \t1\ttrue\n",
            "(0)\t 718\t [ 0.87259954 -1.467202  ] \t0\ttrue\n",
            "(0)\t 719\t [ 0.7588248 -0.9365438] \t0\ttrue\n",
            "(1)\t 720\t [-1.4567242  1.600432 ] \t1\ttrue\n",
            "(1)\t 721\t [-0.2614916  0.1198777] \t1\ttrue\n",
            "(0)\t 722\t [ 0.1522789  -0.36198285] \t0\ttrue\n",
            "(1)\t 723\t [ 0.5116254  -0.22056752] \t0\tfalse\n",
            "(1)\t 724\t [ 0.7281233 -1.1145891] \t0\tfalse\n",
            "(1)\t 725\t [-0.87678874  0.7259156 ] \t1\ttrue\n",
            "(1)\t 726\t [-0.15267655 -0.05418795] \t1\ttrue\n",
            "(1)\t 727\t [ 0.9197113  -0.48779193] \t0\tfalse\n",
            "(0)\t 728\t [ 1.0204424 -1.6544056] \t0\ttrue\n",
            "(0)\t 729\t [ 1.3241832 -1.9377813] \t0\ttrue\n",
            "(1)\t 730\t [-0.66558826  0.39414915] \t1\ttrue\n",
            "(0)\t 731\t [ 1.0957407 -0.7659253] \t0\ttrue\n",
            "(0)\t 732\t [ 1.194712  -1.7191148] \t0\ttrue\n",
            "(0)\t 733\t [ 0.8842937 -1.1196518] \t0\ttrue\n",
            "(0)\t 734\t [ 1.2818068 -1.4194486] \t0\ttrue\n",
            "(0)\t 735\t [-0.13493678  0.18184787] \t1\tfalse\n",
            "(0)\t 736\t [0.31619945 0.10394399] \t0\ttrue\n",
            "(0)\t 737\t [ 1.0521765 -1.0020306] \t0\ttrue\n",
            "(0)\t 738\t [ 1.4152565 -1.4763396] \t0\ttrue\n",
            "(0)\t 739\t [ 0.357249  -0.2796802] \t0\ttrue\n",
            "(0)\t 740\t [ 1.1384292 -1.7502809] \t0\ttrue\n",
            "(1)\t 741\t [ 0.9623252 -1.1392561] \t0\tfalse\n",
            "(1)\t 742\t [ 1.0767274 -1.4978211] \t0\tfalse\n",
            "(1)\t 743\t [-0.5034023  0.9423338] \t1\ttrue\n",
            "(0)\t 744\t [ 1.2628604 -1.8238835] \t0\ttrue\n",
            "(0)\t 745\t [ 0.8003484 -1.0240165] \t0\ttrue\n",
            "(1)\t 746\t [-0.14418723 -0.08347865] \t1\ttrue\n",
            "(1)\t 747\t [ 0.6654924 -0.9361263] \t0\tfalse\n",
            "(1)\t 748\t [-1.5914327  1.7785279] \t1\ttrue\n",
            "(0)\t 749\t [ 0.89160717 -1.2229002 ] \t0\ttrue\n",
            "(1)\t 750\t [ 1.2322348 -1.6994538] \t0\tfalse\n",
            "(1)\t 751\t [ 0.9148799 -0.6809488] \t0\tfalse\n",
            "(1)\t 752\t [-1.1174227  0.9359269] \t1\ttrue\n",
            "(0)\t 753\t [ 1.2590775 -1.1730764] \t0\ttrue\n",
            "(0)\t 754\t [ 1.3396697 -1.8433416] \t0\ttrue\n",
            "(0)\t 755\t [ 1.0361737 -0.8985139] \t0\ttrue\n",
            "(0)\t 756\t [ 0.94562364 -0.9848415 ] \t0\ttrue\n",
            "(0)\t 757\t [ 1.3001008 -1.783992 ] \t0\ttrue\n",
            "(0)\t 758\t [ 1.28881   -1.7538807] \t0\ttrue\n",
            "(0)\t 759\t [ 1.3769009 -1.8268981] \t0\ttrue\n",
            "(0)\t 760\t [ 1.0144161 -1.5843453] \t0\ttrue\n",
            "(0)\t 761\t [ 1.1249027 -1.327416 ] \t0\ttrue\n",
            "(0)\t 762\t [ 1.2774118 -1.7651652] \t0\ttrue\n",
            "(1)\t 763\t [-1.116457   1.4718983] \t1\ttrue\n",
            "(0)\t 764\t [ 0.4870449  -0.54090804] \t0\ttrue\n",
            "(0)\t 765\t [ 1.0933145 -1.7205918] \t0\ttrue\n",
            "(1)\t 766\t [-0.13548972 -0.17497519] \t0\tfalse\n",
            "(0)\t 767\t [ 0.57790625 -0.76878345] \t0\ttrue\n",
            "(0)\t 768\t [ 0.6186903  -0.21236926] \t0\ttrue\n",
            "(0)\t 769\t [-0.85900486  0.55962974] \t1\tfalse\n",
            "(1)\t 770\t [ 0.5064987 -0.0325498] \t0\tfalse\n",
            "(0)\t 771\t [ 1.4772277 -1.9210689] \t0\ttrue\n",
            "(0)\t 772\t [ 1.3550404 -1.8589532] \t0\ttrue\n",
            "(0)\t 773\t [ 1.1194675 -1.0897467] \t0\ttrue\n",
            "(1)\t 774\t [-0.5222559  0.5456045] \t1\ttrue\n",
            "(1)\t 775\t [-1.4077477  1.6980381] \t1\ttrue\n",
            "(0)\t 776\t [-0.5744848   0.61807466] \t1\tfalse\n",
            "(1)\t 777\t [-1.2622901  1.5428905] \t1\ttrue\n",
            "(1)\t 778\t [-0.99195135  1.4015394 ] \t1\ttrue\n",
            "(0)\t 779\t [ 1.519442  -1.6117828] \t0\ttrue\n",
            "(1)\t 780\t [-0.7517945  0.3781503] \t1\ttrue\n",
            "(1)\t 781\t [ 0.38959593 -0.75071394] \t0\tfalse\n",
            "(0)\t 782\t [ 1.3642061 -1.5400537] \t0\ttrue\n",
            "(0)\t 783\t [ 0.7813207 -1.2348912] \t0\ttrue\n",
            "(0)\t 784\t [ 0.5346905 -0.9558809] \t0\ttrue\n",
            "(0)\t 785\t [ 1.2867568 -1.7659724] \t0\ttrue\n",
            "(0)\t 786\t [ 1.2512695 -1.8436463] \t0\ttrue\n",
            "(1)\t 787\t [ 0.02474169 -0.18641299] \t0\tfalse\n",
            "(0)\t 788\t [ 1.3750279 -1.901038 ] \t0\ttrue\n",
            "(1)\t 789\t [ 0.65774953 -0.6526171 ] \t0\tfalse\n",
            "(0)\t 790\t [ 1.261509  -1.7683964] \t0\ttrue\n",
            "(0)\t 791\t [ 1.1811483 -1.7639134] \t0\ttrue\n",
            "(0)\t 792\t [ 1.2501028 -1.2508203] \t0\ttrue\n",
            "(1)\t 793\t [-1.162638   1.3448122] \t1\ttrue\n",
            "(1)\t 794\t [ 0.9359175 -1.533535 ] \t0\tfalse\n",
            "(0)\t 795\t [ 1.4369152 -1.6619062] \t0\ttrue\n",
            "(0)\t 796\t [ 1.128138  -1.0052607] \t0\ttrue\n",
            "(1)\t 797\t [ 1.5292573 -1.5702622] \t0\tfalse\n",
            "(1)\t 798\t [ 1.3455174 -1.842735 ] \t0\tfalse\n",
            "(1)\t 799\t [-0.0482282  -0.18641771] \t0\tfalse\n",
            "(0)\t 800\t [ 1.2618093 -1.6473461] \t0\ttrue\n",
            "(0)\t 801\t [ 1.0845251 -1.8808049] \t0\ttrue\n",
            "(0)\t 802\t [-1.2817868  1.5206993] \t1\tfalse\n",
            "(0)\t 803\t [ 1.1900885 -1.1126157] \t0\ttrue\n",
            "(1)\t 804\t [ 0.30916363 -0.48220137] \t0\tfalse\n",
            "(0)\t 805\t [-0.05266128 -0.14523146] \t0\ttrue\n",
            "(0)\t 806\t [ 1.1811225 -1.3933502] \t0\ttrue\n",
            "(1)\t 807\t [0.3757902  0.11949147] \t0\tfalse\n",
            "(0)\t 808\t [ 0.9974148 -1.4941735] \t0\ttrue\n",
            "(0)\t 809\t [-1.0095984  0.847378 ] \t1\tfalse\n",
            "(1)\t 810\t [ 0.1545776  -0.37390247] \t0\tfalse\n",
            "(1)\t 811\t [ 0.72013026 -1.3459477 ] \t0\tfalse\n",
            "(1)\t 812\t [-1.5582811  1.8032143] \t1\ttrue\n",
            "(1)\t 813\t [-0.3429665  0.6738635] \t1\ttrue\n",
            "(1)\t 814\t [ 0.92773014 -0.982442  ] \t0\tfalse\n",
            "(0)\t 815\t [ 1.0780067 -1.6485845] \t0\ttrue\n",
            "(0)\t 816\t [ 0.00475501 -0.36558542] \t0\ttrue\n",
            "(0)\t 817\t [-1.0796154  1.4457968] \t1\tfalse\n",
            "(0)\t 818\t [-1.3424416  1.4724097] \t1\tfalse\n",
            "(0)\t 819\t [ 0.66476655 -0.7963225 ] \t0\ttrue\n",
            "(0)\t 820\t [-0.4879541   0.30551842] \t1\tfalse\n",
            "(1)\t 821\t [ 0.23429982 -0.6015522 ] \t0\tfalse\n",
            "(1)\t 822\t [-1.4217788  1.7602518] \t1\ttrue\n",
            "(0)\t 823\t [-0.38664302  0.10004214] \t1\tfalse\n",
            "(0)\t 824\t [ 0.8706718 -1.1184752] \t0\ttrue\n",
            "(0)\t 825\t [ 0.7959571 -1.3104563] \t0\ttrue\n",
            "(0)\t 826\t [ 1.3621225 -1.4607583] \t0\ttrue\n",
            "(1)\t 827\t [ 0.52833474 -0.06619903] \t0\tfalse\n",
            "(1)\t 828\t [ 0.8022899 -0.9031022] \t0\tfalse\n",
            "(1)\t 829\t [-1.2733676  1.6253585] \t1\ttrue\n",
            "(0)\t 830\t [ 1.3227179 -1.8438177] \t0\ttrue\n",
            "(0)\t 831\t [-1.3688736  1.5936446] \t1\tfalse\n",
            "(0)\t 832\t [ 0.7282907 -1.1478913] \t0\ttrue\n",
            "(0)\t 833\t [ 1.1174232  -0.91118467] \t0\ttrue\n",
            "(0)\t 834\t [-0.01654438 -0.3140277 ] \t0\ttrue\n",
            "(0)\t 835\t [ 0.72288907 -0.8638552 ] \t0\ttrue\n",
            "(0)\t 836\t [ 0.36293593 -0.45876506] \t0\ttrue\n",
            "(1)\t 837\t [ 0.7476797  -0.92609817] \t0\tfalse\n",
            "(1)\t 838\t [ 1.0212702 -1.4333801] \t0\tfalse\n",
            "(1)\t 839\t [ 0.95537114 -1.4320385 ] \t0\tfalse\n",
            "(0)\t 840\t [ 1.0190583 -1.2563113] \t0\ttrue\n",
            "(1)\t 841\t [-1.2621615  1.5223541] \t1\ttrue\n",
            "(0)\t 842\t [ 1.0627131 -1.7343414] \t0\ttrue\n",
            "(0)\t 843\t [0.10946137 0.01542958] \t0\ttrue\n",
            "(0)\t 844\t [ 1.4243093 -1.3569411] \t0\ttrue\n",
            "(0)\t 845\t [ 1.2977605 -1.8091145] \t0\ttrue\n",
            "(0)\t 846\t [ 1.1520606 -1.156378 ] \t0\ttrue\n",
            "(0)\t 847\t [ 1.3597777 -1.8427827] \t0\ttrue\n",
            "(0)\t 848\t [ 1.2717414 -1.9178646] \t0\ttrue\n",
            "(0)\t 849\t [ 1.2952852 -1.7827791] \t0\ttrue\n",
            "(0)\t 850\t [ 0.5400398  -0.48688447] \t0\ttrue\n",
            "(0)\t 851\t [ 0.00981548 -0.05162875] \t0\ttrue\n",
            "(0)\t 852\t [ 1.3728734 -1.8830985] \t0\ttrue\n",
            "(0)\t 853\t [ 0.7922717 -0.7306685] \t0\ttrue\n",
            "(1)\t 854\t [-1.4892602  1.3348284] \t1\ttrue\n",
            "(0)\t 855\t [ 0.092317 -0.331736] \t0\ttrue\n",
            "(0)\t 856\t [ 0.28898212 -0.14412746] \t0\ttrue\n",
            "(0)\t 857\t [-0.9229627   0.74392176] \t1\tfalse\n",
            "(0)\t 858\t [-0.9282167  0.7818849] \t1\tfalse\n",
            "(0)\t 859\t [ 1.338742  -1.5071236] \t0\ttrue\n",
            "(1)\t 860\t [ 1.1831884 -1.3314941] \t0\tfalse\n",
            "(0)\t 861\t [ 0.4657223  -0.21106654] \t0\ttrue\n",
            "(1)\t 862\t [ 0.9845533 -1.7398567] \t0\tfalse\n",
            "(0)\t 863\t [ 1.324727  -1.0768216] \t0\ttrue\n",
            "(1)\t 864\t [-1.4565728  1.7852862] \t1\ttrue\n",
            "(1)\t 865\t [-0.25230417  0.2495713 ] \t1\ttrue\n",
            "(1)\t 866\t [-0.4520734   0.09479942] \t1\ttrue\n",
            "(1)\t 867\t [ 0.53684723 -0.17032339] \t0\tfalse\n",
            "(0)\t 868\t [ 1.4913025 -1.5673497] \t0\ttrue\n",
            "(0)\t 869\t [ 1.3666868 -1.3299018] \t0\ttrue\n",
            "(1)\t 870\t [-0.59002024  0.4080231 ] \t1\ttrue\n",
            "(0)\t 871\t [ 1.1593101 -1.132916 ] \t0\ttrue\n",
            "(1)\t 872\t [-0.19523261 -0.07938071] \t1\ttrue\n",
            "(1)\t 873\t [ 0.24772584 -0.40498483] \t0\tfalse\n",
            "(0)\t 874\t [ 0.8298178  -0.75668037] \t0\ttrue\n",
            "(1)\t 875\t [ 0.10936922 -0.3451609 ] \t0\tfalse\n",
            "(1)\t 876\t [ 0.36373553 -0.5365182 ] \t0\tfalse\n",
            "(0)\t 877\t [ 1.200629   -0.89351845] \t0\ttrue\n",
            "(1)\t 878\t [-0.22647622  0.0466355 ] \t1\ttrue\n",
            "(0)\t 879\t [-0.2759112  -0.07260127] \t1\tfalse\n",
            "(0)\t 880\t [ 1.3240154 -1.8746159] \t0\ttrue\n",
            "(0)\t 881\t [ 0.89110947 -0.71239686] \t0\ttrue\n",
            "(1)\t 882\t [-0.99294645  0.6295012 ] \t1\ttrue\n",
            "(1)\t 883\t [ 1.2102394 -1.6813158] \t0\tfalse\n",
            "(0)\t 884\t [ 0.74381286 -1.2758418 ] \t0\ttrue\n",
            "(0)\t 885\t [ 1.4325192 -1.9281361] \t0\ttrue\n",
            "(1)\t 886\t [ 0.80140543 -0.58402514] \t0\tfalse\n",
            "(1)\t 887\t [ 0.73069704 -1.0943251 ] \t0\tfalse\n",
            "(0)\t 888\t [ 0.6867852 -1.2005334] \t0\ttrue\n",
            "(0)\t 889\t [ 0.79559493 -0.47584763] \t0\ttrue\n",
            "(0)\t 890\t [ 0.0238254  -0.14181253] \t0\ttrue\n",
            "(1)\t 891\t [ 1.0829318 -1.5532129] \t0\tfalse\n",
            "(1)\t 892\t [ 0.6410608 -1.0109345] \t0\tfalse\n",
            "(1)\t 893\t [-0.02366374  0.0815275 ] \t1\ttrue\n",
            "(1)\t 894\t [ 0.7171062  -0.94449884] \t0\tfalse\n",
            "(0)\t 895\t [ 1.1274886 -1.7324737] \t0\ttrue\n",
            "(0)\t 896\t [ 0.14953665 -0.5293339 ] \t0\ttrue\n",
            "(0)\t 897\t [-0.41830292  0.2543144 ] \t1\tfalse\n",
            "(1)\t 898\t [-0.40132636  0.2994387 ] \t1\ttrue\n",
            "(1)\t 899\t [-0.8729058   0.70799375] \t1\ttrue\n",
            "(1)\t 900\t [ 1.3317261 -1.7229757] \t0\tfalse\n",
            "Number of true predictions: 614\n",
            "Number of false predictions: 286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIgsWWCTKdY",
        "colab_type": "code",
        "outputId": "35374b39-70fa-41d8-8094-8854c94793e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 68.1465038845727 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtGyPreQUa-A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d84f698c-e435-4ea6-d264-d6b7eec57721"
      },
      "source": [
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "scores = [3.0, 1.0, 0.2]\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    predictions[i][j]=softmax(predictions[i][j])\n",
        "    print(predictions[i][j])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.47053304 0.5294669 ]\n",
            "[0.8109738  0.18902618]\n",
            "[0.14619316 0.85380685]\n",
            "[0.12305849 0.87694144]\n",
            "[0.9664953  0.03350473]\n",
            "[0.14229487 0.8577051 ]\n",
            "[0.121066 0.878934]\n",
            "[0.9547162  0.04528382]\n",
            "[0.96064526 0.03935469]\n",
            "[0.03939982 0.96060014]\n",
            "[0.27734867 0.72265136]\n",
            "[0.91067046 0.08932952]\n",
            "[0.81265575 0.18734428]\n",
            "[0.9183358  0.08166426]\n",
            "[0.9610351 0.0389649]\n",
            "[0.9328664  0.06713362]\n",
            "[0.18834376 0.8116562 ]\n",
            "[0.82361406 0.17638591]\n",
            "[0.4845184 0.5154816]\n",
            "[0.9504993  0.04950066]\n",
            "[0.9582993  0.04170065]\n",
            "[0.96489877 0.03510126]\n",
            "[0.5573075  0.44269252]\n",
            "[0.92810035 0.07189965]\n",
            "[0.9634437  0.03655624]\n",
            "[0.962866   0.03713401]\n",
            "[0.9657543  0.03424571]\n",
            "[0.6428445  0.35715547]\n",
            "[0.33614743 0.6638525 ]\n",
            "[0.9595349  0.04046511]\n",
            "[0.90937334 0.09062665]\n",
            "[0.9105538  0.08944623]\n",
            "[0.7949159  0.20508406]\n",
            "[0.945727   0.05427299]\n",
            "[0.9040131  0.09598688]\n",
            "[0.9033384  0.09666161]\n",
            "[0.95532286 0.04467716]\n",
            "[0.04555718 0.95444286]\n",
            "[0.5747514  0.42524865]\n",
            "[0.61511135 0.3848887 ]\n",
            "[0.9572211  0.04277894]\n",
            "[0.0811457  0.91885436]\n",
            "[0.28307822 0.7169218 ]\n",
            "[0.9559687  0.04403126]\n",
            "[0.82526505 0.1747349 ]\n",
            "[0.95830226 0.04169779]\n",
            "[0.9293932  0.07060686]\n",
            "[0.80713695 0.19286312]\n",
            "[0.95959973 0.04040029]\n",
            "[0.47172028 0.5282797 ]\n",
            "[0.9283038  0.07169617]\n",
            "[0.81501937 0.18498069]\n",
            "[0.8127163  0.18728375]\n",
            "[0.27721107 0.72278893]\n",
            "[0.5579812  0.44201875]\n",
            "[0.83330613 0.16669382]\n",
            "[0.9624368  0.03756313]\n",
            "[0.92339057 0.07660946]\n",
            "[0.9399872  0.06001281]\n",
            "[0.95300746 0.04699249]\n",
            "[0.31199875 0.6880013 ]\n",
            "[0.85602707 0.1439729 ]\n",
            "[0.96207356 0.03792647]\n",
            "[0.88469696 0.11530306]\n",
            "[0.94584227 0.05415768]\n",
            "[0.80309665 0.19690333]\n",
            "[0.9585873  0.04141275]\n",
            "[0.5322586  0.46774134]\n",
            "[0.95874256 0.0412575 ]\n",
            "[0.50006187 0.49993816]\n",
            "[0.95391345 0.04608651]\n",
            "[0.08372936 0.9162706 ]\n",
            "[0.9623613  0.03763872]\n",
            "[0.96346915 0.03653084]\n",
            "[0.9572036 0.0427964]\n",
            "[0.9206719  0.07932816]\n",
            "[0.9567741  0.04322585]\n",
            "[0.9419176  0.05808237]\n",
            "[0.8301897 0.1698103]\n",
            "[0.5786858 0.4213142]\n",
            "[0.9077151  0.09228491]\n",
            "[0.8992066  0.10079346]\n",
            "[0.9618974  0.03810265]\n",
            "[0.911187   0.08881301]\n",
            "[0.66719514 0.33280483]\n",
            "[0.67905885 0.32094115]\n",
            "[0.96550804 0.03449191]\n",
            "[0.7352874 0.2647126]\n",
            "[0.9550256  0.04497436]\n",
            "[0.9591914  0.04080861]\n",
            "[0.9491379  0.05086214]\n",
            "[0.93281484 0.06718515]\n",
            "[0.96689534 0.03310461]\n",
            "[0.7467729 0.2532271]\n",
            "[0.10735237 0.8926476 ]\n",
            "[0.67345893 0.32654104]\n",
            "[0.04586661 0.9541334 ]\n",
            "[0.63418514 0.3658149 ]\n",
            "[0.9332822  0.06671778]\n",
            "[0.89418435 0.10581561]\n",
            "[0.94259506 0.05740496]\n",
            "[0.2794456 0.7205544]\n",
            "[0.7222979  0.27770212]\n",
            "[0.70339215 0.29660785]\n",
            "[0.74927473 0.25072524]\n",
            "[0.51573217 0.4842678 ]\n",
            "[0.22538824 0.7746117 ]\n",
            "[0.16474529 0.83525467]\n",
            "[0.95812833 0.04187163]\n",
            "[0.9291484  0.07085165]\n",
            "[0.9360073  0.06399271]\n",
            "[0.6568294 0.3431706]\n",
            "[0.9653545  0.03464549]\n",
            "[0.3127264  0.68727356]\n",
            "[0.09063541 0.9093646 ]\n",
            "[0.6498616  0.35013843]\n",
            "[0.96410996 0.03589008]\n",
            "[0.07447501 0.925525  ]\n",
            "[0.9413954  0.05860461]\n",
            "[0.96371734 0.03628265]\n",
            "[0.77863103 0.22136898]\n",
            "[0.84641564 0.15358435]\n",
            "[0.7991607  0.20083922]\n",
            "[0.6826659  0.31733406]\n",
            "[0.95603484 0.04396515]\n",
            "[0.8055576  0.19444238]\n",
            "[0.96548045 0.0345195 ]\n",
            "[0.4370059  0.56299406]\n",
            "[0.87761855 0.12238146]\n",
            "[0.9653167  0.03468331]\n",
            "[0.96571577 0.03428417]\n",
            "[0.92603993 0.07396004]\n",
            "[0.95097977 0.04902022]\n",
            "[0.87063766 0.12936229]\n",
            "[0.84762216 0.15237783]\n",
            "[0.9571908  0.04280921]\n",
            "[0.7328472  0.26715282]\n",
            "[0.9179284  0.08207155]\n",
            "[0.0919157  0.90808433]\n",
            "[0.68883467 0.31116536]\n",
            "[0.95920223 0.04079781]\n",
            "[0.9459379  0.05406211]\n",
            "[0.86206764 0.13793233]\n",
            "[0.95352626 0.04647375]\n",
            "[0.48655406 0.513446  ]\n",
            "[0.60837674 0.39162326]\n",
            "[0.96502423 0.03497575]\n",
            "[0.93249047 0.06750952]\n",
            "[0.8803318  0.11966813]\n",
            "[0.76147395 0.23852605]\n",
            "[0.57653505 0.42346498]\n",
            "[0.12569821 0.87430185]\n",
            "[0.8813839  0.11861617]\n",
            "[0.83152527 0.16847478]\n",
            "[0.5771878 0.4228122]\n",
            "[0.27466443 0.72533554]\n",
            "[0.26893705 0.73106295]\n",
            "[0.9442336  0.05576642]\n",
            "[0.8534072  0.14659283]\n",
            "[0.0523841 0.9476159]\n",
            "[0.49556383 0.5044362 ]\n",
            "[0.36002856 0.63997144]\n",
            "[0.9542495  0.04575051]\n",
            "[0.6277566 0.3722434]\n",
            "[0.8376113  0.16238867]\n",
            "[0.96828085 0.03171914]\n",
            "[0.8966268  0.10337319]\n",
            "[0.9563411  0.04365887]\n",
            "[0.9701534 0.0298466]\n",
            "[0.9120015  0.08799849]\n",
            "[0.35179302 0.64820695]\n",
            "[0.08930879 0.9106912 ]\n",
            "[0.606794   0.39320594]\n",
            "[0.8216237  0.17837626]\n",
            "[0.9601043  0.03989565]\n",
            "[0.04283342 0.9571666 ]\n",
            "[0.6676122  0.33238778]\n",
            "[0.5959117  0.40408835]\n",
            "[0.8874014  0.11259857]\n",
            "[0.21348998 0.78651005]\n",
            "[0.03742244 0.9625775 ]\n",
            "[0.9303558  0.06964422]\n",
            "[0.8510794 0.1489206]\n",
            "[0.7689736  0.23102644]\n",
            "[0.7814445  0.21855552]\n",
            "[0.95485944 0.0451406 ]\n",
            "[0.8821428  0.11785724]\n",
            "[0.85423857 0.14576149]\n",
            "[0.91716224 0.08283772]\n",
            "[0.96493787 0.03506212]\n",
            "[0.876232   0.12376796]\n",
            "[0.9556407  0.04435936]\n",
            "[0.95265377 0.04734617]\n",
            "[0.17755246 0.82244754]\n",
            "[0.91688097 0.08311909]\n",
            "[0.95613974 0.04386024]\n",
            "[0.9586215  0.04137851]\n",
            "[0.46749884 0.53250116]\n",
            "[0.91479343 0.08520663]\n",
            "[0.15762527 0.8423747 ]\n",
            "[0.8025542 0.1974458]\n",
            "[0.9508936  0.04910647]\n",
            "[0.95349413 0.04650588]\n",
            "[0.46684995 0.5331501 ]\n",
            "[0.9223996  0.07760043]\n",
            "[0.96073604 0.03926399]\n",
            "[0.6491491  0.35085088]\n",
            "[0.9616271  0.03837293]\n",
            "[0.9315345  0.06846546]\n",
            "[0.96224934 0.03775062]\n",
            "[0.9576483  0.04235177]\n",
            "[0.96507436 0.03492564]\n",
            "[0.04766455 0.9523355 ]\n",
            "[0.6724153  0.32758465]\n",
            "[0.95905495 0.04094505]\n",
            "[0.7005462  0.29945382]\n",
            "[0.84793 0.15207]\n",
            "[0.92174566 0.0782544 ]\n",
            "[0.05939314 0.9406069 ]\n",
            "[0.95617026 0.04382978]\n",
            "[0.8863725  0.11362751]\n",
            "[0.717743   0.28225705]\n",
            "[0.8847316  0.11526843]\n",
            "[0.9507652  0.04923481]\n",
            "[0.92141104 0.07858902]\n",
            "[0.19593781 0.8040622 ]\n",
            "[0.8210486  0.17895137]\n",
            "[0.8877191  0.11228093]\n",
            "[0.71609586 0.28390414]\n",
            "[0.041568 0.958432]\n",
            "[0.9448301  0.05516988]\n",
            "[0.27359667 0.7264033 ]\n",
            "[0.89067125 0.10932878]\n",
            "[0.73118234 0.2688176 ]\n",
            "[0.45143047 0.54856956]\n",
            "[0.9488592  0.05114081]\n",
            "[0.1770919  0.82290804]\n",
            "[0.05185751 0.94814247]\n",
            "[0.81622607 0.18377393]\n",
            "[0.8473232  0.15267678]\n",
            "[0.51280093 0.48719904]\n",
            "[0.69789875 0.30210125]\n",
            "[0.60936946 0.3906305 ]\n",
            "[0.9438105  0.05618944]\n",
            "[0.9578522  0.04214776]\n",
            "[0.2777677  0.72223234]\n",
            "[0.5888662 0.4111339]\n",
            "[0.93430555 0.06569442]\n",
            "[0.9022467  0.09775331]\n",
            "[0.6661629  0.33383706]\n",
            "[0.06755294 0.9324471 ]\n",
            "[0.90736026 0.09263975]\n",
            "[0.06782909 0.9321709 ]\n",
            "[0.4015074  0.59849256]\n",
            "[0.2530122 0.7469878]\n",
            "[0.95029885 0.04970117]\n",
            "[0.6901384 0.3098616]\n",
            "[0.8789707  0.12102932]\n",
            "[0.42754635 0.5724536 ]\n",
            "[0.6964013 0.3035987]\n",
            "[0.14816023 0.8518398 ]\n",
            "[0.91766    0.08234003]\n",
            "[0.9493509  0.05064912]\n",
            "[0.5774797 0.4225203]\n",
            "[0.11595735 0.8840426 ]\n",
            "[0.88443774 0.11556224]\n",
            "[0.8622406  0.13775942]\n",
            "[0.19126444 0.80873555]\n",
            "[0.5763686  0.42363143]\n",
            "[0.14111204 0.8588879 ]\n",
            "[0.85140055 0.14859945]\n",
            "[0.46484748 0.53515255]\n",
            "[0.72414815 0.27585188]\n",
            "[0.5241166  0.47588342]\n",
            "[0.11707354 0.8829264 ]\n",
            "[0.8801938  0.11980622]\n",
            "[0.16275817 0.8372418 ]\n",
            "[0.7341977 0.2658023]\n",
            "[0.03899071 0.9610093 ]\n",
            "[0.52012587 0.47987413]\n",
            "[0.8392975  0.16070254]\n",
            "[0.9199769  0.08002307]\n",
            "[0.72481024 0.2751898 ]\n",
            "[0.11558668 0.8844133 ]\n",
            "[0.03756401 0.96243596]\n",
            "[0.04166802 0.958332  ]\n",
            "[0.05530385 0.9446961 ]\n",
            "[0.04761742 0.9523826 ]\n",
            "[0.10899838 0.8910016 ]\n",
            "[0.9120332  0.08796681]\n",
            "[0.70866865 0.29133135]\n",
            "[0.86471885 0.13528122]\n",
            "[0.12926726 0.8707327 ]\n",
            "[0.94438094 0.05561906]\n",
            "[0.4224237 0.5775763]\n",
            "[0.75930876 0.24069117]\n",
            "[0.6395934  0.36040658]\n",
            "[0.6247597  0.37524033]\n",
            "[0.8284661  0.17153391]\n",
            "[0.3075657 0.6924343]\n",
            "[0.6099353 0.3900647]\n",
            "[0.9551161  0.04488393]\n",
            "[0.95984656 0.04015344]\n",
            "[0.8238009  0.17619914]\n",
            "[0.95146495 0.04853505]\n",
            "[0.24122061 0.75877935]\n",
            "[0.96027565 0.03972431]\n",
            "[0.23699923 0.76300085]\n",
            "[0.36617225 0.63382775]\n",
            "[0.91628796 0.08371206]\n",
            "[0.9649604  0.03503966]\n",
            "[0.45468426 0.5453158 ]\n",
            "[0.78639054 0.21360949]\n",
            "[0.9200666  0.07993338]\n",
            "[0.90530133 0.09469865]\n",
            "[0.8786151 0.1213849]\n",
            "[0.96134675 0.03865327]\n",
            "[0.714178 0.285822]\n",
            "[0.9622888  0.03771129]\n",
            "[0.8386478  0.16135219]\n",
            "[0.8686492  0.13135083]\n",
            "[0.87810194 0.12189807]\n",
            "[0.5487269 0.4512731]\n",
            "[0.8876813  0.11231866]\n",
            "[0.03346772 0.9665323 ]\n",
            "[0.9464075  0.05359243]\n",
            "[0.94931245 0.05068758]\n",
            "[0.94181705 0.05818301]\n",
            "[0.9291392  0.07086077]\n",
            "[0.86745113 0.13254887]\n",
            "[0.7383903  0.26160967]\n",
            "[0.9595366  0.04046339]\n",
            "[0.9312306  0.06876937]\n",
            "[0.03636627 0.9636337 ]\n",
            "[0.83412313 0.1658768 ]\n",
            "[0.03922831 0.9607717 ]\n",
            "[0.93462384 0.0653761 ]\n",
            "[0.5977322  0.40226787]\n",
            "[0.7363622 0.2636378]\n",
            "[0.4359545 0.5640455]\n",
            "[0.9152751  0.08472494]\n",
            "[0.6131644  0.38683555]\n",
            "[0.04860724 0.95139277]\n",
            "[0.36138785 0.6386122 ]\n",
            "[0.26186284 0.7381372 ]\n",
            "[0.1217901 0.8782099]\n",
            "[0.03668114 0.9633189 ]\n",
            "[0.10460934 0.8953907 ]\n",
            "[0.7406194  0.25938055]\n",
            "[0.91659623 0.0834038 ]\n",
            "[0.9478728  0.05212714]\n",
            "[0.6562219  0.34377804]\n",
            "[0.04780128 0.95219874]\n",
            "[0.4591336 0.5408664]\n",
            "[0.9648282  0.03517179]\n",
            "[0.18747751 0.8125225 ]\n",
            "[0.08100531 0.9189947 ]\n",
            "[0.11842335 0.8815766 ]\n",
            "[0.73056453 0.2694354 ]\n",
            "[0.8636232  0.13637678]\n",
            "[0.8629707  0.13702925]\n",
            "[0.13310435 0.8668957 ]\n",
            "[0.9361946  0.06380543]\n",
            "[0.8517721  0.14822789]\n",
            "[0.3213498 0.6786502]\n",
            "[0.5643584  0.43564156]\n",
            "[0.03869911 0.9613009 ]\n",
            "[0.03947903 0.960521  ]\n",
            "[0.94797444 0.05202555]\n",
            "[0.03778705 0.9622129 ]\n",
            "[0.8418524  0.15814759]\n",
            "[0.9641063  0.03589366]\n",
            "[0.9549492  0.04505079]\n",
            "[0.72461134 0.27538866]\n",
            "[0.8811652  0.11883479]\n",
            "[0.8670251  0.13297494]\n",
            "[0.16243245 0.83756757]\n",
            "[0.53777367 0.46222636]\n",
            "[0.18851848 0.8114816 ]\n",
            "[0.16069044 0.8393096 ]\n",
            "[0.6262924  0.37370756]\n",
            "[0.7990588 0.2009412]\n",
            "[0.95266813 0.04733188]\n",
            "[0.04183894 0.95816106]\n",
            "[0.76942086 0.23057917]\n",
            "[0.96337146 0.03662852]\n",
            "[0.9660758  0.03392425]\n",
            "[0.68022203 0.31977797]\n",
            "[0.67943186 0.32056817]\n",
            "[0.9550813  0.04491867]\n",
            "[0.04267518 0.95732474]\n",
            "[0.91468    0.08531997]\n",
            "[0.37394762 0.62605244]\n",
            "[0.0570038  0.94299626]\n",
            "[0.37326267 0.6267373 ]\n",
            "[0.9672442  0.03275586]\n",
            "[0.04311091 0.95688903]\n",
            "[0.83458686 0.1654131 ]\n",
            "[0.23698567 0.76301426]\n",
            "[0.24528459 0.7547154 ]\n",
            "[0.74413705 0.2558629 ]\n",
            "[0.92957276 0.07042727]\n",
            "[0.95217097 0.04782903]\n",
            "[0.73704875 0.2629513 ]\n",
            "[0.55970085 0.4402992 ]\n",
            "[0.47991395 0.5200861 ]\n",
            "[0.9601377  0.03986223]\n",
            "[0.9190921  0.08090792]\n",
            "[0.47243753 0.5275625 ]\n",
            "[0.2205069 0.7794931]\n",
            "[0.03676736 0.96323264]\n",
            "[0.87275356 0.12724647]\n",
            "[0.04350234 0.95649767]\n",
            "[0.95376116 0.04623881]\n",
            "[0.9331145  0.06688543]\n",
            "[0.46597087 0.5340292 ]\n",
            "[0.88145703 0.11854301]\n",
            "[0.6052601  0.39473996]\n",
            "[0.46446592 0.5355341 ]\n",
            "[0.33846247 0.6615375 ]\n",
            "[0.82954055 0.17045946]\n",
            "[0.9040321  0.09596789]\n",
            "[0.88751113 0.11248884]\n",
            "[0.36951503 0.630485  ]\n",
            "[0.20289811 0.7971019 ]\n",
            "[0.81091535 0.18908459]\n",
            "[0.90183455 0.09816542]\n",
            "[0.55706507 0.44293496]\n",
            "[0.5072529  0.49274716]\n",
            "[0.9108192  0.08918089]\n",
            "[0.08955346 0.9104466 ]\n",
            "[0.1724134 0.8275866]\n",
            "[0.5523564  0.44764355]\n",
            "[0.3090584  0.69094163]\n",
            "[0.60696065 0.39303932]\n",
            "[0.95596486 0.04403511]\n",
            "[0.8886965  0.11130352]\n",
            "[0.48026875 0.5197313 ]\n",
            "[0.28297147 0.71702856]\n",
            "[0.65449595 0.34550405]\n",
            "[0.13261352 0.86738646]\n",
            "[0.93398637 0.06601367]\n",
            "[0.0387371  0.96126294]\n",
            "[0.9194623  0.08053768]\n",
            "[0.89842415 0.10157584]\n",
            "[0.96238524 0.0376148 ]\n",
            "[0.25962615 0.7403739 ]\n",
            "[0.8622246 0.1377754]\n",
            "[0.9432277  0.05677224]\n",
            "[0.1282112 0.8717888]\n",
            "[0.5548282 0.4451718]\n",
            "[0.79799986 0.20200013]\n",
            "[0.5087836 0.4912164]\n",
            "[0.934865   0.06513497]\n",
            "[0.88659734 0.11340271]\n",
            "[0.04192106 0.958079  ]\n",
            "[0.9643106  0.03568941]\n",
            "[0.7259905  0.27400956]\n",
            "[0.23006923 0.7699307 ]\n",
            "[0.5844461 0.4155539]\n",
            "[0.04355219 0.95644784]\n",
            "[0.32652283 0.6734772 ]\n",
            "[0.2107486  0.78925145]\n",
            "[0.7531459  0.24685417]\n",
            "[0.06601193 0.93398803]\n",
            "[0.47017616 0.52982384]\n",
            "[0.25706646 0.7429335 ]\n",
            "[0.5990544 0.4009456]\n",
            "[0.96243095 0.03756905]\n",
            "[0.9651017  0.03489835]\n",
            "[0.40499723 0.5950028 ]\n",
            "[0.6504347  0.34956533]\n",
            "[0.83009106 0.16990893]\n",
            "[0.64308196 0.356918  ]\n",
            "[0.8597483  0.14025168]\n",
            "[0.79861826 0.20138168]\n",
            "[0.96340454 0.03659549]\n",
            "[0.04037391 0.9596261 ]\n",
            "[0.93192416 0.06807581]\n",
            "[0.90449893 0.09550104]\n",
            "[0.8939382  0.10606178]\n",
            "[0.04226111 0.9577389 ]\n",
            "[0.94935584 0.05064419]\n",
            "[0.45420492 0.5457951 ]\n",
            "[0.17586602 0.824134  ]\n",
            "[0.96414644 0.0358536 ]\n",
            "[0.13872305 0.8612769 ]\n",
            "[0.9565284  0.04347152]\n",
            "[0.8537719 0.1462281]\n",
            "[0.4756509 0.5243491]\n",
            "[0.6302753 0.3697247]\n",
            "[0.6491026 0.3508973]\n",
            "[0.68805325 0.31194678]\n",
            "[0.60599524 0.3940048 ]\n",
            "[0.85327387 0.14672613]\n",
            "[0.8599315 0.1400685]\n",
            "[0.47522852 0.5247715 ]\n",
            "[0.958876   0.04112394]\n",
            "[0.9171631  0.08283693]\n",
            "[0.9244814  0.07551865]\n",
            "[0.36703554 0.63296443]\n",
            "[0.7378322 0.2621678]\n",
            "[0.9498657 0.0501343]\n",
            "[0.11146459 0.88853544]\n",
            "[0.95975107 0.04024901]\n",
            "[0.78829974 0.21170029]\n",
            "[0.95760417 0.04239585]\n",
            "[0.8869245  0.11307548]\n",
            "[0.8641491  0.13585097]\n",
            "[0.19618855 0.8038115 ]\n",
            "[0.9390324 0.0609676]\n",
            "[0.9430988  0.05690118]\n",
            "[0.07543756 0.92456245]\n",
            "[0.3603404  0.63965964]\n",
            "[0.34272626 0.6572737 ]\n",
            "[0.9166488  0.08335122]\n",
            "[0.96234196 0.03765798]\n",
            "[0.9111787 0.0888213]\n",
            "[0.27778456 0.7222155 ]\n",
            "[0.82055223 0.17944779]\n",
            "[0.30777574 0.69222426]\n",
            "[0.05525695 0.9447431 ]\n",
            "[0.04884952 0.9511505 ]\n",
            "[0.9550211  0.04497895]\n",
            "[0.71604174 0.28395823]\n",
            "[0.84549797 0.154502  ]\n",
            "[0.9121397  0.08786027]\n",
            "[0.96342623 0.03657383]\n",
            "[0.03461032 0.96538967]\n",
            "[0.58111054 0.41888952]\n",
            "[0.51800096 0.481999  ]\n",
            "[0.05878568 0.94121426]\n",
            "[0.4605427 0.5394573]\n",
            "[0.9651819  0.03481809]\n",
            "[0.94832563 0.05167436]\n",
            "[0.6468309  0.35316908]\n",
            "[0.95586246 0.04413748]\n",
            "[0.96866924 0.03133069]\n",
            "[0.96207666 0.03792333]\n",
            "[0.9637893  0.03621074]\n",
            "[0.9591871  0.04081292]\n",
            "[0.9304236  0.06957638]\n",
            "[0.91564846 0.08435158]\n",
            "[0.7521144  0.24788557]\n",
            "[0.9281809  0.07181917]\n",
            "[0.90264386 0.09735616]\n",
            "[0.9242866 0.0757134]\n",
            "[0.9393225  0.06067755]\n",
            "[0.79708326 0.2029167 ]\n",
            "[0.90797836 0.09202158]\n",
            "[0.45755497 0.542445  ]\n",
            "[0.9104765  0.08952349]\n",
            "[0.8675148  0.13248517]\n",
            "[0.29094863 0.7090514 ]\n",
            "[0.9485123  0.05148764]\n",
            "[0.9574762  0.04252379]\n",
            "[0.9640847  0.03591531]\n",
            "[0.4116733 0.5883267]\n",
            "[0.08454975 0.9154502 ]\n",
            "[0.19218066 0.80781937]\n",
            "[0.81041473 0.18958527]\n",
            "[0.8152361  0.18476397]\n",
            "[0.6274251  0.37257493]\n",
            "[0.94997305 0.05002699]\n",
            "[0.86177605 0.13822395]\n",
            "[0.94562274 0.05437727]\n",
            "[0.80714965 0.19285029]\n",
            "[0.9285141  0.07148586]\n",
            "[0.9359115  0.06408855]\n",
            "[0.7927316  0.20726839]\n",
            "[0.945287 0.054713]\n",
            "[0.8241588 0.1758412]\n",
            "[0.9485323 0.0514677]\n",
            "[0.49204826 0.50795174]\n",
            "[0.2517365  0.74826354]\n",
            "[0.36736214 0.6326378 ]\n",
            "[0.12557426 0.87442577]\n",
            "[0.95531785 0.04468217]\n",
            "[0.70873684 0.29126313]\n",
            "[0.8417614  0.15823853]\n",
            "[0.71725273 0.28274727]\n",
            "[0.93670505 0.06329491]\n",
            "[0.53759617 0.46240377]\n",
            "[0.85190415 0.14809592]\n",
            "[0.91346586 0.08653416]\n",
            "[0.9594026  0.04059739]\n",
            "[0.8905572  0.10944277]\n",
            "[0.7819414  0.21805857]\n",
            "[0.26854417 0.73145586]\n",
            "[0.93462926 0.06537075]\n",
            "[0.33075425 0.6692457 ]\n",
            "[0.9386542  0.06134584]\n",
            "[0.8956814  0.10431855]\n",
            "[0.92143697 0.07856309]\n",
            "[0.03836113 0.9616389 ]\n",
            "[0.19394952 0.8060504 ]\n",
            "[0.83455    0.16544999]\n",
            "[0.84357065 0.15642937]\n",
            "[0.91845435 0.08154564]\n",
            "[0.06282058 0.93717945]\n",
            "[0.9317575 0.0682425]\n",
            "[0.22054993 0.77945   ]\n",
            "[0.88975996 0.11024008]\n",
            "[0.69367206 0.306328  ]\n",
            "[0.66810083 0.33189914]\n",
            "[0.7473609 0.2526391]\n",
            "[0.9642539  0.03574603]\n",
            "[0.7977763  0.20222375]\n",
            "[0.9620309  0.03796913]\n",
            "[0.04359826 0.9564017 ]\n",
            "[0.5727017 0.4272983]\n",
            "[0.6231616  0.37683842]\n",
            "[0.3927857 0.6072143]\n",
            "[0.95888984 0.04111016]\n",
            "[0.40634325 0.5936568 ]\n",
            "[0.29062358 0.70937645]\n",
            "[0.479394   0.52060604]\n",
            "[0.708446   0.29155397]\n",
            "[0.2646027  0.73539734]\n",
            "[0.3398255 0.6601745]\n",
            "[0.3265978 0.6734022]\n",
            "[0.95517254 0.0448275 ]\n",
            "[0.7149 0.2851]\n",
            "[0.72344464 0.27655542]\n",
            "[0.15584765 0.8441524 ]\n",
            "[0.92279804 0.07720199]\n",
            "[0.2886457  0.71135426]\n",
            "[0.9051466  0.09485344]\n",
            "[0.18407667 0.81592333]\n",
            "[0.3992492 0.6007508]\n",
            "[0.92707616 0.07292387]\n",
            "[0.9501769  0.04982318]\n",
            "[0.05460026 0.9453998 ]\n",
            "[0.9105196  0.08948044]\n",
            "[0.53295916 0.46704078]\n",
            "[0.8143573  0.18564276]\n",
            "[0.9495756  0.05042438]\n",
            "[0.80082566 0.19917431]\n",
            "[0.9534717  0.04652828]\n",
            "[0.84751326 0.15248676]\n",
            "[0.8794898  0.12051016]\n",
            "[0.6315668  0.36843324]\n",
            "[0.7828231  0.21717691]\n",
            "[0.9050224  0.09497763]\n",
            "[0.20764922 0.79235077]\n",
            "[0.9486223 0.0513777]\n",
            "[0.91109335 0.08890662]\n",
            "[0.04708487 0.95291513]\n",
            "[0.9457663  0.05423363]\n",
            "[0.2777515  0.72224844]\n",
            "[0.43103004 0.56896996]\n",
            "[0.08921748 0.9107826 ]\n",
            "[0.8919194  0.10808064]\n",
            "[0.8412916 0.1587084]\n",
            "[0.14790925 0.8520907 ]\n",
            "[0.946117   0.05388297]\n",
            "[0.43642166 0.56357837]\n",
            "[0.94347906 0.05652092]\n",
            "[0.9436485  0.05635148]\n",
            "[0.7884594  0.21154058]\n",
            "[0.12372029 0.8762797 ]\n",
            "[0.96678257 0.03321738]\n",
            "[0.8369658  0.16303416]\n",
            "[0.9634327  0.03656729]\n",
            "[0.9525267  0.04747335]\n",
            "[0.9376152  0.06238476]\n",
            "[0.9615591  0.03844088]\n",
            "[0.26531103 0.73468894]\n",
            "[0.89682764 0.10317235]\n",
            "[0.9235371  0.07646295]\n",
            "[0.9506386  0.04936143]\n",
            "[0.9655388  0.03446118]\n",
            "[0.95338815 0.04661185]\n",
            "[0.9540367  0.04596337]\n",
            "[0.2091264 0.7908736]\n",
            "[0.04596213 0.9540379 ]\n",
            "[0.05944289 0.9405571 ]\n",
            "[0.8409212 0.1590788]\n",
            "[0.4163579 0.5836421]\n",
            "[0.8634303  0.13656966]\n",
            "[0.8625065  0.13749349]\n",
            "[0.39686623 0.60313374]\n",
            "[0.96364325 0.03635672]\n",
            "[0.10213487 0.89786506]\n",
            "[0.1832487  0.81675136]\n",
            "[0.14711389 0.8528861 ]\n",
            "[0.9261528 0.0738472]\n",
            "[0.25049782 0.7495022 ]\n",
            "[0.36704344 0.6329566 ]\n",
            "[0.7568447  0.24315529]\n",
            "[0.21579443 0.7842056 ]\n",
            "[0.11833166 0.8816683 ]\n",
            "[0.9394044  0.06059561]\n",
            "[0.25031146 0.74968857]\n",
            "[0.41886047 0.58113956]\n",
            "[0.74435186 0.2556481 ]\n",
            "[0.86739945 0.13260058]\n",
            "[0.5252445  0.47475553]\n",
            "[0.9444043  0.05559565]\n",
            "[0.9632544  0.03674558]\n",
            "[0.61993134 0.38006866]\n",
            "[0.8370961  0.16290388]\n",
            "[0.8208255  0.17917444]\n",
            "[0.04732451 0.9526755 ]\n",
            "[0.9515198  0.04848016]\n",
            "[0.961901   0.03809896]\n",
            "[0.7907224  0.20927754]\n",
            "[0.9256548  0.07434523]\n",
            "[0.5177859 0.4822141]\n",
            "[0.05714481 0.9428552 ]\n",
            "[0.8976936  0.10230644]\n",
            "[0.9461734  0.05382654]\n",
            "[0.6552053  0.34479466]\n",
            "[0.07702605 0.92297393]\n",
            "[0.889953   0.11004701]\n",
            "[0.7244463  0.27555367]\n",
            "[0.0499136  0.95008636]\n",
            "[0.9121202  0.08787982]\n",
            "[0.8449289  0.15507111]\n",
            "[0.04490953 0.95509046]\n",
            "[0.40579668 0.5942033 ]\n",
            "[0.62580496 0.374195  ]\n",
            "[0.67528635 0.3247137 ]\n",
            "[0.8632692  0.13673082]\n",
            "[0.16760398 0.832396  ]\n",
            "[0.47539774 0.5246023 ]\n",
            "[0.80337185 0.19662817]\n",
            "[0.9355261  0.06447393]\n",
            "[0.9631007  0.03689933]\n",
            "[0.25735965 0.7426404 ]\n",
            "[0.86549103 0.134509  ]\n",
            "[0.9485257  0.05147427]\n",
            "[0.88121074 0.11878931]\n",
            "[0.9371007  0.06289933]\n",
            "[0.42145956 0.57854044]\n",
            "[0.55286556 0.4471345 ]\n",
            "[0.886372   0.11362796]\n",
            "[0.94742936 0.05257056]\n",
            "[0.65405893 0.34594104]\n",
            "[0.9472855 0.0527145]\n",
            "[0.8910568  0.10894322]\n",
            "[0.9292055  0.07079451]\n",
            "[0.19065864 0.8093414 ]\n",
            "[0.95634264 0.04365738]\n",
            "[0.86108905 0.13891093]\n",
            "[0.4848275 0.5151725]\n",
            "[0.8322445 0.1677555]\n",
            "[0.03324758 0.96675235]\n",
            "[0.89230525 0.10769477]\n",
            "[0.9493909  0.05060912]\n",
            "[0.8314346 0.1685654]\n",
            "[0.11371436 0.8862856 ]\n",
            "[0.9192465  0.08075343]\n",
            "[0.96018994 0.03981007]\n",
            "[0.8737674  0.12623264]\n",
            "[0.8733009  0.12669909]\n",
            "[0.95623183 0.0437682 ]\n",
            "[0.9544659  0.04553409]\n",
            "[0.96097696 0.03902301]\n",
            "[0.93078184 0.06921817]\n",
            "[0.9207308  0.07926915]\n",
            "[0.95446104 0.04553903]\n",
            "[0.06989162 0.9301083 ]\n",
            "[0.73651886 0.26348117]\n",
            "[0.9434227  0.05657732]\n",
            "[0.5098701  0.49012992]\n",
            "[0.79358786 0.20641209]\n",
            "[0.6965789  0.30342105]\n",
            "[0.19487573 0.80512434]\n",
            "[0.6315911  0.36840898]\n",
            "[0.9676512  0.03234874]\n",
            "[0.9613575 0.0386425]\n",
            "[0.90107393 0.0989261 ]\n",
            "[0.2558102 0.7441898]\n",
            "[0.04286923 0.9571308 ]\n",
            "[0.23280148 0.7671985 ]\n",
            "[0.05704487 0.94295514]\n",
            "[0.08367041 0.9163296 ]\n",
            "[0.9581625  0.04183748]\n",
            "[0.24417129 0.7558287 ]\n",
            "[0.75773656 0.24226348]\n",
            "[0.94805664 0.0519434 ]\n",
            "[0.8824887  0.11751125]\n",
            "[0.816164   0.18383598]\n",
            "[0.95490015 0.04509979]\n",
            "[0.95668256 0.04331746]\n",
            "[0.5525934  0.44740662]\n",
            "[0.96359855 0.03640146]\n",
            "[0.7875745 0.2124255]\n",
            "[0.9539071  0.04609299]\n",
            "[0.95002955 0.04997043]\n",
            "[0.92420644 0.07579348]\n",
            "[0.07533754 0.9246625 ]\n",
            "[0.9219724  0.07802762]\n",
            "[0.9568441  0.04315589]\n",
            "[0.8941072  0.10589279]\n",
            "[0.9568729  0.04312708]\n",
            "[0.9603898 0.0396102]\n",
            "[0.5344925 0.4655075]\n",
            "[0.94829714 0.05170283]\n",
            "[0.95098305 0.04901695]\n",
            "[0.05718999 0.94281006]\n",
            "[0.9091008  0.09089923]\n",
            "[0.68812436 0.31187564]\n",
            "[0.52312607 0.47687396]\n",
            "[0.92920053 0.07079948]\n",
            "[0.5637262  0.43627378]\n",
            "[0.92355007 0.07644998]\n",
            "[0.13505587 0.86494416]\n",
            "[0.6291285  0.37087145]\n",
            "[0.88756216 0.11243784]\n",
            "[0.03352074 0.96647924]\n",
            "[0.26564533 0.7343546 ]\n",
            "[0.87103856 0.1289615 ]\n",
            "[0.93857765 0.06142239]\n",
            "[0.59154123 0.40845877]\n",
            "[0.07409579 0.9259042 ]\n",
            "[0.0565269 0.9434731]\n",
            "[0.8116992  0.18830082]\n",
            "[0.31142354 0.68857646]\n",
            "[0.6975909 0.3024091]\n",
            "[0.03984757 0.96015245]\n",
            "[0.38067475 0.6193252 ]\n",
            "[0.87965286 0.12034715]\n",
            "[0.891525   0.10847504]\n",
            "[0.94389975 0.05610019]\n",
            "[0.6444047  0.35559523]\n",
            "[0.8462376  0.15376233]\n",
            "[0.05221658 0.9477834 ]\n",
            "[0.95955527 0.04044465]\n",
            "[0.04914819 0.95085186]\n",
            "[0.86717194 0.13282804]\n",
            "[0.88376814 0.11623184]\n",
            "[0.5738272  0.42617282]\n",
            "[0.8301576  0.16984245]\n",
            "[0.6945973 0.3054027]\n",
            "[0.84207886 0.15792115]\n",
            "[0.9209008  0.07909914]\n",
            "[0.9158622  0.08413783]\n",
            "[0.90681654 0.09318348]\n",
            "[0.05816668 0.9418333 ]\n",
            "[0.9425165  0.05748355]\n",
            "[0.52349067 0.47650936]\n",
            "[0.9416542  0.05834582]\n",
            "[0.95717543 0.04282456]\n",
            "[0.9095735  0.09042647]\n",
            "[0.96093047 0.03906948]\n",
            "[0.96044123 0.03955875]\n",
            "[0.9559788 0.0440212]\n",
            "[0.7363191  0.26368082]\n",
            "[0.51535624 0.48464376]\n",
            "[0.9628871  0.03711289]\n",
            "[0.820971   0.17902897]\n",
            "[0.05603627 0.9439637 ]\n",
            "[0.6044527  0.39554733]\n",
            "[0.60661596 0.393384  ]\n",
            "[0.15884002 0.84116   ]\n",
            "[0.15315054 0.84684944]\n",
            "[0.94510454 0.05489541]\n",
            "[0.9251647  0.07483526]\n",
            "[0.6630216 0.3369784]\n",
            "[0.93845177 0.06154825]\n",
            "[0.9169453  0.08305468]\n",
            "[0.03762053 0.9623795 ]\n",
            "[0.37710005 0.6229    ]\n",
            "[0.36659026 0.6334098 ]\n",
            "[0.66977566 0.33022434]\n",
            "[0.9551546 0.0448454]\n",
            "[0.93682504 0.06317496]\n",
            "[0.2693263 0.7306737]\n",
            "[0.90823114 0.09176885]\n",
            "[0.47106937 0.5289306 ]\n",
            "[0.657621   0.34237897]\n",
            "[0.8301228  0.16987716]\n",
            "[0.6117158  0.38828424]\n",
            "[0.71100163 0.28899834]\n",
            "[0.890333   0.10966696]\n",
            "[0.4321433  0.56785667]\n",
            "[0.44934687 0.55065316]\n",
            "[0.96078277 0.03921727]\n",
            "[0.83250785 0.16749212]\n",
            "[0.16486758 0.8351324 ]\n",
            "[0.94742733 0.05257259]\n",
            "[0.8828453 0.1171547]\n",
            "[0.966452   0.03354797]\n",
            "[0.7998618  0.20013824]\n",
            "[0.8611676  0.13883235]\n",
            "[0.8684495 0.1315505]\n",
            "[0.78098965 0.21901043]\n",
            "[0.5413151  0.45868492]\n",
            "[0.9331519  0.06684813]\n",
            "[0.83916056 0.16083947]\n",
            "[0.4737264  0.52627355]\n",
            "[0.8404534  0.15954664]\n",
            "[0.9458314  0.05416863]\n",
            "[0.66348654 0.33651343]\n",
            "[0.33791104 0.662089  ]\n",
            "[0.33164263 0.6683574 ]\n",
            "[0.17066813 0.8293319 ]\n",
            "[0.9549851  0.04501492]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/task1malayalam-results.txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "outputId": "d3a09ff6-b335-46f6-a64a-af2dbbcd9189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "outputId": "186d57a7-4ba9-4a41-8200-1e59b2ea7135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "outputId": "7bc165fc-18e7-4542-b38a-0c3a5f0e8459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-task1malayalam'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task1malayalam\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task1malayalam/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1malayalam/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1malayalam/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}
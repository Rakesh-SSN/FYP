{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c19ee88a49604917a498614c32a99811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f0cd25ff0e3c47ceb70f796c5d7ebc9b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_498b02606df0423bb69b188a560716e2",
              "IPY_MODEL_ee9c7ea44ade4addb84917ad5576f389"
            ]
          }
        },
        "f0cd25ff0e3c47ceb70f796c5d7ebc9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "498b02606df0423bb69b188a560716e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e9ffa4c266ff487aa6c8a1015f100469",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06aabbee92c748a087e097a35427404e"
          }
        },
        "ee9c7ea44ade4addb84917ad5576f389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_05c3bf43fb104f40bf36efd01af3401f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 996k/996k [00:01&lt;00:00, 904kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b0aa2e06d9f48e3b52b781f793af0f4"
          }
        },
        "e9ffa4c266ff487aa6c8a1015f100469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06aabbee92c748a087e097a35427404e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05c3bf43fb104f40bf36efd01af3401f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b0aa2e06d9f48e3b52b781f793af0f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5d6a2439d5d43729daeb86a8ce04547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5f33616512194126bd61b0e2eeded11f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1153270e887f41ac890909f2f07e8854",
              "IPY_MODEL_da946a0af920493883ec66cc107309f6"
            ]
          }
        },
        "5f33616512194126bd61b0e2eeded11f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1153270e887f41ac890909f2f07e8854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_515312a7a0cd46be9e226748244cfe6b",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 569,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 569,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c79e0cab6454458ad69475581431577"
          }
        },
        "da946a0af920493883ec66cc107309f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_01fd133049db4d49a88a0e3bc56c5d04",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 569/569 [00:00&lt;00:00, 19.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e05e575c40764ff5800bdb91e6157de2"
          }
        },
        "515312a7a0cd46be9e226748244cfe6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c79e0cab6454458ad69475581431577": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01fd133049db4d49a88a0e3bc56c5d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e05e575c40764ff5800bdb91e6157de2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6013dea3b8594ca48f3d83c6bfabae12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f494c17d29d24a01b4caf7b76f38beb5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f3b279da83d843ca9ae8075c798eff09",
              "IPY_MODEL_ac5fe86c18cc458d83a69c82c3693c3c"
            ]
          }
        },
        "f494c17d29d24a01b4caf7b76f38beb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3b279da83d843ca9ae8075c798eff09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_809ca9158b284e54a2dce71e1bff104f",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dc469bf670014ddca0b48faec1513640"
          }
        },
        "ac5fe86c18cc458d83a69c82c3693c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e19905998b914f72958c300d93a43b12",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 714M/714M [01:05&lt;00:00, 11.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9c94ee74283a486a897e8d2847e0fe05"
          }
        },
        "809ca9158b284e54a2dce71e1bff104f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dc469bf670014ddca0b48faec1513640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e19905998b914f72958c300d93a43b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9c94ee74283a486a897e8d2847e0fe05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT%20TAMIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "4f86973c-1dda-4e90-e4b5-cad839ce0dfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 12.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 20.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 21.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=af50f8ac9f9011dcb64fb4bbbca89deff6e3792b6de5b823b24b163fd0ff9715\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "cac11fc6-ad4a-4a08-f560-b4e1a7d38bc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "ec7fc464-1727-494a-bb53-bed95cf0c299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/tamil/task1tamil.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>807</th>\n",
              "      <td>TAM0808</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>புயலாக மாறுகிறது காற்றழுத்த தாழ்வு நிலை: தமிழக...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1239</th>\n",
              "      <td>TAM1240</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>நண்பர்கள் மற்றும் உறவினர்கள் வீடுகளில் தேடியும...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1886</th>\n",
              "      <td>TAM1887</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>மும்பை ஐகோட்டை படக் குழுவினர் அணுகினர்.&lt;eol&gt;சி...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1551</th>\n",
              "      <td>TAM1552</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>கேரள மாநிலத்தில் கம்யூனிஸ்டு கட்சிக்கும் பாரதீ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2259</th>\n",
              "      <td>TAM2260</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ஆயிரம் ஆண்டுகள் பழமையான வெண்கல விநாயகர் சிலை உ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>TAM0258</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>டெல்லியில் புதிதாக 3 ஆயிரம் பஸ்களை இயக்க ஆம் ஆ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>928</th>\n",
              "      <td>TAM0929</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>கேரளா, மேற்கு வங்காளம், அசாம் முதல்வர்களுக்கு ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1484</th>\n",
              "      <td>TAM1485</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>சிதம்பரத்தில் சாலையோரம் விநாயகர் சிலையை கடத்தல...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>471</th>\n",
              "      <td>TAM0472</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>அரசு அதிகாரிகள் புரோக்கர்களாக பயன்படுத்தப்படுக...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2115</th>\n",
              "      <td>TAM2116</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>தலைநகர் டாக்கா அருகில் உள்ள ஜத்ரபாரி பஸ் நிலைய...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "807          TAM0808  ...  புயலாக மாறுகிறது காற்றழுத்த தாழ்வு நிலை: தமிழக...\n",
              "1239         TAM1240  ...  நண்பர்கள் மற்றும் உறவினர்கள் வீடுகளில் தேடியும...\n",
              "1886         TAM1887  ...  மும்பை ஐகோட்டை படக் குழுவினர் அணுகினர்.<eol>சி...\n",
              "1551         TAM1552  ...  கேரள மாநிலத்தில் கம்யூனிஸ்டு கட்சிக்கும் பாரதீ...\n",
              "2259         TAM2260  ...  ஆயிரம் ஆண்டுகள் பழமையான வெண்கல விநாயகர் சிலை உ...\n",
              "257          TAM0258  ...  டெல்லியில் புதிதாக 3 ஆயிரம் பஸ்களை இயக்க ஆம் ஆ...\n",
              "928          TAM0929  ...  கேரளா, மேற்கு வங்காளம், அசாம் முதல்வர்களுக்கு ...\n",
              "1484         TAM1485  ...  சிதம்பரத்தில் சாலையோரம் விநாயகர் சிலையை கடத்தல...\n",
              "471          TAM0472  ...  அரசு அதிகாரிகள் புரோக்கர்களாக பயன்படுத்தப்படுக...\n",
              "2115         TAM2116  ...  தலைநகர் டாக்கா அருகில் உள்ள ஜத்ரபாரி பஸ் நிலைய...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "64dd7604-6267-47c5-a6c7-6e3c60ba6d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2273</th>\n",
              "      <td>கிரானைட் உள்ளிட்ட கனிமங்கள் முறைகேடு குறித்து ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2277</th>\n",
              "      <td>முதல்வர் ஜெயலலிதா, சசிகலா, அக்., 1ம் தேதி, தவற...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2101</th>\n",
              "      <td>காஷ்மீர் சேர்ந்த மாணவி ஒருவர் தனது கைபையில் நா...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1977</th>\n",
              "      <td>எதிர் தாக்குதல் நடத்தும் விதமாக பாதுகாப்பு படை...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2217</th>\n",
              "      <td>அணுசக்தி விநியோக நாடுகள் அமைப்பில் (என்.எஸ்.ஜி...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "2273  கிரானைட் உள்ளிட்ட கனிமங்கள் முறைகேடு குறித்து ...      0\n",
              "2277  முதல்வர் ஜெயலலிதா, சசிகலா, அக்., 1ம் தேதி, தவற...      0\n",
              "2101  காஷ்மீர் சேர்ந்த மாணவி ஒருவர் தனது கைபையில் நா...      0\n",
              "1977  எதிர் தாக்குதல் நடத்தும் விதமாக பாதுகாப்பு படை...      0\n",
              "2217  அணுசக்தி விநியோக நாடுகள் அமைப்பில் (என்.எஸ்.ஜி...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "82606a8e-7dd4-4585-8470-199b37299a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "c19ee88a49604917a498614c32a99811",
            "f0cd25ff0e3c47ceb70f796c5d7ebc9b",
            "498b02606df0423bb69b188a560716e2",
            "ee9c7ea44ade4addb84917ad5576f389",
            "e9ffa4c266ff487aa6c8a1015f100469",
            "06aabbee92c748a087e097a35427404e",
            "05c3bf43fb104f40bf36efd01af3401f",
            "1b0aa2e06d9f48e3b52b781f793af0f4"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c19ee88a49604917a498614c32a99811",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=995526, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "47ddb729-79e5-48a7-830d-45c46cade4d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Tokenized:  ['ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'பி', '##ர', '##ச', '##ார', '##ம', 'ச', '##ெ', '##ய', '##தா', '##ர', '.', '<', 'eo', '##l', '>', 'தி', '.', 'மு', '.', 'க', '.', ',', 'வ', '##ே', '##ட', '##பா', '##ள', '##ர', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'சி', '##ன', '##ன', 'ச', '##ே', '##ல', '##ம', 'பகுதி', '##ய', '##ில', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'ஓ', '##ட', '##டு', 'ச', '##ே', '##க', '##ரி', '##த', '##தா', '##ர', '.']\n",
            "Token IDs:  [1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "4c9801a4-dec3-4d22-c49b-5d095e60b2fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Token IDs: [101, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "a1eece35-1847-47ab-be59-48011b5dce55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "0a3eb226-3d79-41b9-8930-eea5080f7c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "65982b05-04a0-4d1c-93c3-b02988c09fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "print(train_inputs)\n",
        "print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "#print(train_masks)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101   1154  35186 ...  55186  95512  31484]\n",
            " [   101   1170 111305 ...   1154 111312  15220]\n",
            " [   101   1163  14124 ...  28065    119    133]\n",
            " ...\n",
            " [   101   1163  27883 ...  27885   1165  18827]\n",
            " [   101  49189  66171 ...  86728  81773  20242]\n",
            " [   101   1142  46168 ...  37076  24171  19894]]\n",
            "[1 0 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "4a66159a-86e0-4489-84f2-632ea9e77d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c5d6a2439d5d43729daeb86a8ce04547",
            "5f33616512194126bd61b0e2eeded11f",
            "1153270e887f41ac890909f2f07e8854",
            "da946a0af920493883ec66cc107309f6",
            "515312a7a0cd46be9e226748244cfe6b",
            "3c79e0cab6454458ad69475581431577",
            "01fd133049db4d49a88a0e3bc56c5d04",
            "e05e575c40764ff5800bdb91e6157de2",
            "6013dea3b8594ca48f3d83c6bfabae12",
            "f494c17d29d24a01b4caf7b76f38beb5",
            "f3b279da83d843ca9ae8075c798eff09",
            "ac5fe86c18cc458d83a69c82c3693c3c",
            "809ca9158b284e54a2dce71e1bff104f",
            "dc469bf670014ddca0b48faec1513640",
            "e19905998b914f72958c300d93a43b12",
            "9c94ee74283a486a897e8d2847e0fe05"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5d6a2439d5d43729daeb86a8ce04547",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=569, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6013dea3b8594ca48f3d83c6bfabae12",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=714314041, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "outputId": "88ada463-a2aa-4c3e-8165-0ccea25ae369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "outputId": "f7a8e2e5-d556-4183-c0aa-1babab63bedf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:10.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epcoh took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epcoh took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epcoh took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.25\n",
            "  Training epcoh took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "outputId": "045d55c6-ddd0-4161-8539-16648130ba0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task1tamil-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "outputId": "c9eb8682-f6d8-4a7e-d94e-232efdbb1e1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "#print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 900 test sentences...\n",
            "    DONE.\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "outputId": "e2325a24-c614-421c-ac7a-7200d6be6895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] ):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] ):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    # elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "    #   #print(\" Semi Paraphrase     \",end=\"\")\n",
        "    #   #count_NP+=1      \n",
        "    #   print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "    #   x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(1)\t 1\t [-0.0916961  0.5871331] \t1\ttrue\n",
            "(1)\t 2\t [ 1.7655591 -1.7905022] \t0\tfalse\n",
            "(1)\t 3\t [ 1.1040487 -0.8226187] \t0\tfalse\n",
            "(1)\t 4\t [ 1.813898  -1.9011356] \t0\tfalse\n",
            "(0)\t 5\t [ 1.8036555 -1.9062835] \t0\ttrue\n",
            "(1)\t 6\t [ 1.7155678 -1.6758815] \t0\tfalse\n",
            "(0)\t 7\t [ 1.2290972 -1.1243919] \t0\ttrue\n",
            "(0)\t 8\t [ 1.6299888 -1.5297271] \t0\ttrue\n",
            "(0)\t 9\t [ 1.8218287 -1.9547037] \t0\ttrue\n",
            "(0)\t 10\t [ 0.8445416  -0.44452074] \t0\ttrue\n",
            "(1)\t 11\t [ 1.7946304 -1.8021232] \t0\tfalse\n",
            "(0)\t 12\t [ 1.8249447 -1.9828066] \t0\ttrue\n",
            "(0)\t 13\t [ 1.4790477 -1.2770814] \t0\ttrue\n",
            "(0)\t 14\t [ 1.8507096 -2.0341225] \t0\ttrue\n",
            "(0)\t 15\t [ 1.8280619 -1.9164468] \t0\ttrue\n",
            "(0)\t 16\t [ 1.2770101 -1.0985533] \t0\ttrue\n",
            "(0)\t 17\t [ 1.8226792 -1.8086665] \t0\ttrue\n",
            "(0)\t 18\t [-0.22183503  0.7731988 ] \t1\tfalse\n",
            "(0)\t 19\t [ 1.7729594 -1.8594297] \t0\ttrue\n",
            "(0)\t 20\t [ 1.7937796 -1.9007851] \t0\ttrue\n",
            "(0)\t 21\t [ 1.7906588 -1.8845345] \t0\ttrue\n",
            "(1)\t 22\t [ 0.5014963  -0.00218298] \t0\tfalse\n",
            "(0)\t 23\t [ 1.7917454 -1.9047183] \t0\ttrue\n",
            "(0)\t 24\t [-0.79144305  1.3162531 ] \t1\tfalse\n",
            "(0)\t 25\t [ 1.0659851 -0.8021967] \t0\ttrue\n",
            "(0)\t 26\t [ 1.4878262 -1.4025619] \t0\ttrue\n",
            "(0)\t 27\t [ 1.7724315 -1.7826127] \t0\ttrue\n",
            "(0)\t 28\t [-0.15557976  0.71525586] \t1\tfalse\n",
            "(0)\t 29\t [ 1.7311015 -1.7231084] \t0\ttrue\n",
            "(0)\t 30\t [ 1.7109555 -1.8218528] \t0\ttrue\n",
            "(0)\t 31\t [-1.3058656  1.8296665] \t1\tfalse\n",
            "(0)\t 32\t [ 1.7173967 -1.73888  ] \t0\ttrue\n",
            "(0)\t 33\t [ 1.716402  -1.7679604] \t0\ttrue\n",
            "(1)\t 34\t [ 1.8171918 -1.9600368] \t0\tfalse\n",
            "(1)\t 35\t [ 1.8171918 -1.9600368] \t0\tfalse\n",
            "(1)\t 36\t [ 1.8652903 -2.0510726] \t0\tfalse\n",
            "(0)\t 37\t [-0.301175   0.6567119] \t1\tfalse\n",
            "(1)\t 38\t [ 1.7885071 -1.9295326] \t0\tfalse\n",
            "(0)\t 39\t [ 1.6978108 -1.729168 ] \t0\ttrue\n",
            "(0)\t 40\t [ 1.5619158 -1.4916967] \t0\ttrue\n",
            "(1)\t 41\t [ 1.1887593  -0.99451214] \t0\tfalse\n",
            "(0)\t 42\t [ 1.0816531  -0.74096686] \t0\ttrue\n",
            "(1)\t 43\t [ 0.6664898  -0.18948235] \t0\tfalse\n",
            "(1)\t 44\t [ 1.6610619 -1.7281762] \t0\tfalse\n",
            "(1)\t 45\t [ 1.6610619 -1.7281762] \t0\tfalse\n",
            "(0)\t 46\t [ 1.2421697 -0.9852029] \t0\ttrue\n",
            "(0)\t 47\t [ 1.8617156 -1.9832349] \t0\ttrue\n",
            "(1)\t 48\t [ 1.760884  -1.8682348] \t0\tfalse\n",
            "(0)\t 49\t [ 1.1851627 -0.9462407] \t0\ttrue\n",
            "(0)\t 50\t [ 1.7944335 -1.9794674] \t0\ttrue\n",
            "(0)\t 51\t [ 1.1182443  -0.93681985] \t0\ttrue\n",
            "(0)\t 52\t [ 1.7429725 -1.7906945] \t0\ttrue\n",
            "(0)\t 53\t [ 1.8614327 -2.031092 ] \t0\ttrue\n",
            "(0)\t 54\t [ 1.8538077 -1.9882338] \t0\ttrue\n",
            "(0)\t 55\t [ 1.8871071 -2.028178 ] \t0\ttrue\n",
            "(1)\t 56\t [ 1.6139122 -1.4463496] \t0\tfalse\n",
            "(1)\t 57\t [-1.2210879  1.7897068] \t1\ttrue\n",
            "(0)\t 58\t [ 1.7564577 -1.842283 ] \t0\ttrue\n",
            "(0)\t 59\t [ 1.711135  -1.7588595] \t0\ttrue\n",
            "(1)\t 60\t [ 0.67357886 -0.45673805] \t0\tfalse\n",
            "(0)\t 61\t [ 1.7123193 -1.7687091] \t0\ttrue\n",
            "(1)\t 62\t [ 1.3978664 -1.1347479] \t0\tfalse\n",
            "(1)\t 63\t [-1.1864151  1.7742149] \t1\ttrue\n",
            "(1)\t 64\t [ 0.8624444 -0.491437 ] \t0\tfalse\n",
            "(1)\t 65\t [ 0.8624444 -0.491437 ] \t0\tfalse\n",
            "(0)\t 66\t [ 1.7473541 -1.7285653] \t0\ttrue\n",
            "(0)\t 67\t [ 1.7667212 -1.8380263] \t0\ttrue\n",
            "(0)\t 68\t [ 1.5704222 -1.6219296] \t0\ttrue\n",
            "(0)\t 69\t [ 1.7794944 -1.9446629] \t0\ttrue\n",
            "(0)\t 70\t [ 1.7885507 -1.813336 ] \t0\ttrue\n",
            "(0)\t 71\t [ 1.132648  -0.8469965] \t0\ttrue\n",
            "(1)\t 72\t [-1.1960522  1.6759036] \t1\ttrue\n",
            "(1)\t 73\t [-1.1960522  1.6759036] \t1\ttrue\n",
            "(1)\t 74\t [-1.1397463  1.2720846] \t1\ttrue\n",
            "(1)\t 75\t [-1.1397463  1.2720846] \t1\ttrue\n",
            "(1)\t 76\t [ 1.8103722 -1.9731666] \t0\tfalse\n",
            "(1)\t 77\t [ 1.8103722 -1.9731666] \t0\tfalse\n",
            "(1)\t 78\t [-0.6109309  1.0918392] \t1\ttrue\n",
            "(0)\t 79\t [ 1.8590423 -1.9991393] \t0\ttrue\n",
            "(0)\t 80\t [ 1.8622453 -1.9599365] \t0\ttrue\n",
            "(0)\t 81\t [ 1.919094  -1.9783295] \t0\ttrue\n",
            "(1)\t 82\t [-1.1309162  1.5704405] \t1\ttrue\n",
            "(0)\t 83\t [ 1.866474  -1.9700507] \t0\ttrue\n",
            "(0)\t 84\t [ 1.861014  -1.9967811] \t0\ttrue\n",
            "(1)\t 85\t [ 1.789508  -1.9258853] \t0\tfalse\n",
            "(1)\t 86\t [ 1.8237991 -1.8719978] \t0\tfalse\n",
            "(1)\t 87\t [ 1.652497 -1.516685] \t0\tfalse\n",
            "(0)\t 88\t [ 1.7976586 -1.933867 ] \t0\ttrue\n",
            "(0)\t 89\t [ 1.6447276 -1.6560024] \t0\ttrue\n",
            "(1)\t 90\t [-0.94917446  1.4610806 ] \t1\ttrue\n",
            "(1)\t 91\t [-1.251599   1.7440798] \t1\ttrue\n",
            "(1)\t 92\t [ 1.0050162 -0.5843926] \t0\tfalse\n",
            "(0)\t 93\t [ 1.1344707 -0.8279544] \t0\ttrue\n",
            "(0)\t 94\t [0.2746165  0.15082154] \t0\ttrue\n",
            "(0)\t 95\t [ 1.8557519 -2.0296333] \t0\ttrue\n",
            "(0)\t 96\t [ 1.6911088 -1.8470136] \t0\ttrue\n",
            "(0)\t 97\t [ 1.7557188 -1.987548 ] \t0\ttrue\n",
            "(0)\t 98\t [ 1.8147684 -1.963586 ] \t0\ttrue\n",
            "(0)\t 99\t [ 1.7935215 -1.8367912] \t0\ttrue\n",
            "(0)\t 100\t [ 1.7879294 -1.8896738] \t0\ttrue\n",
            "(1)\t 101\t [-1.2430382  1.8386296] \t1\ttrue\n",
            "(1)\t 102\t [ 1.8213862 -1.9969862] \t0\tfalse\n",
            "(1)\t 103\t [ 1.5322639 -1.5917171] \t0\tfalse\n",
            "(1)\t 104\t [ 1.6259316 -1.5789663] \t0\tfalse\n",
            "(0)\t 105\t [ 1.8221227 -1.8759811] \t0\ttrue\n",
            "(1)\t 106\t [ 1.3437628 -1.0636421] \t0\tfalse\n",
            "(0)\t 107\t [ 1.6682396 -1.678182 ] \t0\ttrue\n",
            "(0)\t 108\t [ 1.6826023 -1.6932207] \t0\ttrue\n",
            "(0)\t 109\t [ 1.8332119 -1.9740952] \t0\ttrue\n",
            "(0)\t 110\t [ 1.7618997 -1.7653856] \t0\ttrue\n",
            "(0)\t 111\t [ 1.8117231 -1.7523515] \t0\ttrue\n",
            "(1)\t 112\t [0.33290648 0.15681884] \t0\tfalse\n",
            "(1)\t 113\t [ 1.5392369 -1.3792756] \t0\tfalse\n",
            "(1)\t 114\t [ 1.8256043 -1.8429589] \t0\tfalse\n",
            "(1)\t 115\t [ 1.79645   -1.8509289] \t0\tfalse\n",
            "(0)\t 116\t [ 1.7946669 -1.9233835] \t0\ttrue\n",
            "(0)\t 117\t [ 0.71966213 -0.30773285] \t0\ttrue\n",
            "(0)\t 118\t [ 1.7156848 -1.794457 ] \t0\ttrue\n",
            "(0)\t 119\t [ 1.8781255 -2.0339599] \t0\ttrue\n",
            "(0)\t 120\t [ 1.8940588 -2.0079498] \t0\ttrue\n",
            "(0)\t 121\t [ 1.0877022  -0.93208236] \t0\ttrue\n",
            "(0)\t 122\t [ 1.7817584 -1.8109503] \t0\ttrue\n",
            "(1)\t 123\t [-0.8486301  1.3841814] \t1\ttrue\n",
            "(0)\t 124\t [ 1.8680588 -2.0326104] \t0\ttrue\n",
            "(0)\t 125\t [ 1.8702987 -2.0470147] \t0\ttrue\n",
            "(0)\t 126\t [ 1.6996571 -1.7934145] \t0\ttrue\n",
            "(1)\t 127\t [ 1.7903185 -1.9750241] \t0\tfalse\n",
            "(1)\t 128\t [ 1.7903185 -1.9750241] \t0\tfalse\n",
            "(0)\t 129\t [ 1.6725714 -1.636589 ] \t0\ttrue\n",
            "(0)\t 130\t [ 1.7630533 -1.8368922] \t0\ttrue\n",
            "(0)\t 131\t [ 1.9060487 -1.977767 ] \t0\ttrue\n",
            "(0)\t 132\t [ 1.8364807 -1.8703114] \t0\ttrue\n",
            "(1)\t 133\t [ 1.7526333 -1.8341013] \t0\tfalse\n",
            "(0)\t 134\t [ 1.514424  -1.3107997] \t0\ttrue\n",
            "(0)\t 135\t [ 1.8992354 -1.9673125] \t0\ttrue\n",
            "(0)\t 136\t [ 1.6716838 -1.6147035] \t0\ttrue\n",
            "(0)\t 137\t [ 1.2651192 -1.1059548] \t0\ttrue\n",
            "(0)\t 138\t [ 1.7693483 -1.8329376] \t0\ttrue\n",
            "(0)\t 139\t [ 1.8738757 -2.0190163] \t0\ttrue\n",
            "(0)\t 140\t [ 1.5491132 -1.4849507] \t0\ttrue\n",
            "(0)\t 141\t [ 1.7255832 -1.7651454] \t0\ttrue\n",
            "(0)\t 142\t [ 1.6535982 -1.5694479] \t0\ttrue\n",
            "(1)\t 143\t [ 1.8890277 -1.9924151] \t0\tfalse\n",
            "(1)\t 144\t [ 1.582203 -1.570089] \t0\tfalse\n",
            "(1)\t 145\t [ 1.4968022 -1.4833719] \t0\tfalse\n",
            "(0)\t 146\t [ 1.8738514 -1.9652731] \t0\ttrue\n",
            "(0)\t 147\t [-0.721027   1.0875684] \t1\tfalse\n",
            "(1)\t 148\t [ 1.8344145 -1.8944806] \t0\tfalse\n",
            "(1)\t 149\t [ 1.8344145 -1.8944806] \t0\tfalse\n",
            "(1)\t 150\t [ 1.8453007 -1.9454217] \t0\tfalse\n",
            "(1)\t 151\t [ 1.8453007 -1.9454217] \t0\tfalse\n",
            "(1)\t 152\t [ 1.7368373 -1.6580603] \t0\tfalse\n",
            "(1)\t 153\t [ 1.7368373 -1.6580603] \t0\tfalse\n",
            "(0)\t 154\t [ 1.8235768 -1.9541804] \t0\ttrue\n",
            "(1)\t 155\t [ 1.5722216 -1.4936419] \t0\tfalse\n",
            "(1)\t 156\t [ 1.5722216 -1.4936419] \t0\tfalse\n",
            "(1)\t 157\t [ 1.8583795 -2.0644524] \t0\tfalse\n",
            "(0)\t 158\t [ 1.759513  -1.9034373] \t0\ttrue\n",
            "(0)\t 159\t [ 1.8699596 -2.036741 ] \t0\ttrue\n",
            "(1)\t 160\t [ 1.8309664 -1.8953432] \t0\tfalse\n",
            "(0)\t 161\t [ 1.7297299 -1.8443881] \t0\ttrue\n",
            "(1)\t 162\t [ 0.21743767 -0.00854841] \t0\tfalse\n",
            "(0)\t 163\t [ 1.8137752 -1.9690474] \t0\ttrue\n",
            "(0)\t 164\t [ 0.79127026 -0.43733236] \t0\ttrue\n",
            "(0)\t 165\t [ 1.7480854 -1.7613873] \t0\ttrue\n",
            "(0)\t 166\t [-1.0655138  1.6536332] \t1\tfalse\n",
            "(0)\t 167\t [ 1.8469987 -1.9764695] \t0\ttrue\n",
            "(1)\t 168\t [ 1.8043207 -1.8393134] \t0\tfalse\n",
            "(1)\t 169\t [ 1.7969944 -1.8775179] \t0\tfalse\n",
            "(1)\t 170\t [ 1.7969944 -1.8775179] \t0\tfalse\n",
            "(0)\t 171\t [ 0.5759918  -0.29862174] \t0\ttrue\n",
            "(1)\t 172\t [ 0.8992957  -0.39788195] \t0\tfalse\n",
            "(1)\t 173\t [0.08397302 0.4152477 ] \t1\ttrue\n",
            "(1)\t 174\t [ 1.7637976 -1.8143084] \t0\tfalse\n",
            "(0)\t 175\t [-0.69067955  0.92156917] \t1\tfalse\n",
            "(1)\t 176\t [ 1.7878853 -1.8774976] \t0\tfalse\n",
            "(0)\t 177\t [ 1.6912133 -1.7019737] \t0\ttrue\n",
            "(0)\t 178\t [ 1.8550181 -2.0348606] \t0\ttrue\n",
            "(0)\t 179\t [ 1.8447818 -1.8714567] \t0\ttrue\n",
            "(0)\t 180\t [ 1.912352  -1.9851149] \t0\ttrue\n",
            "(0)\t 181\t [ 1.8304113 -1.851085 ] \t0\ttrue\n",
            "(0)\t 182\t [ 1.6770974 -1.614317 ] \t0\ttrue\n",
            "(0)\t 183\t [ 1.740146  -1.6764808] \t0\ttrue\n",
            "(0)\t 184\t [ 1.2209159 -0.8769119] \t0\ttrue\n",
            "(0)\t 185\t [ 1.7813905 -1.8332397] \t0\ttrue\n",
            "(0)\t 186\t [ 1.7972618 -1.8123946] \t0\ttrue\n",
            "(1)\t 187\t [ 1.6960382 -1.6051966] \t0\tfalse\n",
            "(1)\t 188\t [-1.2619087  1.8198042] \t1\ttrue\n",
            "(0)\t 189\t [ 1.1448056  -0.88881356] \t0\ttrue\n",
            "(1)\t 190\t [ 1.5105243 -1.4299185] \t0\tfalse\n",
            "(0)\t 191\t [ 1.8248771 -1.9884095] \t0\ttrue\n",
            "(0)\t 192\t [ 1.8477803 -1.8938938] \t0\ttrue\n",
            "(1)\t 193\t [ 1.3432733 -1.1706415] \t0\tfalse\n",
            "(1)\t 194\t [ 1.5665017 -1.5825242] \t0\tfalse\n",
            "(0)\t 195\t [ 1.4247209 -1.1745473] \t0\ttrue\n",
            "(1)\t 196\t [0.02293102 0.5211655 ] \t1\ttrue\n",
            "(1)\t 197\t [ 1.7911454 -1.880423 ] \t0\tfalse\n",
            "(1)\t 198\t [ 1.5453924 -1.4761702] \t0\tfalse\n",
            "(1)\t 199\t [ 1.3930626 -1.1381946] \t0\tfalse\n",
            "(1)\t 200\t [ 1.3930626 -1.1381946] \t0\tfalse\n",
            "(0)\t 201\t [ 1.7053567 -1.8279735] \t0\ttrue\n",
            "(0)\t 202\t [ 1.5481094 -1.6030098] \t0\ttrue\n",
            "(1)\t 203\t [ 1.8342842 -1.9862117] \t0\tfalse\n",
            "(1)\t 204\t [ 1.8342842 -1.9862117] \t0\tfalse\n",
            "(1)\t 205\t [ 1.7033404 -1.6834345] \t0\tfalse\n",
            "(1)\t 206\t [ 1.8069324 -1.9670705] \t0\tfalse\n",
            "(0)\t 207\t [-0.07132044  0.3725464 ] \t1\tfalse\n",
            "(0)\t 208\t [ 1.6311249 -1.6272875] \t0\ttrue\n",
            "(1)\t 209\t [ 1.7691659 -1.7940542] \t0\tfalse\n",
            "(0)\t 210\t [ 1.5860103 -1.5343921] \t0\ttrue\n",
            "(1)\t 211\t [ 1.2870079 -1.2210314] \t0\tfalse\n",
            "(0)\t 212\t [ 1.3881524 -1.1520457] \t0\ttrue\n",
            "(1)\t 213\t [ 1.5567894 -1.5099454] \t0\tfalse\n",
            "(0)\t 214\t [ 1.8660108 -1.9661902] \t0\ttrue\n",
            "(1)\t 215\t [ 1.8487765 -2.0017955] \t0\tfalse\n",
            "(1)\t 216\t [ 1.7756728 -1.9202291] \t0\tfalse\n",
            "(0)\t 217\t [-0.18351075  0.73333454] \t1\tfalse\n",
            "(1)\t 218\t [ 1.732864  -1.8017453] \t0\tfalse\n",
            "(0)\t 219\t [ 1.3515146 -1.2050079] \t0\ttrue\n",
            "(1)\t 220\t [-0.38954005  0.95525306] \t1\ttrue\n",
            "(1)\t 221\t [ 1.8069091 -1.8921045] \t0\tfalse\n",
            "(1)\t 222\t [ 1.8069091 -1.8921045] \t0\tfalse\n",
            "(1)\t 223\t [0.02834175 0.4625385 ] \t1\ttrue\n",
            "(0)\t 224\t [0.23395446 0.19938281] \t0\ttrue\n",
            "(0)\t 225\t [ 1.3683323 -1.206448 ] \t0\ttrue\n",
            "(0)\t 226\t [-0.65484554  1.24613   ] \t1\tfalse\n",
            "(1)\t 227\t [ 1.8443274 -1.9766783] \t0\tfalse\n",
            "(1)\t 228\t [ 1.8443274 -1.9766783] \t0\tfalse\n",
            "(1)\t 229\t [ 1.6353759 -1.441826 ] \t0\tfalse\n",
            "(0)\t 230\t [ 1.7711401 -1.8095917] \t0\ttrue\n",
            "(1)\t 231\t [ 1.4178643 -1.382652 ] \t0\tfalse\n",
            "(1)\t 232\t [ 1.4178643 -1.382652 ] \t0\tfalse\n",
            "(0)\t 233\t [ 1.8046676 -2.0375638] \t0\ttrue\n",
            "(1)\t 234\t [ 1.7828931 -1.8238363] \t0\tfalse\n",
            "(0)\t 235\t [ 1.7718757 -1.8595978] \t0\ttrue\n",
            "(1)\t 236\t [ 1.9044739 -2.0375423] \t0\tfalse\n",
            "(1)\t 237\t [-1.2131938  1.7584902] \t1\ttrue\n",
            "(0)\t 238\t [ 1.8361621 -1.9687754] \t0\ttrue\n",
            "(1)\t 239\t [ 1.6544394 -1.6217004] \t0\tfalse\n",
            "(0)\t 240\t [ 1.7249862 -1.8015338] \t0\ttrue\n",
            "(0)\t 241\t [ 1.5191517 -1.4775065] \t0\ttrue\n",
            "(1)\t 242\t [ 1.6919312 -1.6075857] \t0\tfalse\n",
            "(1)\t 243\t [ 1.4108692 -1.260708 ] \t0\tfalse\n",
            "(0)\t 244\t [ 1.7564533 -1.7649117] \t0\ttrue\n",
            "(1)\t 245\t [ 1.7354249 -1.774193 ] \t0\tfalse\n",
            "(1)\t 246\t [ 1.8408833 -2.010552 ] \t0\tfalse\n",
            "(1)\t 247\t [ 1.8408833 -2.010552 ] \t0\tfalse\n",
            "(0)\t 248\t [ 1.8232942 -1.9176193] \t0\ttrue\n",
            "(1)\t 249\t [ 1.6376973 -1.482788 ] \t0\tfalse\n",
            "(1)\t 250\t [ 1.6376973 -1.482788 ] \t0\tfalse\n",
            "(0)\t 251\t [ 1.5543593 -1.5838001] \t0\ttrue\n",
            "(1)\t 252\t [ 1.3014637 -1.184704 ] \t0\tfalse\n",
            "(1)\t 253\t [ 1.7964379 -1.9191115] \t0\tfalse\n",
            "(1)\t 254\t [ 1.0582931  -0.73552936] \t0\tfalse\n",
            "(0)\t 255\t [ 0.932366 -0.566655] \t0\ttrue\n",
            "(0)\t 256\t [-0.27576286  0.72309405] \t1\tfalse\n",
            "(1)\t 257\t [ 0.9151737 -0.5368572] \t0\tfalse\n",
            "(0)\t 258\t [-0.6187735  1.0460684] \t1\tfalse\n",
            "(0)\t 259\t [ 1.8229316 -1.9006065] \t0\ttrue\n",
            "(0)\t 260\t [ 1.8541421 -2.0433078] \t0\ttrue\n",
            "(0)\t 261\t [ 1.865954  -2.0512135] \t0\ttrue\n",
            "(0)\t 262\t [ 1.8753759 -1.9660813] \t0\ttrue\n",
            "(0)\t 263\t [ 1.9106573 -2.0313308] \t0\ttrue\n",
            "(0)\t 264\t [ 1.4994595 -1.4012951] \t0\ttrue\n",
            "(0)\t 265\t [ 1.8581128 -2.009904 ] \t0\ttrue\n",
            "(1)\t 266\t [ 1.8147498 -2.0229282] \t0\tfalse\n",
            "(0)\t 267\t [ 1.8801893 -2.0323648] \t0\ttrue\n",
            "(0)\t 268\t [-1.0046648  1.5321575] \t1\tfalse\n",
            "(0)\t 269\t [ 1.1936296  -0.97380847] \t0\ttrue\n",
            "(0)\t 270\t [ 1.0528222 -0.703621 ] \t0\ttrue\n",
            "(1)\t 271\t [0.32536927 0.12990731] \t0\tfalse\n",
            "(1)\t 272\t [ 1.4482964 -1.3098348] \t0\tfalse\n",
            "(1)\t 273\t [ 1.4763929 -1.3978292] \t0\tfalse\n",
            "(1)\t 274\t [ 0.93177706 -0.72633785] \t0\tfalse\n",
            "(1)\t 275\t [ 1.5419805 -1.4373599] \t0\tfalse\n",
            "(1)\t 276\t [-1.0308263  1.589876 ] \t1\ttrue\n",
            "(1)\t 277\t [ 1.8551625 -1.9305278] \t0\tfalse\n",
            "(0)\t 278\t [ 1.7741979 -1.8604766] \t0\ttrue\n",
            "(0)\t 279\t [ 0.97258395 -0.52347594] \t0\ttrue\n",
            "(0)\t 280\t [ 1.7785155 -1.9117566] \t0\ttrue\n",
            "(1)\t 281\t [-0.3625703  0.6326156] \t1\ttrue\n",
            "(0)\t 282\t [ 0.70325774 -0.23884243] \t0\ttrue\n",
            "(1)\t 283\t [-0.44467342  0.91769284] \t1\ttrue\n",
            "(1)\t 284\t [ 1.3062836 -1.1484109] \t0\tfalse\n",
            "(1)\t 285\t [ 1.8580829 -1.8613328] \t0\tfalse\n",
            "(1)\t 286\t [ 1.8580829 -1.8613328] \t0\tfalse\n",
            "(1)\t 287\t [ 1.7468096 -1.711412 ] \t0\tfalse\n",
            "(0)\t 288\t [ 1.7762213 -1.9052187] \t0\ttrue\n",
            "(0)\t 289\t [ 1.7756588 -1.8013911] \t0\ttrue\n",
            "(1)\t 290\t [0.08421752 0.3538054 ] \t1\ttrue\n",
            "(1)\t 291\t [0.31412333 0.09231777] \t0\tfalse\n",
            "(1)\t 292\t [ 1.2623097 -0.9800418] \t0\tfalse\n",
            "(1)\t 293\t [ 1.7862202 -1.820407 ] \t0\tfalse\n",
            "(0)\t 294\t [ 1.7862074 -1.9006428] \t0\ttrue\n",
            "(0)\t 295\t [ 1.5871246 -1.5489565] \t0\ttrue\n",
            "(0)\t 296\t [ 1.677075  -1.7401571] \t0\ttrue\n",
            "(0)\t 297\t [-0.8245303  1.4041094] \t1\tfalse\n",
            "(0)\t 298\t [ 1.7844074 -1.5591205] \t0\ttrue\n",
            "(1)\t 299\t [ 1.273686  -1.0178612] \t0\tfalse\n",
            "(0)\t 300\t [ 1.7405728 -1.7166601] \t0\ttrue\n",
            "(1)\t 301\t [ 1.589341  -1.4409951] \t0\tfalse\n",
            "(1)\t 302\t [ 1.8684701 -2.0669332] \t0\tfalse\n",
            "(1)\t 303\t [ 1.8684701 -2.0669332] \t0\tfalse\n",
            "(1)\t 304\t [-0.79584986  1.2862014 ] \t1\ttrue\n",
            "(1)\t 305\t [ 0.75808567 -0.40240705] \t0\tfalse\n",
            "(1)\t 306\t [0.3772777  0.07717704] \t0\tfalse\n",
            "(0)\t 307\t [ 1.7811054 -1.7936467] \t0\ttrue\n",
            "(1)\t 308\t [ 1.8722736 -2.016615 ] \t0\tfalse\n",
            "(0)\t 309\t [ 1.8688719 -2.0058317] \t0\ttrue\n",
            "(0)\t 310\t [ 1.1965737 -1.0044316] \t0\ttrue\n",
            "(0)\t 311\t [ 0.48694953 -0.0489202 ] \t0\ttrue\n",
            "(0)\t 312\t [ 0.78534085 -0.39366645] \t0\ttrue\n",
            "(0)\t 313\t [ 1.6934856 -1.6944895] \t0\ttrue\n",
            "(0)\t 314\t [ 1.726049  -1.7374684] \t0\ttrue\n",
            "(0)\t 315\t [ 1.5199686 -1.3955597] \t0\ttrue\n",
            "(0)\t 316\t [ 1.4887013 -1.3166398] \t0\ttrue\n",
            "(0)\t 317\t [ 1.8464748 -1.9134115] \t0\ttrue\n",
            "(1)\t 318\t [-1.0907296  1.7539774] \t1\ttrue\n",
            "(0)\t 319\t [ 1.4935626 -1.3648616] \t0\ttrue\n",
            "(1)\t 320\t [ 1.871348  -1.9683384] \t0\tfalse\n",
            "(1)\t 321\t [ 1.9045228 -1.9900119] \t0\tfalse\n",
            "(0)\t 322\t [ 0.7819137  -0.45248362] \t0\ttrue\n",
            "(0)\t 323\t [ 1.8684837 -2.091464 ] \t0\ttrue\n",
            "(0)\t 324\t [ 1.7400237 -1.8551339] \t0\ttrue\n",
            "(0)\t 325\t [ 1.8352094 -1.7619511] \t0\ttrue\n",
            "(1)\t 326\t [ 1.7757798 -1.9150556] \t0\tfalse\n",
            "(0)\t 327\t [ 1.4428859 -1.274124 ] \t0\ttrue\n",
            "(0)\t 328\t [ 1.800226  -1.7211654] \t0\ttrue\n",
            "(0)\t 329\t [-1.0908272  1.4727308] \t1\tfalse\n",
            "(1)\t 330\t [-0.92422485  1.5488529 ] \t1\ttrue\n",
            "(0)\t 331\t [0.2559503 0.2633129] \t1\tfalse\n",
            "(1)\t 332\t [-0.7131923  1.2180701] \t1\ttrue\n",
            "(1)\t 333\t [0.06968241 0.33320034] \t1\ttrue\n",
            "(1)\t 334\t [ 1.7799488 -1.8847493] \t0\tfalse\n",
            "(0)\t 335\t [-0.9281655  1.483737 ] \t1\tfalse\n",
            "(0)\t 336\t [ 1.783173  -1.8389723] \t0\ttrue\n",
            "(0)\t 337\t [ 1.8183546 -1.8905926] \t0\ttrue\n",
            "(1)\t 338\t [ 0.86950505 -0.5194816 ] \t0\tfalse\n",
            "(1)\t 339\t [-0.15700266  0.6242129 ] \t1\ttrue\n",
            "(1)\t 340\t [-1.2583991  1.7678913] \t1\ttrue\n",
            "(0)\t 341\t [0.48131928 0.00656237] \t0\ttrue\n",
            "(1)\t 342\t [ 1.6169727 -1.6892599] \t0\tfalse\n",
            "(1)\t 343\t [ 1.5936315 -1.5507551] \t0\tfalse\n",
            "(0)\t 344\t [-1.1038846  1.6824967] \t1\tfalse\n",
            "(1)\t 345\t [0.1768551  0.26733854] \t1\ttrue\n",
            "(1)\t 346\t [ 1.8409635 -2.0536337] \t0\tfalse\n",
            "(0)\t 347\t [ 1.8443509 -1.8553227] \t0\ttrue\n",
            "(0)\t 348\t [-0.11749581  0.6583609 ] \t1\tfalse\n",
            "(0)\t 349\t [ 1.3787864 -1.2159762] \t0\ttrue\n",
            "(0)\t 350\t [ 1.5346798 -1.486732 ] \t0\ttrue\n",
            "(0)\t 351\t [-0.01756139  0.3858783 ] \t1\tfalse\n",
            "(0)\t 352\t [ 1.7541348 -1.8110167] \t0\ttrue\n",
            "(1)\t 353\t [ 1.7302293 -1.7733132] \t0\tfalse\n",
            "(0)\t 354\t [ 1.6505808 -1.4931444] \t0\ttrue\n",
            "(0)\t 355\t [ 1.7240711 -1.7445118] \t0\ttrue\n",
            "(0)\t 356\t [ 1.5928345 -1.5849241] \t0\ttrue\n",
            "(0)\t 357\t [ 1.709737  -1.7755284] \t0\ttrue\n",
            "(0)\t 358\t [ 1.733127 -1.844607] \t0\ttrue\n",
            "(0)\t 359\t [ 1.8534204 -2.032706 ] \t0\ttrue\n",
            "(0)\t 360\t [ 1.7140008 -1.8200635] \t0\ttrue\n",
            "(0)\t 361\t [ 1.8472847 -1.917776 ] \t0\ttrue\n",
            "(0)\t 362\t [ 1.8193246 -1.9303826] \t0\ttrue\n",
            "(0)\t 363\t [ 1.7066659 -1.7709645] \t0\ttrue\n",
            "(0)\t 364\t [ 1.7789096 -1.8433824] \t0\ttrue\n",
            "(0)\t 365\t [ 1.3412929 -1.255853 ] \t0\ttrue\n",
            "(0)\t 366\t [ 1.7953231 -1.8851323] \t0\ttrue\n",
            "(1)\t 367\t [-0.24297357  0.63370806] \t1\ttrue\n",
            "(0)\t 368\t [ 0.62684816 -0.29323623] \t0\ttrue\n",
            "(0)\t 369\t [ 1.8518327 -1.9958794] \t0\ttrue\n",
            "(0)\t 370\t [-0.2885616  0.6912728] \t1\tfalse\n",
            "(0)\t 371\t [ 1.6757153 -1.6814357] \t0\ttrue\n",
            "(0)\t 372\t [-1.3442043  1.9185036] \t1\tfalse\n",
            "(0)\t 373\t [ 1.5653905 -1.4404238] \t0\ttrue\n",
            "(0)\t 374\t [-1.189521   1.7739512] \t1\tfalse\n",
            "(0)\t 375\t [ 1.8647596 -1.9417056] \t0\ttrue\n",
            "(0)\t 376\t [ 1.8603333 -2.0095463] \t0\ttrue\n",
            "(0)\t 377\t [ 1.82102   -1.9235545] \t0\ttrue\n",
            "(0)\t 378\t [-0.18253303  0.68567604] \t1\tfalse\n",
            "(0)\t 379\t [ 0.6486966 -0.2751269] \t0\ttrue\n",
            "(1)\t 380\t [ 1.7954937 -1.8581604] \t0\tfalse\n",
            "(0)\t 381\t [-1.2661549  1.8184428] \t1\tfalse\n",
            "(0)\t 382\t [ 1.7574977 -1.9343175] \t0\ttrue\n",
            "(0)\t 383\t [ 1.7818626 -1.816392 ] \t0\ttrue\n",
            "(1)\t 384\t [ 1.7613596 -1.8665007] \t0\tfalse\n",
            "(0)\t 385\t [ 1.7571572 -1.7199438] \t0\ttrue\n",
            "(0)\t 386\t [ 1.598549  -1.6045965] \t0\ttrue\n",
            "(0)\t 387\t [ 0.5429525  -0.22258039] \t0\ttrue\n",
            "(1)\t 388\t [-0.1279358  0.5666529] \t1\ttrue\n",
            "(0)\t 389\t [ 1.9009362 -1.995855 ] \t0\ttrue\n",
            "(0)\t 390\t [ 1.8092648 -1.9504079] \t0\ttrue\n",
            "(0)\t 391\t [ 1.7864438 -1.8626775] \t0\ttrue\n",
            "(0)\t 392\t [ 1.769221  -1.8047359] \t0\ttrue\n",
            "(0)\t 393\t [-1.2537341  1.8003576] \t1\tfalse\n",
            "(0)\t 394\t [ 1.2066282 -0.9127695] \t0\ttrue\n",
            "(1)\t 395\t [ 1.728005 -1.798817] \t0\tfalse\n",
            "(0)\t 396\t [ 0.91008466 -0.58586186] \t0\ttrue\n",
            "(0)\t 397\t [ 0.91964746 -0.647217  ] \t0\ttrue\n",
            "(0)\t 398\t [-0.98863286  1.5961132 ] \t1\tfalse\n",
            "(0)\t 399\t [ 1.7042688 -1.7792912] \t0\ttrue\n",
            "(0)\t 400\t [ 1.8089515 -1.9807386] \t0\ttrue\n",
            "(0)\t 401\t [ 1.6280242 -1.607339 ] \t0\ttrue\n",
            "(0)\t 402\t [ 1.5582718 -1.5508856] \t0\ttrue\n",
            "(0)\t 403\t [-0.35037285  0.8405249 ] \t1\tfalse\n",
            "(0)\t 404\t [-1.0806632  1.6259447] \t1\tfalse\n",
            "(0)\t 405\t [ 1.1840008  -0.88384986] \t0\ttrue\n",
            "(0)\t 406\t [ 1.8063682 -1.962828 ] \t0\ttrue\n",
            "(0)\t 407\t [0.11757484 0.30201828] \t1\tfalse\n",
            "(1)\t 408\t [ 0.8554813  -0.45657226] \t0\tfalse\n",
            "(0)\t 409\t [ 1.8247555 -1.9749714] \t0\ttrue\n",
            "(0)\t 410\t [ 1.6360179 -1.5892804] \t0\ttrue\n",
            "(0)\t 411\t [-0.02316157  0.4193491 ] \t1\tfalse\n",
            "(0)\t 412\t [ 1.735849  -1.7732371] \t0\ttrue\n",
            "(0)\t 413\t [0.34297836 0.08813669] \t0\ttrue\n",
            "(0)\t 414\t [-1.3071396  1.8640969] \t1\tfalse\n",
            "(0)\t 415\t [ 1.3483582 -1.1591924] \t0\ttrue\n",
            "(1)\t 416\t [ 1.6986346 -1.7034672] \t0\tfalse\n",
            "(0)\t 417\t [ 1.8502284 -2.0284662] \t0\ttrue\n",
            "(1)\t 418\t [ 1.1176645  -0.81061476] \t0\tfalse\n",
            "(0)\t 419\t [ 1.8660457 -1.8695115] \t0\ttrue\n",
            "(1)\t 420\t [ 1.3624252 -1.3129827] \t0\tfalse\n",
            "(0)\t 421\t [ 1.4481462 -1.4595158] \t0\ttrue\n",
            "(0)\t 422\t [ 1.5477211 -1.4266286] \t0\ttrue\n",
            "(1)\t 423\t [ 1.8782774 -2.009382 ] \t0\tfalse\n",
            "(0)\t 424\t [ 1.8327112 -2.0088747] \t0\ttrue\n",
            "(0)\t 425\t [ 1.5417253 -1.4348053] \t0\ttrue\n",
            "(1)\t 426\t [-0.4700807  1.0104765] \t1\ttrue\n",
            "(0)\t 427\t [ 1.8136913 -1.9173228] \t0\ttrue\n",
            "(0)\t 428\t [ 1.754685  -1.8524145] \t0\ttrue\n",
            "(0)\t 429\t [ 1.4722508 -1.3794593] \t0\ttrue\n",
            "(0)\t 430\t [ 1.8721497 -2.0032005] \t0\ttrue\n",
            "(1)\t 431\t [0.4024316  0.09369484] \t0\tfalse\n",
            "(1)\t 432\t [ 1.5367211 -1.4159311] \t0\tfalse\n",
            "(1)\t 433\t [ 1.5367211 -1.4159311] \t0\tfalse\n",
            "(0)\t 434\t [ 0.8937829 -0.6881445] \t0\ttrue\n",
            "(1)\t 435\t [ 1.8829066 -1.9550322] \t0\tfalse\n",
            "(1)\t 436\t [ 1.8829066 -1.9550322] \t0\tfalse\n",
            "(0)\t 437\t [ 1.824426  -2.0813975] \t0\ttrue\n",
            "(0)\t 438\t [-0.13778916  0.64810467] \t1\tfalse\n",
            "(0)\t 439\t [0.29831025 0.27192605] \t0\ttrue\n",
            "(0)\t 440\t [ 1.782461  -1.9006146] \t0\ttrue\n",
            "(0)\t 441\t [ 1.6641771 -1.6184136] \t0\ttrue\n",
            "(1)\t 442\t [ 1.6836845 -1.6720439] \t0\tfalse\n",
            "(1)\t 443\t [ 1.8099372 -1.7645977] \t0\tfalse\n",
            "(0)\t 444\t [ 1.8576596 -1.9779252] \t0\ttrue\n",
            "(0)\t 445\t [ 1.2219359 -0.9649382] \t0\ttrue\n",
            "(1)\t 446\t [ 1.8489472 -1.9434805] \t0\tfalse\n",
            "(0)\t 447\t [ 1.892414  -2.0296578] \t0\ttrue\n",
            "(0)\t 448\t [ 1.6822673 -1.8351398] \t0\ttrue\n",
            "(0)\t 449\t [ 1.7220125 -1.7566842] \t0\ttrue\n",
            "(1)\t 450\t [ 1.4974989 -1.3571525] \t0\tfalse\n",
            "(1)\t 451\t [ 1.4974989 -1.3571525] \t0\tfalse\n",
            "(1)\t 452\t [ 1.8428682 -1.9720669] \t0\tfalse\n",
            "(0)\t 453\t [ 1.5629781 -1.6345769] \t0\ttrue\n",
            "(0)\t 454\t [ 1.797673  -1.9033982] \t0\ttrue\n",
            "(0)\t 455\t [ 1.8028351 -1.8855594] \t0\ttrue\n",
            "(1)\t 456\t [ 1.5864545 -1.5296205] \t0\tfalse\n",
            "(1)\t 457\t [ 0.9955018 -0.5466728] \t0\tfalse\n",
            "(1)\t 458\t [ 1.579802  -1.4399375] \t0\tfalse\n",
            "(0)\t 459\t [ 1.0009739 -0.6717867] \t0\ttrue\n",
            "(1)\t 460\t [ 1.8559213 -2.0199666] \t0\tfalse\n",
            "(1)\t 461\t [ 1.8559213 -2.0199666] \t0\tfalse\n",
            "(0)\t 462\t [ 1.8277751 -1.9486061] \t0\ttrue\n",
            "(1)\t 463\t [ 1.7460824 -1.8478204] \t0\tfalse\n",
            "(1)\t 464\t [ 1.8066016 -1.8712745] \t0\tfalse\n",
            "(1)\t 465\t [ 1.7909337 -1.8677093] \t0\tfalse\n",
            "(1)\t 466\t [-1.0009092  1.6101111] \t1\ttrue\n",
            "(0)\t 467\t [ 1.9010911 -1.9477706] \t0\ttrue\n",
            "(0)\t 468\t [ 1.8058631 -1.9396913] \t0\ttrue\n",
            "(1)\t 469\t [ 1.8077699 -1.9280721] \t0\tfalse\n",
            "(1)\t 470\t [ 1.7974383 -1.8606557] \t0\tfalse\n",
            "(1)\t 471\t [ 1.8557905 -2.0139065] \t0\tfalse\n",
            "(1)\t 472\t [ 1.8557905 -2.0139065] \t0\tfalse\n",
            "(0)\t 473\t [ 1.2395729 -0.9673329] \t0\ttrue\n",
            "(1)\t 474\t [ 1.1772872 -0.9326877] \t0\tfalse\n",
            "(0)\t 475\t [ 1.78686   -1.8435136] \t0\ttrue\n",
            "(0)\t 476\t [ 1.7994026 -1.8601779] \t0\ttrue\n",
            "(0)\t 477\t [ 1.86375   -1.9896737] \t0\ttrue\n",
            "(0)\t 478\t [ 1.7451223 -1.8902904] \t0\ttrue\n",
            "(1)\t 479\t [ 1.4001373 -1.2179159] \t0\tfalse\n",
            "(1)\t 480\t [ 1.3104929 -1.2237546] \t0\tfalse\n",
            "(1)\t 481\t [ 1.6594979 -1.6776811] \t0\tfalse\n",
            "(1)\t 482\t [ 1.8958926 -1.9799235] \t0\tfalse\n",
            "(0)\t 483\t [ 1.8269254 -2.0137043] \t0\ttrue\n",
            "(1)\t 484\t [ 0.8882056 -0.5724267] \t0\tfalse\n",
            "(0)\t 485\t [ 1.7790555 -1.9149487] \t0\ttrue\n",
            "(1)\t 486\t [ 1.9144095 -1.9998037] \t0\tfalse\n",
            "(1)\t 487\t [ 1.530726  -1.5637764] \t0\tfalse\n",
            "(0)\t 488\t [ 1.7769735 -1.8151056] \t0\ttrue\n",
            "(0)\t 489\t [ 1.6231787 -1.5542592] \t0\ttrue\n",
            "(0)\t 490\t [ 1.8029698 -1.8968991] \t0\ttrue\n",
            "(0)\t 491\t [ 1.5809878 -1.4612325] \t0\ttrue\n",
            "(1)\t 492\t [ 1.813967  -1.9333948] \t0\tfalse\n",
            "(1)\t 493\t [ 1.813967  -1.9333948] \t0\tfalse\n",
            "(0)\t 494\t [ 0.8946096 -0.4977093] \t0\ttrue\n",
            "(0)\t 495\t [ 1.7939199 -1.9573132] \t0\ttrue\n",
            "(1)\t 496\t [ 1.6279106 -1.5553776] \t0\tfalse\n",
            "(1)\t 497\t [-0.75644696  1.3131422 ] \t1\ttrue\n",
            "(0)\t 498\t [ 1.5583061 -1.4372088] \t0\ttrue\n",
            "(0)\t 499\t [ 1.8464066 -1.9865525] \t0\ttrue\n",
            "(0)\t 500\t [ 1.7038907 -1.6755639] \t0\ttrue\n",
            "(1)\t 501\t [ 1.8910052 -2.0381007] \t0\tfalse\n",
            "(1)\t 502\t [ 1.8910052 -2.0381007] \t0\tfalse\n",
            "(0)\t 503\t [ 1.8197516 -1.893844 ] \t0\ttrue\n",
            "(0)\t 504\t [ 1.7560569 -1.9244245] \t0\ttrue\n",
            "(1)\t 505\t [ 0.9981354 -0.8442644] \t0\tfalse\n",
            "(1)\t 506\t [ 1.6012807 -1.4612666] \t0\tfalse\n",
            "(1)\t 507\t [ 0.7811324  -0.41220644] \t0\tfalse\n",
            "(0)\t 508\t [-1.0206019  1.6343824] \t1\tfalse\n",
            "(0)\t 509\t [-1.1717968  1.7305189] \t1\tfalse\n",
            "(0)\t 510\t [ 0.6569207  -0.24565066] \t0\ttrue\n",
            "(1)\t 511\t [-1.2855458  1.822914 ] \t1\ttrue\n",
            "(1)\t 512\t [ 1.7365814 -1.6258997] \t0\tfalse\n",
            "(0)\t 513\t [ 1.3258561 -1.2356364] \t0\ttrue\n",
            "(0)\t 514\t [0.02098934 0.48470485] \t1\tfalse\n",
            "(1)\t 515\t [-1.3009025  1.8730432] \t1\ttrue\n",
            "(0)\t 516\t [ 1.8465183 -1.9727572] \t0\ttrue\n",
            "(1)\t 517\t [0.3103034  0.10597917] \t0\tfalse\n",
            "(0)\t 518\t [ 1.8440167 -1.9736305] \t0\ttrue\n",
            "(0)\t 519\t [ 1.5718887 -1.4729239] \t0\ttrue\n",
            "(0)\t 520\t [ 1.8614658 -1.9638691] \t0\ttrue\n",
            "(0)\t 521\t [ 0.816072  -0.4006199] \t0\ttrue\n",
            "(1)\t 522\t [ 1.821871  -1.8683976] \t0\tfalse\n",
            "(1)\t 523\t [ 1.7925657 -1.8876706] \t0\tfalse\n",
            "(1)\t 524\t [ 1.1630718  -0.92791563] \t0\tfalse\n",
            "(1)\t 525\t [ 1.8275787 -1.9517006] \t0\tfalse\n",
            "(1)\t 526\t [ 1.6521748 -1.6261501] \t0\tfalse\n",
            "(1)\t 527\t [ 1.1984001  -0.99315673] \t0\tfalse\n",
            "(0)\t 528\t [ 1.7470783 -1.8140262] \t0\ttrue\n",
            "(1)\t 529\t [ 1.7304618 -1.8126475] \t0\tfalse\n",
            "(1)\t 530\t [ 1.7304618 -1.8126475] \t0\tfalse\n",
            "(1)\t 531\t [ 1.3059465 -1.1791307] \t0\tfalse\n",
            "(1)\t 532\t [-1.0304716  1.4846286] \t1\ttrue\n",
            "(1)\t 533\t [ 1.8361158 -1.9634869] \t0\tfalse\n",
            "(1)\t 534\t [ 1.8361158 -1.9634869] \t0\tfalse\n",
            "(1)\t 535\t [ 1.4553704 -1.4796964] \t0\tfalse\n",
            "(1)\t 536\t [ 1.4553704 -1.4796964] \t0\tfalse\n",
            "(1)\t 537\t [ 1.8706585 -2.0805566] \t0\tfalse\n",
            "(1)\t 538\t [ 1.0079122 -0.8063637] \t0\tfalse\n",
            "(1)\t 539\t [ 1.0079122 -0.8063637] \t0\tfalse\n",
            "(1)\t 540\t [0.09370334 0.3039042 ] \t1\ttrue\n",
            "(1)\t 541\t [0.09370334 0.3039042 ] \t1\ttrue\n",
            "(0)\t 542\t [ 0.9189494  -0.55049795] \t0\ttrue\n",
            "(1)\t 543\t [ 1.8045682 -1.9323919] \t0\tfalse\n",
            "(1)\t 544\t [ 1.8045682 -1.9323919] \t0\tfalse\n",
            "(1)\t 545\t [ 1.841746  -1.9506526] \t0\tfalse\n",
            "(1)\t 546\t [ 1.853769  -1.9808766] \t0\tfalse\n",
            "(0)\t 547\t [ 0.5446338  -0.19112656] \t0\ttrue\n",
            "(1)\t 548\t [ 1.8266064 -1.991537 ] \t0\tfalse\n",
            "(1)\t 549\t [ 1.8266064 -1.991537 ] \t0\tfalse\n",
            "(0)\t 550\t [ 1.7106274 -1.90351  ] \t0\ttrue\n",
            "(0)\t 551\t [ 0.6589977  -0.26523486] \t0\ttrue\n",
            "(0)\t 552\t [ 1.1407465  -0.81014436] \t0\ttrue\n",
            "(1)\t 553\t [-0.357747    0.84738225] \t1\ttrue\n",
            "(0)\t 554\t [ 1.8587306 -1.9806548] \t0\ttrue\n",
            "(0)\t 555\t [ 1.2592214 -1.020145 ] \t0\ttrue\n",
            "(1)\t 556\t [ 1.3874457 -1.3520056] \t0\tfalse\n",
            "(1)\t 557\t [-0.88797957  1.4306501 ] \t1\ttrue\n",
            "(1)\t 558\t [ 1.1563039  -0.97533566] \t0\tfalse\n",
            "(0)\t 559\t [0.02868987 0.4542155 ] \t1\tfalse\n",
            "(0)\t 560\t [-1.164235  1.724758] \t1\tfalse\n",
            "(0)\t 561\t [-0.8096625  1.3107928] \t1\tfalse\n",
            "(1)\t 562\t [ 1.3771518 -1.341657 ] \t0\tfalse\n",
            "(1)\t 563\t [ 1.3771518 -1.341657 ] \t0\tfalse\n",
            "(1)\t 564\t [ 1.8771435 -1.9844147] \t0\tfalse\n",
            "(0)\t 565\t [ 1.1823195  -0.84404224] \t0\ttrue\n",
            "(1)\t 566\t [ 1.1163253 -0.7586433] \t0\tfalse\n",
            "(1)\t 567\t [ 1.1163253 -0.7586433] \t0\tfalse\n",
            "(0)\t 568\t [ 1.2661554 -1.0358304] \t0\ttrue\n",
            "(0)\t 569\t [-0.03324046  0.5414557 ] \t1\tfalse\n",
            "(0)\t 570\t [ 1.821556  -1.9567094] \t0\ttrue\n",
            "(0)\t 571\t [ 1.83293   -1.9196843] \t0\ttrue\n",
            "(1)\t 572\t [-0.09430936  0.58252454] \t1\ttrue\n",
            "(1)\t 573\t [ 1.720751  -1.7018391] \t0\tfalse\n",
            "(0)\t 574\t [ 1.2598737 -0.9900183] \t0\ttrue\n",
            "(0)\t 575\t [ 1.7286388 -1.7000269] \t0\ttrue\n",
            "(1)\t 576\t [ 1.8518915 -2.0091288] \t0\tfalse\n",
            "(0)\t 577\t [ 0.6257168 -0.1686725] \t0\ttrue\n",
            "(0)\t 578\t [ 1.8352016 -2.005775 ] \t0\ttrue\n",
            "(0)\t 579\t [ 0.9685243 -0.7069548] \t0\ttrue\n",
            "(0)\t 580\t [ 1.8703445 -2.0174892] \t0\ttrue\n",
            "(1)\t 581\t [ 1.7196113 -1.7419034] \t0\tfalse\n",
            "(1)\t 582\t [ 1.7751697 -1.8347391] \t0\tfalse\n",
            "(1)\t 583\t [ 1.7751697 -1.8347391] \t0\tfalse\n",
            "(0)\t 584\t [ 1.7842361 -1.905794 ] \t0\ttrue\n",
            "(1)\t 585\t [ 1.7566495 -1.7737056] \t0\tfalse\n",
            "(0)\t 586\t [ 1.8481396 -2.076098 ] \t0\ttrue\n",
            "(1)\t 587\t [ 1.744753  -1.7772886] \t0\tfalse\n",
            "(0)\t 588\t [ 1.6776053 -1.6543833] \t0\ttrue\n",
            "(1)\t 589\t [ 1.685193  -1.6439768] \t0\tfalse\n",
            "(1)\t 590\t [ 1.1095272  -0.91012555] \t0\tfalse\n",
            "(1)\t 591\t [ 1.2182277 -0.9832775] \t0\tfalse\n",
            "(0)\t 592\t [ 1.8487463 -1.9901322] \t0\ttrue\n",
            "(1)\t 593\t [ 1.688999 -1.825261] \t0\tfalse\n",
            "(1)\t 594\t [ 1.688999 -1.825261] \t0\tfalse\n",
            "(0)\t 595\t [ 1.7602017 -1.8016475] \t0\ttrue\n",
            "(0)\t 596\t [ 1.7759172 -1.8355126] \t0\ttrue\n",
            "(1)\t 597\t [-1.1112646  1.6622555] \t1\ttrue\n",
            "(0)\t 598\t [ 0.48020908 -0.07375669] \t0\ttrue\n",
            "(0)\t 599\t [-0.7711872  1.2597181] \t1\tfalse\n",
            "(0)\t 600\t [-1.2268983  1.8277155] \t1\tfalse\n",
            "(0)\t 601\t [ 1.3372697 -1.1454346] \t0\ttrue\n",
            "(0)\t 602\t [ 1.3359227 -1.1815631] \t0\ttrue\n",
            "(1)\t 603\t [ 1.7300922 -1.6321481] \t0\tfalse\n",
            "(1)\t 604\t [ 1.8083287 -1.7802395] \t0\tfalse\n",
            "(1)\t 605\t [ 1.8083287 -1.7802395] \t0\tfalse\n",
            "(1)\t 606\t [ 1.8680779 -2.067031 ] \t0\tfalse\n",
            "(0)\t 607\t [ 1.601662  -1.5758096] \t0\ttrue\n",
            "(0)\t 608\t [ 1.841885  -1.9305893] \t0\ttrue\n",
            "(1)\t 609\t [ 0.9807354 -0.6523922] \t0\tfalse\n",
            "(1)\t 610\t [ 0.9807354 -0.6523922] \t0\tfalse\n",
            "(0)\t 611\t [ 1.4597018 -1.2642919] \t0\ttrue\n",
            "(1)\t 612\t [ 1.4685127 -1.3476111] \t0\tfalse\n",
            "(1)\t 613\t [ 1.7604207 -1.7665346] \t0\tfalse\n",
            "(0)\t 614\t [ 1.822827  -1.8392481] \t0\ttrue\n",
            "(0)\t 615\t [0.0520665  0.38621995] \t1\tfalse\n",
            "(0)\t 616\t [ 1.7857995 -1.8326863] \t0\ttrue\n",
            "(0)\t 617\t [0.3110086  0.15108413] \t0\ttrue\n",
            "(0)\t 618\t [ 1.781751  -1.7801783] \t0\ttrue\n",
            "(1)\t 619\t [ 1.8395779 -1.9646705] \t0\tfalse\n",
            "(0)\t 620\t [ 1.8148485 -1.9116414] \t0\ttrue\n",
            "(1)\t 621\t [ 1.7210418 -1.6851679] \t0\tfalse\n",
            "(1)\t 622\t [ 1.7210418 -1.6851679] \t0\tfalse\n",
            "(0)\t 623\t [ 1.7429851 -1.7480279] \t0\ttrue\n",
            "(0)\t 624\t [ 1.6450363 -1.7350935] \t0\ttrue\n",
            "(0)\t 625\t [ 1.6580178 -1.6522065] \t0\ttrue\n",
            "(0)\t 626\t [ 1.6223164 -1.667008 ] \t0\ttrue\n",
            "(0)\t 627\t [ 1.8707987 -1.9974948] \t0\ttrue\n",
            "(1)\t 628\t [-1.1711794  1.5195645] \t1\ttrue\n",
            "(0)\t 629\t [ 1.7452291 -1.6938368] \t0\ttrue\n",
            "(1)\t 630\t [ 1.3349    -1.2294259] \t0\tfalse\n",
            "(0)\t 631\t [-0.89068073  1.3755363 ] \t1\tfalse\n",
            "(1)\t 632\t [ 1.8153485 -1.9162779] \t0\tfalse\n",
            "(0)\t 633\t [ 1.6762425 -1.7040907] \t0\ttrue\n",
            "(1)\t 634\t [ 1.6427952 -1.5337507] \t0\tfalse\n",
            "(1)\t 635\t [ 0.895077   -0.51765424] \t0\tfalse\n",
            "(1)\t 636\t [ 1.6631306 -1.6036812] \t0\tfalse\n",
            "(1)\t 637\t [ 1.8394531 -1.8129681] \t0\tfalse\n",
            "(1)\t 638\t [0.33768243 0.14786603] \t0\tfalse\n",
            "(0)\t 639\t [ 0.69706696 -0.31922093] \t0\ttrue\n",
            "(1)\t 640\t [-1.3016263  1.8504063] \t1\ttrue\n",
            "(1)\t 641\t [ 1.7655317 -1.8171002] \t0\tfalse\n",
            "(0)\t 642\t [ 1.8612014 -2.0184712] \t0\ttrue\n",
            "(1)\t 643\t [ 1.4434284 -1.2364405] \t0\tfalse\n",
            "(1)\t 644\t [ 1.221745  -0.9527882] \t0\tfalse\n",
            "(1)\t 645\t [-0.20087968  0.71354645] \t1\ttrue\n",
            "(0)\t 646\t [0.17979473 0.2698395 ] \t1\tfalse\n",
            "(1)\t 647\t [ 0.7919242  -0.44167137] \t0\tfalse\n",
            "(0)\t 648\t [ 1.6524051 -1.597859 ] \t0\ttrue\n",
            "(1)\t 649\t [ 0.60670096 -0.19905728] \t0\tfalse\n",
            "(0)\t 650\t [-1.3355074  1.8884181] \t1\tfalse\n",
            "(0)\t 651\t [ 1.7935885 -1.9615518] \t0\ttrue\n",
            "(1)\t 652\t [0.32390943 0.06070912] \t0\tfalse\n",
            "(1)\t 653\t [ 1.7461332 -1.8116955] \t0\tfalse\n",
            "(0)\t 654\t [-0.88417745  1.4944142 ] \t1\tfalse\n",
            "(1)\t 655\t [ 1.2640014 -0.9381415] \t0\tfalse\n",
            "(1)\t 656\t [ 1.4480454 -1.3196526] \t0\tfalse\n",
            "(0)\t 657\t [ 1.6790403 -1.7155696] \t0\ttrue\n",
            "(0)\t 658\t [ 1.7829173 -1.91865  ] \t0\ttrue\n",
            "(0)\t 659\t [ 1.5525157 -1.511237 ] \t0\ttrue\n",
            "(0)\t 660\t [ 1.8659846 -1.9771739] \t0\ttrue\n",
            "(0)\t 661\t [ 1.6572562 -1.5580852] \t0\ttrue\n",
            "(1)\t 662\t [-0.02098012  0.44753042] \t1\ttrue\n",
            "(1)\t 663\t [ 1.691442  -1.6225665] \t0\tfalse\n",
            "(1)\t 664\t [ 1.691442  -1.6225665] \t0\tfalse\n",
            "(0)\t 665\t [ 1.8091327 -1.9393004] \t0\ttrue\n",
            "(1)\t 666\t [ 1.3148679 -1.1614894] \t0\tfalse\n",
            "(0)\t 667\t [ 1.8411398 -1.9284198] \t0\ttrue\n",
            "(1)\t 668\t [ 1.6461834 -1.6423215] \t0\tfalse\n",
            "(0)\t 669\t [ 1.7252944 -1.7552379] \t0\ttrue\n",
            "(0)\t 670\t [ 1.6668569 -1.7353002] \t0\ttrue\n",
            "(0)\t 671\t [ 1.8782419 -1.9979568] \t0\ttrue\n",
            "(0)\t 672\t [ 1.8674242 -1.9700745] \t0\ttrue\n",
            "(1)\t 673\t [ 1.6115729 -1.5596706] \t0\tfalse\n",
            "(1)\t 674\t [ 1.6481991 -1.6544833] \t0\tfalse\n",
            "(1)\t 675\t [ 1.6481991 -1.6544833] \t0\tfalse\n",
            "(1)\t 676\t [ 1.7257029 -1.8151863] \t0\tfalse\n",
            "(1)\t 677\t [ 1.6335398 -1.5719985] \t0\tfalse\n",
            "(1)\t 678\t [-0.75919324  0.9436999 ] \t1\ttrue\n",
            "(0)\t 679\t [-1.2160352  1.7288601] \t1\tfalse\n",
            "(1)\t 680\t [ 1.7859915 -1.9054446] \t0\tfalse\n",
            "(0)\t 681\t [ 1.8464164 -1.9293019] \t0\ttrue\n",
            "(0)\t 682\t [ 1.7571814 -1.9200668] \t0\ttrue\n",
            "(0)\t 683\t [-0.11345525  0.5797826 ] \t1\tfalse\n",
            "(0)\t 684\t [-1.1026196  1.7011666] \t1\tfalse\n",
            "(1)\t 685\t [-1.1376433  1.6830575] \t1\ttrue\n",
            "(1)\t 686\t [ 1.8431093 -1.7921929] \t0\tfalse\n",
            "(1)\t 687\t [ 1.7378734 -1.825023 ] \t0\tfalse\n",
            "(1)\t 688\t [ 1.8180636 -1.9099749] \t0\tfalse\n",
            "(0)\t 689\t [ 1.8874637 -2.0053914] \t0\ttrue\n",
            "(1)\t 690\t [ 1.6002034 -1.4547007] \t0\tfalse\n",
            "(1)\t 691\t [ 1.578288  -1.5291862] \t0\tfalse\n",
            "(0)\t 692\t [ 1.6427277 -1.6111535] \t0\ttrue\n",
            "(0)\t 693\t [ 0.9175052 -0.5954055] \t0\ttrue\n",
            "(1)\t 694\t [ 1.781157  -1.8327891] \t0\tfalse\n",
            "(1)\t 695\t [ 1.495449  -1.4703567] \t0\tfalse\n",
            "(0)\t 696\t [ 1.8350761 -1.9137737] \t0\ttrue\n",
            "(0)\t 697\t [ 1.174645   -0.90140593] \t0\ttrue\n",
            "(0)\t 698\t [-0.01798197  0.51000345] \t1\tfalse\n",
            "(0)\t 699\t [ 1.2492518 -1.0071199] \t0\ttrue\n",
            "(0)\t 700\t [ 1.6830366 -1.6996866] \t0\ttrue\n",
            "(0)\t 701\t [ 0.8957288 -0.5886858] \t0\ttrue\n",
            "(0)\t 702\t [ 1.8053334 -2.0045123] \t0\ttrue\n",
            "(0)\t 703\t [ 1.8371167 -1.9434501] \t0\ttrue\n",
            "(0)\t 704\t [ 1.3035438 -1.1372329] \t0\ttrue\n",
            "(1)\t 705\t [ 1.880002  -2.0033107] \t0\tfalse\n",
            "(0)\t 706\t [0.02425803 0.43611827] \t1\tfalse\n",
            "(0)\t 707\t [ 1.7098166 -1.8507764] \t0\ttrue\n",
            "(1)\t 708\t [ 1.580059  -1.6370271] \t0\tfalse\n",
            "(0)\t 709\t [-0.8548898  1.4333688] \t1\tfalse\n",
            "(0)\t 710\t [ 1.1584526  -0.82383645] \t0\ttrue\n",
            "(1)\t 711\t [ 1.864221  -2.0169027] \t0\tfalse\n",
            "(1)\t 712\t [0.06318603 0.45215347] \t1\ttrue\n",
            "(1)\t 713\t [ 1.6042913 -1.5941895] \t0\tfalse\n",
            "(1)\t 714\t [-0.24589054  0.69035274] \t1\ttrue\n",
            "(1)\t 715\t [ 1.3400754 -1.1711421] \t0\tfalse\n",
            "(0)\t 716\t [ 1.8159732 -1.8051554] \t0\ttrue\n",
            "(0)\t 717\t [ 1.8315858 -1.9168884] \t0\ttrue\n",
            "(0)\t 718\t [ 1.1383384 -0.7730403] \t0\ttrue\n",
            "(0)\t 719\t [ 1.8633486 -1.9951648] \t0\ttrue\n",
            "(0)\t 720\t [ 1.8806466 -2.0356493] \t0\ttrue\n",
            "(1)\t 721\t [ 1.7688675 -1.8178415] \t0\tfalse\n",
            "(1)\t 722\t [-1.2846566  1.8269932] \t1\ttrue\n",
            "(0)\t 723\t [ 1.7551154 -1.8361769] \t0\ttrue\n",
            "(0)\t 724\t [ 1.7691054 -1.8017554] \t0\ttrue\n",
            "(1)\t 725\t [ 1.8039044 -1.940523 ] \t0\tfalse\n",
            "(0)\t 726\t [ 1.7709564 -1.8758088] \t0\ttrue\n",
            "(0)\t 727\t [ 1.6790473 -1.5568882] \t0\ttrue\n",
            "(1)\t 728\t [-1.2975839  1.8082455] \t1\ttrue\n",
            "(1)\t 729\t [ 1.5094911 -1.296356 ] \t0\tfalse\n",
            "(1)\t 730\t [-0.2889599  0.756901 ] \t1\ttrue\n",
            "(1)\t 731\t [ 1.8682163 -1.9636256] \t0\tfalse\n",
            "(1)\t 732\t [ 1.8682163 -1.9636256] \t0\tfalse\n",
            "(0)\t 733\t [ 1.7107476 -1.8104204] \t0\ttrue\n",
            "(1)\t 734\t [ 1.6597556 -1.6954358] \t0\tfalse\n",
            "(0)\t 735\t [ 1.4886826 -1.4035858] \t0\ttrue\n",
            "(0)\t 736\t [ 1.7759312 -1.9299265] \t0\ttrue\n",
            "(0)\t 737\t [ 1.8434001 -1.9753473] \t0\ttrue\n",
            "(1)\t 738\t [ 1.8922918 -2.041647 ] \t0\tfalse\n",
            "(1)\t 739\t [ 1.8922918 -2.041647 ] \t0\tfalse\n",
            "(0)\t 740\t [ 1.1956694  -0.98789304] \t0\ttrue\n",
            "(1)\t 741\t [ 1.6376425 -1.557593 ] \t0\tfalse\n",
            "(1)\t 742\t [ 1.6376425 -1.557593 ] \t0\tfalse\n",
            "(0)\t 743\t [ 1.8258165 -1.9673054] \t0\ttrue\n",
            "(0)\t 744\t [0.18294963 0.20981926] \t1\tfalse\n",
            "(1)\t 745\t [-0.06480326  0.58520806] \t1\ttrue\n",
            "(0)\t 746\t [ 1.738035  -1.8409815] \t0\ttrue\n",
            "(0)\t 747\t [ 1.8478982 -1.8542701] \t0\ttrue\n",
            "(1)\t 748\t [ 1.8275539 -1.9806321] \t0\tfalse\n",
            "(0)\t 749\t [ 1.7485968 -1.8552618] \t0\ttrue\n",
            "(0)\t 750\t [0.384805  0.1015232] \t0\ttrue\n",
            "(1)\t 751\t [ 1.79015  -1.809688] \t0\tfalse\n",
            "(1)\t 752\t [ 1.8280805 -1.8988235] \t0\tfalse\n",
            "(1)\t 753\t [ 1.8427567 -1.9429849] \t0\tfalse\n",
            "(0)\t 754\t [ 1.8585502 -1.86897  ] \t0\ttrue\n",
            "(0)\t 755\t [ 0.46623757 -0.02830574] \t0\ttrue\n",
            "(0)\t 756\t [ 1.8251601 -1.9442321] \t0\ttrue\n",
            "(0)\t 757\t [-0.6793731  1.2397085] \t1\tfalse\n",
            "(0)\t 758\t [-1.0734248  1.4971637] \t1\tfalse\n",
            "(0)\t 759\t [ 1.7616664 -1.8524892] \t0\ttrue\n",
            "(0)\t 760\t [ 1.7697293 -1.812008 ] \t0\ttrue\n",
            "(0)\t 761\t [ 1.8459207 -1.9867122] \t0\ttrue\n",
            "(0)\t 762\t [ 1.5804981 -1.4377316] \t0\ttrue\n",
            "(1)\t 763\t [ 1.8176135 -1.8863741] \t0\tfalse\n",
            "(1)\t 764\t [ 0.7937872  -0.40467843] \t0\tfalse\n",
            "(0)\t 765\t [-1.3067532  1.8717029] \t1\tfalse\n",
            "(0)\t 766\t [ 1.8074046 -1.8534222] \t0\ttrue\n",
            "(0)\t 767\t [ 1.6657485 -1.7181538] \t0\ttrue\n",
            "(1)\t 768\t [ 1.0313737  -0.82019895] \t0\tfalse\n",
            "(0)\t 769\t [ 0.6516316 -0.2461904] \t0\ttrue\n",
            "(0)\t 770\t [ 1.7368987 -1.8872877] \t0\ttrue\n",
            "(1)\t 771\t [ 1.7972723 -1.8912925] \t0\tfalse\n",
            "(1)\t 772\t [ 1.7972723 -1.8912925] \t0\tfalse\n",
            "(1)\t 773\t [ 1.7957605 -1.900672 ] \t0\tfalse\n",
            "(1)\t 774\t [ 1.7957605 -1.900672 ] \t0\tfalse\n",
            "(1)\t 775\t [ 1.4514395 -1.2877632] \t0\tfalse\n",
            "(1)\t 776\t [ 1.4514395 -1.2877632] \t0\tfalse\n",
            "(1)\t 777\t [ 1.7510217 -1.7996906] \t0\tfalse\n",
            "(0)\t 778\t [ 1.8764354 -1.96623  ] \t0\ttrue\n",
            "(0)\t 779\t [-0.98808026  1.5265332 ] \t1\tfalse\n",
            "(0)\t 780\t [ 1.63394   -1.5068327] \t0\ttrue\n",
            "(0)\t 781\t [ 1.7232834 -1.8583525] \t0\ttrue\n",
            "(0)\t 782\t [ 1.6754745 -1.6650028] \t0\ttrue\n",
            "(0)\t 783\t [0.4493276  0.00638358] \t0\ttrue\n",
            "(0)\t 784\t [ 1.1670607 -0.8779112] \t0\ttrue\n",
            "(0)\t 785\t [-1.2298747  1.7940835] \t1\tfalse\n",
            "(0)\t 786\t [ 1.7355618 -1.6251038] \t0\ttrue\n",
            "(1)\t 787\t [-1.2648852  1.8602368] \t1\ttrue\n",
            "(0)\t 788\t [ 1.7977884 -1.7893791] \t0\ttrue\n",
            "(0)\t 789\t [ 1.7813369 -1.7428876] \t0\ttrue\n",
            "(1)\t 790\t [0.4650918  0.03152666] \t0\tfalse\n",
            "(1)\t 791\t [ 1.1297274 -0.847241 ] \t0\tfalse\n",
            "(1)\t 792\t [ 1.8743964 -1.991027 ] \t0\tfalse\n",
            "(0)\t 793\t [-1.3129617  1.8888267] \t1\tfalse\n",
            "(1)\t 794\t [ 1.5580145 -1.4571251] \t0\tfalse\n",
            "(1)\t 795\t [ 1.7233269 -1.7296288] \t0\tfalse\n",
            "(0)\t 796\t [ 1.6171025 -1.5733584] \t0\ttrue\n",
            "(0)\t 797\t [ 1.763498  -1.8569744] \t0\ttrue\n",
            "(0)\t 798\t [0.1865402  0.31220445] \t1\tfalse\n",
            "(0)\t 799\t [ 0.8022666 -0.4282804] \t0\ttrue\n",
            "(0)\t 800\t [ 1.7966683 -1.9657933] \t0\ttrue\n",
            "(1)\t 801\t [ 1.7251872 -1.737624 ] \t0\tfalse\n",
            "(1)\t 802\t [ 1.7251872 -1.737624 ] \t0\tfalse\n",
            "(0)\t 803\t [ 1.7737654 -1.6155921] \t0\ttrue\n",
            "(0)\t 804\t [-1.2776973  1.8791293] \t1\tfalse\n",
            "(0)\t 805\t [-0.40304133  0.92626995] \t1\tfalse\n",
            "(0)\t 806\t [ 1.7263228 -1.7667607] \t0\ttrue\n",
            "(1)\t 807\t [ 0.77554494 -0.45929655] \t0\tfalse\n",
            "(0)\t 808\t [ 1.5027767 -1.4038897] \t0\ttrue\n",
            "(1)\t 809\t [0.06752032 0.3778269 ] \t1\ttrue\n",
            "(1)\t 810\t [0.06752032 0.3778269 ] \t1\ttrue\n",
            "(1)\t 811\t [ 1.711557  -1.7015448] \t0\tfalse\n",
            "(0)\t 812\t [ 1.2495614 -1.1284131] \t0\ttrue\n",
            "(1)\t 813\t [ 1.8043028 -1.7822434] \t0\tfalse\n",
            "(1)\t 814\t [ 1.8043028 -1.7822434] \t0\tfalse\n",
            "(1)\t 815\t [-1.2777778  1.8717995] \t1\ttrue\n",
            "(0)\t 816\t [ 1.5005317 -1.2547879] \t0\ttrue\n",
            "(0)\t 817\t [ 1.6734096 -1.6257844] \t0\ttrue\n",
            "(0)\t 818\t [ 1.2330133 -1.1110415] \t0\ttrue\n",
            "(0)\t 819\t [ 1.668578  -1.7298523] \t0\ttrue\n",
            "(0)\t 820\t [ 1.8083181 -1.9466442] \t0\ttrue\n",
            "(0)\t 821\t [ 1.855541 -2.028262] \t0\ttrue\n",
            "(1)\t 822\t [ 1.8325647 -1.9992706] \t0\tfalse\n",
            "(0)\t 823\t [ 1.0640658  -0.79431516] \t0\ttrue\n",
            "(0)\t 824\t [ 1.6139766 -1.672021 ] \t0\ttrue\n",
            "(0)\t 825\t [ 1.7849075 -1.8528172] \t0\ttrue\n",
            "(0)\t 826\t [ 1.5987647 -1.5856463] \t0\ttrue\n",
            "(0)\t 827\t [ 1.6370386 -1.5564113] \t0\ttrue\n",
            "(0)\t 828\t [ 1.299902  -1.1724328] \t0\ttrue\n",
            "(0)\t 829\t [ 1.6321952 -1.6239448] \t0\ttrue\n",
            "(0)\t 830\t [ 1.8755246 -2.002952 ] \t0\ttrue\n",
            "(1)\t 831\t [ 1.842657  -1.9263471] \t0\tfalse\n",
            "(0)\t 832\t [ 1.6375512 -1.5062718] \t0\ttrue\n",
            "(0)\t 833\t [ 1.6448423 -1.6297593] \t0\ttrue\n",
            "(1)\t 834\t [ 0.9396995  -0.58745104] \t0\tfalse\n",
            "(1)\t 835\t [ 1.7891972 -1.8419856] \t0\tfalse\n",
            "(1)\t 836\t [ 1.4331545 -1.3585021] \t0\tfalse\n",
            "(1)\t 837\t [ 0.4761596  -0.17790121] \t0\tfalse\n",
            "(0)\t 838\t [ 1.7501976 -1.7809664] \t0\ttrue\n",
            "(0)\t 839\t [ 1.8948227 -1.9841409] \t0\ttrue\n",
            "(0)\t 840\t [ 1.2784909 -1.1468129] \t0\ttrue\n",
            "(0)\t 841\t [0.37353003 0.01875889] \t0\ttrue\n",
            "(0)\t 842\t [ 1.290038  -1.0456374] \t0\ttrue\n",
            "(1)\t 843\t [ 1.8308152 -1.8969642] \t0\tfalse\n",
            "(0)\t 844\t [-0.6850315  1.24552  ] \t1\tfalse\n",
            "(1)\t 845\t [0.00479665 0.47103757] \t1\ttrue\n",
            "(1)\t 846\t [ 1.801662  -1.6625845] \t0\tfalse\n",
            "(1)\t 847\t [ 1.792627  -1.8668181] \t0\tfalse\n",
            "(1)\t 848\t [ 1.792627  -1.8668181] \t0\tfalse\n",
            "(0)\t 849\t [-0.5059453  1.1219016] \t1\tfalse\n",
            "(1)\t 850\t [-0.24830678  0.6646083 ] \t1\ttrue\n",
            "(1)\t 851\t [ 1.8448179 -1.9455265] \t0\tfalse\n",
            "(1)\t 852\t [ 1.5859736 -1.5244088] \t0\tfalse\n",
            "(0)\t 853\t [ 1.5863247 -1.408909 ] \t0\ttrue\n",
            "(1)\t 854\t [ 1.6902632 -1.6082581] \t0\tfalse\n",
            "(0)\t 855\t [ 0.98528844 -0.7220177 ] \t0\ttrue\n",
            "(0)\t 856\t [-0.46430042  0.916174  ] \t1\tfalse\n",
            "(0)\t 857\t [0.16021974 0.29047513] \t1\tfalse\n",
            "(1)\t 858\t [ 1.8066635 -1.9781584] \t0\tfalse\n",
            "(0)\t 859\t [-1.1380047  1.7380487] \t1\tfalse\n",
            "(1)\t 860\t [ 1.7099487 -1.8408151] \t0\tfalse\n",
            "(0)\t 861\t [0.4373944  0.03254975] \t0\ttrue\n",
            "(1)\t 862\t [ 1.8184732 -1.9631582] \t0\tfalse\n",
            "(1)\t 863\t [ 1.8184732 -1.9631582] \t0\tfalse\n",
            "(1)\t 864\t [ 0.55798334 -0.19393152] \t0\tfalse\n",
            "(0)\t 865\t [ 1.6261163 -1.5871469] \t0\ttrue\n",
            "(0)\t 866\t [ 1.7807578 -1.8370737] \t0\ttrue\n",
            "(0)\t 867\t [ 1.7891669 -1.9997646] \t0\ttrue\n",
            "(0)\t 868\t [-1.293348   1.8807002] \t1\tfalse\n",
            "(0)\t 869\t [ 1.4125035 -1.31839  ] \t0\ttrue\n",
            "(0)\t 870\t [ 1.8350569 -1.9680138] \t0\ttrue\n",
            "(0)\t 871\t [ 1.8653666 -1.8994311] \t0\ttrue\n",
            "(0)\t 872\t [ 1.1563632  -0.87043315] \t0\ttrue\n",
            "(1)\t 873\t [ 1.2341007 -0.9412547] \t0\tfalse\n",
            "(1)\t 874\t [ 1.6041265 -1.3478905] \t0\tfalse\n",
            "(1)\t 875\t [ 0.47985324 -0.14802   ] \t0\tfalse\n",
            "(1)\t 876\t [ 1.8023983 -1.8151189] \t0\tfalse\n",
            "(0)\t 877\t [ 1.8868462 -2.048655 ] \t0\ttrue\n",
            "(1)\t 878\t [ 1.7702068 -1.8509312] \t0\tfalse\n",
            "(1)\t 879\t [-1.1081024  1.5659744] \t1\ttrue\n",
            "(1)\t 880\t [ 1.8029587 -1.8991512] \t0\tfalse\n",
            "(0)\t 881\t [-0.71358293  1.2272724 ] \t1\tfalse\n",
            "(0)\t 882\t [ 0.9084037  -0.60124236] \t0\ttrue\n",
            "(1)\t 883\t [ 1.6309797 -1.4904296] \t0\tfalse\n",
            "(1)\t 884\t [ 1.4639167 -1.3096601] \t0\tfalse\n",
            "(1)\t 885\t [-1.1288708  1.5273217] \t1\ttrue\n",
            "(0)\t 886\t [ 1.8638455 -2.0449667] \t0\ttrue\n",
            "(0)\t 887\t [-0.87134594  1.395638  ] \t1\tfalse\n",
            "(1)\t 888\t [ 0.9175044  -0.55787563] \t0\tfalse\n",
            "(0)\t 889\t [ 1.5994865 -1.629594 ] \t0\ttrue\n",
            "(0)\t 890\t [ 1.5773557 -1.4887263] \t0\ttrue\n",
            "(0)\t 891\t [0.40610352 0.05929996] \t0\ttrue\n",
            "(1)\t 892\t [ 1.7946734 -1.7358307] \t0\tfalse\n",
            "(1)\t 893\t [ 1.8606852 -1.9554855] \t0\tfalse\n",
            "(1)\t 894\t [ 1.7866777 -1.9453869] \t0\tfalse\n",
            "(1)\t 895\t [ 1.5494827 -1.4486488] \t0\tfalse\n",
            "(0)\t 896\t [-0.4483016   0.99260163] \t1\tfalse\n",
            "(1)\t 897\t [ 1.3757209 -1.1228962] \t0\tfalse\n",
            "(1)\t 898\t [ 1.1507864 -0.979113 ] \t0\tfalse\n",
            "(1)\t 899\t [-1.2642357  1.7834101] \t1\ttrue\n",
            "(1)\t 900\t [ 1.1384941  -0.80018103] \t0\tfalse\n",
            "Number of true predictions: 493\n",
            "Number of false predictions: 407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIgsWWCTKdY",
        "colab_type": "code",
        "outputId": "53cb22b5-d1dd-4757-81aa-8dda8b5302ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 54.71698113207547 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xk8eT967gJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42c9bbe9-f17d-488d-8bef-3ec5fecbb8e4"
      },
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "#scores = [3.0, 1.0, 0.2]\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    predictions[i][j]=softmax(predictions[i][j])\n",
        "    print(predictions[i][j])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.33652267 0.66347736]\n",
            "[0.97224146 0.02775852]\n",
            "[0.8728801 0.1271199]\n",
            "[0.9762245  0.02377558]\n",
            "[0.97610587 0.02389411]\n",
            "[0.96743625 0.03256376]\n",
            "[0.91321117 0.08678883]\n",
            "[0.95928985 0.04071015]\n",
            "[0.97761077 0.02238921]\n",
            "[0.7839884  0.21601154]\n",
            "[0.9733189  0.02668117]\n",
            "[0.978284   0.02171599]\n",
            "[0.94025856 0.05974144]\n",
            "[0.9798625  0.02013743]\n",
            "[0.9768991  0.02310097]\n",
            "[0.91494477 0.08505519]\n",
            "[0.9742026 0.0257974]\n",
            "[0.26991895 0.7300811 ]\n",
            "[0.9742288  0.02577119]\n",
            "[0.9757447  0.02425533]\n",
            "[0.97528195 0.02471804]\n",
            "[0.6233236  0.37667644]\n",
            "[0.97578955 0.02421043]\n",
            "[0.10835104 0.8916489 ]\n",
            "[0.8662477  0.13375226]\n",
            "[0.9473693  0.05263077]\n",
            "[0.97221404 0.02778599]\n",
            "[0.29508045 0.7049195 ]\n",
            "[0.9693564  0.03064356]\n",
            "[0.97160697 0.02839301]\n",
            "[0.04166515 0.95833486]\n",
            "[0.9694178  0.03058222]\n",
            "[0.9702396  0.02976045]\n",
            "[0.977626   0.02237398]\n",
            "[0.977626   0.02237398]\n",
            "[0.98047537 0.01952459]\n",
            "[0.27730146 0.7226985 ]\n",
            "[0.9762941 0.0237059]\n",
            "[0.96853715 0.03146287]\n",
            "[0.95493823 0.04506177]\n",
            "[0.8987372  0.10126282]\n",
            "[0.8608802 0.1391198]\n",
            "[0.7018184  0.29818156]\n",
            "[0.9673665 0.0326335]\n",
            "[0.9673665 0.0326335]\n",
            "[0.90268075 0.0973192 ]\n",
            "[0.9790604  0.02093962]\n",
            "[0.97414654 0.02585342]\n",
            "[0.8939182  0.10608184]\n",
            "[0.9775531  0.02244688]\n",
            "[0.8864583  0.11354167]\n",
            "[0.97163063 0.02836933]\n",
            "[0.9800137 0.0199862]\n",
            "[0.9790006  0.02099933]\n",
            "[0.98045474 0.01954523]\n",
            "[0.95522344 0.0447765 ]\n",
            "[0.04694058 0.95305943]\n",
            "[0.9733704  0.02662962]\n",
            "[0.9698219  0.03017814]\n",
            "[0.7558974  0.24410261]\n",
            "[0.9701431  0.02985688]\n",
            "[0.9263968  0.07360319]\n",
            "[0.04923651 0.95076346]\n",
            "[0.79476345 0.20523654]\n",
            "[0.79476345 0.20523654]\n",
            "[0.9699948  0.03000521]\n",
            "[0.9735257  0.02647436]\n",
            "[0.9605455  0.03945455]\n",
            "[0.97643524 0.02356473]\n",
            "[0.9734518  0.02654819]\n",
            "[0.87864333 0.12135674]\n",
            "[0.05355742 0.94644254]\n",
            "[0.05355742 0.94644254]\n",
            "[0.08227497 0.917725  ]\n",
            "[0.08227497 0.917725  ]\n",
            "[0.97776365 0.02223637]\n",
            "[0.97776365 0.02223637]\n",
            "[0.15410382 0.8458962 ]\n",
            "[0.9793299  0.02067008]\n",
            "[0.97858846 0.02141153]\n",
            "[0.98010945 0.01989047]\n",
            "[0.06289334 0.93710667]\n",
            "[0.97888696 0.02111305]\n",
            "[0.97932214 0.0206779 ]\n",
            "[0.97623277 0.02376723]\n",
            "[0.97577375 0.02422618]\n",
            "[0.9596579  0.04034207]\n",
            "[0.97660416 0.02339579]\n",
            "[0.9644538  0.03554615]\n",
            "[0.08239404 0.917606  ]\n",
            "[0.04762147 0.9523785 ]\n",
            "[0.8305329  0.16946708]\n",
            "[0.8767952  0.12320483]\n",
            "[0.5309093  0.46909073]\n",
            "[0.97987354 0.02012652]\n",
            "[0.9717532  0.02824678]\n",
            "[0.97687095 0.02312901]\n",
            "[0.97765064 0.02234937]\n",
            "[0.97417665 0.02582337]\n",
            "[0.97534    0.02466001]\n",
            "[0.0438698  0.95613015]\n",
            "[0.97850853 0.0214915 ]\n",
            "[0.9578712  0.04212883]\n",
            "[0.9610182  0.03898183]\n",
            "[0.97582823 0.0241717 ]\n",
            "[0.9173902  0.08260978]\n",
            "[0.96598744 0.03401253]\n",
            "[0.96694034 0.03305966]\n",
            "[0.97827464 0.02172542]\n",
            "[0.9714542  0.02854577]\n",
            "[0.97245693 0.02754308]\n",
            "[0.54390854 0.4560915 ]\n",
            "[0.948754   0.05124598]\n",
            "[0.9751217  0.02487837]\n",
            "[0.97460246 0.0253975 ]\n",
            "[0.97629434 0.02370565]\n",
            "[0.73641056 0.26358944]\n",
            "[0.970975   0.02902504]\n",
            "[0.98039335 0.01960664]\n",
            "[0.9801987  0.01980128]\n",
            "[0.8828587  0.11714128]\n",
            "[0.9732136  0.02678642]\n",
            "[0.09684247 0.9031576 ]\n",
            "[0.9801727 0.0198273]\n",
            "[0.9804936 0.0195064]\n",
            "[0.97049004 0.02951001]\n",
            "[0.9773646  0.02263544]\n",
            "[0.9773646  0.02263544]\n",
            "[0.96474177 0.03525826]\n",
            "[0.97340167 0.02659841]\n",
            "[0.9798425  0.02015749]\n",
            "[0.9760324  0.02396762]\n",
            "[0.9730574  0.02694259]\n",
            "[0.9440237  0.05597626]\n",
            "[0.97949857 0.0205014 ]\n",
            "[0.96395886 0.03604114]\n",
            "[0.9145948  0.08540522]\n",
            "[0.9734621  0.02653788]\n",
            "[0.98002094 0.01997901]\n",
            "[0.9540895  0.04591049]\n",
            "[0.97042274 0.02957718]\n",
            "[0.9616924  0.03830761]\n",
            "[0.9797956  0.02020442]\n",
            "[0.9589989  0.04100106]\n",
            "[0.9516704  0.04832962]\n",
            "[0.97894067 0.02105938]\n",
            "[0.14080797 0.859192  ]\n",
            "[0.976544   0.02345596]\n",
            "[0.976544   0.02345596]\n",
            "[0.9779193  0.02208072]\n",
            "[0.9779193  0.02208072]\n",
            "[0.96754473 0.03245531]\n",
            "[0.96754473 0.03245531]\n",
            "[0.97763765 0.02236242]\n",
            "[0.95546246 0.04453752]\n",
            "[0.95546246 0.04453752]\n",
            "[0.9805989  0.01940113]\n",
            "[0.9749851  0.02501491]\n",
            "[0.98028964 0.01971042]\n",
            "[0.9764847  0.02351526]\n",
            "[0.9727246  0.02727534]\n",
            "[0.5562573 0.4437427]\n",
            "[0.97774804 0.02225194]\n",
            "[0.77357394 0.2264261 ]\n",
            "[0.9709561 0.0290439]\n",
            "[0.06185293 0.93814707]\n",
            "[0.9786154  0.02138459]\n",
            "[0.9745096  0.02549036]\n",
            "[0.97526556 0.02473446]\n",
            "[0.97526556 0.02473446]\n",
            "[0.70570475 0.29429522]\n",
            "[0.7853596  0.21464038]\n",
            "[0.4179305 0.5820695]\n",
            "[0.97283024 0.02716973]\n",
            "[0.16627665 0.8337233 ]\n",
            "[0.97504437 0.02495565]\n",
            "[0.967491   0.03250907]\n",
            "[0.9799619  0.02003809]\n",
            "[0.97625244 0.02374763]\n",
            "[0.9801104  0.01988963]\n",
            "[0.97543347 0.02456655]\n",
            "[0.96413314 0.0358669 ]\n",
            "[0.9682201  0.03177986]\n",
            "[0.8906919  0.10930811]\n",
            "[0.97377914 0.02622084]\n",
            "[0.9736519  0.02634814]\n",
            "[0.9644711  0.03552885]\n",
            "[0.04386792 0.9561321 ]\n",
            "[0.88428193 0.11571807]\n",
            "[0.9498098  0.05019016]\n",
            "[0.97840124 0.0215987 ]\n",
            "[0.97683495 0.02316502]\n",
            "[0.92511153 0.07488843]\n",
            "[0.95887035 0.04112968]\n",
            "[0.93081445 0.06918553]\n",
            "[0.37795568 0.6220444 ]\n",
            "[0.97519445 0.02480558]\n",
            "[0.95353884 0.0464612 ]\n",
            "[0.9263042  0.07369578]\n",
            "[0.9263042  0.07369578]\n",
            "[0.9716214  0.02837862]\n",
            "[0.9589528 0.0410472]\n",
            "[0.97855306 0.02144688]\n",
            "[0.97855306 0.02144688]\n",
            "[0.9672887  0.03271135]\n",
            "[0.9775554  0.02244464]\n",
            "[0.39081997 0.60918003]\n",
            "[0.9629742  0.03702577]\n",
            "[0.97243404 0.02756598]\n",
            "[0.95772654 0.04227348]\n",
            "[0.92470354 0.07529651]\n",
            "[0.92691225 0.07308775]\n",
            "[0.9554996  0.04450046]\n",
            "[0.97879744 0.0212026 ]\n",
            "[0.9791753  0.02082468]\n",
            "[0.97577626 0.0242237 ]\n",
            "[0.2856011  0.71439886]\n",
            "[0.9716566  0.02834337]\n",
            "[0.9280105  0.07198953]\n",
            "[0.20672294 0.793277  ]\n",
            "[0.9758497  0.02415025]\n",
            "[0.9758497  0.02415025]\n",
            "[0.39312464 0.6068754 ]\n",
            "[0.508642   0.49135795]\n",
            "[0.92922074 0.07077926]\n",
            "[0.12999812 0.8700019 ]\n",
            "[0.9785638  0.02143618]\n",
            "[0.9785638  0.02143618]\n",
            "[0.9559425  0.04405751]\n",
            "[0.9728996  0.02710042]\n",
            "[0.9427038  0.05729628]\n",
            "[0.9427038  0.05729628]\n",
            "[0.9790045  0.02099543]\n",
            "[0.97357666 0.02642332]\n",
            "[0.97420585 0.02579419]\n",
            "[0.9809605 0.0190395]\n",
            "[0.04872162 0.9512784 ]\n",
            "[0.9782242  0.02177585]\n",
            "[0.96360105 0.03639887]\n",
            "[0.97143304 0.028567  ]\n",
            "[0.952423   0.04757707]\n",
            "[0.9644122  0.03558777]\n",
            "[0.93532854 0.0646715 ]\n",
            "[0.97128963 0.02871041]\n",
            "[0.97096026 0.02903981]\n",
            "[0.979193   0.02080708]\n",
            "[0.979193   0.02080708]\n",
            "[0.9768177  0.02318224]\n",
            "[0.95772994 0.04227012]\n",
            "[0.95772994 0.04227012]\n",
            "[0.95843965 0.04156038]\n",
            "[0.9231664  0.07683358]\n",
            "[0.9762364  0.02376361]\n",
            "[0.8573953  0.14260471]\n",
            "[0.8174284  0.18257158]\n",
            "[0.26916623 0.73083377]\n",
            "[0.8103108 0.1896892]\n",
            "[0.15911311 0.8408869 ]\n",
            "[0.97642106 0.02357898]\n",
            "[0.98011005 0.01988996]\n",
            "[0.98049086 0.01950919]\n",
            "[0.97898865 0.02101135]\n",
            "[0.98096    0.01904003]\n",
            "[0.9478837  0.05211628]\n",
            "[0.97952807 0.02047192]\n",
            "[0.97891074 0.02108923]\n",
            "[0.98040235 0.01959763]\n",
            "[0.07331678 0.9266832 ]\n",
            "[0.89728713 0.10271291]\n",
            "[0.85276365 0.14723636]\n",
            "[0.54871047 0.45128947]\n",
            "[0.9403709  0.05962907]\n",
            "[0.94655734 0.05344267]\n",
            "[0.8399848  0.16001521]\n",
            "[0.9516321  0.04836797]\n",
            "[0.06781788 0.93218213]\n",
            "[0.9778104  0.02218964]\n",
            "[0.9742861  0.02571387]\n",
            "[0.8169861 0.1830139]\n",
            "[0.9756429  0.02435713]\n",
            "[0.269889   0.73011106]\n",
            "[0.71952367 0.28047633]\n",
            "[0.203856 0.796144]\n",
            "[0.92090404 0.07909592]\n",
            "[0.9763259  0.02367408]\n",
            "[0.9763259  0.02367408]\n",
            "[0.9694754  0.03052462]\n",
            "[0.9754321 0.0245679]\n",
            "[0.97280234 0.02719766]\n",
            "[0.43300828 0.56699175]\n",
            "[0.55522513 0.44477484]\n",
            "[0.9039888  0.09601125]\n",
            "[0.97357404 0.02642595]\n",
            "[0.97556144 0.02443858]\n",
            "[0.9583568  0.04164324]\n",
            "[0.96823883 0.03176124]\n",
            "[0.09720796 0.90279204]\n",
            "[0.96589226 0.03410774]\n",
            "[0.9081746  0.09182543]\n",
            "[0.9694462  0.03055389]\n",
            "[0.9539259  0.04607405]\n",
            "[0.98083663 0.01916341]\n",
            "[0.98083663 0.01916341]\n",
            "[0.11085362 0.8891464 ]\n",
            "[0.76142216 0.23857777]\n",
            "[0.5744671  0.42553288]\n",
            "[0.9727414  0.02725852]\n",
            "[0.97994244 0.02005755]\n",
            "[0.9796617  0.02033826]\n",
            "[0.9003398  0.09966024]\n",
            "[0.6308511 0.3691489]\n",
            "[0.76476926 0.23523073]\n",
            "[0.9673266  0.03267339]\n",
            "[0.96963173 0.0303683 ]\n",
            "[0.94860876 0.05139126]\n",
            "[0.94296384 0.05703623]\n",
            "[0.97724354 0.02275647]\n",
            "[0.05495557 0.94504446]\n",
            "[0.9457525  0.05424749]\n",
            "[0.97895217 0.02104781]\n",
            "[0.9800531  0.01994687]\n",
            "[0.7745873  0.22541273]\n",
            "[0.98129255 0.01870747]\n",
            "[0.97327733 0.02672265]\n",
            "[0.97332937 0.02667061]\n",
            "[0.97565633 0.02434374]\n",
            "[0.938023   0.06197707]\n",
            "[0.9712903  0.02870967]\n",
            "[0.07152091 0.9284791 ]\n",
            "[0.07776721 0.92223275]\n",
            "[0.49815935 0.50184065]\n",
            "[0.1266109 0.8733891]\n",
            "[0.4344991  0.56550086]\n",
            "[0.9750277  0.02497231]\n",
            "[0.08226957 0.91773045]\n",
            "[0.97397035 0.02602964]\n",
            "[0.9760827  0.02391725]\n",
            "[0.80043036 0.19956958]\n",
            "[0.31405798 0.68594205]\n",
            "[0.04625219 0.9537478 ]\n",
            "[0.616509   0.38349098]\n",
            "[0.964642 0.035358]\n",
            "[0.958687   0.04131303]\n",
            "[0.05806456 0.9419355 ]\n",
            "[0.47739455 0.5226054 ]\n",
            "[0.9800544  0.01994565]\n",
            "[0.97586524 0.02413471]\n",
            "[0.31521353 0.68478644]\n",
            "[0.9305237  0.06947624]\n",
            "[0.9535321  0.04646787]\n",
            "[0.4004862  0.59951377]\n",
            "[0.9724858  0.02751425]\n",
            "[0.9707884  0.02921161]\n",
            "[0.9586608  0.04133924]\n",
            "[0.9697805  0.03021948]\n",
            "[0.9599887  0.04001134]\n",
            "[0.9702656 0.0297344]\n",
            "[0.9728204  0.02717957]\n",
            "[0.9798881 0.0201119]\n",
            "[0.97164166 0.02835838]\n",
            "[0.97735834 0.02264169]\n",
            "[0.97701603 0.02298394]\n",
            "[0.9700446  0.02995546]\n",
            "[0.9739741  0.02602591]\n",
            "[0.93067765 0.06932232]\n",
            "[0.97540855 0.0245915 ]\n",
            "[0.29386592 0.70613414]\n",
            "[0.7150593  0.28494072]\n",
            "[0.979117   0.02088308]\n",
            "[0.27292463 0.72707534]\n",
            "[0.9663382  0.03366177]\n",
            "[0.03687292 0.96312714]\n",
            "[0.9528361  0.04716389]\n",
            "[0.04910363 0.9508964 ]\n",
            "[0.9782567  0.02174333]\n",
            "[0.9795654  0.02043459]\n",
            "[0.9769006  0.02309949]\n",
            "[0.2956271 0.7043729]\n",
            "[0.71582055 0.28417948]\n",
            "[0.9747574  0.02524264]\n",
            "[0.04374707 0.9562529 ]\n",
            "[0.9756796  0.02432049]\n",
            "[0.97335786 0.02664222]\n",
            "[0.9741149  0.02588514]\n",
            "[0.9700291  0.02997085]\n",
            "[0.9609525  0.03904752]\n",
            "[0.68255377 0.31744623]\n",
            "[0.3330131  0.66698694]\n",
            "[0.98009723 0.0199028 ]\n",
            "[0.9772388  0.02276122]\n",
            "[0.9746456  0.02535441]\n",
            "[0.9727203  0.02727961]\n",
            "[0.04504116 0.95495886]\n",
            "[0.8927743  0.10722572]\n",
            "[0.9714414  0.02855862]\n",
            "[0.81696916 0.18303087]\n",
            "[0.8273362  0.17266385]\n",
            "[0.07012661 0.92987335]\n",
            "[0.97021633 0.02978363]\n",
            "[0.97789705 0.02210302]\n",
            "[0.96214366 0.03785641]\n",
            "[0.95726895 0.0427311 ]\n",
            "[0.23309842 0.76690155]\n",
            "[0.06258456 0.9374154 ]\n",
            "[0.8877389  0.11226107]\n",
            "[0.97744966 0.02255035]\n",
            "[0.4540194 0.5459806]\n",
            "[0.7878565 0.2121434]\n",
            "[0.9781129  0.02188711]\n",
            "[0.9617753  0.03822472]\n",
            "[0.39114287 0.6088571 ]\n",
            "[0.9709452  0.02905481]\n",
            "[0.56336784 0.43663216]\n",
            "[0.04026261 0.9597374 ]\n",
            "[0.9246695  0.07533054]\n",
            "[0.9677702  0.03222984]\n",
            "[0.9797411  0.02025889]\n",
            "[0.87305886 0.12694117]\n",
            "[0.9766962  0.02330385]\n",
            "[0.9355598  0.06444017]\n",
            "[0.94822395 0.05177611]\n",
            "[0.9514018  0.04859821]\n",
            "[0.97991824 0.02008171]\n",
            "[0.97899127 0.0210087 ]\n",
            "[0.9515025  0.04849748]\n",
            "[0.18534327 0.8146567 ]\n",
            "[0.9765925  0.02340748]\n",
            "[0.97358626 0.02641381]\n",
            "[0.94540703 0.05459299]\n",
            "[0.97967464 0.02032538]\n",
            "[0.5765769 0.4234231]\n",
            "[0.9503887  0.04961131]\n",
            "[0.9503887  0.04961131]\n",
            "[0.8294773  0.17052269]\n",
            "[0.9789162  0.02108385]\n",
            "[0.9789162  0.02108385]\n",
            "[0.9802727  0.01972737]\n",
            "[0.31305102 0.68694896]\n",
            "[0.5065957  0.49340433]\n",
            "[0.97547126 0.02452873]\n",
            "[0.9638267  0.03617329]\n",
            "[0.9662919  0.03370808]\n",
            "[0.9727357  0.02726428]\n",
            "[0.97886753 0.02113249]\n",
            "[0.8990646 0.1009354]\n",
            "[0.97795606 0.02204393]\n",
            "[0.9805844 0.0194156]\n",
            "[0.971179   0.02882099]\n",
            "[0.97007555 0.02992449]\n",
            "[0.94555855 0.05444137]\n",
            "[0.94555855 0.05444137]\n",
            "[0.97843605 0.02156389]\n",
            "[0.9607422  0.03925784]\n",
            "[0.9758982  0.02410181]\n",
            "[0.9755982  0.02440179]\n",
            "[0.95755094 0.04244902]\n",
            "[0.8237806  0.17621937]\n",
            "[0.95345795 0.04654203]\n",
            "[0.84194356 0.15805645]\n",
            "[0.97968525 0.02031467]\n",
            "[0.97968525 0.02031467]\n",
            "[0.9776074  0.02239252]\n",
            "[0.97324467 0.0267553 ]\n",
            "[0.9753465  0.02465345]\n",
            "[0.97487986 0.02512017]\n",
            "[0.06843253 0.9315675 ]\n",
            "[0.9791404  0.02085958]\n",
            "[0.97692263 0.02307738]\n",
            "[0.97670263 0.02329737]\n",
            "[0.97486633 0.02513362]\n",
            "[0.9795617  0.02043825]\n",
            "[0.9795617  0.02043825]\n",
            "[0.9008679  0.09913205]\n",
            "[0.89186895 0.10813109]\n",
            "[0.97417814 0.02582184]\n",
            "[0.97490275 0.02509722]\n",
            "[0.97923344 0.02076661]\n",
            "[0.9743046  0.02569538]\n",
            "[0.9320144  0.06798554]\n",
            "[0.92650807 0.07349191]\n",
            "[0.96568245 0.03431752]\n",
            "[0.9796839 0.0203161]\n",
            "[0.9789716  0.02102838]\n",
            "[0.81162935 0.18837063]\n",
            "[0.97573143 0.0242686 ]\n",
            "[0.98043424 0.01956579]\n",
            "[0.95666546 0.0433346 ]\n",
            "[0.97319716 0.02680283]\n",
            "[0.9599763  0.04002366]\n",
            "[0.9758699  0.02413011]\n",
            "[0.9544455  0.04555453]\n",
            "[0.97696334 0.02303667]\n",
            "[0.97696334 0.02303667]\n",
            "[0.80096215 0.19903782]\n",
            "[0.9770503 0.0229497]\n",
            "[0.9602005  0.03979949]\n",
            "[0.11208792 0.8879121 ]\n",
            "[0.95237106 0.04762891]\n",
            "[0.9788132  0.02118687]\n",
            "[0.9670562  0.03294376]\n",
            "[0.9807179  0.01928214]\n",
            "[0.9807179  0.01928214]\n",
            "[0.97619104 0.02380898]\n",
            "[0.9754091  0.02459088]\n",
            "[0.86323225 0.13676772]\n",
            "[0.95532113 0.04467886]\n",
            "[0.76733774 0.2326623 ]\n",
            "[0.06568247 0.93431747]\n",
            "[0.05203921 0.94796073]\n",
            "[0.71147764 0.2885224 ]\n",
            "[0.04275963 0.95724034]\n",
            "[0.96651125 0.03348882]\n",
            "[0.9283418  0.07165819]\n",
            "[0.3861048  0.61389524]\n",
            "[0.04015805 0.95984197]\n",
            "[0.9785275  0.02147251]\n",
            "[0.5509041  0.44909593]\n",
            "[0.9784933  0.02150675]\n",
            "[0.954558   0.04544195]\n",
            "[0.97865444 0.02134556]\n",
            "[0.7714809  0.22851916]\n",
            "[0.9756428  0.02435721]\n",
            "[0.9754032  0.02459676]\n",
            "[0.8900241  0.10997588]\n",
            "[0.9776708  0.02232917]\n",
            "[0.9636777 0.0363223]\n",
            "[0.89948875 0.10051125]\n",
            "[0.97237724 0.02762274]\n",
            "[0.9718898  0.02811021]\n",
            "[0.9718898  0.02811021]\n",
            "[0.92308897 0.07691097]\n",
            "[0.07480637 0.92519367]\n",
            "[0.97811025 0.02188978]\n",
            "[0.97811025 0.02188978]\n",
            "[0.94955295 0.05044707]\n",
            "[0.94955295 0.05044707]\n",
            "[0.98113155 0.01886845]\n",
            "[0.8598778  0.14012215]\n",
            "[0.8598778  0.14012215]\n",
            "[0.4476424  0.55235755]\n",
            "[0.4476424  0.55235755]\n",
            "[0.8129734  0.18702663]\n",
            "[0.976728   0.02327194]\n",
            "[0.976728   0.02327194]\n",
            "[0.9779554  0.02204455]\n",
            "[0.9788481  0.02115193]\n",
            "[0.67606807 0.32393196]\n",
            "[0.97850364 0.02149631]\n",
            "[0.97850364 0.02149631]\n",
            "[0.9737666  0.02623342]\n",
            "[0.71590376 0.28409627]\n",
            "[0.8755437  0.12445626]\n",
            "[0.230564 0.769436]\n",
            "[0.97894603 0.02105401]\n",
            "[0.90715367 0.0928463 ]\n",
            "[0.9393148  0.06068517]\n",
            "[0.08959176 0.91040826]\n",
            "[0.89394057 0.10605945]\n",
            "[0.39519528 0.60480475]\n",
            "[0.05270036 0.9472996 ]\n",
            "[0.10712452 0.8928755 ]\n",
            "[0.93812746 0.06187256]\n",
            "[0.93812746 0.06187256]\n",
            "[0.9793982  0.02060183]\n",
            "[0.88353723 0.11646277]\n",
            "[0.8670322  0.13296787]\n",
            "[0.8670322  0.13296787]\n",
            "[0.90904135 0.09095864]\n",
            "[0.3601539 0.6398461]\n",
            "[0.9776487  0.02235131]\n",
            "[0.97708124 0.02291876]\n",
            "[0.3369683 0.6630317]\n",
            "[0.9684031  0.03159687]\n",
            "[0.9046413  0.09535878]\n",
            "[0.96858853 0.03141151]\n",
            "[0.9793873  0.02061269]\n",
            "[0.68877304 0.31122696]\n",
            "[0.9789788  0.02102124]\n",
            "[0.842305   0.15769506]\n",
            "[0.9799217  0.02007829]\n",
            "[0.96957266 0.03042732]\n",
            "[0.9736583  0.02634166]\n",
            "[0.9736583  0.02634166]\n",
            "[0.97563714 0.02436288]\n",
            "[0.97153926 0.02846077]\n",
            "[0.9806256  0.01937441]\n",
            "[0.9713084  0.02869155]\n",
            "[0.96551    0.03448995]\n",
            "[0.965416   0.03458394]\n",
            "[0.8828451 0.1171549]\n",
            "[0.9003846 0.0996154]\n",
            "[0.9789355  0.02106446]\n",
            "[0.97109085 0.0289092 ]\n",
            "[0.97109085 0.0289092 ]\n",
            "[0.9723973  0.02760275]\n",
            "[0.97369736 0.02630268]\n",
            "[0.05877199 0.94122803]\n",
            "[0.6350552 0.3649448]\n",
            "[0.11599608 0.88400394]\n",
            "[0.0450187  0.95498127]\n",
            "[0.9229204  0.07707961]\n",
            "[0.9253586  0.07464141]\n",
            "[0.96650344 0.03349662]\n",
            "[0.9731055  0.02689457]\n",
            "[0.9731055  0.02689457]\n",
            "[0.9808311  0.01916895]\n",
            "[0.9599776  0.04002236]\n",
            "[0.9775218  0.02247821]\n",
            "[0.8365976  0.16340235]\n",
            "[0.8365976  0.16340235]\n",
            "[0.9384277  0.06157229]\n",
            "[0.94354093 0.05645907]\n",
            "[0.9714451  0.02855493]\n",
            "[0.97496367 0.02503626]\n",
            "[0.41723037 0.58276963]\n",
            "[0.97387743 0.02612256]\n",
            "[0.53989613 0.46010387]\n",
            "[0.9723994 0.0276006]\n",
            "[0.9782095  0.02179053]\n",
            "[0.9764889  0.02351112]\n",
            "[0.9678981  0.03210196]\n",
            "[0.9678981  0.03210196]\n",
            "[0.97043097 0.02956902]\n",
            "[0.96707773 0.03292226]\n",
            "[0.96477795 0.03522209]\n",
            "[0.9640608  0.03593925]\n",
            "[0.97953355 0.02046637]\n",
            "[0.06352175 0.93647826]\n",
            "[0.9689034  0.03109662]\n",
            "[0.92853004 0.07146994]\n",
            "[0.09395977 0.90604025]\n",
            "[0.97660655 0.02339348]\n",
            "[0.96708417 0.03291579]\n",
            "[0.95994204 0.04005795]\n",
            "[0.8041964  0.19580364]\n",
            "[0.9632726  0.03672745]\n",
            "[0.97472703 0.02527299]\n",
            "[0.54731214 0.4526879 ]\n",
            "[0.73424894 0.2657511 ]\n",
            "[0.04101126 0.9589887 ]\n",
            "[0.9729496  0.02705036]\n",
            "[0.9797606  0.02023949]\n",
            "[0.9358282  0.06417175]\n",
            "[0.8979392  0.10206085]\n",
            "[0.28609496 0.71390504]\n",
            "[0.477504 0.522496]\n",
            "[0.77444726 0.22555272]\n",
            "[0.96268255 0.0373174 ]\n",
            "[0.69120485 0.30879512]\n",
            "[0.03827522 0.9617248 ]\n",
            "[0.9771377  0.02286226]\n",
            "[0.56542283 0.43457717]\n",
            "[0.9722891  0.02771086]\n",
            "[0.08481983 0.91518015]\n",
            "[0.90044177 0.09955821]\n",
            "[0.94090515 0.05909489]\n",
            "[0.9675357  0.03246434]\n",
            "[0.9759099  0.02409015]\n",
            "[0.9553726  0.04462743]\n",
            "[0.9790236  0.02097638]\n",
            "[0.9614076  0.03859246]\n",
            "[0.38496885 0.6150312 ]\n",
            "[0.96490633 0.03509373]\n",
            "[0.96490633 0.03509373]\n",
            "[0.97698736 0.02301257]\n",
            "[0.92246765 0.07753234]\n",
            "[0.97745764 0.02254234]\n",
            "[0.9640323  0.03596765]\n",
            "[0.9701287  0.02987125]\n",
            "[0.9677719  0.03222812]\n",
            "[0.97969157 0.02030849]\n",
            "[0.97890705 0.02109293]\n",
            "[0.95973766 0.04026234]\n",
            "[0.9645207  0.03547928]\n",
            "[0.9645207  0.03547928]\n",
            "[0.97182906 0.02817093]\n",
            "[0.9610421  0.03895783]\n",
            "[0.15408778 0.8459122 ]\n",
            "[0.04997833 0.9500217 ]\n",
            "[0.97567046 0.02432948]\n",
            "[0.97759295 0.02240704]\n",
            "[0.9753314  0.02466855]\n",
            "[0.33331317 0.6666868 ]\n",
            "[0.05711991 0.9428801 ]\n",
            "[0.05621573 0.94378424]\n",
            "[0.9743018  0.02569815]\n",
            "[0.97242534 0.02757465]\n",
            "[0.97652435 0.02347559]\n",
            "[0.9800203  0.01997973]\n",
            "[0.9549938  0.04500623]\n",
            "[0.9572 0.0428]\n",
            "[0.9628123  0.03718767]\n",
            "[0.81949216 0.18050782]\n",
            "[0.9737617  0.02623831]\n",
            "[0.9510052  0.04899479]\n",
            "[0.97699684 0.02300321]\n",
            "[0.8885535  0.11144644]\n",
            "[0.37098688 0.6290131 ]\n",
            "[0.90519875 0.09480127]\n",
            "[0.9671602  0.03283979]\n",
            "[0.8152385  0.18476155]\n",
            "[0.97832847 0.02167154]\n",
            "[0.9776989  0.02230108]\n",
            "[0.9198844  0.08011565]\n",
            "[0.97983253 0.02016743]\n",
            "[0.39846614 0.60153383]\n",
            "[0.9723635  0.02763648]\n",
            "[0.9614723  0.03852778]\n",
            "[0.09210006 0.9079    ]\n",
            "[0.878925   0.12107503]\n",
            "[0.97978926 0.02021074]\n",
            "[0.4039659 0.5960341]\n",
            "[0.96077704 0.03922293]\n",
            "[0.2816598 0.7183402]\n",
            "[0.9249245  0.07507551]\n",
            "[0.9739446  0.02605542]\n",
            "[0.9769884  0.02301165]\n",
            "[0.871174   0.12882604]\n",
            "[0.9793367  0.02066336]\n",
            "[0.9804741  0.01952587]\n",
            "[0.97305673 0.02694326]\n",
            "[0.04262926 0.95737076]\n",
            "[0.9731766  0.02682336]\n",
            "[0.97263813 0.02736189]\n",
            "[0.9768971 0.0231028]\n",
            "[0.97458726 0.0254127 ]\n",
            "[0.9621644  0.03783557]\n",
            "[0.04286743 0.9571325 ]\n",
            "[0.94299096 0.05700902]\n",
            "[0.2600207  0.73997927]\n",
            "[0.97879    0.02121005]\n",
            "[0.97879    0.02121005]\n",
            "[0.97128415 0.0287159 ]\n",
            "[0.96627444 0.03372558]\n",
            "[0.94746286 0.05253709]\n",
            "[0.9760105  0.02398949]\n",
            "[0.9785164 0.0214836]\n",
            "[0.9808091  0.01919096]\n",
            "[0.9808091  0.01919096]\n",
            "[0.89876366 0.10123632]\n",
            "[0.9606546  0.03934542]\n",
            "[0.9606546  0.03934542]\n",
            "[0.977971   0.02202897]\n",
            "[0.493283 0.506717]\n",
            "[0.342987 0.657013]\n",
            "[0.9728544  0.02714568]\n",
            "[0.97592396 0.02407602]\n",
            "[0.97829324 0.02170675]\n",
            "[0.97350276 0.02649728]\n",
            "[0.57035065 0.42964938]\n",
            "[0.9733988  0.02660119]\n",
            "[0.97649837 0.02350161]\n",
            "[0.9778115  0.02218852]\n",
            "[0.97651255 0.02348748]\n",
            "[0.6211761  0.37882385]\n",
            "[0.977454   0.02254603]\n",
            "[0.127964   0.87203604]\n",
            "[0.07105545 0.9289445 ]\n",
            "[0.9737671  0.02623295]\n",
            "[0.97292614 0.02707392]\n",
            "[0.9788063  0.02119363]\n",
            "[0.9533909  0.04660907]\n",
            "[0.97596663 0.02403331]\n",
            "[0.7682517 0.2317483]\n",
            "[0.03998456 0.9600154 ]\n",
            "[0.9749332  0.02506675]\n",
            "[0.96719766 0.03280236]\n",
            "[0.8643117  0.13568835]\n",
            "[0.71050173 0.28949827]\n",
            "[0.97402203 0.02597793]\n",
            "[0.97560227 0.02439773]\n",
            "[0.97560227 0.02439773]\n",
            "[0.97578883 0.02421116]\n",
            "[0.97578883 0.02421116]\n",
            "[0.9393007  0.06069934]\n",
            "[0.9393007  0.06069934]\n",
            "[0.9720968  0.02790325]\n",
            "[0.97901356 0.02098651]\n",
            "[0.07484005 0.92516   ]\n",
            "[0.9585436 0.0414564]\n",
            "[0.9729234  0.02707659]\n",
            "[0.9657916  0.03420838]\n",
            "[0.60896033 0.39103967]\n",
            "[0.88543856 0.11456142]\n",
            "[0.04635518 0.9536448 ]\n",
            "[0.9664523  0.03354763]\n",
            "[0.04208281 0.95791715]\n",
            "[0.9730688  0.02693125]\n",
            "[0.97136927 0.02863077]\n",
            "[0.6067247  0.39327532]\n",
            "[0.87835765 0.1216424 ]\n",
            "[0.97947603 0.02052399]\n",
            "[0.03909848 0.96090156]\n",
            "[0.9532534  0.04674659]\n",
            "[0.96931916 0.03068084]\n",
            "[0.96047366 0.03952628]\n",
            "[0.973928   0.02607208]\n",
            "[0.46862522 0.53137475]\n",
            "[0.7739143  0.22608572]\n",
            "[0.9773007  0.02269927]\n",
            "[0.96961087 0.03038909]\n",
            "[0.96961087 0.03038909]\n",
            "[0.9673702  0.03262972]\n",
            "[0.04082314 0.9591769 ]\n",
            "[0.20927332 0.79072666]\n",
            "[0.97049034 0.02950967]\n",
            "[0.7746648  0.22533517]\n",
            "[0.94817495 0.051825  ]\n",
            "[0.4230399 0.5769601]\n",
            "[0.4230399 0.5769601]\n",
            "[0.9681115 0.0318885]\n",
            "[0.9151322  0.08486774]\n",
            "[0.97305244 0.02694753]\n",
            "[0.97305244 0.02694753]\n",
            "[0.04110794 0.95889205]\n",
            "[0.940213   0.05978692]\n",
            "[0.9644011  0.03559885]\n",
            "[0.9124605  0.08753949]\n",
            "[0.9676555  0.03234456]\n",
            "[0.97713375 0.02286623]\n",
            "[0.97984225 0.02015775]\n",
            "[0.97878975 0.02121019]\n",
            "[0.8651082  0.13489187]\n",
            "[0.9639453  0.03605469]\n",
            "[0.9743625  0.02563757]\n",
            "[0.96024346 0.0397566 ]\n",
            "[0.9605871  0.03941296]\n",
            "[0.9221795  0.07782051]\n",
            "[0.96289307 0.03710688]\n",
            "[0.97973675 0.02026322]\n",
            "[0.9774455  0.02255459]\n",
            "[0.95866466 0.04133536]\n",
            "[0.96354717 0.03645286]\n",
            "[0.82158905 0.17841099]\n",
            "[0.9741985  0.02580149]\n",
            "[0.94222325 0.05777671]\n",
            "[0.657925   0.34207502]\n",
            "[0.9715617  0.02843841]\n",
            "[0.97974646 0.02025356]\n",
            "[0.9187366  0.08126338]\n",
            "[0.58777404 0.4122259 ]\n",
            "[0.9117889  0.08821113]\n",
            "[0.97651845 0.02348153]\n",
            "[0.12668955 0.87331045]\n",
            "[0.38550636 0.61449367]\n",
            "[0.9696531  0.03034682]\n",
            "[0.9748994  0.02510054]\n",
            "[0.9748994  0.02510054]\n",
            "[0.16412552 0.8358745 ]\n",
            "[0.2864037  0.71359634]\n",
            "[0.9779112  0.02208889]\n",
            "[0.95731896 0.04268101]\n",
            "[0.9523583  0.04764167]\n",
            "[0.96437806 0.03562195]\n",
            "[0.84648657 0.15351345]\n",
            "[0.2009328 0.7990672]\n",
            "[0.4674821  0.53251785]\n",
            "[0.9777915  0.02220849]\n",
            "[0.05335011 0.94664985]\n",
            "[0.9720981  0.02790185]\n",
            "[0.5998511 0.4001489]\n",
            "[0.97772205 0.02227787]\n",
            "[0.97772205 0.02227787]\n",
            "[0.67959577 0.3204042 ]\n",
            "[0.96133035 0.03866965]\n",
            "[0.9738608  0.02613922]\n",
            "[0.9778806  0.02211943]\n",
            "[0.0401541 0.9598459]\n",
            "[0.9388252  0.06117482]\n",
            "[0.9781844  0.02181565]\n",
            "[0.97735256 0.02264751]\n",
            "[0.883582   0.11641806]\n",
            "[0.8980144  0.10198551]\n",
            "[0.9503587  0.04964127]\n",
            "[0.65200704 0.3479929 ]\n",
            "[0.97385275 0.02614722]\n",
            "[0.9808384  0.01916157]\n",
            "[0.9739448  0.02605518]\n",
            "[0.06452046 0.9354796 ]\n",
            "[0.9759226  0.02407739]\n",
            "[0.12555392 0.87444603]\n",
            "[0.81900877 0.18099126]\n",
            "[0.9577673  0.04223273]\n",
            "[0.9412312  0.05876885]\n",
            "[0.06560836 0.9343916 ]\n",
            "[0.9803303  0.01966966]\n",
            "[0.0938945 0.9061055]\n",
            "[0.8138737  0.18612626]\n",
            "[0.96191406 0.03808592]\n",
            "[0.95547175 0.04452822]\n",
            "[0.58584225 0.41415775]\n",
            "[0.9715433  0.02845664]\n",
            "[0.9784622  0.02153784]\n",
            "[0.97661656 0.02338347]\n",
            "[0.9524897  0.04751036]\n",
            "[0.19140552 0.80859447]\n",
            "[0.9240448  0.07595518]\n",
            "[0.89377546 0.10622453]\n",
            "[0.04531922 0.95468074]\n",
            "[0.87420654 0.12579349]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/task1tamil-results.txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print('True positives: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "outputId": "f89df5dd-26ed-461a-a76a-0c012fd2ed56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI2K1bqaR5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "outputId": "753e3afb-eb3f-4255-acab-24ebd4c6230e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "outputId": "865e9397-f0f6-4eea-ae28-0c627996f35e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-task1tamil'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task1tamil\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task1tamil/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1tamil/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1tamil/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}
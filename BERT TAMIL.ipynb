{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e68b4ad4e4e14d92863ba9b21cb1adbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4aaeee6e18f84d1788b5d2253ce4b91a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4a51a695b4a04f5c89cd03ffce28742f",
              "IPY_MODEL_c857f3eae279405fab14857a8c2b34c3"
            ]
          }
        },
        "4aaeee6e18f84d1788b5d2253ce4b91a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a51a695b4a04f5c89cd03ffce28742f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_da75b6c9672340eba0668e2776c64a86",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db31b1cb2bad4c52b9cd6b63a14ad725"
          }
        },
        "c857f3eae279405fab14857a8c2b34c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5cae6dcdf0274b418d47fc1b9e418383",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 996k/996k [00:00&lt;00:00, 1.94MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b381b71030c40ca8cb640bff1d34eb9"
          }
        },
        "da75b6c9672340eba0668e2776c64a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db31b1cb2bad4c52b9cd6b63a14ad725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5cae6dcdf0274b418d47fc1b9e418383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b381b71030c40ca8cb640bff1d34eb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85c054fc52f14ad3803d065b2396d3bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_44191ea26f8848e0a89914bbe813658f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e14b279b0317482d92c8f668d584ebec",
              "IPY_MODEL_da61e8cc06e2484f8335e7540f917227"
            ]
          }
        },
        "44191ea26f8848e0a89914bbe813658f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e14b279b0317482d92c8f668d584ebec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cdded25c56494ae8b0c17525456567fc",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 569,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 569,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_53f58451e784491f85012f3434afab2a"
          }
        },
        "da61e8cc06e2484f8335e7540f917227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5fb5398fe58e4b1886abfc9cfdd79e46",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 569/569 [00:00&lt;00:00, 19.1kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d9dd959b0d96474283b7fb18130dee79"
          }
        },
        "cdded25c56494ae8b0c17525456567fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "53f58451e784491f85012f3434afab2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5fb5398fe58e4b1886abfc9cfdd79e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d9dd959b0d96474283b7fb18130dee79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ed84523d05a4769ad793bb309543331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8d6ae1395f604a10870bdd182b10e78c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5d7f32c68f1c41b0b1d1c830e6e5059a",
              "IPY_MODEL_1b05cda387a64efab268c9024e9b7077"
            ]
          }
        },
        "8d6ae1395f604a10870bdd182b10e78c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d7f32c68f1c41b0b1d1c830e6e5059a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_77199ab5d3af4da0b90b34500924e5d2",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b3c1a6bf69d4ed69bac363b4d69f7c2"
          }
        },
        "1b05cda387a64efab268c9024e9b7077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_25abbf6b866049b58e1ec845520db9df",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 714M/714M [00:25&lt;00:00, 27.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5732715b87d74449b18af8344e8f8a09"
          }
        },
        "77199ab5d3af4da0b90b34500924e5d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b3c1a6bf69d4ed69bac363b4d69f7c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25abbf6b866049b58e1ec845520db9df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5732715b87d74449b18af8344e8f8a09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT%20TAMIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "3f92d1c1-29b6-469e-deca-2e2cc6644a9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 7.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 69.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 58.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 54.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=9725467afc67d97a019b03adca1d62897951027ea9a466b2f5e0384794af4252\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "4211165e-3b63-4605-df93-48da0ae07cac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "3ebf0c2d-f598-41fe-8fda-b973c5e6f355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/tamil/task1tamil.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>459</th>\n",
              "      <td>TAM0460</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>தென் அமெரிக்க நாடுகளில் மீட்கப்பட்ட 33 சர்க்கஸ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2403</th>\n",
              "      <td>TAM2404</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>டில்லி முதல்வர் கெஜ்ரிவாலுக்கு, பெங்களூருவில் ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018</th>\n",
              "      <td>TAM2019</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>500 டாஸ்மாக் மதுக்கடைகள் மூடப்பட்டதற்கு, முதல்...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>TAM0093</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>சந்திரபாபு நாயுடுவுக்கு எதிராக ஜனாதிபதியிடம் ப...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>TAM1457</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>அதற்கு பருவ நிலை மாற்றம் காரணமாக வடபுலத்தில் உ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1345</th>\n",
              "      <td>TAM1346</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>தாராபுரத்தில் 7 பேரை திருமணம் செய்து நகை-பணத்த...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1807</th>\n",
              "      <td>TAM1808</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>இந்த விமர்சனங்களை தொடர்ந்து பஞ்சாப் மாநில தேர்...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855</th>\n",
              "      <td>TAM0856</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>நடிகர் வாகை சந்திரசேகர் வேளச்சேரி தொகுதியில் வ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>TAM0119</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>சென்னை கத்திப்பாரா மேம்பாலத்தில் திடீரென தீப்ப...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1611</th>\n",
              "      <td>TAM1612</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>அதிகமான இணையதளங்கள் மும்பையை அடிப்படையாக கொண்ட...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "459          TAM0460  ...  தென் அமெரிக்க நாடுகளில் மீட்கப்பட்ட 33 சர்க்கஸ...\n",
              "2403         TAM2404  ...  டில்லி முதல்வர் கெஜ்ரிவாலுக்கு, பெங்களூருவில் ...\n",
              "2018         TAM2019  ...  500 டாஸ்மாக் மதுக்கடைகள் மூடப்பட்டதற்கு, முதல்...\n",
              "92           TAM0093  ...  சந்திரபாபு நாயுடுவுக்கு எதிராக ஜனாதிபதியிடம் ப...\n",
              "1456         TAM1457  ...  அதற்கு பருவ நிலை மாற்றம் காரணமாக வடபுலத்தில் உ...\n",
              "1345         TAM1346  ...  தாராபுரத்தில் 7 பேரை திருமணம் செய்து நகை-பணத்த...\n",
              "1807         TAM1808  ...  இந்த விமர்சனங்களை தொடர்ந்து பஞ்சாப் மாநில தேர்...\n",
              "855          TAM0856  ...  நடிகர் வாகை சந்திரசேகர் வேளச்சேரி தொகுதியில் வ...\n",
              "118          TAM0119  ...  சென்னை கத்திப்பாரா மேம்பாலத்தில் திடீரென தீப்ப...\n",
              "1611         TAM1612  ...  அதிகமான இணையதளங்கள் மும்பையை அடிப்படையாக கொண்ட...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "1afde33d-5bb9-409c-be4e-a42ab0310345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1547</th>\n",
              "      <td>சிவாஜி கணேசனுடன் இணைந்து அதிக படங்களை இயக்கிய ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1788</th>\n",
              "      <td>பாதுகாப்பு படை தரப்பில் ஒரு வீரர் பலியானார்.&lt;e...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1863</th>\n",
              "      <td>வேலைக்காக வரும் வெளி மாநிலத்தவர்கள் நமக்கே வேட...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2120</th>\n",
              "      <td>சண்டையில் காயமடைந்த குலாம் பைசுல்லா பாஹிம் ஆஸ்...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1027</th>\n",
              "      <td>சாம்பியன்ஸ் கோப்பை ஆக்கி போட்டியில் பெல்ஜியம் ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1547  சிவாஜி கணேசனுடன் இணைந்து அதிக படங்களை இயக்கிய ...      0\n",
              "1788  பாதுகாப்பு படை தரப்பில் ஒரு வீரர் பலியானார்.<e...      0\n",
              "1863  வேலைக்காக வரும் வெளி மாநிலத்தவர்கள் நமக்கே வேட...      0\n",
              "2120  சண்டையில் காயமடைந்த குலாம் பைசுல்லா பாஹிம் ஆஸ்...      0\n",
              "1027  சாம்பியன்ஸ் கோப்பை ஆக்கி போட்டியில் பெல்ஜியம் ...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "653fa15d-d541-4759-fd9e-b82d4510f20a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "e68b4ad4e4e14d92863ba9b21cb1adbc",
            "4aaeee6e18f84d1788b5d2253ce4b91a",
            "4a51a695b4a04f5c89cd03ffce28742f",
            "c857f3eae279405fab14857a8c2b34c3",
            "da75b6c9672340eba0668e2776c64a86",
            "db31b1cb2bad4c52b9cd6b63a14ad725",
            "5cae6dcdf0274b418d47fc1b9e418383",
            "0b381b71030c40ca8cb640bff1d34eb9"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e68b4ad4e4e14d92863ba9b21cb1adbc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=995526, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "78527dd8-b011-4018-b2bf-e49080d2582b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Tokenized:  ['ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'பி', '##ர', '##ச', '##ார', '##ம', 'ச', '##ெ', '##ய', '##தா', '##ர', '.', '<', 'eo', '##l', '>', 'தி', '.', 'மு', '.', 'க', '.', ',', 'வ', '##ே', '##ட', '##பா', '##ள', '##ர', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'சி', '##ன', '##ன', 'ச', '##ே', '##ல', '##ம', 'பகுதி', '##ய', '##ில', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'ஓ', '##ட', '##டு', 'ச', '##ே', '##க', '##ரி', '##த', '##தா', '##ர', '.']\n",
            "Token IDs:  [1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "1ca464f2-34fe-44ad-ab27-398d764ae32b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Token IDs: [101, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "825b4eb6-93d0-44e9-88db-256e2f964f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "5712bc2f-8714-4274-9c29-8291441f0a2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "be24f3cf-5d48-492b-923e-d3dce0fcfbe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "print(train_inputs)\n",
        "print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "#print(train_masks)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101   1154  35186 ...  55186  95512  31484]\n",
            " [   101   1170 111305 ...   1154 111312  15220]\n",
            " [   101   1163  14124 ...  28065    119    133]\n",
            " ...\n",
            " [   101   1163  27883 ...  27885   1165  18827]\n",
            " [   101  49189  66171 ...  86728  81773  20242]\n",
            " [   101   1142  46168 ...  37076  24171  19894]]\n",
            "[1 0 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "3be699ee-ffcd-45b3-cb91-bed088d3c517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "85c054fc52f14ad3803d065b2396d3bd",
            "44191ea26f8848e0a89914bbe813658f",
            "e14b279b0317482d92c8f668d584ebec",
            "da61e8cc06e2484f8335e7540f917227",
            "cdded25c56494ae8b0c17525456567fc",
            "53f58451e784491f85012f3434afab2a",
            "5fb5398fe58e4b1886abfc9cfdd79e46",
            "d9dd959b0d96474283b7fb18130dee79",
            "2ed84523d05a4769ad793bb309543331",
            "8d6ae1395f604a10870bdd182b10e78c",
            "5d7f32c68f1c41b0b1d1c830e6e5059a",
            "1b05cda387a64efab268c9024e9b7077",
            "77199ab5d3af4da0b90b34500924e5d2",
            "9b3c1a6bf69d4ed69bac363b4d69f7c2",
            "25abbf6b866049b58e1ec845520db9df",
            "5732715b87d74449b18af8344e8f8a09"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85c054fc52f14ad3803d065b2396d3bd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=569, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ed84523d05a4769ad793bb309543331",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=714314041, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "outputId": "62a5dd39-738b-49d3-f79d-e6d8e513dc40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 50\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "outputId": "a3e04e02-3609-4fbe-9318-37e989e35a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.88\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 9 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 10 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 11 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 12 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 13 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 14 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 15 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 16 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 17 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 18 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 19 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 20 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 21 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 22 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.88\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 23 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 24 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 25 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 26 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 27 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.88\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 28 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 29 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 30 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 31 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 32 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 33 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 34 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 35 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 36 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 37 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 38 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 39 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 40 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 41 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 42 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 43 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 44 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 45 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 46 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 47 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 48 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 49 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 50 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "outputId": "676a8345-7a43-424e-8e11-d7de96661c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task1tamil-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "outputId": "2c5c8c3b-72cc-407b-9798-fb6ce4413c2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "#print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 900 test sentences...\n",
            "    DONE.\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "outputId": "827b4350-f760-4a81-fe68-4f585bd007a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] ):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] ):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    # elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "    #   #print(\" Semi Paraphrase     \",end=\"\")\n",
        "    #   #count_NP+=1      \n",
        "    #   print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "    #   x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(1)\t 1\t [ 0.29213414 -0.1094121 ] \t0\tfalse\n",
            "(1)\t 2\t [ 5.472153  -5.5735545] \t0\tfalse\n",
            "(1)\t 3\t [ 4.878096  -5.2324095] \t0\tfalse\n",
            "(1)\t 4\t [ 5.467407 -5.563573] \t0\tfalse\n",
            "(0)\t 5\t [ 5.4684734 -5.610629 ] \t0\ttrue\n",
            "(1)\t 6\t [ 5.322789  -5.4889474] \t0\tfalse\n",
            "(0)\t 7\t [ 5.2753177 -5.4057326] \t0\ttrue\n",
            "(0)\t 8\t [ 5.478846 -5.534384] \t0\ttrue\n",
            "(0)\t 9\t [ 5.450144 -5.620637] \t0\ttrue\n",
            "(0)\t 10\t [ 5.479233 -5.585036] \t0\ttrue\n",
            "(1)\t 11\t [ 5.4840226 -5.497156 ] \t0\tfalse\n",
            "(0)\t 12\t [ 5.481389 -5.607583] \t0\ttrue\n",
            "(0)\t 13\t [ 5.405538  -5.5279117] \t0\ttrue\n",
            "(0)\t 14\t [ 5.4663806 -5.6254787] \t0\ttrue\n",
            "(0)\t 15\t [ 5.4927073 -5.5620923] \t0\ttrue\n",
            "(0)\t 16\t [ 5.28597   -5.4050574] \t0\ttrue\n",
            "(0)\t 17\t [ 5.478172  -5.5964794] \t0\ttrue\n",
            "(0)\t 18\t [-3.5100923  3.9205692] \t1\tfalse\n",
            "(0)\t 19\t [ 5.4645805 -5.5478477] \t0\ttrue\n",
            "(0)\t 20\t [ 5.486712 -5.546144] \t0\ttrue\n",
            "(0)\t 21\t [ 5.400062 -5.567812] \t0\ttrue\n",
            "(1)\t 22\t [-5.2721004  5.4726624] \t1\ttrue\n",
            "(0)\t 23\t [ 5.460932  -5.6244226] \t0\ttrue\n",
            "(0)\t 24\t [ 4.918118  -5.2324815] \t0\ttrue\n",
            "(0)\t 25\t [ 5.4749484 -5.6035385] \t0\ttrue\n",
            "(0)\t 26\t [ 5.448798 -5.611622] \t0\ttrue\n",
            "(0)\t 27\t [ 5.456232  -5.5709534] \t0\ttrue\n",
            "(0)\t 28\t [ 5.125443 -5.176841] \t0\ttrue\n",
            "(0)\t 29\t [ 5.4711227 -5.620798 ] \t0\ttrue\n",
            "(0)\t 30\t [ 4.9825044 -5.3107967] \t0\ttrue\n",
            "(0)\t 31\t [-4.6097856  4.9419928] \t1\tfalse\n",
            "(0)\t 32\t [ 5.473765  -5.5109935] \t0\ttrue\n",
            "(0)\t 33\t [ 5.433628 -5.564899] \t0\ttrue\n",
            "(1)\t 34\t [ 5.4571295 -5.608154 ] \t0\tfalse\n",
            "(1)\t 35\t [ 5.4571295 -5.608154 ] \t0\tfalse\n",
            "(1)\t 36\t [ 5.4468975 -5.567585 ] \t0\tfalse\n",
            "(0)\t 37\t [-5.169804  4.963082] \t1\tfalse\n",
            "(1)\t 38\t [ 5.4877944 -5.535597 ] \t0\tfalse\n",
            "(0)\t 39\t [ 5.431172 -5.550445] \t0\ttrue\n",
            "(0)\t 40\t [ 5.3249855 -5.5214815] \t0\ttrue\n",
            "(1)\t 41\t [ 5.4005985 -5.5466313] \t0\tfalse\n",
            "(0)\t 42\t [ 5.2987895 -5.505356 ] \t0\ttrue\n",
            "(1)\t 43\t [ 4.485349  -4.6923714] \t0\tfalse\n",
            "(1)\t 44\t [ 5.360449 -5.526928] \t0\tfalse\n",
            "(1)\t 45\t [ 5.360449 -5.526928] \t0\tfalse\n",
            "(0)\t 46\t [ 5.338529  -5.4570246] \t0\ttrue\n",
            "(0)\t 47\t [ 5.473708  -5.5858545] \t0\ttrue\n",
            "(1)\t 48\t [ 5.4728227 -5.6092033] \t0\tfalse\n",
            "(0)\t 49\t [-4.5184293  4.8640585] \t1\tfalse\n",
            "(0)\t 50\t [ 5.475637 -5.594688] \t0\ttrue\n",
            "(0)\t 51\t [-3.9910152  4.4817867] \t1\tfalse\n",
            "(0)\t 52\t [ 5.1757126 -5.430158 ] \t0\ttrue\n",
            "(0)\t 53\t [ 5.4423785 -5.6192346] \t0\ttrue\n",
            "(0)\t 54\t [ 5.4742928 -5.5213456] \t0\ttrue\n",
            "(0)\t 55\t [ 5.4415936 -5.6269956] \t0\ttrue\n",
            "(1)\t 56\t [ 5.472286  -5.5242157] \t0\tfalse\n",
            "(1)\t 57\t [-5.350074   5.4560947] \t1\ttrue\n",
            "(0)\t 58\t [ 4.8251224 -5.132484 ] \t0\ttrue\n",
            "(0)\t 59\t [ 5.478017 -5.603001] \t0\ttrue\n",
            "(1)\t 60\t [ 5.270699 -5.480209] \t0\tfalse\n",
            "(0)\t 61\t [ 4.5840545 -4.9830356] \t0\ttrue\n",
            "(1)\t 62\t [ 5.4522533 -5.4947567] \t0\tfalse\n",
            "(1)\t 63\t [-5.121905   5.4215064] \t1\ttrue\n",
            "(1)\t 64\t [ 5.132965  -5.2799983] \t0\tfalse\n",
            "(1)\t 65\t [ 5.132965  -5.2799983] \t0\tfalse\n",
            "(0)\t 66\t [ 5.4949307 -5.577591 ] \t0\ttrue\n",
            "(0)\t 67\t [ 5.218665  -5.4857054] \t0\ttrue\n",
            "(0)\t 68\t [ 5.4748983 -5.5962615] \t0\ttrue\n",
            "(0)\t 69\t [ 5.449655 -5.562261] \t0\ttrue\n",
            "(0)\t 70\t [ 5.4762297 -5.608289 ] \t0\ttrue\n",
            "(0)\t 71\t [ 5.46787  -5.585554] \t0\ttrue\n",
            "(1)\t 72\t [-5.303807  5.445087] \t1\ttrue\n",
            "(1)\t 73\t [-5.303807  5.445087] \t1\ttrue\n",
            "(1)\t 74\t [-5.2794595  5.42561  ] \t1\ttrue\n",
            "(1)\t 75\t [-5.2794595  5.42561  ] \t1\ttrue\n",
            "(1)\t 76\t [ 5.4912786 -5.592749 ] \t0\tfalse\n",
            "(1)\t 77\t [ 5.4912786 -5.592749 ] \t0\tfalse\n",
            "(1)\t 78\t [-4.7318854  5.1461473] \t1\ttrue\n",
            "(0)\t 79\t [ 5.4691086 -5.6074586] \t0\ttrue\n",
            "(0)\t 80\t [ 4.566957 -4.535769] \t0\ttrue\n",
            "(0)\t 81\t [ 5.4889765 -5.5889072] \t0\ttrue\n",
            "(1)\t 82\t [-5.1727314  5.361694 ] \t1\ttrue\n",
            "(0)\t 83\t [ 5.4372687 -5.640961 ] \t0\ttrue\n",
            "(0)\t 84\t [ 5.5052466 -5.5511374] \t0\ttrue\n",
            "(1)\t 85\t [ 5.475318 -5.537289] \t0\tfalse\n",
            "(1)\t 86\t [ 5.2788997 -5.4887366] \t0\tfalse\n",
            "(1)\t 87\t [ 5.476299 -5.620879] \t0\tfalse\n",
            "(0)\t 88\t [ 5.4945216 -5.541055 ] \t0\ttrue\n",
            "(0)\t 89\t [ 5.4973264 -5.5455837] \t0\ttrue\n",
            "(1)\t 90\t [-5.291733   5.4505124] \t1\ttrue\n",
            "(1)\t 91\t [-5.32986    5.4476104] \t1\ttrue\n",
            "(1)\t 92\t [-3.6244743  3.6620598] \t1\ttrue\n",
            "(0)\t 93\t [ 4.869281  -5.2533617] \t0\ttrue\n",
            "(0)\t 94\t [ 5.359375 -5.504721] \t0\ttrue\n",
            "(0)\t 95\t [ 5.4882655 -5.5542855] \t0\ttrue\n",
            "(0)\t 96\t [ 5.317415  -5.4083495] \t0\ttrue\n",
            "(0)\t 97\t [ 5.283937  -5.5418844] \t0\ttrue\n",
            "(0)\t 98\t [ 5.4185414 -5.638993 ] \t0\ttrue\n",
            "(0)\t 99\t [ 5.484725  -5.5921535] \t0\ttrue\n",
            "(0)\t 100\t [ 5.47604   -5.5492487] \t0\ttrue\n",
            "(1)\t 101\t [-5.262821   5.4278483] \t1\ttrue\n",
            "(1)\t 102\t [ 5.4996767 -5.579901 ] \t0\tfalse\n",
            "(1)\t 103\t [ 5.404163 -5.642516] \t0\tfalse\n",
            "(1)\t 104\t [ 5.4691997 -5.623403 ] \t0\tfalse\n",
            "(0)\t 105\t [ 5.4224687 -5.5168724] \t0\ttrue\n",
            "(1)\t 106\t [ 2.8358243 -2.9304602] \t0\tfalse\n",
            "(0)\t 107\t [ 5.4705343 -5.5642347] \t0\ttrue\n",
            "(0)\t 108\t [ 5.4817147 -5.5565386] \t0\ttrue\n",
            "(0)\t 109\t [ 5.465339  -5.6207676] \t0\ttrue\n",
            "(0)\t 110\t [ 5.477816  -5.5713673] \t0\ttrue\n",
            "(0)\t 111\t [ 5.445799 -5.468022] \t0\ttrue\n",
            "(1)\t 112\t [-1.3527919  1.7387449] \t1\ttrue\n",
            "(1)\t 113\t [ 5.454954  -5.6397767] \t0\tfalse\n",
            "(1)\t 114\t [ 5.4717426 -5.520717 ] \t0\tfalse\n",
            "(1)\t 115\t [ 5.399243  -5.4658675] \t0\tfalse\n",
            "(0)\t 116\t [ 5.4910417 -5.6025414] \t0\ttrue\n",
            "(0)\t 117\t [ 5.2480392 -5.3665495] \t0\ttrue\n",
            "(0)\t 118\t [ 5.3897877 -5.587053 ] \t0\ttrue\n",
            "(0)\t 119\t [ 5.434431  -5.6315837] \t0\ttrue\n",
            "(0)\t 120\t [ 5.4437995 -5.560229 ] \t0\ttrue\n",
            "(0)\t 121\t [ 0.4102946 -0.727004 ] \t0\ttrue\n",
            "(0)\t 122\t [ 5.458722 -5.545236] \t0\ttrue\n",
            "(1)\t 123\t [ 4.997654 -5.238091] \t0\tfalse\n",
            "(0)\t 124\t [ 5.450936 -5.62845 ] \t0\ttrue\n",
            "(0)\t 125\t [ 5.463709 -5.628599] \t0\ttrue\n",
            "(0)\t 126\t [ 5.4333215 -5.5720906] \t0\ttrue\n",
            "(1)\t 127\t [ 5.481609 -5.606424] \t0\tfalse\n",
            "(1)\t 128\t [ 5.481609 -5.606424] \t0\tfalse\n",
            "(0)\t 129\t [ 5.472362  -5.5745716] \t0\ttrue\n",
            "(0)\t 130\t [ 5.4905524 -5.584253 ] \t0\ttrue\n",
            "(0)\t 131\t [ 5.45062  -5.537111] \t0\ttrue\n",
            "(0)\t 132\t [ 5.485164  -5.6040025] \t0\ttrue\n",
            "(1)\t 133\t [ 5.4940076 -5.5849433] \t0\tfalse\n",
            "(0)\t 134\t [ 5.5090923 -5.5646915] \t0\ttrue\n",
            "(0)\t 135\t [ 5.4879174 -5.6028805] \t0\ttrue\n",
            "(0)\t 136\t [ 3.320713  -3.7809832] \t0\ttrue\n",
            "(0)\t 137\t [ 3.317275 -3.525826] \t0\ttrue\n",
            "(0)\t 138\t [ 5.4588747 -5.5175457] \t0\ttrue\n",
            "(0)\t 139\t [ 5.4783983 -5.594447 ] \t0\ttrue\n",
            "(0)\t 140\t [ 5.481067 -5.584432] \t0\ttrue\n",
            "(0)\t 141\t [ 5.4664392 -5.620749 ] \t0\ttrue\n",
            "(0)\t 142\t [-4.4703097  4.9224873] \t1\tfalse\n",
            "(1)\t 143\t [ 5.4965343 -5.5917883] \t0\tfalse\n",
            "(1)\t 144\t [ 5.19014  -5.436265] \t0\tfalse\n",
            "(1)\t 145\t [-4.1208797  4.4351525] \t1\ttrue\n",
            "(0)\t 146\t [ 5.421517  -5.5373836] \t0\ttrue\n",
            "(0)\t 147\t [-5.2587104  5.3744965] \t1\tfalse\n",
            "(1)\t 148\t [ 5.5011687 -5.5855813] \t0\tfalse\n",
            "(1)\t 149\t [ 5.5011687 -5.5855813] \t0\tfalse\n",
            "(1)\t 150\t [ 5.4300246 -5.507746 ] \t0\tfalse\n",
            "(1)\t 151\t [ 5.4300246 -5.507746 ] \t0\tfalse\n",
            "(1)\t 152\t [ 5.4459825 -5.5381994] \t0\tfalse\n",
            "(1)\t 153\t [ 5.4459825 -5.5381994] \t0\tfalse\n",
            "(0)\t 154\t [ 5.4934363 -5.5966206] \t0\ttrue\n",
            "(1)\t 155\t [-5.012497  5.296089] \t1\ttrue\n",
            "(1)\t 156\t [-5.012497  5.296089] \t1\ttrue\n",
            "(1)\t 157\t [ 5.455476 -5.625973] \t0\tfalse\n",
            "(0)\t 158\t [ 5.2355347 -5.450584 ] \t0\ttrue\n",
            "(0)\t 159\t [ 5.4892964 -5.5951395] \t0\ttrue\n",
            "(1)\t 160\t [ 5.4809527 -5.593256 ] \t0\tfalse\n",
            "(0)\t 161\t [ 5.2521067 -5.436406 ] \t0\ttrue\n",
            "(1)\t 162\t [ 5.1419992 -5.316804 ] \t0\tfalse\n",
            "(0)\t 163\t [ 5.4959946 -5.5694356] \t0\ttrue\n",
            "(0)\t 164\t [-5.214639  5.430287] \t1\tfalse\n",
            "(0)\t 165\t [ 5.466279 -5.519064] \t0\ttrue\n",
            "(0)\t 166\t [ 5.22161   -5.2775984] \t0\ttrue\n",
            "(0)\t 167\t [ 5.4485326 -5.631654 ] \t0\ttrue\n",
            "(1)\t 168\t [ 5.397389 -5.535618] \t0\tfalse\n",
            "(1)\t 169\t [ 5.4286323 -5.540054 ] \t0\tfalse\n",
            "(1)\t 170\t [ 5.4286323 -5.540054 ] \t0\tfalse\n",
            "(0)\t 171\t [-5.030226  5.393443] \t1\tfalse\n",
            "(1)\t 172\t [ 3.6989834 -4.1606765] \t0\tfalse\n",
            "(1)\t 173\t [ 5.4542933 -5.5705953] \t0\tfalse\n",
            "(1)\t 174\t [ 5.4931197 -5.6006255] \t0\tfalse\n",
            "(0)\t 175\t [-5.335137   5.3505645] \t1\tfalse\n",
            "(1)\t 176\t [ 5.4738574 -5.598843 ] \t0\tfalse\n",
            "(0)\t 177\t [ 4.380004  -4.5633903] \t0\ttrue\n",
            "(0)\t 178\t [ 5.466418  -5.6133494] \t0\ttrue\n",
            "(0)\t 179\t [ 5.3879375 -5.5244274] \t0\ttrue\n",
            "(0)\t 180\t [ 5.488563 -5.606028] \t0\ttrue\n",
            "(0)\t 181\t [ 5.447083  -5.5202727] \t0\ttrue\n",
            "(0)\t 182\t [ 5.362198 -5.339603] \t0\ttrue\n",
            "(0)\t 183\t [ 5.4871354 -5.5050483] \t0\ttrue\n",
            "(0)\t 184\t [ 3.9337387 -3.6722507] \t0\ttrue\n",
            "(0)\t 185\t [ 5.4733577 -5.6183333] \t0\ttrue\n",
            "(0)\t 186\t [ 5.4952507 -5.580959 ] \t0\ttrue\n",
            "(1)\t 187\t [ 5.3916807 -5.5113025] \t0\tfalse\n",
            "(1)\t 188\t [-5.3405085  5.467969 ] \t1\ttrue\n",
            "(0)\t 189\t [ 5.4443483 -5.5653353] \t0\ttrue\n",
            "(1)\t 190\t [ 4.7908015 -4.682634 ] \t0\tfalse\n",
            "(0)\t 191\t [ 5.4441147 -5.549104 ] \t0\ttrue\n",
            "(0)\t 192\t [ 5.472741  -5.5575323] \t0\ttrue\n",
            "(1)\t 193\t [ 3.4552238 -3.7249222] \t0\tfalse\n",
            "(1)\t 194\t [ 5.45598   -5.5784373] \t0\tfalse\n",
            "(0)\t 195\t [ 4.2201037 -4.5099745] \t0\ttrue\n",
            "(1)\t 196\t [ 5.1248183 -5.2876825] \t0\tfalse\n",
            "(1)\t 197\t [ 5.498925 -5.570592] \t0\tfalse\n",
            "(1)\t 198\t [-4.208548   4.6459455] \t1\ttrue\n",
            "(1)\t 199\t [ 5.4850774 -5.5422254] \t0\tfalse\n",
            "(1)\t 200\t [ 5.4850774 -5.5422254] \t0\tfalse\n",
            "(0)\t 201\t [ 5.429066  -5.5441093] \t0\ttrue\n",
            "(0)\t 202\t [-1.7695367  2.1909273] \t1\tfalse\n",
            "(1)\t 203\t [ 5.469264 -5.622257] \t0\tfalse\n",
            "(1)\t 204\t [ 5.469264 -5.622257] \t0\tfalse\n",
            "(1)\t 205\t [ 5.1642957 -5.30206  ] \t0\tfalse\n",
            "(1)\t 206\t [ 5.1210756 -5.3502345] \t0\tfalse\n",
            "(0)\t 207\t [ 4.736202  -5.2168536] \t0\ttrue\n",
            "(0)\t 208\t [ 5.2423654 -5.252266 ] \t0\ttrue\n",
            "(1)\t 209\t [ 5.477444  -5.5682206] \t0\tfalse\n",
            "(0)\t 210\t [ 5.4425373 -5.591022 ] \t0\ttrue\n",
            "(1)\t 211\t [ 5.385961 -5.57135 ] \t0\tfalse\n",
            "(0)\t 212\t [ 5.2885766 -5.4810066] \t0\ttrue\n",
            "(1)\t 213\t [ 5.4068813 -5.5730033] \t0\tfalse\n",
            "(0)\t 214\t [ 5.5013094 -5.589818 ] \t0\ttrue\n",
            "(1)\t 215\t [ 5.4333534 -5.635033 ] \t0\tfalse\n",
            "(1)\t 216\t [ 5.4836736 -5.5913796] \t0\tfalse\n",
            "(0)\t 217\t [-1.5421239  2.3667529] \t1\tfalse\n",
            "(1)\t 218\t [ 5.215855 -5.434365] \t0\tfalse\n",
            "(0)\t 219\t [ 2.9282246 -3.1752565] \t0\ttrue\n",
            "(1)\t 220\t [ 0.82323855 -0.9286204 ] \t0\tfalse\n",
            "(1)\t 221\t [ 5.4908814 -5.5354424] \t0\tfalse\n",
            "(1)\t 222\t [ 5.4908814 -5.5354424] \t0\tfalse\n",
            "(1)\t 223\t [-4.738497   5.1454587] \t1\ttrue\n",
            "(0)\t 224\t [ 5.2765083 -5.414402 ] \t0\ttrue\n",
            "(0)\t 225\t [ 4.905075 -5.258992] \t0\ttrue\n",
            "(0)\t 226\t [-5.0490346  5.3861513] \t1\tfalse\n",
            "(1)\t 227\t [ 5.494284  -5.5863624] \t0\tfalse\n",
            "(1)\t 228\t [ 5.494284  -5.5863624] \t0\tfalse\n",
            "(1)\t 229\t [ 4.900851  -5.2108865] \t0\tfalse\n",
            "(0)\t 230\t [ 5.4666657 -5.6264243] \t0\ttrue\n",
            "(1)\t 231\t [ 5.2016134 -5.373581 ] \t0\tfalse\n",
            "(1)\t 232\t [ 5.2016134 -5.373581 ] \t0\tfalse\n",
            "(0)\t 233\t [ 5.420933 -5.502649] \t0\ttrue\n",
            "(1)\t 234\t [ 5.1840596 -5.466254 ] \t0\tfalse\n",
            "(0)\t 235\t [ 5.469443 -5.625391] \t0\ttrue\n",
            "(1)\t 236\t [ 5.475005  -5.6234403] \t0\tfalse\n",
            "(1)\t 237\t [-5.235926   5.4606028] \t1\ttrue\n",
            "(0)\t 238\t [ 5.4352646 -5.5555124] \t0\ttrue\n",
            "(1)\t 239\t [ 5.480739 -5.589316] \t0\tfalse\n",
            "(0)\t 240\t [ 3.1181939 -2.9370804] \t0\ttrue\n",
            "(0)\t 241\t [-5.020532   5.3788285] \t1\tfalse\n",
            "(1)\t 242\t [ 5.400959 -5.562337] \t0\tfalse\n",
            "(1)\t 243\t [ 5.4675364 -5.5657797] \t0\tfalse\n",
            "(0)\t 244\t [ 5.455108 -5.519953] \t0\ttrue\n",
            "(1)\t 245\t [ 5.4830637 -5.5996213] \t0\tfalse\n",
            "(1)\t 246\t [ 5.500893  -5.5810776] \t0\tfalse\n",
            "(1)\t 247\t [ 5.500893  -5.5810776] \t0\tfalse\n",
            "(0)\t 248\t [ 5.4777975 -5.5689125] \t0\ttrue\n",
            "(1)\t 249\t [ 5.492757  -5.5630317] \t0\tfalse\n",
            "(1)\t 250\t [ 5.492757  -5.5630317] \t0\tfalse\n",
            "(0)\t 251\t [ 5.436511 -5.540366] \t0\ttrue\n",
            "(1)\t 252\t [ 5.4195623 -5.605948 ] \t0\tfalse\n",
            "(1)\t 253\t [ 5.4747243 -5.599588 ] \t0\tfalse\n",
            "(1)\t 254\t [ 3.8137317 -4.2317567] \t0\tfalse\n",
            "(0)\t 255\t [ 5.4994016 -5.5524178] \t0\ttrue\n",
            "(0)\t 256\t [ 4.255402 -4.097602] \t0\ttrue\n",
            "(1)\t 257\t [-4.914879   5.3206444] \t1\ttrue\n",
            "(0)\t 258\t [-5.2568784  5.4796114] \t1\tfalse\n",
            "(0)\t 259\t [ 5.4432445 -5.530187 ] \t0\ttrue\n",
            "(0)\t 260\t [ 5.4762363 -5.6181507] \t0\ttrue\n",
            "(0)\t 261\t [ 5.468395  -5.6180463] \t0\ttrue\n",
            "(0)\t 262\t [ 5.4753923 -5.6109633] \t0\ttrue\n",
            "(0)\t 263\t [ 5.4430594 -5.5512323] \t0\ttrue\n",
            "(0)\t 264\t [ 5.506274 -5.576366] \t0\ttrue\n",
            "(0)\t 265\t [ 5.4909067 -5.57115  ] \t0\ttrue\n",
            "(1)\t 266\t [ 5.4886355 -5.6028576] \t0\tfalse\n",
            "(0)\t 267\t [ 5.479097  -5.5439596] \t0\ttrue\n",
            "(0)\t 268\t [-5.2534046  5.4706354] \t1\tfalse\n",
            "(0)\t 269\t [-4.3204374  4.7721744] \t1\tfalse\n",
            "(0)\t 270\t [ 5.174344 -5.418315] \t0\ttrue\n",
            "(1)\t 271\t [ 5.427008 -5.525632] \t0\tfalse\n",
            "(1)\t 272\t [ 3.6597903 -3.8960524] \t0\tfalse\n",
            "(1)\t 273\t [-4.3909    4.867438] \t1\ttrue\n",
            "(1)\t 274\t [ 5.301062  -5.5645404] \t0\tfalse\n",
            "(1)\t 275\t [ 5.4675574 -5.5114183] \t0\tfalse\n",
            "(1)\t 276\t [ 1.1921089 -1.1939663] \t0\tfalse\n",
            "(1)\t 277\t [ 5.459629  -5.6330266] \t0\tfalse\n",
            "(0)\t 278\t [ 5.4809527 -5.6059303] \t0\ttrue\n",
            "(0)\t 279\t [ 4.656503 -4.988994] \t0\ttrue\n",
            "(0)\t 280\t [ 5.474468  -5.6011567] \t0\ttrue\n",
            "(1)\t 281\t [-5.214341  5.479125] \t1\ttrue\n",
            "(0)\t 282\t [-2.9207256  3.3351038] \t1\tfalse\n",
            "(1)\t 283\t [-4.7317243  5.103178 ] \t1\ttrue\n",
            "(1)\t 284\t [-4.4645724  4.7880163] \t1\ttrue\n",
            "(1)\t 285\t [ 5.4697657 -5.533402 ] \t0\tfalse\n",
            "(1)\t 286\t [ 5.4697657 -5.533402 ] \t0\tfalse\n",
            "(1)\t 287\t [ 4.0593505 -4.5464735] \t0\tfalse\n",
            "(0)\t 288\t [ 5.4852552 -5.5801215] \t0\ttrue\n",
            "(0)\t 289\t [ 5.446729  -5.5498385] \t0\ttrue\n",
            "(1)\t 290\t [-4.87264    5.3033924] \t1\ttrue\n",
            "(1)\t 291\t [-5.17925    5.3944335] \t1\ttrue\n",
            "(1)\t 292\t [ 5.456727 -5.581151] \t0\tfalse\n",
            "(1)\t 293\t [ 5.268569  -5.4975996] \t0\tfalse\n",
            "(0)\t 294\t [ 5.4300623 -5.491773 ] \t0\ttrue\n",
            "(0)\t 295\t [ 5.449299  -5.6142535] \t0\ttrue\n",
            "(0)\t 296\t [ 3.397305  -3.8685122] \t0\ttrue\n",
            "(0)\t 297\t [-4.7946286  5.214152 ] \t1\tfalse\n",
            "(0)\t 298\t [ 5.406443  -5.5676274] \t0\ttrue\n",
            "(1)\t 299\t [ 1.67106   -1.8955642] \t0\tfalse\n",
            "(0)\t 300\t [ 5.430335  -5.5578575] \t0\ttrue\n",
            "(1)\t 301\t [-4.8122797  5.0933094] \t1\ttrue\n",
            "(1)\t 302\t [ 5.47842   -5.6220856] \t0\tfalse\n",
            "(1)\t 303\t [ 5.47842   -5.6220856] \t0\tfalse\n",
            "(1)\t 304\t [-5.2980523  5.4191175] \t1\ttrue\n",
            "(1)\t 305\t [-5.049499   5.4115243] \t1\ttrue\n",
            "(1)\t 306\t [-4.579675   5.0447273] \t1\ttrue\n",
            "(0)\t 307\t [ 5.4884233 -5.5792108] \t0\ttrue\n",
            "(1)\t 308\t [ 5.484977 -5.601855] \t0\tfalse\n",
            "(0)\t 309\t [ 5.462545  -5.6300225] \t0\ttrue\n",
            "(0)\t 310\t [ 5.3715873 -5.5352273] \t0\ttrue\n",
            "(0)\t 311\t [ 5.242289 -5.353569] \t0\ttrue\n",
            "(0)\t 312\t [ 5.369086  -5.5417256] \t0\ttrue\n",
            "(0)\t 313\t [ 5.4663734 -5.621687 ] \t0\ttrue\n",
            "(0)\t 314\t [ 5.2916927 -5.4600744] \t0\ttrue\n",
            "(0)\t 315\t [ 5.460536 -5.562077] \t0\ttrue\n",
            "(0)\t 316\t [ 5.464317  -5.5657263] \t0\ttrue\n",
            "(0)\t 317\t [ 5.479034 -5.53454 ] \t0\ttrue\n",
            "(1)\t 318\t [-5.0500717  5.355653 ] \t1\ttrue\n",
            "(0)\t 319\t [ 5.494722 -5.481717] \t0\ttrue\n",
            "(1)\t 320\t [ 5.455197 -5.629199] \t0\tfalse\n",
            "(1)\t 321\t [ 5.458787 -5.625864] \t0\tfalse\n",
            "(0)\t 322\t [ 4.72858   -5.0525656] \t0\ttrue\n",
            "(0)\t 323\t [ 5.4821506 -5.6018915] \t0\ttrue\n",
            "(0)\t 324\t [ 5.3776865 -5.4809804] \t0\ttrue\n",
            "(0)\t 325\t [ 5.4878793 -5.567639 ] \t0\ttrue\n",
            "(1)\t 326\t [ 5.4240522 -5.5208683] \t0\tfalse\n",
            "(0)\t 327\t [ 3.1471956 -3.1809235] \t0\ttrue\n",
            "(0)\t 328\t [ 5.4689198 -5.6081257] \t0\ttrue\n",
            "(0)\t 329\t [-5.168157  5.447404] \t1\tfalse\n",
            "(1)\t 330\t [-5.1711717  5.428338 ] \t1\ttrue\n",
            "(0)\t 331\t [ 5.3083873 -5.5002623] \t0\ttrue\n",
            "(1)\t 332\t [-5.194807  5.466359] \t1\ttrue\n",
            "(1)\t 333\t [-5.189277   5.2006583] \t1\ttrue\n",
            "(1)\t 334\t [ 5.37786   -5.5651016] \t0\tfalse\n",
            "(0)\t 335\t [-5.305956  5.469091] \t1\tfalse\n",
            "(0)\t 336\t [ 5.4685445 -5.6136527] \t0\ttrue\n",
            "(0)\t 337\t [ 5.4577007 -5.514701 ] \t0\ttrue\n",
            "(1)\t 338\t [ 5.116718  -5.4444656] \t0\tfalse\n",
            "(1)\t 339\t [-5.1689873  5.3549953] \t1\ttrue\n",
            "(1)\t 340\t [ 4.507528 -4.994928] \t0\tfalse\n",
            "(0)\t 341\t [ 3.9068983 -3.8881295] \t0\ttrue\n",
            "(1)\t 342\t [-4.952268   5.2497377] \t1\ttrue\n",
            "(1)\t 343\t [ 5.4660306 -5.5202055] \t0\tfalse\n",
            "(0)\t 344\t [-5.051571   5.2355113] \t1\tfalse\n",
            "(1)\t 345\t [ 0.35649288 -0.5017976 ] \t0\tfalse\n",
            "(1)\t 346\t [ 5.471542  -5.6280136] \t0\tfalse\n",
            "(0)\t 347\t [ 5.463027 -5.535661] \t0\ttrue\n",
            "(0)\t 348\t [-5.055256  5.369201] \t1\tfalse\n",
            "(0)\t 349\t [ 5.4336267 -5.542114 ] \t0\ttrue\n",
            "(0)\t 350\t [ 5.4562955 -5.5297713] \t0\ttrue\n",
            "(0)\t 351\t [-2.4046705  1.7009611] \t1\tfalse\n",
            "(0)\t 352\t [ 5.490775 -5.567216] \t0\ttrue\n",
            "(1)\t 353\t [-4.7442145  5.1684318] \t1\ttrue\n",
            "(0)\t 354\t [ 5.109713 -5.407966] \t0\ttrue\n",
            "(0)\t 355\t [ 5.454078  -5.6139393] \t0\ttrue\n",
            "(0)\t 356\t [ 5.4377303 -5.6349196] \t0\ttrue\n",
            "(0)\t 357\t [ 5.385605 -5.561841] \t0\ttrue\n",
            "(0)\t 358\t [ 5.4706798 -5.6210694] \t0\ttrue\n",
            "(0)\t 359\t [ 5.4496017 -5.6113443] \t0\ttrue\n",
            "(0)\t 360\t [ 3.9715445 -3.9926496] \t0\ttrue\n",
            "(0)\t 361\t [ 5.444511  -5.5480156] \t0\ttrue\n",
            "(0)\t 362\t [ 5.490107 -5.579196] \t0\ttrue\n",
            "(0)\t 363\t [ 5.491209 -5.566298] \t0\ttrue\n",
            "(0)\t 364\t [ 5.47009  -5.557988] \t0\ttrue\n",
            "(0)\t 365\t [ 5.345063  -5.4399424] \t0\ttrue\n",
            "(0)\t 366\t [ 5.4179473 -5.6353297] \t0\ttrue\n",
            "(1)\t 367\t [ 5.3746767 -5.5325184] \t0\tfalse\n",
            "(0)\t 368\t [ 5.439423  -5.5952396] \t0\ttrue\n",
            "(0)\t 369\t [ 5.487512 -5.584869] \t0\ttrue\n",
            "(0)\t 370\t [-4.6138783  4.558081 ] \t1\tfalse\n",
            "(0)\t 371\t [ 5.3660436 -5.5184126] \t0\ttrue\n",
            "(0)\t 372\t [-5.084957   5.2070584] \t1\tfalse\n",
            "(0)\t 373\t [ 5.399941  -5.5161576] \t0\ttrue\n",
            "(0)\t 374\t [ 1.328102  -1.4920677] \t0\ttrue\n",
            "(0)\t 375\t [ 5.4928546 -5.5960407] \t0\ttrue\n",
            "(0)\t 376\t [ 5.43655  -5.623433] \t0\ttrue\n",
            "(0)\t 377\t [ 5.4467    -5.6331525] \t0\ttrue\n",
            "(0)\t 378\t [ 5.4075375 -5.6282096] \t0\ttrue\n",
            "(0)\t 379\t [ 4.4076514 -4.7905335] \t0\ttrue\n",
            "(1)\t 380\t [ 5.3265624 -5.444955 ] \t0\tfalse\n",
            "(0)\t 381\t [-5.329457  5.461394] \t1\tfalse\n",
            "(0)\t 382\t [ 5.481955 -5.591893] \t0\ttrue\n",
            "(0)\t 383\t [ 5.4204955 -5.4945436] \t0\ttrue\n",
            "(1)\t 384\t [ 5.238518 -5.494884] \t0\tfalse\n",
            "(0)\t 385\t [ 5.4918585 -5.5683713] \t0\ttrue\n",
            "(0)\t 386\t [ 5.1167617 -5.241879 ] \t0\ttrue\n",
            "(0)\t 387\t [ 5.368898 -5.504988] \t0\ttrue\n",
            "(1)\t 388\t [-5.1466684  5.450157 ] \t1\ttrue\n",
            "(0)\t 389\t [ 5.484176  -5.6001444] \t0\ttrue\n",
            "(0)\t 390\t [ 5.488445 -5.589422] \t0\ttrue\n",
            "(0)\t 391\t [ 5.4940057 -5.548558 ] \t0\ttrue\n",
            "(0)\t 392\t [ 5.47594   -5.5386715] \t0\ttrue\n",
            "(0)\t 393\t [-4.985737  5.400879] \t1\tfalse\n",
            "(0)\t 394\t [-0.7482044  1.0707419] \t1\tfalse\n",
            "(1)\t 395\t [ 5.3493896 -5.497202 ] \t0\tfalse\n",
            "(0)\t 396\t [ 5.343476  -5.5304337] \t0\ttrue\n",
            "(0)\t 397\t [-5.1527576  5.4566455] \t1\tfalse\n",
            "(0)\t 398\t [ 4.289106 -4.404252] \t0\ttrue\n",
            "(0)\t 399\t [ 5.4477882 -5.6119323] \t0\ttrue\n",
            "(0)\t 400\t [ 5.443418 -5.561962] \t0\ttrue\n",
            "(0)\t 401\t [ 4.677623 -5.139922] \t0\ttrue\n",
            "(0)\t 402\t [ 5.3179007 -5.5272136] \t0\ttrue\n",
            "(0)\t 403\t [ 3.6375124 -3.9855042] \t0\ttrue\n",
            "(0)\t 404\t [-4.7966037  5.1396484] \t1\tfalse\n",
            "(0)\t 405\t [ 4.301871 -4.764882] \t0\ttrue\n",
            "(0)\t 406\t [ 5.455815  -5.6329613] \t0\ttrue\n",
            "(0)\t 407\t [ 5.103221  -5.3554373] \t0\ttrue\n",
            "(1)\t 408\t [ 5.4212294 -5.5491853] \t0\tfalse\n",
            "(0)\t 409\t [ 5.482357  -5.5890803] \t0\ttrue\n",
            "(0)\t 410\t [-0.72736454  0.7638605 ] \t1\tfalse\n",
            "(0)\t 411\t [ 5.3816257 -5.5458393] \t0\ttrue\n",
            "(0)\t 412\t [ 5.4780316 -5.5358047] \t0\ttrue\n",
            "(0)\t 413\t [ 5.486165 -5.580897] \t0\ttrue\n",
            "(0)\t 414\t [-5.0140047  5.3122234] \t1\tfalse\n",
            "(0)\t 415\t [ 5.4597063 -5.6181283] \t0\ttrue\n",
            "(1)\t 416\t [ 5.477699  -5.5833874] \t0\tfalse\n",
            "(0)\t 417\t [ 5.47558   -5.6131496] \t0\ttrue\n",
            "(1)\t 418\t [-4.81697    5.1158624] \t1\ttrue\n",
            "(0)\t 419\t [ 5.4585376 -5.613063 ] \t0\ttrue\n",
            "(1)\t 420\t [-4.999401  5.244709] \t1\ttrue\n",
            "(0)\t 421\t [ 5.4594326 -5.6270947] \t0\ttrue\n",
            "(0)\t 422\t [ 5.5001817 -5.5450654] \t0\ttrue\n",
            "(1)\t 423\t [ 5.5021353 -5.5634723] \t0\tfalse\n",
            "(0)\t 424\t [ 5.4543185 -5.6172323] \t0\ttrue\n",
            "(0)\t 425\t [ 5.4703755 -5.5788565] \t0\ttrue\n",
            "(1)\t 426\t [-5.2459636  5.448087 ] \t1\ttrue\n",
            "(0)\t 427\t [ 5.4671946 -5.6254034] \t0\ttrue\n",
            "(0)\t 428\t [ 5.4946303 -5.577666 ] \t0\ttrue\n",
            "(0)\t 429\t [ 4.4003806 -4.910568 ] \t0\ttrue\n",
            "(0)\t 430\t [ 5.4699297 -5.6153345] \t0\ttrue\n",
            "(1)\t 431\t [-0.43165696 -0.07506245] \t1\ttrue\n",
            "(1)\t 432\t [ 5.447978 -5.642471] \t0\tfalse\n",
            "(1)\t 433\t [ 5.447978 -5.642471] \t0\tfalse\n",
            "(0)\t 434\t [ 3.7882886 -3.8903184] \t0\ttrue\n",
            "(1)\t 435\t [ 5.408556  -5.5482516] \t0\tfalse\n",
            "(1)\t 436\t [ 5.408556  -5.5482516] \t0\tfalse\n",
            "(0)\t 437\t [ 5.4731536 -5.5497336] \t0\ttrue\n",
            "(0)\t 438\t [-4.138049  4.627526] \t1\tfalse\n",
            "(0)\t 439\t [-0.17618342  0.5542555 ] \t1\tfalse\n",
            "(0)\t 440\t [ 5.476782 -5.542568] \t0\ttrue\n",
            "(0)\t 441\t [ 5.4384875 -5.57625  ] \t0\ttrue\n",
            "(1)\t 442\t [ 5.465892 -5.563111] \t0\tfalse\n",
            "(1)\t 443\t [ 5.4750557 -5.5818214] \t0\tfalse\n",
            "(0)\t 444\t [ 5.466761  -5.5397034] \t0\ttrue\n",
            "(0)\t 445\t [ 5.470154 -5.594159] \t0\ttrue\n",
            "(1)\t 446\t [ 5.390205  -5.5438433] \t0\tfalse\n",
            "(0)\t 447\t [ 5.470714 -5.605403] \t0\ttrue\n",
            "(0)\t 448\t [ 5.384005  -5.5444746] \t0\ttrue\n",
            "(0)\t 449\t [ 5.4991174 -5.5430074] \t0\ttrue\n",
            "(1)\t 450\t [ 5.380902 -5.499724] \t0\tfalse\n",
            "(1)\t 451\t [ 5.380902 -5.499724] \t0\tfalse\n",
            "(1)\t 452\t [ 5.477317  -5.6216693] \t0\tfalse\n",
            "(0)\t 453\t [-4.1853585  4.6395693] \t1\tfalse\n",
            "(0)\t 454\t [ 5.488346  -5.5838604] \t0\ttrue\n",
            "(0)\t 455\t [ 5.441398  -5.5303698] \t0\ttrue\n",
            "(1)\t 456\t [ 5.3818974 -5.600234 ] \t0\tfalse\n",
            "(1)\t 457\t [ 5.3840957 -5.569839 ] \t0\tfalse\n",
            "(1)\t 458\t [ 4.7694554 -5.100528 ] \t0\tfalse\n",
            "(0)\t 459\t [ 5.0632844 -5.350095 ] \t0\ttrue\n",
            "(1)\t 460\t [ 5.4662304 -5.6258454] \t0\tfalse\n",
            "(1)\t 461\t [ 5.4662304 -5.6258454] \t0\tfalse\n",
            "(0)\t 462\t [ 5.480019 -5.58524 ] \t0\ttrue\n",
            "(1)\t 463\t [ 5.4388647 -5.637835 ] \t0\tfalse\n",
            "(1)\t 464\t [-3.0263271  3.0256674] \t1\ttrue\n",
            "(1)\t 465\t [ 5.4440684 -5.6270275] \t0\tfalse\n",
            "(1)\t 466\t [-4.9679704  5.177016 ] \t1\ttrue\n",
            "(0)\t 467\t [ 5.493847  -5.5903573] \t0\ttrue\n",
            "(0)\t 468\t [ 5.25067   -5.3800592] \t0\ttrue\n",
            "(1)\t 469\t [ 5.2760715 -5.460559 ] \t0\tfalse\n",
            "(1)\t 470\t [ 5.127255 -5.239277] \t0\tfalse\n",
            "(1)\t 471\t [ 5.4654226 -5.6040707] \t0\tfalse\n",
            "(1)\t 472\t [ 5.4654226 -5.6040707] \t0\tfalse\n",
            "(0)\t 473\t [-5.0698776  5.328002 ] \t1\tfalse\n",
            "(1)\t 474\t [ 5.307711  -5.4793344] \t0\tfalse\n",
            "(0)\t 475\t [ 5.4772024 -5.6063924] \t0\ttrue\n",
            "(0)\t 476\t [ 5.455165 -5.631137] \t0\ttrue\n",
            "(0)\t 477\t [ 5.4920983 -5.568675 ] \t0\ttrue\n",
            "(0)\t 478\t [ 5.4764395 -5.586099 ] \t0\ttrue\n",
            "(1)\t 479\t [ 5.1023726 -5.28217  ] \t0\tfalse\n",
            "(1)\t 480\t [ 5.087868 -5.351225] \t0\tfalse\n",
            "(1)\t 481\t [-4.8837748  5.2977753] \t1\ttrue\n",
            "(1)\t 482\t [ 4.481364  -4.7234526] \t0\tfalse\n",
            "(0)\t 483\t [ 5.4769034 -5.6005898] \t0\ttrue\n",
            "(1)\t 484\t [ 3.1952748 -3.3237934] \t0\tfalse\n",
            "(0)\t 485\t [ 5.451728  -5.5112333] \t0\ttrue\n",
            "(1)\t 486\t [ 5.399506  -5.5195065] \t0\tfalse\n",
            "(1)\t 487\t [ 4.8036904 -5.080671 ] \t0\tfalse\n",
            "(0)\t 488\t [ 5.4906845 -5.5928645] \t0\ttrue\n",
            "(0)\t 489\t [ 5.4368763 -5.5426855] \t0\ttrue\n",
            "(0)\t 490\t [ 5.4861546 -5.6037197] \t0\ttrue\n",
            "(0)\t 491\t [ 4.62392   -5.1199923] \t0\ttrue\n",
            "(1)\t 492\t [ 5.492161  -5.5516043] \t0\tfalse\n",
            "(1)\t 493\t [ 5.492161  -5.5516043] \t0\tfalse\n",
            "(0)\t 494\t [ 5.301074  -5.5041246] \t0\ttrue\n",
            "(0)\t 495\t [ 5.4525266 -5.5574174] \t0\ttrue\n",
            "(1)\t 496\t [ 4.5986495 -4.7800817] \t0\tfalse\n",
            "(1)\t 497\t [-5.0421534  4.9194365] \t1\ttrue\n",
            "(0)\t 498\t [ 5.17946  -5.450987] \t0\ttrue\n",
            "(0)\t 499\t [ 5.436305 -5.632607] \t0\ttrue\n",
            "(0)\t 500\t [ 0.6169388  -0.45866516] \t0\ttrue\n",
            "(1)\t 501\t [ 5.465736 -5.602375] \t0\tfalse\n",
            "(1)\t 502\t [ 5.465736 -5.602375] \t0\tfalse\n",
            "(0)\t 503\t [ 5.484887 -5.537384] \t0\ttrue\n",
            "(0)\t 504\t [ 5.4544353 -5.5198274] \t0\ttrue\n",
            "(1)\t 505\t [ 5.427036  -5.5126166] \t0\tfalse\n",
            "(1)\t 506\t [ 5.2643986 -5.4844913] \t0\tfalse\n",
            "(1)\t 507\t [-3.2922716  3.3793876] \t1\ttrue\n",
            "(0)\t 508\t [-5.0401797  5.2195263] \t1\tfalse\n",
            "(0)\t 509\t [-5.112135   5.4093556] \t1\tfalse\n",
            "(0)\t 510\t [ 5.3735414 -5.5049787] \t0\ttrue\n",
            "(1)\t 511\t [-5.325625  5.475538] \t1\ttrue\n",
            "(1)\t 512\t [-3.229784  3.646701] \t1\ttrue\n",
            "(0)\t 513\t [ 5.3919597 -5.4864373] \t0\ttrue\n",
            "(0)\t 514\t [ 5.3593397 -5.5115123] \t0\ttrue\n",
            "(1)\t 515\t [-5.3462553  5.4348607] \t1\ttrue\n",
            "(0)\t 516\t [ 5.4554143 -5.627784 ] \t0\ttrue\n",
            "(1)\t 517\t [ 5.3683414 -5.4541197] \t0\tfalse\n",
            "(0)\t 518\t [ 5.4493537 -5.628965 ] \t0\ttrue\n",
            "(0)\t 519\t [ 5.473193 -5.608457] \t0\ttrue\n",
            "(0)\t 520\t [ 5.4680758 -5.619133 ] \t0\ttrue\n",
            "(0)\t 521\t [-4.2436986  4.695499 ] \t1\tfalse\n",
            "(1)\t 522\t [-3.8958805  4.2734556] \t1\ttrue\n",
            "(1)\t 523\t [ 5.387937  -5.5097637] \t0\tfalse\n",
            "(1)\t 524\t [ 5.428635  -5.5495973] \t0\tfalse\n",
            "(1)\t 525\t [ 5.5011387 -5.5418754] \t0\tfalse\n",
            "(1)\t 526\t [ 5.18226   -5.2665415] \t0\tfalse\n",
            "(1)\t 527\t [-3.6932101  3.3146923] \t1\ttrue\n",
            "(0)\t 528\t [ 5.254107 -5.458641] \t0\ttrue\n",
            "(1)\t 529\t [ 5.464323 -5.511665] \t0\tfalse\n",
            "(1)\t 530\t [ 5.464323 -5.511665] \t0\tfalse\n",
            "(1)\t 531\t [-4.890989   5.2022104] \t1\ttrue\n",
            "(1)\t 532\t [-5.1519647  5.4335785] \t1\ttrue\n",
            "(1)\t 533\t [ 5.4732785 -5.61497  ] \t0\tfalse\n",
            "(1)\t 534\t [ 5.4732785 -5.61497  ] \t0\tfalse\n",
            "(1)\t 535\t [ 5.4859376 -5.563433 ] \t0\tfalse\n",
            "(1)\t 536\t [ 5.4859376 -5.563433 ] \t0\tfalse\n",
            "(1)\t 537\t [ 5.4179716 -5.6273046] \t0\tfalse\n",
            "(1)\t 538\t [ 4.9808764 -5.282566 ] \t0\tfalse\n",
            "(1)\t 539\t [ 4.9808764 -5.282566 ] \t0\tfalse\n",
            "(1)\t 540\t [-5.2931323  5.3911347] \t1\ttrue\n",
            "(1)\t 541\t [-5.2931323  5.3911347] \t1\ttrue\n",
            "(0)\t 542\t [ 4.716013  -5.0362983] \t0\ttrue\n",
            "(1)\t 543\t [ 5.4663143 -5.5526347] \t0\tfalse\n",
            "(1)\t 544\t [ 5.4663143 -5.5526347] \t0\tfalse\n",
            "(1)\t 545\t [ 5.4900312 -5.6017966] \t0\tfalse\n",
            "(1)\t 546\t [ 5.4731603 -5.5610933] \t0\tfalse\n",
            "(0)\t 547\t [ 5.3934984 -5.5231185] \t0\ttrue\n",
            "(1)\t 548\t [ 5.5031843 -5.5746226] \t0\tfalse\n",
            "(1)\t 549\t [ 5.5031843 -5.5746226] \t0\tfalse\n",
            "(0)\t 550\t [ 5.496101  -5.5641375] \t0\ttrue\n",
            "(0)\t 551\t [ 5.4639974 -5.5925097] \t0\ttrue\n",
            "(0)\t 552\t [ 5.3222165 -5.430612 ] \t0\ttrue\n",
            "(1)\t 553\t [-5.2488174  5.4753976] \t1\ttrue\n",
            "(0)\t 554\t [ 5.456553  -5.6244226] \t0\ttrue\n",
            "(0)\t 555\t [ 5.4709287 -5.5390663] \t0\ttrue\n",
            "(1)\t 556\t [-5.3201885  5.4330006] \t1\ttrue\n",
            "(1)\t 557\t [-5.2931623  5.4472675] \t1\ttrue\n",
            "(1)\t 558\t [ 5.467458 -5.562944] \t0\tfalse\n",
            "(0)\t 559\t [-5.0716476  5.414023 ] \t1\tfalse\n",
            "(0)\t 560\t [-5.174271   5.4320803] \t1\tfalse\n",
            "(0)\t 561\t [-4.6974206  5.045209 ] \t1\tfalse\n",
            "(1)\t 562\t [ 5.391416  -5.5859566] \t0\tfalse\n",
            "(1)\t 563\t [ 5.391416  -5.5859566] \t0\tfalse\n",
            "(1)\t 564\t [ 5.3956857 -5.53956  ] \t0\tfalse\n",
            "(0)\t 565\t [ 5.282106  -5.3421316] \t0\ttrue\n",
            "(1)\t 566\t [-3.2388048  3.1208384] \t1\ttrue\n",
            "(1)\t 567\t [-3.2388048  3.1208384] \t1\ttrue\n",
            "(0)\t 568\t [ 5.4826427 -5.5798464] \t0\ttrue\n",
            "(0)\t 569\t [ 1.7840595 -1.4018166] \t0\ttrue\n",
            "(0)\t 570\t [ 5.444421  -5.5985684] \t0\ttrue\n",
            "(0)\t 571\t [ 5.4135814 -5.496092 ] \t0\ttrue\n",
            "(1)\t 572\t [ 1.8644863 -2.02889  ] \t0\tfalse\n",
            "(1)\t 573\t [-4.828687   4.9604006] \t1\ttrue\n",
            "(0)\t 574\t [ 5.4402533 -5.493105 ] \t0\ttrue\n",
            "(0)\t 575\t [ 5.0500026 -5.334957 ] \t0\ttrue\n",
            "(1)\t 576\t [ 5.4774485 -5.6036654] \t0\tfalse\n",
            "(0)\t 577\t [ 5.461935  -5.5689135] \t0\ttrue\n",
            "(0)\t 578\t [ 5.466206  -5.6173906] \t0\ttrue\n",
            "(0)\t 579\t [-5.205448  5.413699] \t1\tfalse\n",
            "(0)\t 580\t [ 5.4849243 -5.5660806] \t0\ttrue\n",
            "(1)\t 581\t [ 5.4102626 -5.522749 ] \t0\tfalse\n",
            "(1)\t 582\t [ 5.0867863 -5.281017 ] \t0\tfalse\n",
            "(1)\t 583\t [ 5.0867863 -5.281017 ] \t0\tfalse\n",
            "(0)\t 584\t [ 5.470455 -5.619112] \t0\ttrue\n",
            "(1)\t 585\t [ 5.4599357 -5.5995045] \t0\tfalse\n",
            "(0)\t 586\t [ 5.4866014 -5.6075325] \t0\ttrue\n",
            "(1)\t 587\t [ 5.3270874 -5.4307456] \t0\tfalse\n",
            "(0)\t 588\t [ 5.477212  -5.6094713] \t0\ttrue\n",
            "(1)\t 589\t [ 0.27894294 -0.25720805] \t0\tfalse\n",
            "(1)\t 590\t [ 5.332964  -5.4752803] \t0\tfalse\n",
            "(1)\t 591\t [ 5.046596 -5.414711] \t0\tfalse\n",
            "(0)\t 592\t [ 5.439924 -5.609164] \t0\ttrue\n",
            "(1)\t 593\t [ 5.4378448 -5.552387 ] \t0\tfalse\n",
            "(1)\t 594\t [ 5.4378448 -5.552387 ] \t0\tfalse\n",
            "(0)\t 595\t [ 5.4862146 -5.6135235] \t0\ttrue\n",
            "(0)\t 596\t [ 5.4472384 -5.638876 ] \t0\ttrue\n",
            "(1)\t 597\t [-4.8894825  5.1960745] \t1\ttrue\n",
            "(0)\t 598\t [ 3.684674  -3.7678862] \t0\ttrue\n",
            "(0)\t 599\t [-4.723563  5.133272] \t1\tfalse\n",
            "(0)\t 600\t [-5.3091245  5.4112   ] \t1\tfalse\n",
            "(0)\t 601\t [-1.108648   1.3718948] \t1\tfalse\n",
            "(0)\t 602\t [-3.746275   4.0217443] \t1\tfalse\n",
            "(1)\t 603\t [ 5.423783 -5.53109 ] \t0\tfalse\n",
            "(1)\t 604\t [ 5.101363 -5.36764 ] \t0\tfalse\n",
            "(1)\t 605\t [ 5.101363 -5.36764 ] \t0\tfalse\n",
            "(1)\t 606\t [ 5.481804 -5.570097] \t0\tfalse\n",
            "(0)\t 607\t [ 5.06422  -5.252181] \t0\ttrue\n",
            "(0)\t 608\t [ 5.4985585 -5.5877056] \t0\ttrue\n",
            "(1)\t 609\t [ 5.459488  -5.5185003] \t0\tfalse\n",
            "(1)\t 610\t [ 5.459488  -5.5185003] \t0\tfalse\n",
            "(0)\t 611\t [ 4.205133  -4.7208548] \t0\ttrue\n",
            "(1)\t 612\t [ 5.4697976 -5.601696 ] \t0\tfalse\n",
            "(1)\t 613\t [ 5.494829  -5.5763993] \t0\tfalse\n",
            "(0)\t 614\t [ 5.475156  -5.5350823] \t0\ttrue\n",
            "(0)\t 615\t [ 5.3369946 -5.4881554] \t0\ttrue\n",
            "(0)\t 616\t [ 5.5015273 -5.531436 ] \t0\ttrue\n",
            "(0)\t 617\t [-4.7009363  5.102296 ] \t1\tfalse\n",
            "(0)\t 618\t [ 5.472587  -5.4635367] \t0\ttrue\n",
            "(1)\t 619\t [ 5.4683394 -5.5095654] \t0\tfalse\n",
            "(0)\t 620\t [ 5.419949  -5.5469165] \t0\ttrue\n",
            "(1)\t 621\t [ 5.4832935 -5.549653 ] \t0\tfalse\n",
            "(1)\t 622\t [ 5.4832935 -5.549653 ] \t0\tfalse\n",
            "(0)\t 623\t [ 5.5115886 -5.558391 ] \t0\ttrue\n",
            "(0)\t 624\t [ 5.4747405 -5.6114635] \t0\ttrue\n",
            "(0)\t 625\t [ 5.0524497 -5.324255 ] \t0\ttrue\n",
            "(0)\t 626\t [ 4.6994557 -4.931942 ] \t0\ttrue\n",
            "(0)\t 627\t [ 5.488994 -5.579846] \t0\ttrue\n",
            "(1)\t 628\t [-5.362928  5.431746] \t1\ttrue\n",
            "(0)\t 629\t [ 4.899608 -5.250145] \t0\ttrue\n",
            "(1)\t 630\t [ 5.4020715 -5.5761795] \t0\tfalse\n",
            "(0)\t 631\t [ 5.0905685 -5.22173  ] \t0\ttrue\n",
            "(1)\t 632\t [ 5.448791  -5.6335053] \t0\tfalse\n",
            "(0)\t 633\t [ 5.4719214 -5.487911 ] \t0\ttrue\n",
            "(1)\t 634\t [ 5.4152403 -5.506947 ] \t0\tfalse\n",
            "(1)\t 635\t [ 4.8041    -5.1288886] \t0\tfalse\n",
            "(1)\t 636\t [-1.0827677   0.86699104] \t1\ttrue\n",
            "(1)\t 637\t [ 5.4935713 -5.515466 ] \t0\tfalse\n",
            "(1)\t 638\t [-5.060572  5.284506] \t1\ttrue\n",
            "(0)\t 639\t [ 5.045689  -5.3795567] \t0\ttrue\n",
            "(1)\t 640\t [-5.3193808  5.4742665] \t1\ttrue\n",
            "(1)\t 641\t [ 5.2067766 -5.4175525] \t0\tfalse\n",
            "(0)\t 642\t [ 5.4685574 -5.5564265] \t0\ttrue\n",
            "(1)\t 643\t [ 5.329539 -5.531635] \t0\tfalse\n",
            "(1)\t 644\t [ 5.3414035 -5.52817  ] \t0\tfalse\n",
            "(1)\t 645\t [-4.926402   5.3396306] \t1\ttrue\n",
            "(0)\t 646\t [ 1.8199995 -1.5410812] \t0\ttrue\n",
            "(1)\t 647\t [-4.7886767  4.82875  ] \t1\ttrue\n",
            "(0)\t 648\t [ 4.801508  -4.9115133] \t0\ttrue\n",
            "(1)\t 649\t [ 5.0487266 -5.2643046] \t0\tfalse\n",
            "(0)\t 650\t [-5.2660933  5.490711 ] \t1\tfalse\n",
            "(0)\t 651\t [ 5.489921 -5.571796] \t0\ttrue\n",
            "(1)\t 652\t [-5.130923   5.4242306] \t1\ttrue\n",
            "(1)\t 653\t [ 5.462501 -5.630574] \t0\tfalse\n",
            "(0)\t 654\t [-5.1688266  5.386943 ] \t1\tfalse\n",
            "(1)\t 655\t [ 5.476962 -5.576826] \t0\tfalse\n",
            "(1)\t 656\t [ 5.2564797 -5.5497446] \t0\tfalse\n",
            "(0)\t 657\t [ 5.489618  -5.6030016] \t0\ttrue\n",
            "(0)\t 658\t [ 5.382433  -5.5287476] \t0\ttrue\n",
            "(0)\t 659\t [ 5.2318068 -5.498936 ] \t0\ttrue\n",
            "(0)\t 660\t [ 5.4145007 -5.644427 ] \t0\ttrue\n",
            "(0)\t 661\t [ 5.4616323 -5.59573  ] \t0\ttrue\n",
            "(1)\t 662\t [-5.0921764  5.357347 ] \t1\ttrue\n",
            "(1)\t 663\t [ 5.425502  -5.4800444] \t0\tfalse\n",
            "(1)\t 664\t [ 5.425502  -5.4800444] \t0\tfalse\n",
            "(0)\t 665\t [ 5.4859886 -5.5563293] \t0\ttrue\n",
            "(1)\t 666\t [ 5.484013 -5.515603] \t0\tfalse\n",
            "(0)\t 667\t [ 5.4226565 -5.6387444] \t0\ttrue\n",
            "(1)\t 668\t [ 3.1168523 -3.155166 ] \t0\tfalse\n",
            "(0)\t 669\t [ 5.4439225 -5.629743 ] \t0\ttrue\n",
            "(0)\t 670\t [ 5.461639 -5.625761] \t0\ttrue\n",
            "(0)\t 671\t [ 5.4401765 -5.63945  ] \t0\ttrue\n",
            "(0)\t 672\t [ 5.46987  -5.575752] \t0\ttrue\n",
            "(1)\t 673\t [ 5.13726   -5.4237037] \t0\tfalse\n",
            "(1)\t 674\t [ 5.456699  -5.5005126] \t0\tfalse\n",
            "(1)\t 675\t [ 5.456699  -5.5005126] \t0\tfalse\n",
            "(1)\t 676\t [ 5.4950256 -5.588061 ] \t0\tfalse\n",
            "(1)\t 677\t [ 5.4385486 -5.6062903] \t0\tfalse\n",
            "(1)\t 678\t [-1.4141467  1.7338375] \t1\ttrue\n",
            "(0)\t 679\t [ 5.12716  -5.401813] \t0\ttrue\n",
            "(1)\t 680\t [ 5.224351 -5.392358] \t0\tfalse\n",
            "(0)\t 681\t [ 5.4494143 -5.626136 ] \t0\ttrue\n",
            "(0)\t 682\t [ 5.470781  -5.6169934] \t0\ttrue\n",
            "(0)\t 683\t [ 5.001589 -5.337431] \t0\ttrue\n",
            "(0)\t 684\t [-5.342574  5.409816] \t1\tfalse\n",
            "(1)\t 685\t [-5.2020936  5.4638553] \t1\ttrue\n",
            "(1)\t 686\t [ 5.4881635 -5.5737205] \t0\tfalse\n",
            "(1)\t 687\t [ 5.4094305 -5.605619 ] \t0\tfalse\n",
            "(1)\t 688\t [ 5.4127817 -5.530733 ] \t0\tfalse\n",
            "(0)\t 689\t [ 5.402652  -5.6438594] \t0\ttrue\n",
            "(1)\t 690\t [ 3.6105065 -4.112271 ] \t0\tfalse\n",
            "(1)\t 691\t [-4.789797   5.1124773] \t1\ttrue\n",
            "(0)\t 692\t [ 5.496512 -5.572122] \t0\ttrue\n",
            "(0)\t 693\t [ 5.4815855 -5.5607786] \t0\ttrue\n",
            "(1)\t 694\t [ 5.4570785 -5.630121 ] \t0\tfalse\n",
            "(1)\t 695\t [ 5.406245  -5.5332713] \t0\tfalse\n",
            "(0)\t 696\t [ 5.457582  -5.6207147] \t0\ttrue\n",
            "(0)\t 697\t [ 5.1459756 -5.461731 ] \t0\ttrue\n",
            "(0)\t 698\t [ 2.5627816 -3.1393926] \t0\ttrue\n",
            "(0)\t 699\t [-4.8771377  5.307881 ] \t1\tfalse\n",
            "(0)\t 700\t [ 5.461198 -5.530259] \t0\ttrue\n",
            "(0)\t 701\t [ 5.4417586 -5.5475397] \t0\ttrue\n",
            "(0)\t 702\t [ 5.4960194 -5.5491066] \t0\ttrue\n",
            "(0)\t 703\t [ 5.498113  -5.5536404] \t0\ttrue\n",
            "(0)\t 704\t [ 2.2981222 -2.3343184] \t0\ttrue\n",
            "(1)\t 705\t [ 5.326405  -5.5247145] \t0\tfalse\n",
            "(0)\t 706\t [ 5.4227185 -5.5758924] \t0\ttrue\n",
            "(0)\t 707\t [ 5.4496408 -5.633314 ] \t0\ttrue\n",
            "(1)\t 708\t [ 5.3885846 -5.5347443] \t0\tfalse\n",
            "(0)\t 709\t [ 1.1467274 -1.2511387] \t0\ttrue\n",
            "(0)\t 710\t [ 4.315954  -4.8247757] \t0\ttrue\n",
            "(1)\t 711\t [ 5.4692254 -5.6196694] \t0\tfalse\n",
            "(1)\t 712\t [-5.1778026  5.4188876] \t1\ttrue\n",
            "(1)\t 713\t [-4.7386303  5.1159697] \t1\ttrue\n",
            "(1)\t 714\t [-3.7569878  4.24723  ] \t1\ttrue\n",
            "(1)\t 715\t [-5.063179  5.399862] \t1\ttrue\n",
            "(0)\t 716\t [ 5.1789107 -5.4697633] \t0\ttrue\n",
            "(0)\t 717\t [ 5.367965 -5.585802] \t0\ttrue\n",
            "(0)\t 718\t [ 5.269916  -5.4991097] \t0\ttrue\n",
            "(0)\t 719\t [ 5.4724655 -5.577168 ] \t0\ttrue\n",
            "(0)\t 720\t [ 5.492556  -5.5133553] \t0\ttrue\n",
            "(1)\t 721\t [ 5.4642925 -5.510883 ] \t0\tfalse\n",
            "(1)\t 722\t [-5.288206  5.470546] \t1\ttrue\n",
            "(0)\t 723\t [ 5.304706  -5.4366226] \t0\ttrue\n",
            "(0)\t 724\t [ 3.2664034 -3.480384 ] \t0\ttrue\n",
            "(1)\t 725\t [ 5.471195 -5.623137] \t0\tfalse\n",
            "(0)\t 726\t [ 5.4742837 -5.628901 ] \t0\ttrue\n",
            "(0)\t 727\t [ 5.4339485 -5.5155096] \t0\ttrue\n",
            "(1)\t 728\t [-5.382944   5.3951626] \t1\ttrue\n",
            "(1)\t 729\t [ 5.495598  -5.5154057] \t0\tfalse\n",
            "(1)\t 730\t [-4.6365204  4.8762307] \t1\ttrue\n",
            "(1)\t 731\t [ 5.5011673 -5.55397  ] \t0\tfalse\n",
            "(1)\t 732\t [ 5.5011673 -5.55397  ] \t0\tfalse\n",
            "(0)\t 733\t [ 5.4738946 -5.583186 ] \t0\ttrue\n",
            "(1)\t 734\t [ 5.463801  -5.5419397] \t0\tfalse\n",
            "(0)\t 735\t [-4.021648   3.6189315] \t1\tfalse\n",
            "(0)\t 736\t [ 5.4569273 -5.6295943] \t0\ttrue\n",
            "(0)\t 737\t [ 5.4844155 -5.5735264] \t0\ttrue\n",
            "(1)\t 738\t [ 5.485579  -5.5998406] \t0\tfalse\n",
            "(1)\t 739\t [ 5.485579  -5.5998406] \t0\tfalse\n",
            "(0)\t 740\t [ 4.7979536 -5.201734 ] \t0\ttrue\n",
            "(1)\t 741\t [ 4.014975  -4.1723804] \t0\tfalse\n",
            "(1)\t 742\t [ 4.014975  -4.1723804] \t0\tfalse\n",
            "(0)\t 743\t [ 5.5096884 -5.539975 ] \t0\ttrue\n",
            "(0)\t 744\t [ 2.9591286 -3.023149 ] \t0\ttrue\n",
            "(1)\t 745\t [ 4.12716   -4.5372143] \t0\tfalse\n",
            "(0)\t 746\t [ 5.442193  -5.5551405] \t0\ttrue\n",
            "(0)\t 747\t [ 5.460352 -5.619657] \t0\ttrue\n",
            "(1)\t 748\t [ 5.4874434 -5.575535 ] \t0\tfalse\n",
            "(0)\t 749\t [ 5.469507  -5.6187024] \t0\ttrue\n",
            "(0)\t 750\t [-2.4617074  2.9047256] \t1\tfalse\n",
            "(1)\t 751\t [-5.0835323  5.1761966] \t1\ttrue\n",
            "(1)\t 752\t [ 5.4918065 -5.5439262] \t0\tfalse\n",
            "(1)\t 753\t [ 5.477048 -5.544399] \t0\tfalse\n",
            "(0)\t 754\t [ 5.4667807 -5.62134  ] \t0\ttrue\n",
            "(0)\t 755\t [ 4.02166   -4.0526404] \t0\ttrue\n",
            "(0)\t 756\t [ 5.490837  -5.5896454] \t0\ttrue\n",
            "(0)\t 757\t [ 5.4032726 -5.508118 ] \t0\ttrue\n",
            "(0)\t 758\t [-5.0617423  4.989313 ] \t1\tfalse\n",
            "(0)\t 759\t [ 5.4009075 -5.517431 ] \t0\ttrue\n",
            "(0)\t 760\t [ 5.367152 -5.527378] \t0\ttrue\n",
            "(0)\t 761\t [ 5.4606457 -5.603664 ] \t0\ttrue\n",
            "(0)\t 762\t [ 5.4733686 -5.593888 ] \t0\ttrue\n",
            "(1)\t 763\t [ 5.0132403 -5.391861 ] \t0\tfalse\n",
            "(1)\t 764\t [ 5.18207  -5.106574] \t0\tfalse\n",
            "(0)\t 765\t [-5.2632556  5.4568334] \t1\tfalse\n",
            "(0)\t 766\t [ 5.4803934 -5.60929  ] \t0\ttrue\n",
            "(0)\t 767\t [ 5.498885  -5.5845394] \t0\ttrue\n",
            "(1)\t 768\t [ 5.4188995 -5.542353 ] \t0\tfalse\n",
            "(0)\t 769\t [-4.436841   4.9178085] \t1\tfalse\n",
            "(0)\t 770\t [ 5.423801  -5.6037436] \t0\ttrue\n",
            "(1)\t 771\t [ 5.4383225 -5.637959 ] \t0\tfalse\n",
            "(1)\t 772\t [ 5.4383225 -5.637959 ] \t0\tfalse\n",
            "(1)\t 773\t [ 5.302953  -5.5646443] \t0\tfalse\n",
            "(1)\t 774\t [ 5.302953  -5.5646443] \t0\tfalse\n",
            "(1)\t 775\t [ 5.479533  -5.4836373] \t0\tfalse\n",
            "(1)\t 776\t [ 5.479533  -5.4836373] \t0\tfalse\n",
            "(1)\t 777\t [ 5.4547486 -5.5653725] \t0\tfalse\n",
            "(0)\t 778\t [ 5.3848796 -5.5275073] \t0\ttrue\n",
            "(0)\t 779\t [ 5.155852 -5.47348 ] \t0\ttrue\n",
            "(0)\t 780\t [ 4.5044184 -4.9020314] \t0\ttrue\n",
            "(0)\t 781\t [ 5.234699  -5.3538914] \t0\ttrue\n",
            "(0)\t 782\t [ 5.48684  -5.558134] \t0\ttrue\n",
            "(0)\t 783\t [-4.888777   4.9408603] \t1\tfalse\n",
            "(0)\t 784\t [ 3.131111  -3.1341493] \t0\ttrue\n",
            "(0)\t 785\t [ 4.1106305 -4.5510383] \t0\ttrue\n",
            "(0)\t 786\t [ 5.420479 -5.569022] \t0\ttrue\n",
            "(1)\t 787\t [-5.316694   5.3604546] \t1\ttrue\n",
            "(0)\t 788\t [ 5.478575  -5.5853233] \t0\ttrue\n",
            "(0)\t 789\t [ 5.4465623 -5.46715  ] \t0\ttrue\n",
            "(1)\t 790\t [-4.843688  5.263323] \t1\ttrue\n",
            "(1)\t 791\t [-4.4968567  4.942098 ] \t1\ttrue\n",
            "(1)\t 792\t [ 5.476765 -5.550866] \t0\tfalse\n",
            "(0)\t 793\t [-5.2847395  5.484598 ] \t1\tfalse\n",
            "(1)\t 794\t [-5.1651244  5.4398847] \t1\ttrue\n",
            "(1)\t 795\t [ 5.071999 -5.352227] \t0\tfalse\n",
            "(0)\t 796\t [ 5.35079  -5.455692] \t0\ttrue\n",
            "(0)\t 797\t [ 5.443148  -5.6377015] \t0\ttrue\n",
            "(0)\t 798\t [-4.222321   4.5277386] \t1\tfalse\n",
            "(0)\t 799\t [ 5.2331653 -5.4962554] \t0\ttrue\n",
            "(0)\t 800\t [ 5.372108 -5.523166] \t0\ttrue\n",
            "(1)\t 801\t [ 5.389474 -5.525763] \t0\tfalse\n",
            "(1)\t 802\t [ 5.389474 -5.525763] \t0\tfalse\n",
            "(0)\t 803\t [ 5.4041367 -5.4754443] \t0\ttrue\n",
            "(0)\t 804\t [-5.0887876  5.375803 ] \t1\tfalse\n",
            "(0)\t 805\t [-5.255218   5.4205303] \t1\tfalse\n",
            "(0)\t 806\t [ 5.413505  -5.5030336] \t0\ttrue\n",
            "(1)\t 807\t [ 5.327997  -5.5719075] \t0\tfalse\n",
            "(0)\t 808\t [ 5.194332 -5.425153] \t0\ttrue\n",
            "(1)\t 809\t [-0.9478726   0.84332275] \t1\ttrue\n",
            "(1)\t 810\t [-0.9478726   0.84332275] \t1\ttrue\n",
            "(1)\t 811\t [ 5.470649 -5.564477] \t0\tfalse\n",
            "(0)\t 812\t [ 5.3496003 -5.5829844] \t0\ttrue\n",
            "(1)\t 813\t [ 5.207191  -5.4907756] \t0\tfalse\n",
            "(1)\t 814\t [ 5.207191  -5.4907756] \t0\tfalse\n",
            "(1)\t 815\t [-5.3033195  5.4374833] \t1\ttrue\n",
            "(0)\t 816\t [ 5.209722  -5.4583445] \t0\ttrue\n",
            "(0)\t 817\t [ 5.4307137 -5.5938263] \t0\ttrue\n",
            "(0)\t 818\t [ 3.7307863 -4.038038 ] \t0\ttrue\n",
            "(0)\t 819\t [ 5.4800334 -5.542321 ] \t0\ttrue\n",
            "(0)\t 820\t [ 5.4908576 -5.5371423] \t0\ttrue\n",
            "(0)\t 821\t [ 5.5033736 -5.5763917] \t0\ttrue\n",
            "(1)\t 822\t [ 5.437481  -5.6347165] \t0\tfalse\n",
            "(0)\t 823\t [ 5.438092 -5.545656] \t0\ttrue\n",
            "(0)\t 824\t [ 5.470707 -5.603841] \t0\ttrue\n",
            "(0)\t 825\t [ 5.15621  -5.274979] \t0\ttrue\n",
            "(0)\t 826\t [ 5.4305115 -5.58084  ] \t0\ttrue\n",
            "(0)\t 827\t [ 5.2228184 -5.4300313] \t0\ttrue\n",
            "(0)\t 828\t [ 5.46483   -5.5448966] \t0\ttrue\n",
            "(0)\t 829\t [ 5.496712  -5.5735536] \t0\ttrue\n",
            "(0)\t 830\t [ 5.491828  -5.5840216] \t0\ttrue\n",
            "(1)\t 831\t [ 5.4557524 -5.6337895] \t0\tfalse\n",
            "(0)\t 832\t [ 5.4875665 -5.566168 ] \t0\ttrue\n",
            "(0)\t 833\t [ 4.9488974 -5.3225684] \t0\ttrue\n",
            "(1)\t 834\t [-5.257944   5.4814405] \t1\ttrue\n",
            "(1)\t 835\t [ 5.434244  -5.5235486] \t0\tfalse\n",
            "(1)\t 836\t [ 0.11436081 -0.3401211 ] \t0\tfalse\n",
            "(1)\t 837\t [ 3.6302645 -4.1114697] \t0\tfalse\n",
            "(0)\t 838\t [ 5.4606433 -5.5902953] \t0\ttrue\n",
            "(0)\t 839\t [ 5.4699793 -5.5673466] \t0\ttrue\n",
            "(0)\t 840\t [ 5.4877086 -5.5898914] \t0\ttrue\n",
            "(0)\t 841\t [-5.2584553  5.399419 ] \t1\tfalse\n",
            "(0)\t 842\t [ 5.4093733 -5.5532184] \t0\ttrue\n",
            "(1)\t 843\t [ 5.495709  -5.5275316] \t0\tfalse\n",
            "(0)\t 844\t [-5.0628815  5.286299 ] \t1\tfalse\n",
            "(1)\t 845\t [-5.0392394  5.335672 ] \t1\ttrue\n",
            "(1)\t 846\t [ 5.281736  -5.4452357] \t0\tfalse\n",
            "(1)\t 847\t [ 4.4964004 -4.9456983] \t0\tfalse\n",
            "(1)\t 848\t [ 4.4964004 -4.9456983] \t0\tfalse\n",
            "(0)\t 849\t [ 5.2839046 -5.3900447] \t0\ttrue\n",
            "(1)\t 850\t [ 4.9618907 -5.0837245] \t0\tfalse\n",
            "(1)\t 851\t [ 5.383027  -5.4073505] \t0\tfalse\n",
            "(1)\t 852\t [ 5.4476843 -5.515058 ] \t0\tfalse\n",
            "(0)\t 853\t [ 5.4639144 -5.553719 ] \t0\ttrue\n",
            "(1)\t 854\t [-3.6310797  4.2348895] \t1\ttrue\n",
            "(0)\t 855\t [ 5.021257  -5.1702127] \t0\ttrue\n",
            "(0)\t 856\t [-3.6622996  4.249313 ] \t1\tfalse\n",
            "(0)\t 857\t [ 5.435674  -5.5616894] \t0\ttrue\n",
            "(1)\t 858\t [ 5.4981422 -5.548157 ] \t0\tfalse\n",
            "(0)\t 859\t [-5.173831  5.43227 ] \t1\tfalse\n",
            "(1)\t 860\t [ 5.179036 -5.283695] \t0\tfalse\n",
            "(0)\t 861\t [-4.865263  5.303423] \t1\tfalse\n",
            "(1)\t 862\t [ 5.495619  -5.5885944] \t0\tfalse\n",
            "(1)\t 863\t [ 5.495619  -5.5885944] \t0\tfalse\n",
            "(1)\t 864\t [-4.4908595  4.9462113] \t1\ttrue\n",
            "(0)\t 865\t [ 5.471753 -5.592108] \t0\ttrue\n",
            "(0)\t 866\t [ 5.3038816 -5.4052796] \t0\ttrue\n",
            "(0)\t 867\t [ 5.4074564 -5.4411693] \t0\ttrue\n",
            "(0)\t 868\t [-3.8518806  4.3007355] \t1\tfalse\n",
            "(0)\t 869\t [-0.23328963  0.7779078 ] \t1\tfalse\n",
            "(0)\t 870\t [ 5.4807367 -5.5599003] \t0\ttrue\n",
            "(0)\t 871\t [ 5.421787 -5.553064] \t0\ttrue\n",
            "(0)\t 872\t [ 5.467766 -5.594346] \t0\ttrue\n",
            "(1)\t 873\t [ 5.4513125 -5.5295305] \t0\tfalse\n",
            "(1)\t 874\t [ 5.5024223 -5.533431 ] \t0\tfalse\n",
            "(1)\t 875\t [0.09874839 0.4346634 ] \t1\ttrue\n",
            "(1)\t 876\t [ 5.1703095 -5.392135 ] \t0\tfalse\n",
            "(0)\t 877\t [ 5.468719  -5.6082964] \t0\ttrue\n",
            "(1)\t 878\t [ 1.2938731 -1.4949493] \t0\tfalse\n",
            "(1)\t 879\t [-5.3513975  5.365133 ] \t1\ttrue\n",
            "(1)\t 880\t [ 5.5037837 -5.5422587] \t0\tfalse\n",
            "(0)\t 881\t [-5.0408044  5.3047113] \t1\tfalse\n",
            "(0)\t 882\t [ 3.3120024 -3.6901934] \t0\ttrue\n",
            "(1)\t 883\t [ 5.4379673 -5.4754214] \t0\tfalse\n",
            "(1)\t 884\t [ 3.589983  -3.8203402] \t0\tfalse\n",
            "(1)\t 885\t [-5.269528   5.4173074] \t1\ttrue\n",
            "(0)\t 886\t [ 5.466654  -5.6214867] \t0\ttrue\n",
            "(0)\t 887\t [-5.2806416  5.478998 ] \t1\tfalse\n",
            "(1)\t 888\t [ 5.2580476 -5.388791 ] \t0\tfalse\n",
            "(0)\t 889\t [ 5.4545517 -5.562256 ] \t0\ttrue\n",
            "(0)\t 890\t [ 5.266525  -5.4135585] \t0\ttrue\n",
            "(0)\t 891\t [-2.7268     3.0120778] \t1\tfalse\n",
            "(1)\t 892\t [ 5.433397 -5.536398] \t0\tfalse\n",
            "(1)\t 893\t [ 5.482241 -5.558926] \t0\tfalse\n",
            "(1)\t 894\t [ 5.4077888 -5.543711 ] \t0\tfalse\n",
            "(1)\t 895\t [ 5.474334  -5.4912057] \t0\tfalse\n",
            "(0)\t 896\t [ 3.9886541 -4.568372 ] \t0\ttrue\n",
            "(1)\t 897\t [ 5.0471926 -5.4127607] \t0\tfalse\n",
            "(1)\t 898\t [ 5.4371204 -5.552581 ] \t0\tfalse\n",
            "(1)\t 899\t [-2.447302   3.0829155] \t1\ttrue\n",
            "(1)\t 900\t [ 5.434915  -5.5394135] \t0\tfalse\n",
            "Number of true predictions: 526\n",
            "Number of false predictions: 374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIgsWWCTKdY",
        "colab_type": "code",
        "outputId": "92a4ee9e-0c5e-46ac-f228-ff0edd7cc74a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 58.379578246392896 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xk8eT967gJ1",
        "colab_type": "code",
        "outputId": "3f989010-65e6-4d3d-c958-0cbb9e89764a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "#scores = [3.0, 1.0, 0.2]\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    predictions[i][j]=softmax(predictions[i][j])\n",
        "    print(predictions[i][j])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5990591 0.4009409]\n",
            "[9.9998403e-01 1.5955233e-05]\n",
            "[9.9995935e-01 4.0648578e-05]\n",
            "[9.999838e-01 1.619195e-05]\n",
            "[9.999846e-01 1.543122e-05]\n",
            "[9.9997985e-01 2.0161067e-05]\n",
            "[9.9997699e-01 2.2975702e-05]\n",
            "[9.9998355e-01 1.6481932e-05]\n",
            "[9.9998438e-01 1.5560165e-05]\n",
            "[9.9998438e-01 1.5661835e-05]\n",
            "[9.9998295e-01 1.7018743e-05]\n",
            "[9.9998474e-01 1.5279671e-05]\n",
            "[9.9998212e-01 1.7850705e-05]\n",
            "[9.9998474e-01 1.5235610e-05]\n",
            "[9.9998415e-01 1.5810821e-05]\n",
            "[9.9997723e-01 2.2747610e-05]\n",
            "[9.999845e-01 1.550005e-05]\n",
            "[5.9244421e-04 9.9940753e-01]\n",
            "[9.9998355e-01 1.6495142e-05]\n",
            "[9.9998379e-01 1.6161604e-05]\n",
            "[9.9998271e-01 1.7246664e-05]\n",
            "[2.155756e-05 9.999784e-01]\n",
            "[9.9998462e-01 1.5335054e-05]\n",
            "[9.9996090e-01 3.9051134e-05]\n",
            "[9.9998450e-01 1.5440712e-05]\n",
            "[9.9998426e-01 1.5722217e-05]\n",
            "[9.999838e-01 1.625351e-05]\n",
            "[9.9996650e-01 3.3555232e-05]\n",
            "[9.9998474e-01 1.5234680e-05]\n",
            "[9.9996614e-01 3.3858025e-05]\n",
            "[7.1069677e-05 9.9992895e-01]\n",
            "[9.9998307e-01 1.6957923e-05]\n",
            "[9.9998331e-01 1.6726033e-05]\n",
            "[9.999844e-01 1.564595e-05]\n",
            "[9.999844e-01 1.564595e-05]\n",
            "[9.9998355e-01 1.6461290e-05]\n",
            "[3.9749008e-05 9.9996030e-01]\n",
            "[9.9998367e-01 1.6315287e-05]\n",
            "[9.9998295e-01 1.7011278e-05]\n",
            "[9.9998057e-01 1.9472907e-05]\n",
            "[9.9998236e-01 1.7606419e-05]\n",
            "[9.9997973e-01 2.0314696e-05]\n",
            "[9.9989665e-01 1.0330504e-04]\n",
            "[9.9998128e-01 1.8692364e-05]\n",
            "[9.9998128e-01 1.8692364e-05]\n",
            "[9.9997950e-01 2.0489997e-05]\n",
            "[9.9998426e-01 1.5735703e-05]\n",
            "[9.999846e-01 1.538618e-05]\n",
            "[8.4178384e-05 9.9991584e-01]\n",
            "[9.999844e-01 1.556726e-05]\n",
            "[2.0903449e-04 9.9979097e-01]\n",
            "[9.9997520e-01 2.4769537e-05]\n",
            "[9.9998426e-01 1.5703470e-05]\n",
            "[9.9998319e-01 1.6774433e-05]\n",
            "[9.9998438e-01 1.5594303e-05]\n",
            "[9.9998319e-01 1.6759945e-05]\n",
            "[2.0273645e-05 9.9997973e-01]\n",
            "[9.9995267e-01 4.7363737e-05]\n",
            "[9.9998462e-01 1.5401682e-05]\n",
            "[9.9997854e-01 2.1425487e-05]\n",
            "[9.9993002e-01 6.9989874e-05]\n",
            "[9.9998236e-01 1.7610280e-05]\n",
            "[2.6365939e-05 9.9997365e-01]\n",
            "[9.9996996e-01 3.0039604e-05]\n",
            "[9.9996996e-01 3.0039604e-05]\n",
            "[9.999845e-01 1.553311e-05]\n",
            "[9.9997759e-01 2.2446116e-05]\n",
            "[9.9998450e-01 1.5554277e-05]\n",
            "[9.9998355e-01 1.6503591e-05]\n",
            "[9.999846e-01 1.534787e-05]\n",
            "[9.9998415e-01 1.5832580e-05]\n",
            "[2.1468686e-05 9.9997854e-01]\n",
            "[2.1468686e-05 9.9997854e-01]\n",
            "[2.2430433e-05 9.9997759e-01]\n",
            "[2.2430433e-05 9.9997759e-01]\n",
            "[9.9998462e-01 1.5355396e-05]\n",
            "[9.9998462e-01 1.5355396e-05]\n",
            "[5.1286450e-05 9.9994874e-01]\n",
            "[9.9998450e-01 1.5470398e-05]\n",
            "[9.9988866e-01 1.1134943e-04]\n",
            "[9.9998450e-01 1.5450036e-05]\n",
            "[2.6603911e-05 9.9997342e-01]\n",
            "[9.9998450e-01 1.5444688e-05]\n",
            "[9.9998426e-01 1.5785798e-05]\n",
            "[9.9998355e-01 1.6492184e-05]\n",
            "[9.9997890e-01 2.1070062e-05]\n",
            "[9.9998486e-01 1.5154809e-05]\n",
            "[9.9998391e-01 1.6117692e-05]\n",
            "[9.9998403e-01 1.5999938e-05]\n",
            "[2.1611904e-05 9.9997842e-01]\n",
            "[2.0863874e-05 9.9997914e-01]\n",
            "[6.8422838e-04 9.9931574e-01]\n",
            "[9.999598e-01 4.015825e-05]\n",
            "[9.999809e-01 1.913264e-05]\n",
            "[9.9998403e-01 1.6005677e-05]\n",
            "[9.9997807e-01 2.1971000e-05]\n",
            "[9.9998009e-01 1.9879115e-05]\n",
            "[9.9998426e-01 1.5767655e-05]\n",
            "[9.9998450e-01 1.5465574e-05]\n",
            "[9.9998367e-01 1.6284368e-05]\n",
            "[2.2755748e-05 9.9997723e-01]\n",
            "[9.9998462e-01 1.5423879e-05]\n",
            "[9.9998403e-01 1.5939751e-05]\n",
            "[9.99984741e-01 1.52242965e-05]\n",
            "[9.9998224e-01 1.7745861e-05]\n",
            "[0.9968784 0.0031216]\n",
            "[9.9998391e-01 1.6130718e-05]\n",
            "[9.9998391e-01 1.6074606e-05]\n",
            "[9.9998462e-01 1.5323505e-05]\n",
            "[9.9998415e-01 1.5899883e-05]\n",
            "[9.9998176e-01 1.8204559e-05]\n",
            "[0.04345771 0.9565423 ]\n",
            "[9.9998486e-01 1.5191940e-05]\n",
            "[9.9998319e-01 1.6827822e-05]\n",
            "[9.9998093e-01 1.9113237e-05]\n",
            "[9.9998474e-01 1.5209378e-05]\n",
            "[9.9997544e-01 2.4554553e-05]\n",
            "[9.9998295e-01 1.7092718e-05]\n",
            "[9.9998438e-01 1.5634496e-05]\n",
            "[9.9998331e-01 1.6634278e-05]\n",
            "[0.7571833  0.24281669]\n",
            "[9.9998331e-01 1.6635451e-05]\n",
            "[9.999641e-01 3.586386e-05]\n",
            "[9.999846e-01 1.542685e-05]\n",
            "[9.9998474e-01 1.5228783e-05]\n",
            "[9.9998343e-01 1.6611279e-05]\n",
            "[9.9998474e-01 1.5294030e-05]\n",
            "[9.9998474e-01 1.5294030e-05]\n",
            "[9.9998403e-01 1.5935677e-05]\n",
            "[9.9998450e-01 1.5497671e-05]\n",
            "[9.999831e-01 1.690759e-05]\n",
            "[9.9998474e-01 1.5276699e-05]\n",
            "[9.999846e-01 1.543356e-05]\n",
            "[9.999845e-01 1.551351e-05]\n",
            "[9.9998474e-01 1.5251805e-05]\n",
            "[9.991769e-01 8.230288e-04]\n",
            "[0.9989343  0.00106565]\n",
            "[9.9998295e-01 1.7099908e-05]\n",
            "[9.9998450e-01 1.5528074e-05]\n",
            "[9.9998438e-01 1.5642565e-05]\n",
            "[9.9998474e-01 1.5306945e-05]\n",
            "[8.331510e-05 9.999167e-01]\n",
            "[9.9998474e-01 1.5289597e-05]\n",
            "[9.9997568e-01 2.4266128e-05]\n",
            "[1.9234412e-04 9.9980766e-01]\n",
            "[9.9998260e-01 1.7402132e-05]\n",
            "[2.4101622e-05 9.9997592e-01]\n",
            "[9.9998474e-01 1.5313661e-05]\n",
            "[9.9998474e-01 1.5313661e-05]\n",
            "[9.9998224e-01 1.7773737e-05]\n",
            "[9.9998224e-01 1.7773737e-05]\n",
            "[9.9998307e-01 1.6967695e-05]\n",
            "[9.9998307e-01 1.6967695e-05]\n",
            "[9.9998474e-01 1.5263096e-05]\n",
            "[3.334444e-05 9.999666e-01]\n",
            "[3.334444e-05 9.999666e-01]\n",
            "[9.9998462e-01 1.5395044e-05]\n",
            "[9.999771e-01 2.285954e-05]\n",
            "[9.999846e-01 1.534913e-05]\n",
            "[9.9998450e-01 1.5506912e-05]\n",
            "[9.9997723e-01 2.2804890e-05]\n",
            "[9.999713e-01 2.869373e-05]\n",
            "[9.9998438e-01 1.5643653e-05]\n",
            "[2.3820836e-05 9.9997616e-01]\n",
            "[9.9998307e-01 1.6948014e-05]\n",
            "[9.9997246e-01 2.7557497e-05]\n",
            "[9.9998462e-01 1.5414496e-05]\n",
            "[9.9998212e-01 1.7858622e-05]\n",
            "[9.9998271e-01 1.7232673e-05]\n",
            "[9.9998271e-01 1.7232673e-05]\n",
            "[2.9719728e-05 9.9997032e-01]\n",
            "[9.9961412e-01 3.8585608e-04]\n",
            "[9.9998367e-01 1.6290876e-05]\n",
            "[9.9998474e-01 1.5206912e-05]\n",
            "[2.2869091e-05 9.9997711e-01]\n",
            "[9.9998450e-01 1.5530326e-05]\n",
            "[9.9986935e-01 1.3058003e-04]\n",
            "[9.9998462e-01 1.5420967e-05]\n",
            "[9.9998176e-01 1.8231070e-05]\n",
            "[9.9998486e-01 1.5194056e-05]\n",
            "[9.9998271e-01 1.7255614e-05]\n",
            "[9.9997747e-01 2.2503857e-05]\n",
            "[9.9998319e-01 1.6832475e-05]\n",
            "[9.9950278e-01 4.9721566e-04]\n",
            "[9.9998474e-01 1.5238182e-05]\n",
            "[9.9998450e-01 1.5475916e-05]\n",
            "[9.9998164e-01 1.8402921e-05]\n",
            "[2.0226891e-05 9.9997973e-01]\n",
            "[9.9998343e-01 1.6540474e-05]\n",
            "[9.9992311e-01 7.6860975e-05]\n",
            "[9.9998319e-01 1.6815051e-05]\n",
            "[9.9998379e-01 1.6203398e-05]\n",
            "[9.992391e-01 7.609770e-04]\n",
            "[9.9998391e-01 1.6136395e-05]\n",
            "[9.9983835e-01 1.6162377e-04]\n",
            "[9.9996996e-01 3.0053530e-05]\n",
            "[9.999844e-01 1.557984e-05]\n",
            "[1.4271845e-04 9.9985731e-01]\n",
            "[9.9998379e-01 1.6251604e-05]\n",
            "[9.9998379e-01 1.6251604e-05]\n",
            "[9.9998283e-01 1.7155491e-05]\n",
            "[0.01869799 0.98130196]\n",
            "[9.9998474e-01 1.5240769e-05]\n",
            "[9.9998474e-01 1.5240769e-05]\n",
            "[9.9997151e-01 2.8477827e-05]\n",
            "[9.9997163e-01 2.8337117e-05]\n",
            "[9.9995244e-01 4.7579768e-05]\n",
            "[9.9997234e-01 2.7683929e-05]\n",
            "[9.9998403e-01 1.5955919e-05]\n",
            "[9.9998391e-01 1.6150236e-05]\n",
            "[9.9998260e-01 1.7429802e-05]\n",
            "[9.9997902e-01 2.1029091e-05]\n",
            "[9.9998295e-01 1.7040764e-05]\n",
            "[9.9998474e-01 1.5246773e-05]\n",
            "[9.9998438e-01 1.5597472e-05]\n",
            "[9.999845e-01 1.549383e-05]\n",
            "[0.01966841 0.98033154]\n",
            "[9.9997628e-01 2.3695067e-05]\n",
            "[0.99776995 0.00223009]\n",
            "[0.85218716 0.14781289]\n",
            "[9.999838e-01 1.626753e-05]\n",
            "[9.999838e-01 1.626753e-05]\n",
            "[5.0983628e-05 9.9994898e-01]\n",
            "[9.9997723e-01 2.2750279e-05]\n",
            "[9.9996150e-01 3.8528753e-05]\n",
            "[2.9379431e-05 9.9997067e-01]\n",
            "[9.9998462e-01 1.5407411e-05]\n",
            "[9.9998462e-01 1.5407411e-05]\n",
            "[9.9995935e-01 4.0598563e-05]\n",
            "[9.9998474e-01 1.5216879e-05]\n",
            "[9.9997449e-01 2.5541145e-05]\n",
            "[9.9997449e-01 2.5541145e-05]\n",
            "[9.9998200e-01 1.8027718e-05]\n",
            "[9.999763e-01 2.369283e-05]\n",
            "[9.9998486e-01 1.5190375e-05]\n",
            "[9.9998486e-01 1.5135599e-05]\n",
            "[2.2622806e-05 9.9997735e-01]\n",
            "[9.999832e-01 1.685617e-05]\n",
            "[9.9998438e-01 1.5571462e-05]\n",
            "[0.99766004 0.00233997]\n",
            "[3.045102e-05 9.999696e-01]\n",
            "[9.999827e-01 1.732581e-05]\n",
            "[9.9998379e-01 1.6154178e-05]\n",
            "[9.9998283e-01 1.7123160e-05]\n",
            "[9.9998462e-01 1.5376028e-05]\n",
            "[9.999846e-01 1.538703e-05]\n",
            "[9.999846e-01 1.538703e-05]\n",
            "[9.999840e-01 1.593925e-05]\n",
            "[9.9998415e-01 1.5795209e-05]\n",
            "[9.9998415e-01 1.5795209e-05]\n",
            "[9.9998295e-01 1.7092099e-05]\n",
            "[9.9998367e-01 1.6280750e-05]\n",
            "[9.9998450e-01 1.5505315e-05]\n",
            "[9.9967957e-01 3.2044214e-04]\n",
            "[9.9998415e-01 1.5858028e-05]\n",
            "[9.9976438e-01 2.3563181e-04]\n",
            "[3.587180e-05 9.999641e-01]\n",
            "[2.1736625e-05 9.9997830e-01]\n",
            "[9.9998283e-01 1.7151091e-05]\n",
            "[9.9998486e-01 1.5197157e-05]\n",
            "[9.999846e-01 1.531839e-05]\n",
            "[9.9998462e-01 1.5319705e-05]\n",
            "[9.9998319e-01 1.6797036e-05]\n",
            "[9.9998462e-01 1.5376732e-05]\n",
            "[9.9998426e-01 1.5696507e-05]\n",
            "[9.9998474e-01 1.5241190e-05]\n",
            "[9.9998367e-01 1.6320764e-05]\n",
            "[2.2008933e-05 9.9997795e-01]\n",
            "[1.1248127e-04 9.9988747e-01]\n",
            "[9.9997485e-01 2.5098960e-05]\n",
            "[9.9998248e-01 1.7511407e-05]\n",
            "[9.994772e-01 5.227719e-04]\n",
            "[9.5304516e-05 9.9990463e-01]\n",
            "[9.9998093e-01 1.9103834e-05]\n",
            "[9.9998295e-01 1.7056276e-05]\n",
            "[0.9157593  0.08424073]\n",
            "[9.9998474e-01 1.5223498e-05]\n",
            "[9.9998474e-01 1.5311616e-05]\n",
            "[9.9993527e-01 6.4712120e-05]\n",
            "[9.999845e-01 1.548498e-05]\n",
            "[2.2692209e-05 9.9997735e-01]\n",
            "[0.00191556 0.9980844 ]\n",
            "[5.3546755e-05 9.9994648e-01]\n",
            "[9.5854019e-05 9.9990416e-01]\n",
            "[9.9998331e-01 1.6648593e-05]\n",
            "[9.9998331e-01 1.6648593e-05]\n",
            "[9.9981707e-01 1.8300329e-04]\n",
            "[9.999844e-01 1.564449e-05]\n",
            "[9.9998319e-01 1.6758842e-05]\n",
            "[3.807049e-05 9.999620e-01]\n",
            "[2.5579779e-05 9.9997437e-01]\n",
            "[9.9998391e-01 1.6080647e-05]\n",
            "[9.9997890e-01 2.1101008e-05]\n",
            "[9.9998200e-01 1.8059241e-05]\n",
            "[9.999844e-01 1.567304e-05]\n",
            "[9.9930143e-01 6.9854152e-04]\n",
            "[4.5001016e-05 9.9995494e-01]\n",
            "[9.9998283e-01 1.7140135e-05]\n",
            "[0.9725251  0.02747487]\n",
            "[9.9998307e-01 1.6899789e-05]\n",
            "[4.9892540e-05 9.9995005e-01]\n",
            "[9.9998486e-01 1.5104453e-05]\n",
            "[9.9998486e-01 1.5104453e-05]\n",
            "[2.216066e-05 9.999778e-01]\n",
            "[2.863010e-05 9.999714e-01]\n",
            "[6.6091590e-05 9.9993396e-01]\n",
            "[9.9998438e-01 1.5609197e-05]\n",
            "[9.9998474e-01 1.5312406e-05]\n",
            "[9.9998474e-01 1.5224834e-05]\n",
            "[9.9998164e-01 1.8332539e-05]\n",
            "[9.9997497e-01 2.5018810e-05]\n",
            "[9.9998176e-01 1.8259416e-05]\n",
            "[9.9998474e-01 1.5293608e-05]\n",
            "[9.9997854e-01 2.1407084e-05]\n",
            "[9.9998367e-01 1.6327987e-05]\n",
            "[9.9998379e-01 1.6207137e-05]\n",
            "[9.9998355e-01 1.6476242e-05]\n",
            "[3.0257852e-05 9.9996972e-01]\n",
            "[9.9998295e-01 1.7099599e-05]\n",
            "[9.9998462e-01 1.5349744e-05]\n",
            "[9.9998462e-01 1.5345835e-05]\n",
            "[9.9994349e-01 5.6503857e-05]\n",
            "[9.999846e-01 1.535519e-05]\n",
            "[9.9998081e-01 1.9236779e-05]\n",
            "[9.9998415e-01 1.5799473e-05]\n",
            "[9.9998236e-01 1.7647117e-05]\n",
            "[0.99821776 0.00178221]\n",
            "[9.9998450e-01 1.5462992e-05]\n",
            "[2.4530702e-05 9.9997544e-01]\n",
            "[2.4927596e-05 9.9997509e-01]\n",
            "[9.9997973e-01 2.0223419e-05]\n",
            "[2.3437115e-05 9.9997652e-01]\n",
            "[3.0739371e-05 9.9996924e-01]\n",
            "[9.9998236e-01 1.7681718e-05]\n",
            "[2.0914493e-05 9.9997914e-01]\n",
            "[9.9998462e-01 1.5383539e-05]\n",
            "[9.9998283e-01 1.7168766e-05]\n",
            "[9.9997413e-01 2.5901498e-05]\n",
            "[2.6883175e-05 9.9997306e-01]\n",
            "[9.999254e-01 7.466267e-05]\n",
            "[9.9958843e-01 4.1160788e-04]\n",
            "[3.7094476e-05 9.9996293e-01]\n",
            "[9.9998307e-01 1.6932876e-05]\n",
            "[3.4069202e-05 9.9996591e-01]\n",
            "[0.7023034  0.29769665]\n",
            "[9.9998486e-01 1.5118806e-05]\n",
            "[9.9998331e-01 1.6723354e-05]\n",
            "[2.9696324e-05 9.9997032e-01]\n",
            "[9.9998283e-01 1.7111539e-05]\n",
            "[9.999831e-01 1.693575e-05]\n",
            "[0.01621243 0.98378754]\n",
            "[9.9998426e-01 1.5760454e-05]\n",
            "[4.9541697e-05 9.9995041e-01]\n",
            "[9.9997294e-01 2.7053173e-05]\n",
            "[9.9998438e-01 1.5603215e-05]\n",
            "[9.999845e-01 1.553111e-05]\n",
            "[9.9998236e-01 1.7602608e-05]\n",
            "[9.9998474e-01 1.5237296e-05]\n",
            "[9.9998426e-01 1.5713957e-05]\n",
            "[9.9965239e-01 3.4757087e-04]\n",
            "[9.9998319e-01 1.6826698e-05]\n",
            "[9.999844e-01 1.558317e-05]\n",
            "[9.9998426e-01 1.5768090e-05]\n",
            "[9.9998379e-01 1.6239008e-05]\n",
            "[9.9997926e-01 2.0707257e-05]\n",
            "[9.9998415e-01 1.5834921e-05]\n",
            "[9.9998164e-01 1.8325565e-05]\n",
            "[9.999839e-01 1.613244e-05]\n",
            "[9.9998450e-01 1.5535288e-05]\n",
            "[1.03901955e-04 9.99896049e-01]\n",
            "[9.9998128e-01 1.8747027e-05]\n",
            "[3.3901582e-05 9.9996614e-01]\n",
            "[9.9998188e-01 1.8163133e-05]\n",
            "[0.9437561  0.05624393]\n",
            "[9.9998474e-01 1.5280837e-05]\n",
            "[9.9998426e-01 1.5729085e-05]\n",
            "[9.9998462e-01 1.5419642e-05]\n",
            "[9.9998391e-01 1.6114942e-05]\n",
            "[9.9989879e-01 1.0121271e-04]\n",
            "[9.999790e-01 2.098846e-05]\n",
            "[2.0586578e-05 9.9997938e-01]\n",
            "[9.9998450e-01 1.5512504e-05]\n",
            "[9.9998176e-01 1.8182385e-05]\n",
            "[9.999782e-01 2.180385e-05]\n",
            "[9.9998426e-01 1.5725202e-05]\n",
            "[9.9996829e-01 3.1716536e-05]\n",
            "[9.9998105e-01 1.8946241e-05]\n",
            "[2.4994602e-05 9.9997497e-01]\n",
            "[9.9998462e-01 1.5350915e-05]\n",
            "[9.9998450e-01 1.5450287e-05]\n",
            "[9.9998403e-01 1.6005464e-05]\n",
            "[9.9998355e-01 1.6459155e-05]\n",
            "[3.0841584e-05 9.9996912e-01]\n",
            "[0.13956036 0.86043966]\n",
            "[9.9998057e-01 1.9470473e-05]\n",
            "[9.9998105e-01 1.8945808e-05]\n",
            "[2.4682198e-05 9.9997532e-01]\n",
            "[9.9983227e-01 1.6766792e-04]\n",
            "[9.9998426e-01 1.5733211e-05]\n",
            "[9.9998343e-01 1.6611802e-05]\n",
            "[9.9994552e-01 5.4484222e-05]\n",
            "[9.9998045e-01 1.9499274e-05]\n",
            "[9.995111e-01 4.888253e-04]\n",
            "[4.838595e-05 9.999516e-01]\n",
            "[9.998846e-01 1.154274e-04]\n",
            "[9.9998474e-01 1.5282672e-05]\n",
            "[9.999713e-01 2.869789e-05]\n",
            "[9.9998283e-01 1.7202905e-05]\n",
            "[9.9998450e-01 1.5549947e-05]\n",
            "[0.18373795 0.81626207]\n",
            "[9.9998200e-01 1.7957846e-05]\n",
            "[9.9998355e-01 1.6471939e-05]\n",
            "[9.9998438e-01 1.5618132e-05]\n",
            "[3.2761356e-05 9.9996722e-01]\n",
            "[9.9998450e-01 1.5450787e-05]\n",
            "[9.9998426e-01 1.5711739e-05]\n",
            "[9.9998474e-01 1.5283373e-05]\n",
            "[4.8551705e-05 9.9995148e-01]\n",
            "[9.9998450e-01 1.5547426e-05]\n",
            "[3.5565106e-05 9.9996448e-01]\n",
            "[9.9998474e-01 1.5317079e-05]\n",
            "[9.9998403e-01 1.5962585e-05]\n",
            "[9.9998438e-01 1.5640879e-05]\n",
            "[9.9998450e-01 1.5548198e-05]\n",
            "[9.9998415e-01 1.5899095e-05]\n",
            "[2.2678949e-05 9.9997735e-01]\n",
            "[9.9998474e-01 1.5224369e-05]\n",
            "[9.9998450e-01 1.5536607e-05]\n",
            "[9.999095e-01 9.042048e-05]\n",
            "[9.9998462e-01 1.5336429e-05]\n",
            "[0.4117842 0.5882158]\n",
            "[9.9998474e-01 1.5257130e-05]\n",
            "[9.9998474e-01 1.5257130e-05]\n",
            "[9.9953759e-01 4.6240495e-04]\n",
            "[9.999826e-01 1.743858e-05]\n",
            "[9.999826e-01 1.743858e-05]\n",
            "[9.9998367e-01 1.6323518e-05]\n",
            "[1.5598800e-04 9.9984396e-01]\n",
            "[0.32509843 0.67490155]\n",
            "[9.9998367e-01 1.6381362e-05]\n",
            "[9.9998355e-01 1.6457083e-05]\n",
            "[9.9998379e-01 1.6223992e-05]\n",
            "[9.9998426e-01 1.5778018e-05]\n",
            "[9.9998343e-01 1.6593815e-05]\n",
            "[9.9998438e-01 1.5661133e-05]\n",
            "[9.9998212e-01 1.7840033e-05]\n",
            "[9.9998450e-01 1.5477362e-05]\n",
            "[9.9998212e-01 1.7939636e-05]\n",
            "[9.9998403e-01 1.6012502e-05]\n",
            "[9.9998116e-01 1.8818981e-05]\n",
            "[9.9998116e-01 1.8818981e-05]\n",
            "[9.9998486e-01 1.5127432e-05]\n",
            "[1.4700039e-04 9.9985301e-01]\n",
            "[9.999845e-01 1.553800e-05]\n",
            "[9.9998283e-01 1.7179658e-05]\n",
            "[9.9998295e-01 1.7002520e-05]\n",
            "[9.9998248e-01 1.7488761e-05]\n",
            "[9.9994826e-01 5.1700899e-05]\n",
            "[9.9996996e-01 3.0027117e-05]\n",
            "[9.9998474e-01 1.5232326e-05]\n",
            "[9.9998474e-01 1.5232326e-05]\n",
            "[9.9998438e-01 1.5646325e-05]\n",
            "[9.9998450e-01 1.5468331e-05]\n",
            "[0.00234764 0.99765235]\n",
            "[9.9998450e-01 1.5555255e-05]\n",
            "[3.9270955e-05 9.9996078e-01]\n",
            "[9.9998462e-01 1.5352702e-05]\n",
            "[9.9997580e-01 2.4161409e-05]\n",
            "[9.9997830e-01 2.1733578e-05]\n",
            "[9.999685e-01 3.146726e-05]\n",
            "[9.9998438e-01 1.5580212e-05]\n",
            "[9.9998438e-01 1.5580212e-05]\n",
            "[3.0496149e-05 9.9996948e-01]\n",
            "[9.9997938e-01 2.0665062e-05]\n",
            "[9.9998462e-01 1.5362046e-05]\n",
            "[9.9998462e-01 1.5320524e-05]\n",
            "[9.9998426e-01 1.5716654e-05]\n",
            "[9.9998426e-01 1.5688949e-05]\n",
            "[9.9996912e-01 3.0905594e-05]\n",
            "[9.9997079e-01 2.9264897e-05]\n",
            "[3.786104e-05 9.999621e-01]\n",
            "[9.9989939e-01 1.0054377e-04]\n",
            "[9.9998450e-01 1.5456064e-05]\n",
            "[0.99852705 0.00147287]\n",
            "[9.9998271e-01 1.7331611e-05]\n",
            "[9.9998188e-01 1.8110291e-05]\n",
            "[9.9994898e-01 5.0962917e-05]\n",
            "[9.9998462e-01 1.5362748e-05]\n",
            "[9.9998295e-01 1.7046275e-05]\n",
            "[9.9998474e-01 1.5265892e-05]\n",
            "[9.9994135e-01 5.8647216e-05]\n",
            "[9.999840e-01 1.598626e-05]\n",
            "[9.999840e-01 1.598626e-05]\n",
            "[9.9997973e-01 2.0293317e-05]\n",
            "[9.9998343e-01 1.6536169e-05]\n",
            "[9.9991548e-01 8.4495245e-05]\n",
            "[4.7175447e-05 9.9995279e-01]\n",
            "[9.999758e-01 2.416823e-05]\n",
            "[9.9998438e-01 1.5589263e-05]\n",
            "[0.7456612  0.25433883]\n",
            "[9.9998438e-01 1.5601756e-05]\n",
            "[9.9998438e-01 1.5601756e-05]\n",
            "[9.9998367e-01 1.6333579e-05]\n",
            "[9.9998283e-01 1.7136850e-05]\n",
            "[9.9998224e-01 1.7740327e-05]\n",
            "[9.9997854e-01 2.1468768e-05]\n",
            "[0.00126469 0.99873537]\n",
            "[3.5014738e-05 9.9996495e-01]\n",
            "[2.6950276e-05 9.9997306e-01]\n",
            "[9.9998116e-01 1.8858651e-05]\n",
            "[2.0375384e-05 9.9997962e-01]\n",
            "[0.0010307 0.9989693]\n",
            "[9.9998116e-01 1.8860972e-05]\n",
            "[9.9998105e-01 1.9003821e-05]\n",
            "[2.0787948e-05 9.9997926e-01]\n",
            "[9.9998462e-01 1.5368141e-05]\n",
            "[9.999801e-01 1.994602e-05]\n",
            "[9.9998450e-01 1.5443318e-05]\n",
            "[9.9998462e-01 1.5391963e-05]\n",
            "[9.9998474e-01 1.5306638e-05]\n",
            "[1.3112904e-04 9.9986887e-01]\n",
            "[2.8312573e-04 9.9971682e-01]\n",
            "[9.9998152e-01 1.8500372e-05]\n",
            "[9.9998295e-01 1.7068951e-05]\n",
            "[9.9998403e-01 1.5998263e-05]\n",
            "[9.9997103e-01 2.8982162e-05]\n",
            "[9.0388686e-04 9.9909616e-01]\n",
            "[9.9997771e-01 2.2258871e-05]\n",
            "[9.9998283e-01 1.7107297e-05]\n",
            "[9.9998283e-01 1.7107297e-05]\n",
            "[4.1358187e-05 9.9995863e-01]\n",
            "[2.5278207e-05 9.9997473e-01]\n",
            "[9.9998474e-01 1.5290720e-05]\n",
            "[9.9998474e-01 1.5290720e-05]\n",
            "[9.9998415e-01 1.5896896e-05]\n",
            "[9.9998415e-01 1.5896896e-05]\n",
            "[9.9998403e-01 1.5962112e-05]\n",
            "[9.9996507e-01 3.4884153e-05]\n",
            "[9.9996507e-01 3.4884153e-05]\n",
            "[2.2901917e-05 9.9997711e-01]\n",
            "[2.2901917e-05 9.9997711e-01]\n",
            "[9.9994183e-01 5.8156675e-05]\n",
            "[9.9998367e-01 1.6387925e-05]\n",
            "[9.9998367e-01 1.6387925e-05]\n",
            "[9.9998474e-01 1.5236104e-05]\n",
            "[9.9998391e-01 1.6139027e-05]\n",
            "[9.9998188e-01 1.8153729e-05]\n",
            "[9.9998450e-01 1.5451229e-05]\n",
            "[9.9998450e-01 1.5451229e-05]\n",
            "[9.9998426e-01 1.5725082e-05]\n",
            "[9.9998426e-01 1.5783857e-05]\n",
            "[9.9997866e-01 2.1384378e-05]\n",
            "[2.2005093e-05 9.9997795e-01]\n",
            "[9.9998462e-01 1.5402344e-05]\n",
            "[9.9998343e-01 1.6535332e-05]\n",
            "[2.1376669e-05 9.9997866e-01]\n",
            "[2.165116e-05 9.999783e-01]\n",
            "[9.9998379e-01 1.6201328e-05]\n",
            "[2.7933107e-05 9.9997211e-01]\n",
            "[2.4757635e-05 9.9997520e-01]\n",
            "[5.8722431e-05 9.9994123e-01]\n",
            "[9.9998295e-01 1.7083625e-05]\n",
            "[9.9998295e-01 1.7083625e-05]\n",
            "[9.9998224e-01 1.7818678e-05]\n",
            "[9.9997568e-01 2.4318788e-05]\n",
            "[0.001727 0.998273]\n",
            "[0.001727 0.998273]\n",
            "[9.9998426e-01 1.5689728e-05]\n",
            "[0.9602993 0.0397007]\n",
            "[9.999840e-01 1.599866e-05]\n",
            "[9.9998176e-01 1.8280203e-05]\n",
            "[0.9800305  0.01996952]\n",
            "[5.6056902e-05 9.9994397e-01]\n",
            "[9.9998212e-01 1.7852339e-05]\n",
            "[9.9996912e-01 3.0892686e-05]\n",
            "[9.9998462e-01 1.5400214e-05]\n",
            "[9.9998379e-01 1.6194082e-05]\n",
            "[9.9998462e-01 1.5362017e-05]\n",
            "[2.4442877e-05 9.9997556e-01]\n",
            "[9.9998415e-01 1.5870948e-05]\n",
            "[9.9998212e-01 1.7858518e-05]\n",
            "[9.9996853e-01 3.1427255e-05]\n",
            "[9.9996853e-01 3.1427255e-05]\n",
            "[9.9998474e-01 1.5270580e-05]\n",
            "[9.9998426e-01 1.5737623e-05]\n",
            "[9.9998474e-01 1.5201011e-05]\n",
            "[9.9997878e-01 2.1277621e-05]\n",
            "[9.9998474e-01 1.5314683e-05]\n",
            "[0.6309166  0.36908343]\n",
            "[9.9997973e-01 2.0231597e-05]\n",
            "[9.9997139e-01 2.8621966e-05]\n",
            "[9.9998415e-01 1.5901400e-05]\n",
            "[9.9998319e-01 1.6865351e-05]\n",
            "[9.9998319e-01 1.6865351e-05]\n",
            "[9.9998486e-01 1.5116053e-05]\n",
            "[9.9998462e-01 1.5323387e-05]\n",
            "[4.1675434e-05 9.9995828e-01]\n",
            "[9.994204e-01 5.796186e-04]\n",
            "[5.2385134e-05 9.9994767e-01]\n",
            "[2.2090861e-05 9.9997795e-01]\n",
            "[0.07723352 0.9227665 ]\n",
            "[4.2287153e-04 9.9957711e-01]\n",
            "[9.9998248e-01 1.7472374e-05]\n",
            "[9.9997163e-01 2.8402537e-05]\n",
            "[9.9997163e-01 2.8402537e-05]\n",
            "[9.9998415e-01 1.5856727e-05]\n",
            "[9.9996686e-01 3.3084900e-05]\n",
            "[9.9998462e-01 1.5321108e-05]\n",
            "[9.9998295e-01 1.7073120e-05]\n",
            "[9.9998295e-01 1.7073120e-05]\n",
            "[9.9986708e-01 1.3287255e-04]\n",
            "[9.9998450e-01 1.5549087e-05]\n",
            "[9.9998450e-01 1.5553209e-05]\n",
            "[9.9998343e-01 1.6531296e-05]\n",
            "[9.9998009e-01 1.9892466e-05]\n",
            "[9.9998379e-01 1.6159878e-05]\n",
            "[5.5269604e-05 9.9994469e-01]\n",
            "[9.9998224e-01 1.7803035e-05]\n",
            "[9.9998295e-01 1.7074537e-05]\n",
            "[9.9998271e-01 1.7264074e-05]\n",
            "[9.999838e-01 1.616014e-05]\n",
            "[9.999838e-01 1.616014e-05]\n",
            "[9.9998438e-01 1.5572636e-05]\n",
            "[9.9998462e-01 1.5322013e-05]\n",
            "[9.9996889e-01 3.1148757e-05]\n",
            "[9.999343e-01 6.563097e-05]\n",
            "[9.9998438e-01 1.5590393e-05]\n",
            "[2.0508023e-05 9.9997950e-01]\n",
            "[9.9996090e-01 3.9084185e-05]\n",
            "[9.9998295e-01 1.7068642e-05]\n",
            "[9.9996674e-01 3.3220877e-05]\n",
            "[9.9998462e-01 1.5382013e-05]\n",
            "[9.999826e-01 1.738591e-05]\n",
            "[9.9998200e-01 1.8052871e-05]\n",
            "[9.9995148e-01 4.8544112e-05]\n",
            "[0.12457967 0.8754204 ]\n",
            "[9.9998343e-01 1.6551172e-05]\n",
            "[3.214963e-05 9.999678e-01]\n",
            "[9.9997032e-01 2.9672943e-05]\n",
            "[2.0529078e-05 9.9997950e-01]\n",
            "[9.9997568e-01 2.4316561e-05]\n",
            "[9.9998367e-01 1.6289323e-05]\n",
            "[9.9998081e-01 1.9188627e-05]\n",
            "[9.9998093e-01 1.9028119e-05]\n",
            "[3.4793917e-05 9.9996519e-01]\n",
            "[0.9664658  0.03353418]\n",
            "[6.655424e-05 9.999335e-01]\n",
            "[9.9993956e-01 6.0487022e-05]\n",
            "[9.9996686e-01 3.3196560e-05]\n",
            "[2.1299524e-05 9.9997866e-01]\n",
            "[9.9998426e-01 1.5701838e-05]\n",
            "[2.6058175e-05 9.9997389e-01]\n",
            "[9.9998474e-01 1.5217111e-05]\n",
            "[2.604213e-05 9.999740e-01]\n",
            "[9.9998415e-01 1.5826829e-05]\n",
            "[9.9997973e-01 2.0272504e-05]\n",
            "[9.9998474e-01 1.5224049e-05]\n",
            "[9.9998176e-01 1.8252678e-05]\n",
            "[9.9997818e-01 2.1861899e-05]\n",
            "[9.9998426e-01 1.5745700e-05]\n",
            "[9.9998426e-01 1.5770376e-05]\n",
            "[2.8961220e-05 9.9997103e-01]\n",
            "[9.9998164e-01 1.8355808e-05]\n",
            "[9.9998164e-01 1.8355808e-05]\n",
            "[9.9998403e-01 1.6009402e-05]\n",
            "[9.9998331e-01 1.6707827e-05]\n",
            "[9.9998426e-01 1.5706795e-05]\n",
            "[0.9981152  0.00188485]\n",
            "[9.9998450e-01 1.5515345e-05]\n",
            "[9.9998474e-01 1.5303704e-05]\n",
            "[9.9998462e-01 1.5423142e-05]\n",
            "[9.9998403e-01 1.5956603e-05]\n",
            "[9.9997413e-01 2.5907204e-05]\n",
            "[9.9998260e-01 1.7431548e-05]\n",
            "[9.9998260e-01 1.7431548e-05]\n",
            "[9.9998462e-01 1.5369871e-05]\n",
            "[9.99984e-01 1.59691e-05]\n",
            "[0.04117079 0.9588292 ]\n",
            "[9.9997330e-01 2.6749378e-05]\n",
            "[9.9997544e-01 2.4502551e-05]\n",
            "[9.9998450e-01 1.5486134e-05]\n",
            "[9.9998474e-01 1.5297985e-05]\n",
            "[9.9996769e-01 3.2344968e-05]\n",
            "[2.1393760e-05 9.9997866e-01]\n",
            "[2.3325292e-05 9.9997663e-01]\n",
            "[9.9998426e-01 1.5699217e-05]\n",
            "[9.9998355e-01 1.6451968e-05]\n",
            "[9.9998236e-01 1.7671940e-05]\n",
            "[9.9998403e-01 1.5942427e-05]\n",
            "[9.9955755e-01 4.4243372e-04]\n",
            "[5.0058203e-05 9.9994993e-01]\n",
            "[9.9998438e-01 1.5593605e-05]\n",
            "[9.999840e-01 1.600867e-05]\n",
            "[9.9998474e-01 1.5306770e-05]\n",
            "[9.9998224e-01 1.7742746e-05]\n",
            "[9.9998450e-01 1.5443657e-05]\n",
            "[9.9997532e-01 2.4724133e-05]\n",
            "[0.9966724  0.00332759]\n",
            "[3.772995e-05 9.999622e-01]\n",
            "[9.9998319e-01 1.6844711e-05]\n",
            "[9.9998307e-01 1.6881118e-05]\n",
            "[9.9998403e-01 1.5964519e-05]\n",
            "[9.9998415e-01 1.5859057e-05]\n",
            "[0.9903628 0.0096372]\n",
            "[9.9998057e-01 1.9382511e-05]\n",
            "[9.9998331e-01 1.6724629e-05]\n",
            "[9.9998462e-01 1.5371894e-05]\n",
            "[9.9998200e-01 1.8032293e-05]\n",
            "[0.9166644  0.08333555]\n",
            "[9.9989283e-01 1.0719756e-04]\n",
            "[9.9998474e-01 1.5280852e-05]\n",
            "[2.4997988e-05 9.9997497e-01]\n",
            "[5.2502361e-05 9.9994755e-01]\n",
            "[3.3393904e-04 9.9966609e-01]\n",
            "[2.8572384e-05 9.9997139e-01]\n",
            "[9.9997628e-01 2.3731725e-05]\n",
            "[9.9998248e-01 1.7491679e-05]\n",
            "[9.9997890e-01 2.1040805e-05]\n",
            "[9.9998415e-01 1.5892727e-05]\n",
            "[9.9998343e-01 1.6602995e-05]\n",
            "[9.9998283e-01 1.7121201e-05]\n",
            "[2.1258089e-05 9.9997878e-01]\n",
            "[9.9997842e-01 2.1631698e-05]\n",
            "[0.9988267  0.00117327]\n",
            "[9.99984860e-01 1.51979975e-05]\n",
            "[9.9998498e-01 1.5064045e-05]\n",
            "[9.9998248e-01 1.7567225e-05]\n",
            "[2.0850606e-05 9.9997914e-01]\n",
            "[9.9998343e-01 1.6518658e-05]\n",
            "[7.3898016e-05 9.9992609e-01]\n",
            "[9.9998415e-01 1.5805485e-05]\n",
            "[9.9998415e-01 1.5805485e-05]\n",
            "[9.9998426e-01 1.5774798e-05]\n",
            "[9.9998343e-01 1.6605814e-05]\n",
            "[4.803192e-04 9.995197e-01]\n",
            "[9.9998474e-01 1.5317166e-05]\n",
            "[9.9998426e-01 1.5761234e-05]\n",
            "[9.9998462e-01 1.5334044e-05]\n",
            "[9.9998462e-01 1.5334044e-05]\n",
            "[9.999546e-01 4.541207e-05]\n",
            "[9.9972194e-01 2.7807127e-04]\n",
            "[9.9972194e-01 2.7807127e-04]\n",
            "[9.9998415e-01 1.5892243e-05]\n",
            "[0.9974833  0.00251672]\n",
            "[9.9982738e-01 1.7259773e-04]\n",
            "[9.9998331e-01 1.6746015e-05]\n",
            "[9.999846e-01 1.541723e-05]\n",
            "[9.9998426e-01 1.5682039e-05]\n",
            "[9.9998474e-01 1.5291333e-05]\n",
            "[0.00464905 0.99535096]\n",
            "[3.5013934e-05 9.9996495e-01]\n",
            "[9.9998391e-01 1.6115187e-05]\n",
            "[9.9998367e-01 1.6347058e-05]\n",
            "[9.9998474e-01 1.5292690e-05]\n",
            "[9.996886e-01 3.113440e-04]\n",
            "[9.999846e-01 1.540994e-05]\n",
            "[9.9998176e-01 1.8248849e-05]\n",
            "[4.3138309e-05 9.9995685e-01]\n",
            "[9.9998188e-01 1.8122488e-05]\n",
            "[9.9998140e-01 1.8559125e-05]\n",
            "[9.9998438e-01 1.5661179e-05]\n",
            "[9.9998438e-01 1.5615095e-05]\n",
            "[9.9996972e-01 3.0276702e-05]\n",
            "[9.999660e-01 3.401606e-05]\n",
            "[2.2096065e-05 9.9997795e-01]\n",
            "[9.9998474e-01 1.5268804e-05]\n",
            "[9.9998462e-01 1.5364669e-05]\n",
            "[9.9998260e-01 1.7361253e-05]\n",
            "[8.6554515e-05 9.9991345e-01]\n",
            "[9.9998379e-01 1.6247683e-05]\n",
            "[9.9998450e-01 1.5474809e-05]\n",
            "[9.9998450e-01 1.5474809e-05]\n",
            "[9.9998093e-01 1.9065757e-05]\n",
            "[9.9998093e-01 1.9065757e-05]\n",
            "[9.9998271e-01 1.7327975e-05]\n",
            "[9.9998271e-01 1.7327975e-05]\n",
            "[9.9998367e-01 1.6368744e-05]\n",
            "[9.9998176e-01 1.8230670e-05]\n",
            "[9.9997580e-01 2.4195211e-05]\n",
            "[9.9991786e-01 8.2185441e-05]\n",
            "[9.9997485e-01 2.5201281e-05]\n",
            "[9.9998403e-01 1.5966954e-05]\n",
            "[5.3829368e-05 9.9994612e-01]\n",
            "[0.99810237 0.00189761]\n",
            "[9.9982697e-01 1.7306524e-04]\n",
            "[9.999831e-01 1.687769e-05]\n",
            "[2.3065517e-05 9.9997699e-01]\n",
            "[9.999844e-01 1.566763e-05]\n",
            "[9.9998176e-01 1.8206520e-05]\n",
            "[4.0790899e-05 9.9995923e-01]\n",
            "[7.955716e-05 9.999205e-01]\n",
            "[9.9998379e-01 1.6246257e-05]\n",
            "[2.1034246e-05 9.9997902e-01]\n",
            "[2.4790901e-05 9.9997520e-01]\n",
            "[9.999703e-01 2.970318e-05]\n",
            "[9.9997973e-01 2.0267284e-05]\n",
            "[9.9998462e-01 1.5404283e-05]\n",
            "[1.5842685e-04 9.9984157e-01]\n",
            "[9.9997807e-01 2.1890834e-05]\n",
            "[9.9998140e-01 1.8545325e-05]\n",
            "[9.999819e-01 1.817878e-05]\n",
            "[9.999819e-01 1.817878e-05]\n",
            "[9.9998116e-01 1.8838644e-05]\n",
            "[2.852817e-05 9.999715e-01]\n",
            "[2.3097828e-05 9.9997687e-01]\n",
            "[9.9998188e-01 1.8155148e-05]\n",
            "[9.999815e-01 1.845966e-05]\n",
            "[9.9997556e-01 2.4434627e-05]\n",
            "[0.14292623 0.8570738 ]\n",
            "[0.14292623 0.8570738 ]\n",
            "[9.9998391e-01 1.6124965e-05]\n",
            "[9.999821e-01 1.786615e-05]\n",
            "[9.9997735e-01 2.2590317e-05]\n",
            "[9.9997735e-01 2.2590317e-05]\n",
            "[2.1643087e-05 9.9997830e-01]\n",
            "[9.9997675e-01 2.3275967e-05]\n",
            "[9.9998367e-01 1.6296564e-05]\n",
            "[9.9957746e-01 4.2253142e-04]\n",
            "[9.9998367e-01 1.6332224e-05]\n",
            "[9.9998379e-01 1.6240278e-05]\n",
            "[9.9998462e-01 1.5420996e-05]\n",
            "[9.9998450e-01 1.5538148e-05]\n",
            "[9.999831e-01 1.697506e-05]\n",
            "[9.9998450e-01 1.5501662e-05]\n",
            "[9.9997056e-01 2.9497118e-05]\n",
            "[9.9998343e-01 1.6512909e-05]\n",
            "[9.999764e-01 2.363285e-05]\n",
            "[9.9998343e-01 1.6539763e-05]\n",
            "[9.9998438e-01 1.5568181e-05]\n",
            "[9.9998450e-01 1.5481497e-05]\n",
            "[9.9998474e-01 1.5270960e-05]\n",
            "[9.9998415e-01 1.5827689e-05]\n",
            "[9.9996543e-01 3.4605433e-05]\n",
            "[2.167380e-05 9.999783e-01]\n",
            "[9.9998260e-01 1.7421426e-05]\n",
            "[0.6117043  0.38829568]\n",
            "[9.9956590e-01 4.3412906e-04]\n",
            "[9.9998415e-01 1.5871994e-05]\n",
            "[9.9998391e-01 1.6089529e-05]\n",
            "[9.9998450e-01 1.5454412e-05]\n",
            "[2.3514398e-05 9.9997652e-01]\n",
            "[9.9998271e-01 1.7338025e-05]\n",
            "[9.9998367e-01 1.6317745e-05]\n",
            "[3.2018001e-05 9.9996793e-01]\n",
            "[3.1204676e-05 9.9996877e-01]\n",
            "[9.9997807e-01 2.1944510e-05]\n",
            "[9.999207e-01 7.930750e-05]\n",
            "[9.999207e-01 7.930750e-05]\n",
            "[9.9997687e-01 2.3139433e-05]\n",
            "[9.9995661e-01 4.3373646e-05]\n",
            "[9.9997938e-01 2.0596317e-05]\n",
            "[9.9998271e-01 1.7335413e-05]\n",
            "[9.9998355e-01 1.6409505e-05]\n",
            "[3.8343028e-04 9.9961650e-01]\n",
            "[9.9996257e-01 3.7487363e-05]\n",
            "[3.6632895e-04 9.9963367e-01]\n",
            "[9.9998331e-01 1.6745504e-05]\n",
            "[9.9998403e-01 1.5945801e-05]\n",
            "[2.4763844e-05 9.9997520e-01]\n",
            "[9.999714e-01 2.858124e-05]\n",
            "[3.8351216e-05 9.9996161e-01]\n",
            "[9.9998462e-01 1.5352554e-05]\n",
            "[9.9998462e-01 1.5352554e-05]\n",
            "[7.9707199e-05 9.9992025e-01]\n",
            "[9.9998438e-01 1.5668213e-05]\n",
            "[9.9997771e-01 2.2338832e-05]\n",
            "[9.9998057e-01 1.9430925e-05]\n",
            "[2.8789794e-04 9.9971205e-01]\n",
            "[0.26674557 0.73325443]\n",
            "[9.999839e-01 1.603634e-05]\n",
            "[9.9998283e-01 1.7126769e-05]\n",
            "[9.9998426e-01 1.5695639e-05]\n",
            "[9.9998295e-01 1.7024455e-05]\n",
            "[9.9998391e-01 1.6113236e-05]\n",
            "[0.4168021 0.5831979]\n",
            "[9.9997413e-01 2.5868861e-05]\n",
            "[9.9998450e-01 1.5463464e-05]\n",
            "[0.94206876 0.05793119]\n",
            "[2.2174845e-05 9.9997783e-01]\n",
            "[9.9998403e-01 1.5949894e-05]\n",
            "[3.213553e-05 9.999678e-01]\n",
            "[9.9909091e-01 9.0905465e-04]\n",
            "[9.9998176e-01 1.8212408e-05]\n",
            "[9.993954e-01 6.046094e-04]\n",
            "[2.2843176e-05 9.9997711e-01]\n",
            "[9.9998474e-01 1.5292382e-05]\n",
            "[2.1239222e-05 9.9997878e-01]\n",
            "[9.9997628e-01 2.3775312e-05]\n",
            "[9.9998355e-01 1.6423061e-05]\n",
            "[9.999770e-01 2.299793e-05]\n",
            "[0.00320805 0.9967919 ]\n",
            "[9.9998283e-01 1.7213573e-05]\n",
            "[9.9998403e-01 1.6027841e-05]\n",
            "[9.9998248e-01 1.7531393e-05]\n",
            "[9.9998271e-01 1.7286975e-05]\n",
            "[9.9980789e-01 1.9215312e-04]\n",
            "[9.9997139e-01 2.8660752e-05]\n",
            "[9.999831e-01 1.687431e-05]\n",
            "[0.00394947 0.99605054]\n",
            "[9.9998283e-01 1.7135706e-05]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/task1tamil-results.txt\",predictions, fmt=\"%s\")\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/buffer/true labels/task1tamil-labels.txt\",true_labels, fmt=\"%s\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print('True positives: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "outputId": "d169c48a-d001-47b2-b24b-53950a9095c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI2K1bqaR5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "outputId": "6dd01c0d-9a75-48f6-cba9-5d5421d7d4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "outputId": "6d642aeb-d049-4186-efda-9d8da3fcb0b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-task1tamil'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task1tamil\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task1tamil/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1tamil/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1tamil/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    }
  ]
}
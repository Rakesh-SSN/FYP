{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT%20TAMIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "944f84e2-1371-48b7-b3aa-da9aeb6fd241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "1cbf12dc-15e1-4d50-a2a4-e56b4114ebc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "67694178-089c-4e99-eca7-618b4df12884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/tamil/task1tamil.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1447</th>\n",
              "      <td>TAM1448</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>கலாபவன்மணிக்கு மது பழக்கம் காரணமாக கல்லீரல் பா...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114</th>\n",
              "      <td>TAM1115</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>இந்த 112 சிலைகளையும் ஆய்வு செய்ய மத்திய தொல்லி...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1064</th>\n",
              "      <td>TAM1065</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>போராட்டம் நடந்து வரும் நிலையில் மதுரையில் ஒரு ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2287</th>\n",
              "      <td>TAM2288</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>இந்தியாவுடன், ஐதராபாத் பகுதியை இணைக்க, நிஜாம் ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1537</th>\n",
              "      <td>TAM1538</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>பழம்பெரும் திரைப்பட இயக்குனர் திருலோகசந்தர் மற...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>668</th>\n",
              "      <td>TAM0669</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>ஜல்லிக்கட்டு விவகாரதில் விலங்குகள் நலவாரியத்து...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1583</th>\n",
              "      <td>TAM1584</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>இந்த சம்பவம் பற்றி புதிய தகவல்களை இந்திய ராணுவ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2404</th>\n",
              "      <td>TAM2405</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ஜம்மு - காஷ்மீர் மாநிலத்தில், பா.ஜ., மற்றும் ம...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>TAM0498</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>சந்திரபாபு நாயுடு அரசை கண்டித்து ஜெகன் மோகன் ர...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2480</th>\n",
              "      <td>TAM2481</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>லெக் பீஸ்' வேண்டாம்; முழு கோழின்னா 'ஓ.கே.,!'அம...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "1447         TAM1448  ...  கலாபவன்மணிக்கு மது பழக்கம் காரணமாக கல்லீரல் பா...\n",
              "1114         TAM1115  ...  இந்த 112 சிலைகளையும் ஆய்வு செய்ய மத்திய தொல்லி...\n",
              "1064         TAM1065  ...  போராட்டம் நடந்து வரும் நிலையில் மதுரையில் ஒரு ...\n",
              "2287         TAM2288  ...  இந்தியாவுடன், ஐதராபாத் பகுதியை இணைக்க, நிஜாம் ...\n",
              "1537         TAM1538  ...  பழம்பெரும் திரைப்பட இயக்குனர் திருலோகசந்தர் மற...\n",
              "668          TAM0669  ...  ஜல்லிக்கட்டு விவகாரதில் விலங்குகள் நலவாரியத்து...\n",
              "1583         TAM1584  ...  இந்த சம்பவம் பற்றி புதிய தகவல்களை இந்திய ராணுவ...\n",
              "2404         TAM2405  ...  ஜம்மு - காஷ்மீர் மாநிலத்தில், பா.ஜ., மற்றும் ம...\n",
              "497          TAM0498  ...  சந்திரபாபு நாயுடு அரசை கண்டித்து ஜெகன் மோகன் ர...\n",
              "2480         TAM2481  ...  லெக் பீஸ்' வேண்டாம்; முழு கோழின்னா 'ஓ.கே.,!'அம...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "57a104ed-3142-41a8-c73c-aedabe18f067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1673</th>\n",
              "      <td>கோயம்பேடு மார்க்கெட்டில் ஒரு கிலோ தக்காளி ரூ. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1802</th>\n",
              "      <td>ஏழை மக்களும் மருத்துவ வசதிகளை எளிதாகப் பெறுவதை...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1351</th>\n",
              "      <td>புரோக்கர்கள் பவித்ராவுக்கு பெற்றோர் கிடையாது.&lt;...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1714</th>\n",
              "      <td>இதைமுன்னிட்டு ஜனாதிபதி தேர்தல் நடத்தப்பட உள்ளத...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>தமிழகத்தில் மீண்டும் ஆட்சியில் அமர அ.தி.மு.க.-...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1673  கோயம்பேடு மார்க்கெட்டில் ஒரு கிலோ தக்காளி ரூ. ...      0\n",
              "1802  ஏழை மக்களும் மருத்துவ வசதிகளை எளிதாகப் பெறுவதை...      0\n",
              "1351  புரோக்கர்கள் பவித்ராவுக்கு பெற்றோர் கிடையாது.<...      0\n",
              "1714  இதைமுன்னிட்டு ஜனாதிபதி தேர்தல் நடத்தப்பட உள்ளத...      0\n",
              "1001  தமிழகத்தில் மீண்டும் ஆட்சியில் அமர அ.தி.மு.க.-...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "1658be7f-0ec4-46b7-fd5f-c26fbf3038dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "2ffc0e61-ac9c-417f-f439-d6e420c5e8fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Tokenized:  ['ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'பி', '##ர', '##ச', '##ார', '##ம', 'ச', '##ெ', '##ய', '##தா', '##ர', '.', '<', 'eo', '##l', '>', 'தி', '.', 'மு', '.', 'க', '.', ',', 'வ', '##ே', '##ட', '##பா', '##ள', '##ர', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'சி', '##ன', '##ன', 'ச', '##ே', '##ல', '##ம', 'பகுதி', '##ய', '##ில', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'ஓ', '##ட', '##டு', 'ச', '##ே', '##க', '##ரி', '##த', '##தா', '##ர', '.']\n",
            "Token IDs:  [1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "85fc3968-1b2e-40c8-9f02-3c0a85f9f922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Token IDs: [101, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "a088e980-4d01-4900-d52c-627e30ae9e87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "c035dcca-40fe-4a68-983d-fa1c7d3ccce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "85507d28-1258-414d-a8c6-fc84e3614086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "print(train_inputs)\n",
        "print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "#print(train_masks)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101   1154  35186 ...  55186  95512  31484]\n",
            " [   101   1170 111305 ...   1154 111312  15220]\n",
            " [   101   1163  14124 ...  28065    119    133]\n",
            " ...\n",
            " [   101   1163  27883 ...  27885   1165  18827]\n",
            " [   101  49189  66171 ...  86728  81773  20242]\n",
            " [   101   1142  46168 ...  37076  24171  19894]]\n",
            "[1 0 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "c7e44cd4-48b8-4bfd-a3cd-8c3a82b826d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "outputId": "d9c2367b-c1c8-4165-f4e6-e3c86a2258ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "outputId": "425341ea-e988-41b3-8717-028a8815b47a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:17.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifs2QgT9HeUe",
        "colab_type": "code",
        "outputId": "a6dd255a-5379-44fd-8ee5-20de42a693a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeVxWZf7/8dd9s8omi4AIN4gLiygo\ni0uau4SmZqVmWS45Tk01zfSdvi2T2ozV18ls2ianr6WTmvtuOam4VxqbCy6IipogLiiCK4vC749+\n8h1yxdBzA+/n4+HjIdc55zrvm+sBfLi47uuYysvLyxERERERkRrBbHQAERERERG5fSrgRURERERq\nEBXwIiIiIiI1iAp4EREREZEaRAW8iIiIiEgNogJeRERERKQGUQEvIlJHTZo0idDQUPLy8u7o+uLi\nYkJDQxk3blw1J6uaOXPmEBoayvbt2w3NISJyr9gaHUBEpC4LDQ297XPXrl1LQEDAXUwjIiI1gQp4\nEREDTZw4sdLHaWlpzJs3j8cee4yYmJhKxzw9Pav13n/84x/5/e9/j4ODwx1d7+DgQHp6OjY2NtWa\nS0REbk4FvIiIgR566KFKH1+5coV58+bRunXra47dSHl5OZcuXcLJyalK97a1tcXW9tf9GLjT4l9E\nRO6c1sCLiNQgmzZtIjQ0lG+++Ybp06eTkJBAq1at+OqrrwDYunUrr7zyCvHx8URFRREdHc3QoUNZ\nv379NX1dbw381bbs7Gzeffdd7r//flq1asXDDz/MDz/8UOn6662B/8+2lJQUHn/8caKiomjfvj3j\nxo3j0qVL1+TYvHkzgwYNolWrVnTq1Im//e1v7Nmzh9DQUKZMmXLHn6tTp04xbtw4OnfuTMuWLenW\nrRtvv/02hYWFlc67ePEiH3zwAQ888ACRkZHExcXRr18/Pvjgg0rnrVmzhscff5x27doRGRlJt27d\nePHFF8nOzr7jjCIid0Iz8CIiNdDnn3/OuXPnePTRR/Hy8sJisQCwcuVKsrOz6dOnD40aNSI/P58l\nS5bw7LPP8sknnxAfH39b/f/pT3/CwcGB3/zmNxQXF/Pll1/yu9/9jsTERHx9fW95/c6dO1m1ahUD\nBw6kf//+bNmyhXnz5mFvb8+YMWMqztuyZQujR4/G09OTZ555BhcXF1asWEFycvKdfWL+v4KCAh57\n7DFyc3MZNGgQYWFh7Ny5k6+++oqkpCTmz59PvXr1ABg7diwrVqzg4YcfpnXr1pSWlnL48GF+/PHH\niv6+//57XnjhBVq0aMGzzz6Li4sLJ06c4IcffiAnJ6fi8y8ici+ogBcRqYFOnjzJt99+i7u7e6X2\nP/7xj9cspXnqqafo378///znP2+7gPf19eXjjz/GZDIBVMzkL1iwgBdeeOGW12dmZrJw4UJatGgB\nwOOPP87w4cOZN28er7zyCvb29gBMmDABOzs75s+fj5+fHwBPPPEEQ4YMua2cN/LZZ5+Rk5PDO++8\nw8CBAyvamzdvzrvvvlvxC0l5eTnr1q2jZ8+eTJgw4Yb9rVmzBoDp06fj6upa0X47nwsRkeqmJTQi\nIjXQo48+ek3xDlQq3i9dusSZM2coLi6mbdu2ZGRkUFJSclv9Dx8+vKJ4B4iJicHOzo7Dhw/f1vVx\ncXEVxftV7du3p6SkhGPHjgFw9OhRMjMzeeCBByqKdwB7e3uGDRt2W/e5kat/KXjkkUcqtT/55JO4\nurqSmJgIgMlkwtnZmczMTLKysm7Yn6urK+Xl5axatYorV678qmwiIr+WZuBFRGqgxo0bX7f95MmT\nfPDBB6xfv54zZ85cc/zcuXN4eXndsv9fLgkxmUzUr1+fgoKC28p3vSUlV3/hKCgoICgoiJycHACC\ng4OvOfd6bbervLyc3Nxc2rdvj9lceZ7K3t6ewMDAinsDvPHGG/z5z3+mT58+BAUF0a5dO7p3707X\nrl0rfokZPnw4GzZs4I033uBvf/sbsbGx3H///fTp0wcPD487zioicidUwIuI1EBX12//pytXrjBi\nxAhycnIYNmwYERERuLq6YjabmTt3LqtWraKsrOy2+v9l4XtVeXn5r7q+Kn3cK71796Zdu3Zs2rSJ\n5ORkvv/+e+bPn0+HDh344osvsLW1pUGDBixZsoSUlBQ2b95MSkoKb7/9Nh9//DFTp06lZcuWRr8M\nEalDVMCLiNQSu3btIisri//6r//imWeeqXTs6i411sTf3x+AQ4cOXXPsem23y2Qy4e/vz8GDBykr\nK6v0y0RJSQlHjhwhMDCw0jWenp4MGDCAAQMGUF5ezv/8z/8wY8YMNm3aRPfu3YGft93s0KEDHTp0\nAH7+fA8cOJD//d//5ZNPPrnjvCIiVaU18CIitcTVQvWXM9y7d+9m48aNRkS6qYCAAEJCQli1alXF\nunj4ucieMWPGr+q7Z8+eHD9+nKVLl1Zqnz17NufOnaNXr14AlJaWcv78+UrnmEwmwsPDASq2nMzP\nz7/mHs2aNcPe3v62lxWJiFQXzcCLiNQSoaGhNG7cmH/+85+cPXuWxo0bk5WVxfz58wkNDWX37t1G\nR7zGa6+9xujRoxk8eDBDhgzB2dmZFStWVHoD7Z149tlnWb16NWPGjGHHjh2Ehoaya9cuFi9eTEhI\nCCNGjAB+Xo/fs2dPevbsSWhoKJ6enmRnZzNnzhw8PDzo0qULAK+88gpnz56lQ4cO+Pv7c/HiRb75\n5huKi4sZMGDAr/00iIhUiQp4EZFawt7ens8//5yJEyeyaNEiiouLCQkJ4e9//ztpaWlWWcB37NiR\nKVOm8MEHH/DZZ59Rv359+vbtS8+ePRk6dCiOjo531K+7uzvz5s3jk08+Ye3atSxatAgvLy+efPJJ\nfv/731e8h8DV1ZUnn3ySLVu28N1333Hp0iW8vb2Jj4/nmWeewdPTE4BHHnmEZcuWsXjxYs6cOYOr\nqyvNmzdn8uTJ9OjRo9o+HyIit8NUbm3vJhIRkTpv+fLl/Pd//zeffvopPXv2NDqOiIhV0Rp4EREx\nTFlZ2TV705eUlDB9+nTs7e2JjY01KJmIiPXSEhoRETHM+fPn6dOnD/369aNx48bk5+ezYsUK9u/f\nzwsvvHDdh1WJiNR1KuBFRMQwjo6OdOzYkdWrV3Pq1CkAmjRpwltvvcXgwYMNTiciYp20Bl5ERERE\npAbRGngRERERkRpEBbyIiIiISA2iNfBVdObMBcrK7v2qIy8vF06fPn/rE+We0ZhYJ42L9dGYWCeN\ni/XRmFgnI8bFbDbh4eF8w+Mq4KuorKzckAL+6r3FumhMrJPGxfpoTKyTxsX6aEysk7WNi5bQiIiI\niIjUICrgRURERERqEBXwIiIiIiI1iAp4EREREZEaRAW8iIiIiEgNogJeRERERKQGUQEvIiIiIlKD\nqIAXEREREalBVMCLiIiIiNQgehKrlduy+ziLN2aRf7YYTzcHHunSlA4RDY2OJSIiIiIGUQFvxbbs\nPs70b/dScrkMgNNni5n+7V4AFfEiIiIidZSW0FixxRuzKor3q0oul7F4Y5ZBiURERETEaCrgrdjp\ns8VVahcRERGR2k8FvBXzcnO4brujnQ1FJZfvcRoRERERsQYq4K3YI12aYm9beYjMJhNFpVcYNzWZ\n3YfyDUomIiIiIkZRAW/FOkQ0ZHjvMLzcHDDx84z8qL7hvDY0GhsbM+/P2860f2dwsajU6KgiIiIi\nco9oFxor1yGiIR0iGuLt7Upe3rmK9r+OjGPZD4dYlZTNzoOnGRYfSpsQbwOTioiIiMi9oBn4Gsre\nzoZBXZsxZngMrvXs+WTxTj5btouzF0uMjiYiIiIid5EK+BqucUM3xo2IZcD9waRl5jHm8ySS9pyg\nvLzc6GgiIiIicheogK8FbG3M9O8YzJsj4/B2d+R/l+/mk0U7OXNO202KiIiI1DYq4GuRAG8X/vxU\nDIO7NWP34XzGfJHEph25mo0XERERqUVUwNcyNmYzCe0CGf90Wyw+Lnz57V7en7edUwWXjI4mIiIi\nItVABXwt5evpxCtPtOGp+BCycs8ydmoya1KzKdNsvIiIiEiNpgK+FjObTHSLDuCtUW1pHlCf2Wv2\n8+6srRw7fcHoaCIiIiJyh1TA1wEN6tfjpcFRjHownKN5F3hzWgr//vEnrpSVGR1NRERERKpID3Kq\nI0wmEx1b+RER7MlXq/excEMWKXtP8nSfcCw+LkbHExEREZHbZOgMfElJCe+99x6dOnUiMjKSwYMH\ns2XLllte98knnxAaGnrNv44dO173/AULFtC7d29atWrFAw88wKxZs6r7pdQY7i4OPP9wS343oCVn\nzhYx/ssUln53kMtXNBsvIiIiUhMYOgP/2muvsXr1aoYNG0ZQUBBLlixh9OjRzJw5kzZt2tzy+vHj\nx+Po6Fjx8X/+/6q5c+fy5ptvkpCQwMiRI0lNTWX8+PEUFxfz9NNPV+vrqSlMJhNxYT6EB3kwZ80+\nlv9wmLTMPEb2CadJIzej44mIiIjITRhWwKenp7NixQpef/11RowYAcCAAQPo27cvkyZNuq1Z8t69\ne+PmduOCs6ioiA8++IAePXrw0UcfATB48GDKysr4xz/+waBBg3B1da2W11MTudSzY3S/CNqG+zJj\nVSbvzEzlgbhAHro/GAc7G6PjiYiIiMh1GLaEZuXKldjZ2TFo0KCKNgcHBwYOHEhaWhonT568ZR/l\n5eWcP3/+hg8qSkpKoqCggCeeeKJS+9ChQ7lw4QKbNm36dS+ilohq1oC3RrWjc1QjViYf4c1pyWQe\nOWN0LBERERG5DsMK+IyMDIKDg3F2dq7UHhkZSXl5ORkZGbfso2vXrsTExBATE8Prr79OQUFBpeN7\n9uwBoGXLlpXaIyIiMJvNFccFnBxtGZ4Qxn8PaU1ZWTnvzt7GzNWZXCq+bHQ0EREREfkPhi2hycvL\nw9fX95p2b29vgJvOwLu5ufHUU08RFRWFnZ0dP/74I/PmzWPPnj0sWLAAe3v7invY29vj7u5e6fqr\nbbczy1/XhDf25K1R7Vi86SBrUrNJP3CK4QlhtGziZXQ0EREREcHAAr6oqAg7O7tr2h0cHAAoLi6+\n4bXDhw+v9HFCQgLNmzdn/PjxLF26lMGDB9/0Hlfvc7N73IiXl3FbLnp737v1+i8+Hk18h8Z8NG8b\nf5+/g+6xFn7zUEtcnezvWYaa4F6Oidw+jYv10ZhYJ42L9dGYWCdrGxfDCnhHR0dKS0uvab9aVF8t\n5G/X448/znvvvceWLVsqCnhHR0dKSkque35xcXGV7wFw+vR5ysquv+b+bvL2diUv79w9vaeXsx1j\nh8Ww/IfDfPvjEdIyTvBkfCgxod73NIe1MmJM5NY0LtZHY2KdNC7WR2NinYwYF7PZdNNJY8PWwHt7\ne193CUteXh4APj4+VerPbDbj6+tLYWFhpXuUlpZesza+pKSEgoKCKt+jLrKzteHRLk0ZOzyW+s72\nfLpkJ5OX7uLshev/YiQiIiIid5dhBXxYWBiHDh3iwoULldp37NhRcbwqSktLOXbsGB4eHhVt4eHh\nAOzatavSubt27aKsrKziuNxaUENXxgyP5eHOTdi+P48xXySxZffxG+4AJCIiIiJ3h2EFfEJCAqWl\npSxYsKCiraSkhMWLFxMdHV3xBtfc3FyysrIqXZufn39Nf1OnTqW4uJj777+/oq19+/a4u7sze/bs\nSufOmTMHJycnOnfuXJ0vqdaztTHT777GvDmyLb4e9fj86z18tDCd/LNFRkcTERERqTMMWwMfFRVF\nQkICkyZNIi8vj8DAQJYsWUJubi4TJkyoOO/VV18lOTmZzMzMirZu3brRp08fQkJCsLe3JykpiVWr\nVhETE0Pfvn0rznN0dOTFF19k/Pjx/OEPf6BTp06kpqayfPlyXn755Zs+BEpuzL+BM68/GcOa1GwW\nbzrI2KlJDO7WjM5RjTCZTEbHExEREanVDCvgASZOnMiHH37IsmXLKCwsJDQ0lClTphATE3PT6/r1\n68fWrVtZuXIlpaWl+Pv789xzz/HMM89ga1v5JQ0dOhQ7OzumTZvG2rVr8fPz44033mDYsGF386XV\nemazifi2gbRu3oAvv93L9JWZJGecZHjvMHzc6xkdT0RERKTWMpVrEXOV1KVdaG5XWXk5m3bkMn/d\nAcrKy3m0c1N6xARgNtfu2XhrHpO6TONifTQm1knjYn00JtZJu9BIrWQ2meja2p+3f9OOsEAP5qzd\nz4RZaeSeunDri0VERESkSlTAS7XxdHPkDwMjGd23BcdPX+Qv/0rmm82HuXylzOhoIiIiIrWGoWvg\npfYxmUx0aNmQFsGezFqdyeJNB0nNPMnTfcIJ9LWup5iJiIiI1ESagZe7or6zPc893IrnH25JwfkS\n3pqeyuJNWZRe1my8iIiIyK+hGXi5q2JCfQgN9GDu2v18s/kntu47xcjeYTT1r290NBEREZEaSTPw\ncte51LPjN31b8MdBURSVXOZ/ZqYxd+1+ikuvGB1NREREpMZRAS/3TGRTL94a1Y6ubfxZnZLNuKlJ\nZPx0xuhYIiIiIjWKCni5p+o52PLUA6G88ngbTJh4b842Zqzcy6Xiy0ZHExEREakRVMCLIcKCPPjr\nqLY80NbCxh25jPkiifSsU0bHEhEREbF6KuDFMA52NjzWvTl/fiqGeg62fLggnc+/3sP5S6VGRxMR\nERGxWirgxXBNG9XnzRFx9LuvMckZJxjz+Y+k7j1pdCwRERERq6QCXqyCna2Zhzs3YezwWDxcHZm8\ndBefLtlJ4flio6OJiIiIWBUV8GJVAn1dGTM8hke7NGHHgdOM+SKJH3Yeo7y83OhoIiIiIlZBBbxY\nHRuzmQc7NOavT8fh5+XM1BUZfLggnfyzRUZHExERETGcCnixWn5ezrw2NJrHezYnM/sMY75IYv22\no5RpNl5ERETqMBXwYtXMZhO9Yi28NaodwX5uzFyVyXuzt3HizEWjo4mIiIgYQgW81Aje7vV4eUhr\nRvQO48jJc7w5NZlVyUcoK9NsvIiIiNQttkYHELldJpOJzlGNaNXEi5mrMpm37gApe08ysncY/t4u\nRscTERERuSc0Ay81joerA79/tBW/7d+Ck2cu8dcvU/j6h0NcvlJmdDQRERGRu04z8FIjmUwm2rdo\nSIsgT2av2ceS7w6RmpnH033CCWroanQ8ERERkbtGM/BSo7k52/PsQy154ZFWnL1QwlvTU1m0MYvS\ny1eMjiYiIiJyV2gGXmqF6BBvQgPdmbf2ACu2/MTWfXmM7B1Os4D6RkcTERERqVaagZdaw9nRjqcf\nDOe/HouipLSMCV+lMTtxH8Ulmo0XERGR2kMFvNQ6LYO9GD+qLd2i/VmTlsPYqUnsOZxvdCwRERGR\naqECXmqleg62PBkfymtDo7Exm5g0dztffpvBxaLLRkcTERER+VVUwEutFmJx569PtyWhXSDfpR9j\n7NQkth84ZXQsERERkTumAl5qPXs7GwZ3a8aYYbE4Odry8cJ0pizfzbmLJUZHExEREakyFfBSZwT7\nufHmiDge6hRMyt6TjPkiieSME5SXlxsdTUREROS2qYCXOsXWxsxDnYJ5c0QcXm6OfLZsN/9YvJOC\n88VGRxMRERG5LSrgpU4K8HHhjWExDOrWlF2H8hnzeRLfpx/TbLyIiIhYPRXwUmfZmM30bhfEX59u\ni7+3M9P+ncEH83dwqvCS0dFEREREbkgFvNR5DT2deHVoNEN7hbA/p5CxU5NZm5ZDmWbjRURExAqp\ngBcBzCYTPWICeOs3bWnmX59ZifuYOGsrx/MvGh1NREREpBIV8CL/oUH9evzX4ChG9gkjJ+8Cb05L\n5tukn7hSVmZ0NBEREREAbI0OIGJtTCYT90c2omWwF1+tzmTB+ixS955kZJ9wArxdjI4nIiIidZxm\n4EVuwMPVgRceacWzD0VwqrCIv/4rhWXfH+LyFc3Gi4iIiHE0Ay9yEyaTibbhvoQHeTBnzX6WfX+I\ntMyfZ+O9vV2NjiciIiJ1kGbgRW6Dq5M9v+0fwYuPRnL+Uilvz0jly292U1J6xehoIiIiUsdoBl6k\nClo3b0CIpR3z1x9g0foDfL/9KCP7hBNicTc6moiIiNQRmoEXqSInRztG9A7nrWc6cKWsnHdnbWXW\n6n0UlVw2OpqIiIjUASrgRe5Q6xAfxo9qS4+YANZtzWHsF8nsPpRvdCwRERGp5Qwt4EtKSnjvvffo\n1KkTkZGRDB48mC1btlS5n9GjRxMaGso777xzzbHQ0NDr/pszZ051vASp4xztbXmiVwivPRmNna2Z\n9+dtZ9qKDC4UlRodTURERGopQ9fAv/baa6xevZphw4YRFBTEkiVLGD16NDNnzqRNmza31ceGDRtI\nTU296TmdOnWif//+ldqioqLuOLfILzUPcOevT8ex/IfDfPvjEXYeOs2w+FDahHgbHU1ERERqGcMK\n+PT0dFasWMHrr7/OiBEjABgwYAB9+/Zl0qRJzJo165Z9lJSUMGHCBEaNGsUnn3xyw/OaNGnCQw89\nVF3RRa7LztaGR7s0JTbUh6krMvhk8U7ahvvwRK8Q3JzsjY4nIiIitYRhS2hWrlyJnZ0dgwYNqmhz\ncHBg4MCBpKWlcfLkyVv2MWPGDIqKihg1atQtzy0qKqK4uPhXZRa5HUENXRk3IpYB9weTlpnHmM+T\n+HHPccrLy42OJiIiIrWAYQV8RkYGwcHBODs7V2qPjIykvLycjIyMm16fl5fH5MmTeemll6hXr95N\nz124cCGtW7cmMjKSfv36kZiY+Kvzi9yMrY2Z/h2D+cvIOLzd6zFl+R4+WbSTM+f0S6SIiIj8OoYV\n8Hl5efj4+FzT7u3985rhW83A//3vfyc4OPiWS2PatGnDSy+9xOTJkxk3bhwlJSW88MILfPPNN3ce\nXuQ2+Xu78MZTMTzWvRl7Ducz5oskNu3I1Wy8iIiI3DHD1sAXFRVhZ2d3TbuDgwPATZe7pKens3Tp\nUmbOnInJZLrpfebOnVvp44cffpi+ffvy3nvv8eCDD97y+l/y8nKp0vnVydvb1bB7y/Xd7pg8+WAE\n3dsF8cn87Xz57V62HzjN84OiaOjlfOuLpcr0tWJ9NCbWSeNifTQm1snaxsWwAt7R0ZHS0mu32rta\nuF8t5H+pvLycd955h/j4eGJjY6t8XycnJ4YMGcL777/PwYMHadq0aZWuP336PGVl93721Nvblby8\nc/f8vnJjVR0TO+CPAyPZuD2X+esP8MJ763m0SxO6xwRgruIvknJj+lqxPhoT66RxsT4aE+tkxLiY\nzaabThobVsB7e3tfd5lMXl4ewHWX1wAkJiaSnp7OSy+9RE5OTqVj58+fJycnhwYNGuDo6HjDe/v5\n+QFQWFh4p/FF7ojZZKJbG38im3gxfdVeZq/ZT/Lek4zsHYafZuNFRETkNhi2Bj4sLIxDhw5x4cKF\nSu07duyoOH49ubm5lJWVMXz4cHr06FHxD2Dx4sX06NGD5OTkm947OzsbAE9Pz1/7MkTuiFd9R14a\nFMWoB8M5duoCb05LYcWWw1wpKzM6moiIiFg5w2bgExISmDZtGgsWLKjYB76kpITFixcTHR2Nr68v\n8HPBfunSpYqlLt27dycgIOCa/p5//nm6devGwIEDiYiIACA/P/+aIv3MmTPMnj2bgIAAGjdufPde\noMgtmEwmOrbyo2WwJ18l7mPRxoOkZubxdJ9wLD7GvddCRERErJthBXxUVBQJCQlMmjSJvLw8AgMD\nWbJkCbm5uUyYMKHivFdffZXk5GQyMzMBCAwMJDAw8Lp9WiwWevbsWfHxrFmzWLt2LV27dqVRo0ac\nOHGCefPmkZ+fz6effnp3X6DIbarv4sDzD7cide9JvlqdyfgvU+jTPoi+9zXGztawP5KJiIiIlTKs\ngAeYOHEiH374IcuWLaOwsJDQ0FCmTJlCTExMtfTfpk0btm7dyoIFCygsLMTJyYnWrVvzzDPPVNs9\nRKpLbJgPYUEezFmzj683H2brvjxG9gmnSSM3o6OJiIiIFTGVa0PqKtEuNHLV3RyT9KxTTF+ZScH5\nYuLjLAy4vwkOdjZ35V61jb5WrI/GxDppXKyPxsQ6WeMuNPr7vIgVimzagLd/044uUY1YlZzNm9OS\nyTxyxuhYIiIiYgVUwItYqXoOtgxLCOO/H29DeXk5787exsxVmVwqvmx0NBERETGQCngRKxce5MH4\np9sRH2dhw7ajjJ2axM6Dp42OJSIiIgZRAS9SAzjY2zCkR3NefyoGBzsbPpi/g6nf7OH8pWufZiwi\nIiK1mwp4kRqkmX99/jKyLX3vC2LL7hOM+SKJtMw8o2OJiIjIPaQCXqSGsbM180jnpowbEYu7iz2f\nLtnJ5KW7KLxQYnQ0ERERuQdUwIvUUIG+rowZFsujXZqwfX8eYz7/kS27jqOdYUVERGo3FfAiNZit\njZkHOzTmLyPb0tDLic+/2cNHC9PJP1tkdDQRERG5S1TAi9QCjRo48/rQGIb0aM7eI2cYOzWJDduP\najZeRESkFlIBL1JLmM0m4uMsjB/VjsYN3ZixMpP35mzjZMElo6OJiIhINVIBL1LL+LjX4+UhrRme\nEMpPJ84xbmoSq1OyKSvTbLyIiEhtoAJepBYymUx0ae3PW6PaERbowdy1+5kwK43cUxeMjiYiIiK/\nkgp4kVrM082RPwyMZHS/Fhw/fZG//CuZbzYf5vKVMqOjiYiIyB2yNTqAiNxdJpOJDhENadHYk1mJ\n+1i86SCpe0/y9IPhBPq6Gh1PREREqkgz8CJ1RH1ne54b0JLnH25F4YUS3pqeyuJNWZRe1my8iIhI\nTaIZeJE6JibUm7Agd+au3c83m38iLTOPp/uE09S/vtHRRERE5DZoBl6kDnJ2tGPUgy14aXAUxaVX\n+J+ZacxZs5/ikitGRxMREZFbUAEvUoe1auLFW6Pa0TXan8TUbMZNSyLjpzNGxxIREZGbUAEvUsfV\nc7DlqfhQXn2iDSaTiffmbGPGyr1cKr5sdDQRERG5DhXwIgJAaKAHf326LQltA9m4I5cxXySRnnXK\n6FgiIiLyCyrgRaSCg50Ng7s3442nYnFysOXDBel8/vVuzl8qNTqaiIiI/H8q4EXkGk0auTFuRBz9\nOzYmOeMkYz7/kdS9J42OJWSu/CYAACAASURBVCIiIqiAF5EbsLM1M+D+JowdHouHmyOTl+7i08U7\nKTxfbHQ0ERGROk0FvIjcVKCvK2OGxTCwa1N2ZJ1mzBdJ/LDzGOXl5UZHExERqZNUwIvILdmYzfRp\nH8Rfn47Dr4EzU1dk8MGCHZwuLDI6moiISJ2jAl5EbpuflzOvDY1maK8Q9mcXMmZqEuu35lCm2XgR\nEZF7RgW8iFSJ2WSiR0wAb41qS9NGbsxcvY/3Zm/jxJmLRkcTERGpE1TAi8gdaeBejz891poRvcM4\ncvI8b05NZmXSEcrKNBsvIiJyN9kaHUBEai6TyUTnqEa0auLFzFWZzF9/gNTMk4zsHYa/t4vR8URE\nRGolzcCLyK/m4erA7x9txTP9Izh55hJ/+VcKy384xOUrZUZHExERqXU0Ay8i1cJkMtGuhS/hjT2Y\nnbiPpd8dIi0zj6f7hBPU0NXoeCIiIrWGZuBFpFq5Odnz7EMt+f2jrTh7sYS3pqeycEMWpZevGB1N\nRESkVtAMvIjcFW2aexNqcWfuugP8+8ef2Lovj5F9wmge4G50NBERkRpNM/Aictc4OdrxdJ9w/vRY\na0ovl/G3r7YyO3EfRSWXjY4mIiJSY6mAF5G7LiLYk7d+05buMQGsScth3NRkdh/ONzqWiIhIjaQC\nXkTuCUd7W4b2CuG1odHY2Jh5f+52vvw2g4tFmo0XERGpChXwInJPhVjc+evIOHq3D+S79GOM+eJH\ntu8/ZXQsERGRGkMFvIjcc/Z2Ngzq2owxw2JxqWfHx4vSmbJ8N+culhgdTURExOqpgBcRwwT7uTFu\nRBwDOgWTsvckY75IIjnjBOXl5UZHExERsVoq4EXEULY2Zvp3CubNkXE0qO/IZ8t284/FOyk4X2x0\nNBEREaukAl5ErEKAtwt/fiqGwd2asetQPmM+T+K79FzNxouIiPyCCngRsRo2ZjMJ7QIZ/3RbAryd\n+de/9/L3+Ts4VXjJ6GgiIiJWw9ACvqSkhPfee49OnToRGRnJ4MGD2bJlS5X7GT16NKGhobzzzjvX\nPb5gwQJ69+5Nq1ateOCBB5g1a9avjS4id5GvpxOvDI3myfgQDhwtZOwXyaxNy6FMs/EiIiLGFvCv\nvfYa06dPp3///rzxxhuYzWZGjx7Ntm3bbruPDRs2kJqaesPjc+fOZcyYMYSEhDB27FiioqIYP348\n06ZNq46XICJ3idlkont0AG+NakvzgPrMStzHxFlbOZ5/0ehoIiIihjKsgE9PT2fFihW8/PLLvPLK\nKzz22GNMnz4dPz8/Jk2adFt9lJSUMGHCBEaNGnXd40VFRXzwwQf06NGDjz76iMGDBzNx4kT69evH\nP/7xD86dO1edL0lE7oIG9evx0uAonu4TTk7eBd6clsy3ST9xpazM6GgiIiKGMKyAX7lyJXZ2dgwa\nNKiizcHBgYEDB5KWlsbJkydv2ceMGTMoKiq6YQGflJREQUEBTzzxRKX2oUOHcuHCBTZt2vTrXoSI\n3BMmk4lOkX68PbodrZp4sWB9Fu/MSCPn5Hmjo4mIiNxzhhXwGRkZBAcH4+zsXKk9MjKS8vJyMjIy\nbnp9Xl4ekydP5qWXXqJevXrXPWfPnj0AtGzZslJ7REQEZrO54riI1AzuLg48/3BLfjegJafPFvHX\nL1NY+t1BLl/RbLyIiNQdtkbdOC8vD19f32vavb29AW45A//3v/+d4OBgHnrooZvew97eHnd390rt\nV9tuZ5b/l7y8XKp8TXXx9nY17N5yfRoTY/TxcaNjmwC+WLaL5T8cJv1gPvdF+pGYfIRTZy7RwKMe\nw3qH0zXGYnRU+f/0tWKdNC7WR2NinaxtXAwr4IuKirCzs7um3cHBAYDi4hs/xCU9PZ2lS5cyc+ZM\nTCZTle9x9T43u8eNnD59nrKye78Thre3K3l5WrNvTTQmxhsWH0JkE0+++Ho3s1dlVrTnnbnEJ/O3\nc/ZcER0iGhqYUEBfK9ZK42J9NCbWyYhxMZtNN500rvISmp9++umateM7duzg2WefZciQIcybN++2\n+nF0dKS0tPSa9qtF9dVC/pfKy8t55513iI+PJzY29pb3KCkpue6x4uLiG95DRGqO1s0a4GB/7VxE\nyeUyFm/MMiCRiIjI3VXlGfhJkyZRUFBA586dAcjPz2f06NFcvHgRBwcH/vKXv+Dl5UXPnj1v2o+3\nt/d1l7Dk5eUB4OPjc93rEhMTSU9P56WXXiInJ6fSsfPnz5OTk0ODBg1wdHTE29ub0tJSCgoKKi2j\nKSkpoaCg4Ib3EJGa5cy56/817fTZYg4dO0uwn9s9TiQiInL3VHkGfteuXdx3330VH69YsYLz58+z\nePFitmzZQlRUFNOnT79lP2FhYRw6dIgLFy5Uat+xY0fF8evJzc2lrKyM4cOH06NHj4p/AIsXL6ZH\njx4kJycDEB4eXpH5l6+hrKys4riI1Gxebjf+a9pb01OZ8FUaqXtPGrL8TUREpLpVeQY+Pz+/0sz1\nd999R3R0NCEhIQD06dOHzz777Jb9JCQkMG3aNBYsWMCIESOAn2fGFy9eTHR0dMUbXHNzc7l06RJN\nmzYFoHv37gQEBFzT3/PPP0+3bt0YOHAgERERALRv3x53d3dmz55Np06dKs6dM2cOTk5OFX9FEJGa\n7ZEuTZn+7V5KLv/fbjT2tmYe79mc4tIy1qRmM3npLhrUd6RHTAD3RzbCydGwtwCJiIj8KlX+CVav\nXr2KByBduXKFtLQ0nnrqqYrjjo6OnD9/672Zo6KiSEhIYNKkSeTl5REYGMiSJUvIzc1lwoQJFee9\n+uqrJCcnk5n58xvUAgMDCQwMvG6fFoul0tIdR0dHXnzxRcaPH88f/vAHOnXqRGpqKsuXL+fll1/G\nzU1/VhepDa6+UXXxxizyzxbj6ebAI12aVrT3jAlg2/5TJKYcYd66Ayz9/hD3t/KjZ2wAPh5ORkYX\nERGpsioX8M2bN2fp0qU89NBDrFy5kosXL9KxY8eK40ePHsXT0/O2+po4cSIffvghy5Yto7CwkNDQ\nUKZMmUJMTExVY93Q0KFDsbOzY9q0aaxduxY/Pz/eeOMNhg0bVm33EBHjdYhoSIeIhtfdLcBsNhET\n6k1MqDeHj58lMSWb9duOsjYth9bNGxAfZyHE4n7TXa1ERESsham8vLxKi0I3bNjAc889x9XLwsPD\nWbRoUcUPvoEDB+Lj48PkyZOrP60V0DaScpXGxDrd7ricOVfM+m1H2bDtKOcvlRLo60KvWAttw32x\nszXsGXe1kr5WrJPGxfpoTKyTNW4jWeUZ+K5duzJ9+nTWrl2Li4sLTz75ZEXxfubMGRo2bMiAAQPu\nPLGIyD3g4erAI52b0LdDEFt2HycxNYepKzJYuCGLbtH+dG3tj5uzvdExRURErlHlGfi6TjPwcpXG\nxDrd6biUl5ez+3A+iSk57Dx4GlsbM+0jfImPtRDgY9wTmGsDfa1YJ42L9dGYWKdaMQN/PZcvX2bt\n2rUUFhbSrVs3vL29q6NbEZF7xmQy0TLYi5bBXhw7fYHE1Bw27zzG9+nHaNHYg16xFlo19cKsdfIi\nImKwKhfwEydOJCkpiUWLFgE/z1qNHDmS1NRUysvLcXd3Z/78+TfcKUZExNr5eTkz7IFQHunchI3b\nj7Ju61E+WpiOr6cTvWID6NjSDwd7G6NjiohIHVXld2p99913xMbGVny8bt06UlJSGDVqFO+//z4A\nU6ZMqb6EIiIGcalnx4MdGvPusx34bf8WODnY8NXqffzp0x9YsP4A+WeLjI4oIiJ1UJVn4I8fP05Q\nUFDFx+vXrycgIICXX34ZgP379/P1119XX0IREYPZ2php36Ih7cJ9yTp6ltWp2axMPsKq5Gxiw7zp\nFWuhqX99o2OKiEgdUeUCvrS0FFvb/7ssKSmJ++67r+Jji8VCXl5e9aQTEbEiJpOJZgH1aRZQn1OF\nl1iXdpSNO3JJzjhJ00Zu9IqzEBPqjY1Z21CKiMjdU+WfMg0bNmTbtm3Az7Pt2dnZxMXFVRw/ffo0\nTk56sqGI1G4N6tdjcPdmvP/8fQztFcK5S6V8tmw3r362hW9//IkLRaVGRxQRkVqqyjPwDz74IJMn\nTyY/P5/9+/fj4uJCly5dKo5nZGToDawiUmc42tvSIyaAbtH+pB84TWJqNgs2ZLHsh0N0bOVHr1gL\nDT01qSEiItWnygX8M888w7Fjxyoe5PTuu+/i5uYGwLlz51i3bh0jRoyo7pwiIlbNbDLRunkDWjdv\nwJET51iTmsN3O3JZv/UokU29iI+zEB7kUfHgOxERkTtVrQ9yKisr48KFCzg6OmJnZ1dd3VoVPchJ\nrtKYWCdrGpfCCyVs2HaU9VtzOHuxFH9vZ3rFWugQ4Yudbd3ZhtKaxkT+j8bF+mhMrFOtfZDT/93M\njKura3V2KSJSY9V3tuehTsH0aR9I0p6TrE7J5stv97JoYxZdW/vTPdqf+i4ORscUEZEa5o4K+IsX\nL/LFF1+QmJhITk4OAAEBAcTHxzNq1Ci9iVVE5D/Y2drQKdKPjq0asvdIAYkp2Xyz+TD//vEn2rXw\nJT7OQqCvJj9EROT2VLmALygoYOjQoWRlZeHp6Ul4eDgAhw8f5tNPP2XlypXMmjULd3f3ag8rIlKT\nmUwmwoM8CA/y4ET+Rdak5vD9zmNs3nWcUIs78XEWopo1wGzWOnkREbmxKhfwH3/8MQcPHmTs2LEM\nGTIEG5uf13FeuXKFefPm8fbbb/OPf/yDMWPGVHtYEZHawtfTiaHxITzcOZhNO46xNi2bTxbvxMe9\nHj1iA+jUyo96DtW6ylFERGqJKu8Dv27dOgYNGsTQoUMrincAGxsbnnjiCR599FHWrFlTrSFFRGor\nJ0c7EtoF8rdnO/DcgJa4OdszZ81+Xp78A3PX7udUwSWjI4qIiJWp8vTOqVOnKpbNXE+LFi1YsmTJ\nrwolIlLX2JjNxIb5EBvmw8HcsySmZrM2LYfE1GyiQ7zpFWuheUB9bUMpIiJVL+AbNGhARkbGDY9n\nZGTQoEGDXxVKRKQua9LIjWf6RzCoa1PWbT3Kxu1HScvMo3FDV3rFWYgL88HWpsp/QBURkVqiyj8B\nunXrxsKFC5k7dy5lZWUV7WVlZcybN49FixbRvXv3ag0pIlIXebo5MrBrUyY915GnHgilqOQKn3+9\nh1f+uZlvNh/m/KVSoyOKiIgBqvwgpzNnzjBkyBCOHDmCp6cnwcHBABw6dIj8/HwCAwOZO3cuHh4e\ndyWw0fQgJ7lKY2KdavO4lJWXs+tgPomp2ew+lI+drZn7WjakZ6wF/wbORse7odo8JjWZxsX6aEys\nU614kJOHhweLFi3i888/Z82aNezcuRMAi8XCwIEDGT16NC4uN76hiIjcGbPJRGRTLyKbenE07zyJ\nqTls3nWcjdtzaRnsSa84Cy2DPbVOXkSklqvyDPytzJ07lxkzZvDvf/+7Oru1GpqBl6s0Jtapro3L\n2YslbNyey7qtORSeL8HPy4lesRY6tGyIg53NrTu4B+ramNQUGhfrozGxTrViBv5Wzpw5w6FDh6q7\nWxERuQ43J3v63deY3u0CSck4yeqUbGasymTRxiy6tvGne3QAHq4ORscUEZFqpKeEiIjUArY2Zjq0\nbEj7CF/25xSyOiWbf2/5iZVJR4gL86FXnIVgPzejY4qISDVQAS8iUouYTCZCLO6EWNw5WXCJdWk5\nbNqRy497TtAsoD7xsRbahDTAxqxtKEVEaioV8CIitZSPez2G9GjOQ52C+T79GImp2UxeugsvN0d6\nxgZwf2QjnBz1Y0BEpKbRd24RkVqunoMtveIs9IgJYPuBU6xOyWbeugMs/f4QnVr50TM2AF8PJ6Nj\niojIbbqtAv5f//rXbXe4devWOw4jIiJ3j9lsIjrEm+gQb346fo7VKdls2HaUdWk5RDVrQHychdBA\nd21DKSJi5W6rgH/33Xer1Km++YuIWLeghq6M7teCQd2asm7rUTZsO8r2A6cI9HGhV5yFtuG+2Nlq\nnbyIiDW6rQJ+xowZdzuHiIgYwN3FgUc6N6FvhyB+3HOCxJRspq7IYMGGLLq38adrG3/cnO2Njiki\nIv/htgr4tm3b3u0cIiJiIHs7GzpHNeL+SD/2HD5DYmo2S78/xDdbfqJ9hC/xsRYCfPSUbRERa6A3\nsYqISAWTyUREsCcRwZ4cO32BNak5/LDrGN+nHyM8yINecRYim3ph1lJJERHDqIAXEZHr8vNy5qkH\nQnm4cxM27chlbVoOHy9Mx9ejHj1jLXRs1RBHe/0YERG51/SdV0REbsqlnh192gcRH2chLTOP1SnZ\nzErcx5JNB+ncuhE9ogPwqu9odEwRkTpDBbyIiNwWWxsz7Vr40q6FLweOFpKYks3q5J//xYR6Ex9n\noal/faNjiojUeirgRUSkypr516eZf31OFxaxdmsOG7fnkrL3JE0audEr1kJMqDe2NtqGUkTkblAB\nLyIid8yrviODuzWjf8fG/LDzOGtSs/nf5bvxcHWgR0wAXVo3wtvokCIitYwKeBER+dUc7W3pERNA\nt2h/0rNOk5iSzcINWSz/4RA94gLpFOGLn5ez0TFFRGoFFfAiIlJtzCYTrZs1oHWzBmSfPE9iSjaJ\nSUf4dvNhIpt60SvOQosgDz2xW0TkV1ABLyIid4XFx4WnHwznt49GsmjNPtZvzeH9udvx93amV6yF\n9i18sbezMTqmiEiNowJeRETuKg9XRx7qFEyf9kEkZ5xgdUo2X367l4UbsujWxp9u0f64uzgYHVNE\npMYwtIAvKSnho48+YtmyZZw9e5awsDBeeuklOnTocNPrli9fzsKFC8nKyqKwsBAfHx/atWvHCy+8\ngL+/f6VzQ0NDr9vHX/7yFx5//PFqey0iInJzdrZmOrby476WDck8UsDqlGy+2XyYf//4E+1a+NIr\n1kJQQ1ejY4qIWD1DC/jXXnuN1atXM2zYMIKCgliyZAmjR49m5syZtGnT5obX7d27F19fX7p06UL9\n+vXJzc1l/vz5bNiwgeXLl+PtXXnPg06dOtG/f/9KbVFRUXflNYmIyM2ZTCbCgjwIC/LgxJmLrEnN\n4fv0Y2zedZwQizvxcRZaN2uA2ax18iIi12MqLy8vN+LG6enpDBo0iNdff50RI0YAUFxcTN++ffHx\n8WHWrFlV6m/37t088sgjvPLKK4waNaqiPTQ0lGHDhvHGG29US+7Tp89TVnbvP2Xe3q7k5Z275/eV\nG9OYWCeNi/W5nTG5WFTKph3HWJuWw+mzRXi7O9IzxkKnSD/qOWi1592grxXrozGxTkaMi9lswsvL\n5cbH72GWSlauXImdnR2DBg2qaHNwcGDgwIGkpaVx8uTJKvXXqFEjAM6ePXvd40VFRRQXF995YBER\nuWucHO1IaBfI355tz3MDWlLfxYE5a/fz8uQfmLt2P3kFl4yOKCJiNQyb1sjIyCA4OBhn58r7AkdG\nRlJeXk5GRgY+Pj437aOgoIArV66Qm5vLp59+CnDd9fMLFy5k5syZlJeXExISwosvvkivXr2q78WI\niEi1sDGbiQ3zITbMh0PHzpKYks3atBwSU7OJbu5NrzgLzQPqaxtKEanTDCvg8/Ly8PX1vab96vr1\n25mBf+CBBygoKADA3d2dcePG0b59+0rntGnThj59+hAQEMCxY8eYMWMGL7zwAu+//z59+/athlci\nIiJ3Q7CfG7/tH8HArk1Zv+0oG7YdJW1fHkENXYmPtRAX7oOtjWF/SBYRMYxha+B79uxJs2bN+Oyz\nzyq1Z2dn07NnT8aOHcuTTz550z5SUlK4ePEihw4dYvny5SQkJPDb3/72ptdcvHiRvn37cuXKFTZs\n2KBZHBGRGqKo5DLr03JYvimLnJPn8XRzoE/HYBLaN6a+tqEUkTrEsBl4R0dHSktLr2m/uk7dweHW\n34zj4uIA6NKlCz169KBfv344OTndtPB3cnJiyJAhvP/++xw8eJCmTZtWKbfexCpXaUysk8bF+lTn\nmMQ28yK6qSe7D+WzOiWbr77dy7zEfXSIaEiv2AD8vW/8pi+pTF8r1kdjYp2s8U2shhXw3t7e110m\nk5eXB3DL9e+/ZLFYiIiI4Ouvv77lzL2fnx8AhYWFVbqHiIgYz2wy0aqJF62aeHE07zxr0nLYvOs4\nm3bkEhHsSXychYhgT8z6C6uI1FKGLR4MCwvj0KFDXLhwoVL7jh07Ko5XVVFREefO3fo3pOzsbAA8\nPT2rfA8REbEe/t4uDE8IY9Jz9/FI5ybk5J3ng/k7GPtFEhu2HaW49IrREUVEqp1hBXxCQgKlpaUs\nWLCgoq2kpITFixcTHR1d8QbX3NxcsrKyKl2bn59/TX+7du1i7969RERE3PS8M2fOMHv2bAICAmjc\nuHE1vRoRETGSq5M9fe9rzHu/u4/R/Vpgb2fDjFWZvPzpDyzckMWZc9pGWERqD8OW0ERFRZGQkMCk\nSZPIy8sjMDCQJUuWkJuby4QJEyrOe/XVV0lOTiYzM7OirVu3bvTu3ZuQkBCcnJw4cOAAixYtwtnZ\nmeeee67ivFmzZrF27Vq6du1Ko0aNOHHiBPPmzSM/P79i20kREak9bG3MdIhoSPsWvuzPKSQxJZtv\nk35iVfIRYsN8iI+zEOznZnRMEZFfxdDH202cOJEPP/yQZcuWUVhYSGhoKFOmTCEmJuam1z3xxBNs\n2bKFNWvWUFRUhLe3NwkJCTz33HNYLJaK89q0acPWrVtZsGABhYWFODk50bp1a5555plb3kNERGou\nk8lEiMWdEIs7eQWXWJuWw6YduSTtOUEz//rEx1loE9IAG7O2oRSRmsewbSRrKu1CI1dpTKyTxsX6\nWMuYXCq+zPfpx1iTlk1eQRFebo70iAmgc5QfTo52Rse756xlXOT/aEysk3ahERERMUg9B1t6xVno\nERPA9gOnSEzJZv76Ayz7/hCdIv3oGRuAr4eT0TFFRG5JBbyIiNQpZrOJ6BBvokO8+en4ORJTs9mw\n7Sjr0nKIataAXnEWwgLd9aA/EbFaKuBFRKTOCmroym/6tmBQ16as23qU9duOsv3AKSw+LvSKtdCu\nhS92tlonLyLWRQW8iIjUefVdHHi4cxMe7BDEj3tOkJiazbR/Z7BwwwG6RQfQrY0/bs72RscUEQFU\nwIuIiFSwt7Ohc1Qj7o/0Y89PZ0hMyWbZ94dYseUw7Vs0pFecBYvPjd9YJiJyL6iAFxER+QWTyURE\nY08iGnty7PQF1qTl8MPOY3y/8xjhQR70irUQ2cwLs9bJi4gBVMCLiIjchJ+XM0/Fh/Lw/U34bkcu\na9Jy+HhROj4e9egVa6Fjq4Y42uvHqYjcO/qOIyIichtc6tnRu30QveIsbN2Xx+qUbGYl7mPxpoN0\niWpEj5gAvOo7Gh1TROoAFfAiIiJVYGtjpm24L23Dfck6WkhiajarU37+Fx3qTXychaaN3LQNpYjc\nNSrgRURE7lBT//o09a/P6a5FrNuaw8btuaTuPUmwnxu94gKIDfXB1kbbUIpI9VIBLyIi8it51Xdk\nULdm9OvYmM27jpOYks2U5XtY4JpF92h/urT2x6WendExRaSWUAEvIiJSTRztbekeHUDXNv7szDrN\n6pRsFm08yNebD9OxpR89YwPw83I2OqaI1HAq4EVERKqZ2WQiqlkDopo1IOfkeVanZvNd+jHWbztK\nqyZexMdZaNHYQ+vkReSOqIAXERG5iwJ8XHi6TzgDuzRlw7ajrNt2lPfnbce/gTO94iy0b+GLvZ2N\n0TFFpAZRAS8iInIPuDnb079TML3bB5GccYLElGy+/HYvCzdk0bWNP92j/XF3cTA6pojUACrgRURE\n7iE7WzMdW/lxX8uGZB4pIDE1mxWbD/Ptjz/RNtyX+DgLQQ1djY4pIlZMBbyIiIgBTCYTYUEehAV5\ncOLMRdam5vDdzmNs2X2cEIs7vWIttGneALNZ6+RFpDIV8CIiIgbz9XDiiV4hDLi/Cd+l57ImNYdP\nl+ykQX1HesZauD/Sj3oO+pEtIj/TdwMREREr4eRoywNtA+kZG8C2fadYnZrN3LX7WfrdQe6PbESP\n2AB83OsZHVNEDKYCXkRExMrYmM3EhvkQG+bDoWNnSUzNZt3WHNakZdOmuTe9YgMIsbhrG0qROkoF\nvIiIiBUL9nPjt/0iGNS1Geu25rBh21G27ssjyNeV+DgLceE+2NqYjY4pIveQCngREZEawMPVgUe7\nNKXvfY3Zsvs4iSnZfP7NHuavP0D3aH+6tvHH1cne6Jgicg+ogBcREalBHOxs6Nrany5Rjdh9KJ/V\nKdks+e4Q32z5iQ4RvvSKteDv7WJ0TBG5i1TAi4iI1EAmk4mWTbxo2cSLo6cusCY1m827jrNpxzEi\nGnvQKy6Qlk08MWudvEitowJeRESkhvNv4MzwhDAe7dKUjduPsjYthw8X7MDPy4mesRbui2iIg72N\n0TFFpJqogBcREaklXOrZ8WCHxjzQNpDUvSdZnZLNzFWZLN6YRefWjegRHUBmdgGLN2aRf7YYTzcH\nHuny/9q784Co6/x/4M8ZGE45BIdDuRRlUIZLMmTwwDMySi1d86IyXcvcyrZdc93dNnfT/ZWVZvVb\nrw5dy7wQtVRMLYtBXLXQ4dBATAwGRhAQEAbl8/1DmBw55BpmBp6Pv+I97/fMe3jx6fNy+Hye+CMq\nyMPYWyeiNmADT0RE1M1YWogxPMgDkUPckf1rGZL+l4dDqVdw8OQViEVAnXBnXnF5DT47mAUAbOKJ\nzAhzp4iIiLopkUiEQV7OeGFqMP7fwijYWFnomvcG2lt12PNdjnE2SETtwgaeiIioB+jjbItq7e0m\nHysur8H/sopQe6vpx4nItPASGiIioh7C1dEaxeU1jcbFIuD/71XB1toSDw52Q7TcE/79HPmXXolM\nFBt4IiKiHuLx0f747GAWtLfqdGNWlmLEx8rg1MsayvNqpKSr8d1P+XBztoVC7oEouQekzrZG3DUR\n3YsNPBERUQ/RcKNq+sWu4wAAIABJREFUcyk0QX4umFMTgLMXNVCq1Ej8IRd7f8hFgLczFHIPDAt0\ng601WwciYxMJgiDcfxo1KC6uQN29dwB1AanUARrNjS5/XWoea2KaWBfTw5qYptbUpbisGinpaiSr\n1CgsqYLEUoyhAVIo5B4Y4tcbFmLeSteZeKyYJmPURSwWwdW1+b+ozH9GExERUZNcnWwQp/DDI1G+\nuFRQDqVKjVMZhUjNKIRTLytEDfGAQu4BL7fmGw0i6nxs4ImIiKhFIpEI/n2d4N/XCU+OHYRzOdeg\nVKlx5HQeDp26Ah+3XlAEeyJyiDuc7K2MvV2ibo8NPBEREbWaxFKMCJkbImRuKK/S4lRGIZQqNbYf\n/Rk7jmUjeIALFMGeCBvoComlhbG3S9QtsYEnIiKidnG0s8L4B7wx/gFv/HqtEkpVAU6mFyJtrwp2\n9ZGUCkZSEnU6NvBERETUYf362GN6zEA8Mcofmb9ch1JVAGW6Gt/+lA+33nciKRVBHujDSEqiDmMD\nT0RERJ1GLBYhqL8Lgvq7YE7NLZy9qEHy+QLs/T4Xe7+/E0kZLffAA4ykJGo3HjlERERkELbWlogO\n9kR0sCeuld1ESvqd6+U/OZiF/x65iKEBUkTLPTDEzwViMS+xIWotNvBERERkcH2cbPGowg9xDZGU\n59U4lXlPJGWwB7ykjKQkuh+jNvBarRZr165FYmIiysvLERgYiCVLliAqKqrFdfv27cOuXbuQk5OD\nsrIyuLm5ITIyEosXL0a/fv0azd+5cyc+/vhjXL16FX379kV8fDxmz55tqLdFREREzdCLpBzXRCSl\ney8o5J4YPsQdjoykJGqSURv41157DUlJSYiPj4evry8SEhKwYMECbN26FeHh4c2uy8rKgru7O0aP\nHg0nJyfk5+djx44d+Pbbb7Fv3z5IpVLd3O3bt+P1119HbGwsnnnmGZw+fRorVqxATU0N5s2b1xVv\nk4iIiJrQVCRlMiMpie5LJAiCYIwXPnfuHKZPn45ly5bh6aefBgDU1NQgLi4Obm5u2LZtW5ueLz09\nHY8//jj+/Oc/49lnnwUAVFdXY/To0YiIiMBHH32km/vqq6/i2LFj+O677+Dg4NCm1ykurkBdXdd/\ny/jnlU0Pa2KaWBfTw5qYJlOuy6+aCijT1UhRqVFaoe0xkZSmXJOezBh1EYtFcHVt/nIycRfuRc+h\nQ4cgkUgwffp03Zi1tTWmTZuGM2fOoKioqE3P17dvXwBAeXm5biw1NRWlpaWYNWuW3tzZs2ejsrIS\nJ06c6MA7ICIiIkPoJ+2F6TEDsXpRNP44IwyhA12hTFdj5X/PYNmGk9iXnItrpTeNvU0iozHaJTSZ\nmZno378/7O3t9cZDQkIgCAIyMzPh5ubW4nOUlpbi9u3byM/Px4cffggAetfPZ2RkAADkcrneuqCg\nIIjFYmRkZOCRRx7pjLdDREREnezeSMozFzRQqn6LpJR5O0PBSErqgYz2067RaODu7t5ovOH69dZ8\nAv/QQw+htLQUAODs7Iy///3vGD58uN5rWFlZwdnZWW9dw1hbP+UnIiIi47C1tsSIEE+MCLkrkvJ8\nAT45mIVt9ZGUCkZSUg9htAa+uroaEomk0bi1tTWAO9fD388HH3yAqqoq5ObmYt++faisrGzVazS8\nTmte414tXY9kaFJp267XJ8NjTUwT62J6WBPTZK51kUodMHigG555TI4LV67j2Ok8fP/jrziZUQgX\nR2vEDPXG2Ae84evpaOyttpm51qS7M7W6GK2Bt7GxQW1tbaPxhqa6oZFvybBhwwAAo0ePxrhx4/Do\no4/Czs4Oc+bM0b2GVqttcm1NTU2rXuNevImVGrAmpol1MT2siWnqLnVxtZNg+qgBmKLwQ1r2nUjK\nxBM52PNtNnzdHaCQeyDSTCIpu0tNuhtTvInVaA28VCpt8hIWjUYDAPe9/v1e3t7eCAoKwv79+3UN\nvFQqRW1tLUpLS/Uuo9FqtSgtLW3zaxAREZFpkliK8UCgGx4IvBNJmZpx56++fnH0Z3xZH0kZHeyJ\nUEZSUjdgtAY+MDAQW7duRWVlpd6NrGlpabrH26q6uho3b/52V/rgwYMBACqVCiNGjNCNq1Qq1NXV\n6R4nIiKi7sPRzgoTHvDGhAe870RSqtRISVcjLaf4t0jKYE/49+2+kZTUvRktRjI2Nha1tbXYuXOn\nbkyr1WLPnj0YOnSo7gbX/Px85OTk6K0tKSlp9HwqlQpZWVkICgrSjQ0fPhzOzs74/PPP9eZ+8cUX\nsLOzw6hRozrzLREREZGJ6SftheljfoukDBnoCqVKjZVbz+AvG05iPyMpyQwZ7RP40NBQxMbGYvXq\n1dBoNPDx8UFCQgLy8/OxatUq3bylS5fi1KlTuHDhgm5szJgxePjhhxEQEAA7OztkZ2dj9+7dsLe3\nx6JFi3TzbGxs8OKLL2LFihV46aWXMGLECJw+fRr79u3Dq6++CkdH87u5hYiIiNru7kjKmxN/i6RM\n+D4XCYykJDNj1J/Qt956C2vWrEFiYiLKysogk8mwYcMGREREtLhu1qxZSElJwTfffIPq6mpIpVLE\nxsZi0aJF8Pb21ps7e/ZsSCQSfPzxxzh69Cg8PT2xfPlyxMfHG/KtERERkYlqVSRlsAeG+DKSkkyT\nSBCEro9UMWNMoaEGrIlpYl1MD2timlgXfYIg4FJ+OZQqNU5lFqKy+hace1lheJAHouUe6Cc1fIw0\na2KamEJDREREZIJEIhH8+znBv58Tnhw3SBdJeeR/eTiUesXsIimpe2MDT0RERHQXvUjKSi1SM3+L\npNxxPBvBA1yhkHswkpKMhg08ERERUTMc7X+LpLyqqUBKfSTlT9nX7kRSDnGHQu7BSErqUmzgiYiI\niFrBqz6S8onR/sj4pQRKlRrK8wX49sdf4d7bFgq5B6KCPNDH2dbYW6Vujg08ERERURuIxSLI+7tC\n3t8VNyfewukLRUhRqXWRlIE+zoiSe+ABGSMpyTD4U0VERETUTrbWlhgZ0hcjQ/riWulNpKSroVSp\n8cnXWdiWdBFDZVIo5IykpM7FBp6IiIioE/RxtsWj0f0Rp/BDTkMkZUYhTqYXwrmXFaKCPKDookhK\n6t7YwBMRERF1IpFIhIH9nDCwnxNmjhuItOxiKFVqJP0vDwcbIimD6yMp7RhJSW3HBp6IiIjIQCSW\nFvqRlBn1kZTf/Iwdx+6OpOxj7K2SGWEDT0RERNQFHO2tMGGYNyYMuxNJqbwrktLexhKjwr0wdKAr\nBjCSku6DDTwRERFRF/OS9sLvxgzEtIZIyvNqHD2dh4Mpl3+LpJR7oI8TIympMTbwREREREZydySl\nvYMNDiVfgvI8IympZfxJICIiIjIBdjaSRpGUyYykpCawgSciIiIyMYykpJawgSciIiIyUc1FUh4+\nVR9J6eEAhZyRlD0NG3giIiIiM9BUJGWyqqDJSEqJpdjY2yUDYgNPREREZGb0IimLKqBM14+kfHCw\nOxRyD0ZSdlNs4ImIiIjMmJdbL/zOrT6S8nIJlCo1ks8X4PiPv8Ldxe5OJGWQOyMpuxE28ERERETd\ngFgsgnyAK+QDXHGz5hZOZxVBqVIj4cQlJJy4hEAfZyjknoiQSRlJaeZYPSIiIqJuxtbaEiND+2Jk\naF9o6iMplSo1Pv46E/89cgERAVIo5J4Y7NubkZRmiA08ERERUTcmdbbFY9H98WhDJOX5ApzKLELK\n3ZGUwZ7o18fe2FulVmIDT0RERNQD6EVSjh+En7KLoTxfwEhKM8QGnoiIiKiHkVhaYFigG4bVR1Ke\nzCiE8p5IyuhgD4T4M5LSFLGBJyIiIurBHO2tMHGYNyY2RFKqGElp6tjAExERERGA+kjKsQPxRMwA\nZF6+jmSVGj/cE0mpCPKAq5ONsbfao7GBJyIiIiI9FmJxo0jKZEZSmgx+x4mIiIioWYykND1s4ImI\niIioVfQiKX8th1JVgNT6SMreDtYYHuQOhZyRlIbGBp6IiIiI2kQkEmGglxMGet0TSZmah4Mnr8Cv\nPpLyQUZSGgQbeCIiIiJqt7sjKcsqtUitj6T8/Juf8eWxbIT4u0IhZyRlZ2IDT0RERESdwqmZSMof\nf66PpBxSH0npyUjKjmADT0RERESd7u5IyozL16FUqfHDuQIcP/srPOojKaMYSdkubOCJiIiIyGAs\nxGIED3BF8ABXVFXfwukLRVCq1NhTH0kp83FGdLAnhgYwkrK1+F0iIiIioi5hZ2OJUaF9MaohklJ1\nJ5Jy81eZ2JpUH0kZ7InBPoykbAkbeCIiIiLqclJnWzw2oj8ejfZD9q9lUKrUOHVPJGW03BN9GUnZ\nCBt4IiIiIjIakUiEQV7OGOTljFn1kZTJTURSRg5xhwMjKQGwgSciIiIiE9EokrL+r74yklIfG3gi\nIiIiMjlO9laY+KAPJj7og7yiCihVBTiZXshISrCBJyIiIiIT5+3WCzPGDsK0GH9kXL6O5PMFPTqS\nkg08EREREZmFJiMpzxfoIikDfXtDIfdAhEwKG6vu2+Z233dGRERERN3W3ZGURaU3cbJRJKUbFMEe\n3TKSkg08EREREZk1t2YjKdXo7WCNqCAPKOQe3SaS0qgNvFarxdq1a5GYmIjy8nIEBgZiyZIliIqK\nanFdUlISvv76a5w7dw7FxcXw9PTEmDFjsGjRIjg4OOjNlclkTT7HP/7xD8ycObPT3gsRERERGdfd\nkZQzxw3CT9nXoFSpcSj1Cr4++Qv8PBwQHeyJBwe7mXUkpUgQBMFYL/7KK68gKSkJ8fHx8PX1RUJC\nAlQqFbZu3Yrw8PBm10VGRsLNzQ3jx49H3759ceHCBWzfvh1+fn7YvXs3rK2tdXNlMhlGjBiBxx57\nTO85QkND4efn1+Y9FxdXoK6u679lUqkDNJobXf661DzWxDSxLqaHNTFNrIvpYU0Mp6yiBqkZhVCq\n1LhSVAELsag+ktITIf6uLUZSGqMuYrEIrq69mn3caJ/Anzt3Dl999RWWLVuGp59+GgAwZcoUxMXF\nYfXq1di2bVuza99//31ERkbqjcnlcixduhRfffUVHn/8cb3HBgwYgMmTJ3f6eyAiIiIi0+fUy/q+\nkZTRck/093TQRVKmpKux57sclJTXwMXRGo+P9kdUkIeR38kdRmvgDx06BIlEgunTp+vGrK2tMW3a\nNLz33nsoKiqCm5tbk2vvbd4BYPz48QCAnJycJtdUV1dDJBLpfTpPRERERD3L3ZGU6bnXoVT9Fknp\n6XonktJKYoHd3+ZAe6sOAFBcXoPPDmYBgEk08UZr4DMzM9G/f3/Y2+vfTBASEgJBEJCZmdlsA9+U\na9euAQB69+7d6LFdu3Zh69atEAQBAQEBePHFFzFhwoSOvQEiIiIiMlsWYjFC/F0R4q8fSbn7u0tN\nztfeqsOe73J6dgOv0Wjg7u7eaFwqlQIAioqK2vR8GzduhIWFBSZOnKg3Hh4ejkmTJsHLywsFBQXY\nsmULFi9ejHfeeQdxcXFt3ndL1yMZmlTqcP9J1KVYE9PEupge1sQ0sS6mhzUxHl/v3nhivAzq4kos\nWPlNk3NKymtMokZGa+Crq6shkUgajTdc4lJTU9Pq59q/fz927dqFhQsXwsfHR++x7du36309depU\nxMXF4e2338YjjzzS5j+9y5tYqQFrYppYF9PDmpgm1sX0sCamwQKAq6M1issb96IujtZdUqP73cTa\n/C23BmZjY4Pa2tpG4w2Ne2uvVT99+jSWL1+OmJgYvPTSS/edb2dnhyeffBJqtRqXLjX9KxIiIiIi\n6rkeH+0Pq3uSaawsxXh8tL+RdqTPaJ/AS6XSJi+T0Wg0ANCq69+zsrLw/PPPQyaT4b333oOFhUWr\nXtvT0xMAUFZW1oYdExEREVFP0HCdO1No7hEYGIitW7eisrJS70bWtLQ03eMtuXLlCubPnw8XFxes\nX78ednZ2rX7tvLw8AICLi0s7dk5ERERE3V1UkAeigjxM8tImo11CExsbi9raWuzcuVM3ptVqsWfP\nHgwdOlR3g2t+fn6jaEiNRoN58+ZBJBJh8+bNzTbiJSUljcauX7+Ozz//HF5eXu36Q05ERERERMZk\ntE/gQ0NDERsbi9WrV0Oj0cDHxwcJCQnIz8/HqlWrdPOWLl2KU6dO4cKFC7qx+fPnIy8vD/Pnz8eZ\nM2dw5swZ3WM+Pj66v+K6bds2HD16FDExMejbty8KCwvx5ZdfoqSkBB9++GHXvVkiIiIiok5itAYe\nAN566y2sWbMGiYmJKCsrg0wmw4YNGxAREdHiuqysO0H6mzZtavTY1KlTdQ18eHg4zp49i507d6Ks\nrAx2dnYICwvDwoUL7/saRERERESmSCQIQtdnIpoxxkhSA9bENLEupoc1MU2si+lhTUyTMepisjGS\nRERERETUdmzgiYiIiIjMCBt4IiIiIiIzwgaeiIiIiMiMsIEnIiIiIjIjRo2RNEdisahHvjY1jTUx\nTayL6WFNTBPrYnpYE9PU1XW53+sxRpKIiIiIyIzwEhoiIiIiIjPCBp6IiIiIyIywgSciIiIiMiNs\n4ImIiIiIzAgbeCIiIiIiM8IGnoiIiIjIjLCBJyIiIiIyI2zgiYiIiIjMCBt4IiIiIiIzwgaeiIiI\niMiMWBp7Az2ZVqvF2rVrkZiYiPLycgQGBmLJkiWIioq679rCwkKsXLkSycnJqKurw/Dhw7Fs2TJ4\ne3t3wc67r/bWZN26dfjggw8ajffp0wfJycmG2m6PUFRUhC1btiAtLQ0qlQpVVVXYsmULIiMjW7U+\nJycHK1euxNmzZyGRSDBmzBgsXboULi4uBt5599aRurz22mtISEhoNB4aGoodO3YYYrs9wrlz55CQ\nkIDU1FTk5+fD2dkZ4eHhePnll+Hr63vf9TyvdL6O1ITnFcM5f/48/vOf/yAjIwPFxcVwcHBAYGAg\nXnjhBQwdOvS+603hWGEDb0SvvfYakpKSEB8fD19fXyQkJGDBggXYunUrwsPDm11XWVmJ+Ph4VFZW\n4rnnnoOlpSU+/fRTxMfHY+/evXBycurCd9G9tLcmDVasWAEbGxvd13f/N7VPbm4uNm7cCF9fX8hk\nMvz444+tXqtWqzF79mw4OjpiyZIlqKqqwscff4yLFy9ix44dkEgkBtx599aRugCAra0t3njjDb0x\n/qOqYzZt2oSzZ88iNjYWMpkMGo0G27Ztw5QpU7Br1y74+/s3u5bnFcPoSE0a8LzS+fLy8nD79m1M\nnz4dUqkUN27cwP79+zFnzhxs3LgR0dHRza41mWNFIKNIS0sTAgIChE8++UQ3Vl1dLYwfP16YNWtW\ni2s3bNggyGQyIT09XTeWnZ0tDB48WFizZo2httztdaQm77//vhAQECCUlZUZeJc9z40bN4SSkhJB\nEAThyJEjQkBAgHDy5MlWrX399deFsLAwQa1W68aSk5OFgIAAYefOnQbZb0/RkbosXbpUiIiIMOT2\neqQzZ84INTU1emO5ubmCXC4Xli5d2uJanlcMoyM14Xmla1VVVQkKhUL4/e9/3+I8UzlWeA28kRw6\ndAgSiQTTp0/XjVlbW2PatGk4c+YMioqKml17+PBhhIWFYciQIboxf39/REVF4eDBgwbdd3fWkZo0\nEAQBFRUVEATBkFvtUXr16oXevXu3a21SUhLGjh0Ld3d33ZhCoYCfnx+PlQ7qSF0a3L59GxUVFZ20\nIxo6dCisrKz0xvz8/DBo0CDk5OS0uJbnFcPoSE0a8LzSNWxtbeHi4oLy8vIW55nKscIG3kgyMzPR\nv39/2Nvb642HhIRAEARkZmY2ua6urg4XLlyAXC5v9FhwcDAuX76MmzdvGmTP3V17a3K3mJgYRERE\nICIiAsuWLUNpaamhtkv3UVhYiOLi4iaPlZCQkFbVkwynsrJSd6xERkZi1apVqKmpMfa2uh1BEHDt\n2rUW/7HF80rXak1N7sbziuFUVFSgpKQEly5dwrvvvouLFy+2eM+bKR0rvAbeSDQajd6ngg2kUikA\nNPtpb2lpKbRarW7evWsFQYBGo4GPj0/nbrgHaG9NAMDR0RFz585FaGgoJBIJTp48iS+//BIZGRnY\nuXNno09gyPAa6tXcsVJcXIzbt2/DwsKiq7fW40mlUsyfPx+DBw9GXV0djh8/jk8//RQ5OTnYtGmT\nsbfXrezbtw+FhYVYsmRJs3N4XularakJwPNKV/jLX/6Cw4cPAwAkEgmefPJJPPfcc83ON6VjhQ28\nkVRXVzd5A521tTUANPtJVMN4Uwduw9rq6urO2maP0t6aAMBTTz2l93VsbCwGDRqEFStWYO/evfjd\n737XuZul+2rtsXLvb1zI8P74xz/qfR0XFwd3d3ds3rwZycnJLd5ARq2Xk5ODFStWICIiApMnT252\nHs8rXae1NQF4XukKL7zwAmbMmAG1Wo3ExERotVrU1tY2+48jUzpWeAmNkdjY2KC2trbReMMPR8MP\nwr0axrVabbNreYd6+7S3Js2ZOXMmbG1tkZKS0in7o7bhsWJe5s2bBwA8XjqJRqPBwoUL4eTkhLVr\n10Isbv50z2Ola7SlJs3heaVzyWQyREdH44knnsDmzZuRnp6OZcuWNTvflI4VNvBGIpVKm7wkQ6PR\nAADc3NyaXOfs7AwrKyvdvHvXikSiJn+1Q/fX3po0RywWw93dHWVlZZ2yP2qbhno1d6y4urry8hkT\n0qdPH0gkEh4vneDGjRtYsGABbty4gU2bNt33nMDziuG1tSbN4XnFcCQSCcaNG4ekpKRmP0U3pWOF\nDbyRBAYGIjc3F5WVlXrjaWlpusebIhaLERAQAJVK1eixc+fOwdfXF7a2tp2/4R6gvTVpTm1tLQoK\nCjqc1EHt4+7uDhcXl2aPlcGDBxthV9QctVqN2tpaZsF3UE1NDZ577jlcvnwZ69evx4ABA+67hucV\nw2pPTZrD84phVVdXQxCERn1AA1M6VtjAG0lsbCxqa2uxc+dO3ZhWq8WePXswdOhQ3c2U+fn5jaKm\nHnroIfz000/IyMjQjV26dAknT55EbGxs17yBbqgjNSkpKWn0fJs3b0ZNTQ1Gjhxp2I0TAODKlSu4\ncuWK3tjEiRNx7NgxFBYW6sZSUlJw+fJlHitd5N661NTUNBkd+dFHHwEARowY0WV7625u376Nl19+\nGT/99BPWrl2LsLCwJufxvNJ1OlITnlcMp6nvbUVFBQ4fPgxPT0+4uroCMO1jRSQwWNRoXnrpJRw9\nehRPPfUUfHx8kJCQAJVKhc8++wwREREAgLlz5+LUqVO4cOGCbl1FRQWmTp2Kmzdv4plnnoGFhQU+\n/fRTCIKAvXv38l/mHdDemoSGhmLSpEkICAiAlZUVUlNTcfjwYURERGDLli2wtOT94h3R0Nzl5OTg\nwIEDeOKJJ+Dl5QVHR0fMmTMHADB27FgAwLFjx3TrCgoKMGXKFDg7O2POnDmoqqrC5s2b4enpyRSH\nTtCeuly9ehVTp05FXFwcBgwYoEuhSUlJwaRJk/Dee+8Z5810A2+++Sa2bNmCMWPG4OGHH9Z7zN7e\nHuPHjwfA80pX6khNeF4xnPj4eFhbWyM8PBxSqRQFBQXYs2cP1Go13n33XUyaNAmAaR8rbOCNqKam\nBmvWrMH+/ftRVlYGmUyGV155BQqFQjenqR8e4M6vm1euXInk5GTU1dUhMjISy5cvh7e3d1e/jW6l\nvTX561//irNnz6KgoAC1tbXo168fJk2ahIULF/Lmr04gk8maHO/Xr5+uMWyqgQeAn3/+Gf/+979x\n5swZSCQSxMTEYNmyZbxUoxO0py7l5eX45z//ibS0NBQVFaGurg5+fn6YOnUq4uPjeV9CBzT8v6kp\nd9eE55Wu05Ga8LxiOLt27UJiYiKys7NRXl4OBwcHhIWFYd68eXjwwQd180z5WGEDT0RERERkRngN\nPBERERGRGWEDT0RERERkRtjAExERERGZETbwRERERERmhA08EREREZEZYQNPRERERGRG2MATERER\nEZkRNvBERGTy5s6dq/ujUEREPR3/Di8RUQ+VmpqK+Pj4Zh+3sLBARkZGF+6IiIhagw08EVEPFxcX\nh1GjRjUaF4v5S1oiIlPEBp6IqIcbMmQIJk+ebOxtEBFRK/HjFSIiatHVq1chk8mwbt06HDhwAI8+\n+iiCg4MRExODdevW4datW43WZGVl4YUXXkBkZCSCg4MxadIkbNy4Ebdv3240V6PR4F//+hfGjRsH\nuVyOqKgoPPPMM0hOTm40t7CwEK+88gqGDRuG0NBQPPvss8jNzTXI+yYiMlX8BJ6IqIe7efMmSkpK\nGo1bWVmhV69euq+PHTuGvLw8zJ49G3369MGxY8fwwQcfID8/H6tWrdLNO3/+PObOnQtLS0vd3OPH\nj2P16tXIysrCO++8o5t79epVzJw5E8XFxZg8eTLkcjlu3ryJtLQ0KJVKREdH6+ZWVVVhzpw5CA0N\nxZIlS3D16lVs2bIFixYtwoEDB2BhYWGg7xARkWlhA09E1MOtW7cO69atazQeExOD9evX677OysrC\nrl27EBQUBACYM2cOFi9ejD179mDGjBkICwsDALz55pvQarXYvn07AgMDdXNffvllHDhwANOmTUNU\nVBQA4I033kBRURE2bdqEkSNH6r1+XV2d3tfXr1/Hs88+iwULFujGXFxc8Pbbb0OpVDZaT0TUXbGB\nJyLq4WbMmIHY2NhG4y4uLnpfKxQKXfMOACKRCPPnz8c333yDI0eOICwsDMXFxfjxxx8xYcIEXfPe\nMPf555/HoUOHcOTIEURFRaG0tBTff/89Ro4c2WTzfe9NtGKxuFFqzvDhwwEAv/zyCxt4Iuox2MAT\nEfVwvr6+UCgU953n7+/faGzgwIEAgLy8PAB3Lom5e/xuAwYMgFgs1s29cuUKBEHAkCFDWrVPNzc3\nWFtb6405OzsDAEpLS1v1HERE3QFvYiUiIrPQ0jXugiB04U6IiIyLDTwREbVKTk5Oo7Hs7GwAgLe3\nNwDAy8tLb/yaXNaHAAAB0ElEQVRuly5dQl1dnW6uj48PRCIRMjMzDbVlIqJuiQ08ERG1ilKpRHp6\nuu5rQRCwadMmAMD48eMBAK6urggPD8fx48dx8eJFvbkbNmwAAEyYMAHAnctfRo0ahRMnTkCpVDZ6\nPX6qTkTUNF4DT0TUw2VkZCAxMbHJxxoacwAIDAzEU089hdmzZ0MqleLo0aNQKpWYPHkywsPDdfOW\nL1+OuXPnYvbs2Zg1axakUimOHz+OH374AXFxcboEGgD429/+hoyMDCxYsABTpkxBUFAQampqkJaW\nhn79+uFPf/qT4d44EZGZYgNPRNTDHThwAAcOHGjysaSkJN2152PHjkX//v2xfv165ObmwtXVFYsW\nLcKiRYv01gQHB2P79u14//338cUXX6Cqqgre3t549dVXMW/ePL253t7e2L17Nz788EOcOHECiYmJ\ncHR0RGBgIGbMmGGYN0xEZOZEAn9HSURELbh69SrGjRuHxYsX4w9/+IOxt0NE1OPxGngiIiIiIjPC\nBp6IiIiIyIywgSciIiIiMiO8Bp6IiIiIyIzwE3giIiIiIjPCBp6IiIiIyIywgSciIiIiMiNs4ImI\niIiIzAgbeCIiIiIiM8IGnoiIiIjIjPwflw4zYj4GMXEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "outputId": "ea578cc3-a1cc-4435-cf49-ff2daf720c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task1tamil-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "outputId": "0ed87860-bcc1-49af-af2c-cc4515b40397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "#print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 900 test sentences...\n",
            "    DONE.\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/task1tamil-results.txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "outputId": "8314082d-71e4-44a7-97c5-747416f8e8c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] ):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] ):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    # elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "    #   #print(\" Semi Paraphrase     \",end=\"\")\n",
        "    #   #count_NP+=1      \n",
        "    #   print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "    #   x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(1)\t 1\t [-0.5456994  0.732782 ] \t1\ttrue\n",
            "(1)\t 2\t [ 0.5166146 -0.426691 ] \t0\tfalse\n",
            "(1)\t 3\t [ 1.2967467 -1.3812047] \t0\tfalse\n",
            "(1)\t 4\t [ 1.8449718 -2.0012856] \t0\tfalse\n",
            "(0)\t 5\t [ 1.91249   -2.0178878] \t0\ttrue\n",
            "(1)\t 6\t [ 1.7854174 -1.997591 ] \t0\tfalse\n",
            "(0)\t 7\t [ 1.7481251 -1.6004115] \t0\ttrue\n",
            "(0)\t 8\t [ 0.4306795 -1.2885145] \t0\ttrue\n",
            "(0)\t 9\t [ 1.9767358 -1.9538733] \t0\ttrue\n",
            "(0)\t 10\t [ 1.6641582 -1.4584348] \t0\ttrue\n",
            "(1)\t 11\t [ 0.79266983 -1.5292628 ] \t0\tfalse\n",
            "(0)\t 12\t [ 1.869667  -1.8243428] \t0\ttrue\n",
            "(0)\t 13\t [ 0.94122535 -0.7965241 ] \t0\ttrue\n",
            "(0)\t 14\t [ 2.011632  -2.0073414] \t0\ttrue\n",
            "(0)\t 15\t [ 0.69286263 -1.4848948 ] \t0\ttrue\n",
            "(0)\t 16\t [ 1.2394059 -1.0832901] \t0\ttrue\n",
            "(0)\t 17\t [ 1.9161264 -2.0124786] \t0\ttrue\n",
            "(0)\t 18\t [ 0.78607786 -0.9764685 ] \t0\ttrue\n",
            "(0)\t 19\t [ 0.92464817 -1.6084503 ] \t0\ttrue\n",
            "(0)\t 20\t [ 1.9609706 -1.9203229] \t0\ttrue\n",
            "(0)\t 21\t [ 1.4463152 -1.3738232] \t0\ttrue\n",
            "(1)\t 22\t [-0.1975999   0.76718605] \t1\ttrue\n",
            "(0)\t 23\t [ 1.9623102 -1.9752854] \t0\ttrue\n",
            "(0)\t 24\t [-0.00198125  0.36102653] \t1\tfalse\n",
            "(0)\t 25\t [ 1.5745567 -1.5028908] \t0\ttrue\n",
            "(0)\t 26\t [ 1.7101431 -1.5808055] \t0\ttrue\n",
            "(0)\t 27\t [ 1.9106761 -1.8088517] \t0\ttrue\n",
            "(0)\t 28\t [-1.2697722  1.9452087] \t1\tfalse\n",
            "(0)\t 29\t [ 1.6677682 -1.7406892] \t0\ttrue\n",
            "(0)\t 30\t [0.00426347 0.20276211] \t1\tfalse\n",
            "(0)\t 31\t [-1.2247742  1.5298682] \t1\tfalse\n",
            "(0)\t 32\t [ 0.20210578 -0.12310211] \t0\ttrue\n",
            "(0)\t 33\t [ 1.9121593 -1.8763957] \t0\ttrue\n",
            "(1)\t 34\t [ 1.0727048 -1.6376771] \t0\tfalse\n",
            "(1)\t 35\t [ 1.0727048 -1.6376771] \t0\tfalse\n",
            "(1)\t 36\t [ 1.9151546 -1.9513414] \t0\tfalse\n",
            "(0)\t 37\t [-0.18491133  0.49931383] \t1\tfalse\n",
            "(1)\t 38\t [ 1.014878  -1.6891719] \t0\tfalse\n",
            "(0)\t 39\t [ 1.2111717 -1.0269096] \t0\ttrue\n",
            "(0)\t 40\t [ 0.6172797 -1.4252542] \t0\ttrue\n",
            "(1)\t 41\t [ 1.1921616  -0.98080575] \t0\tfalse\n",
            "(0)\t 42\t [ 1.047918   -0.87890667] \t0\ttrue\n",
            "(1)\t 43\t [-0.9323236  1.3530223] \t1\ttrue\n",
            "(1)\t 44\t [ 0.5900646 -1.4027156] \t0\tfalse\n",
            "(1)\t 45\t [ 0.5900646 -1.4027156] \t0\tfalse\n",
            "(0)\t 46\t [ 1.7646278 -1.4719142] \t0\ttrue\n",
            "(0)\t 47\t [ 2.000206  -1.9854184] \t0\ttrue\n",
            "(1)\t 48\t [ 0.6172207 -1.3800223] \t0\tfalse\n",
            "(0)\t 49\t [ 0.88875735 -1.3953149 ] \t0\ttrue\n",
            "(0)\t 50\t [ 0.55564606 -1.3937101 ] \t0\ttrue\n",
            "(0)\t 51\t [ 0.38311934 -1.2781094 ] \t0\ttrue\n",
            "(0)\t 52\t [ 0.44847897 -1.3210069 ] \t0\ttrue\n",
            "(0)\t 53\t [ 1.9872861 -1.922874 ] \t0\ttrue\n",
            "(0)\t 54\t [ 1.8658699 -1.9293195] \t0\ttrue\n",
            "(0)\t 55\t [ 2.014008  -1.9843131] \t0\ttrue\n",
            "(1)\t 56\t [ 0.71846753 -1.4575421 ] \t0\tfalse\n",
            "(1)\t 57\t [-1.1776671  1.7458344] \t1\ttrue\n",
            "(0)\t 58\t [ 1.8878531 -1.8195843] \t0\ttrue\n",
            "(0)\t 59\t [ 1.9166309 -1.8829824] \t0\ttrue\n",
            "(1)\t 60\t [ 1.111868 -1.050567] \t0\tfalse\n",
            "(0)\t 61\t [ 1.7176372 -1.6461675] \t0\ttrue\n",
            "(1)\t 62\t [ 0.5666682 -1.4018589] \t0\tfalse\n",
            "(1)\t 63\t [-1.3128959  1.8602701] \t1\ttrue\n",
            "(1)\t 64\t [ 0.3765496 -1.2712002] \t0\tfalse\n",
            "(1)\t 65\t [ 0.3765496 -1.2712002] \t0\tfalse\n",
            "(0)\t 66\t [ 0.47004288 -1.3339808 ] \t0\ttrue\n",
            "(0)\t 67\t [ 0.5618196 -1.3960505] \t0\ttrue\n",
            "(0)\t 68\t [ 1.9361689 -1.8459728] \t0\ttrue\n",
            "(0)\t 69\t [ 0.9694185 -1.6356344] \t0\ttrue\n",
            "(0)\t 70\t [ 0.7770994 -1.5300516] \t0\ttrue\n",
            "(0)\t 71\t [ 1.0510739  -0.91015184] \t0\ttrue\n",
            "(1)\t 72\t [-1.1983705  1.8290672] \t1\ttrue\n",
            "(1)\t 73\t [-1.1983705  1.8290672] \t1\ttrue\n",
            "(1)\t 74\t [-0.78014946  1.1081407 ] \t1\ttrue\n",
            "(1)\t 75\t [-0.78014946  1.1081407 ] \t1\ttrue\n",
            "(1)\t 76\t [ 0.8956771 -1.6007781] \t0\tfalse\n",
            "(1)\t 77\t [ 0.8956771 -1.6007781] \t0\tfalse\n",
            "(1)\t 78\t [-0.79858124  1.1154518 ] \t1\ttrue\n",
            "(0)\t 79\t [ 0.90145904 -1.6070913 ] \t0\ttrue\n",
            "(0)\t 80\t [ 1.7748847 -1.904369 ] \t0\ttrue\n",
            "(0)\t 81\t [ 1.2242897 -1.7740167] \t0\ttrue\n",
            "(1)\t 82\t [-1.3802559  1.9551944] \t1\ttrue\n",
            "(0)\t 83\t [ 1.9813218 -1.8493456] \t0\ttrue\n",
            "(0)\t 84\t [ 1.2451535 -1.755358 ] \t0\ttrue\n",
            "(1)\t 85\t [ 0.898178 -1.61089 ] \t0\tfalse\n",
            "(1)\t 86\t [ 0.5478286 -1.3742629] \t0\tfalse\n",
            "(1)\t 87\t [ 0.74477226 -1.4708375 ] \t0\tfalse\n",
            "(0)\t 88\t [ 0.7103533 -1.4849179] \t0\ttrue\n",
            "(0)\t 89\t [ 0.42440653 -1.2919551 ] \t0\ttrue\n",
            "(1)\t 90\t [-1.2370206  1.8543093] \t1\ttrue\n",
            "(1)\t 91\t [-1.0383328  1.5981414] \t1\ttrue\n",
            "(1)\t 92\t [-1.2253011  1.5999323] \t1\ttrue\n",
            "(0)\t 93\t [ 1.0887609 -1.0065745] \t0\ttrue\n",
            "(0)\t 94\t [ 1.5082729 -1.4137586] \t0\ttrue\n",
            "(0)\t 95\t [ 1.218612  -1.7797229] \t0\ttrue\n",
            "(0)\t 96\t [ 1.7636827 -1.475241 ] \t0\ttrue\n",
            "(0)\t 97\t [ 1.5890161 -1.4497632] \t0\ttrue\n",
            "(0)\t 98\t [ 1.9488012 -2.0175276] \t0\ttrue\n",
            "(0)\t 99\t [ 1.8255495 -1.8191736] \t0\ttrue\n",
            "(0)\t 100\t [ 1.7893561 -1.8351139] \t0\ttrue\n",
            "(1)\t 101\t [-1.2511604  1.7121568] \t1\ttrue\n",
            "(1)\t 102\t [ 1.1078682 -1.7433084] \t0\tfalse\n",
            "(1)\t 103\t [ 1.6039222 -1.6420189] \t0\tfalse\n",
            "(1)\t 104\t [ 0.45857352 -1.3262167 ] \t0\tfalse\n",
            "(0)\t 105\t [ 0.96336204 -1.5934159 ] \t0\ttrue\n",
            "(1)\t 106\t [-1.4840912  1.9593154] \t1\ttrue\n",
            "(0)\t 107\t [ 1.6960515 -1.7663227] \t0\ttrue\n",
            "(0)\t 108\t [ 1.8610431 -1.8523111] \t0\ttrue\n",
            "(0)\t 109\t [ 1.8579568 -2.0302696] \t0\ttrue\n",
            "(0)\t 110\t [ 0.7897647 -1.5223602] \t0\ttrue\n",
            "(0)\t 111\t [ 0.59841794 -1.4093481 ] \t0\ttrue\n",
            "(1)\t 112\t [ 0.35523832 -1.2606091 ] \t0\tfalse\n",
            "(1)\t 113\t [ 0.5405267 -1.3821268] \t0\tfalse\n",
            "(1)\t 114\t [ 0.99152404 -1.6411964 ] \t0\tfalse\n",
            "(1)\t 115\t [ 0.81451285 -1.5616055 ] \t0\tfalse\n",
            "(0)\t 116\t [ 2.0208778 -2.0290926] \t0\ttrue\n",
            "(0)\t 117\t [ 0.39272988 -1.2835208 ] \t0\ttrue\n",
            "(0)\t 118\t [ 1.0960542 -0.9206475] \t0\ttrue\n",
            "(0)\t 119\t [ 1.9705352 -1.9112692] \t0\ttrue\n",
            "(0)\t 120\t [ 1.1937473 -1.7407703] \t0\ttrue\n",
            "(0)\t 121\t [ 1.6080829 -1.5434994] \t0\ttrue\n",
            "(0)\t 122\t [ 1.8747861 -1.9210851] \t0\ttrue\n",
            "(1)\t 123\t [-0.8522359  1.4576732] \t1\ttrue\n",
            "(0)\t 124\t [ 1.290075  -1.7959709] \t0\ttrue\n",
            "(0)\t 125\t [ 0.84247184 -1.6061196 ] \t0\ttrue\n",
            "(0)\t 126\t [ 1.7976    -1.9185809] \t0\ttrue\n",
            "(1)\t 127\t [ 1.2047241 -1.71595  ] \t0\tfalse\n",
            "(1)\t 128\t [ 1.2047241 -1.71595  ] \t0\tfalse\n",
            "(0)\t 129\t [ 1.8160256 -1.798727 ] \t0\ttrue\n",
            "(0)\t 130\t [ 1.2647104 -1.7876366] \t0\ttrue\n",
            "(0)\t 131\t [ 1.8735269 -1.9194173] \t0\ttrue\n",
            "(0)\t 132\t [ 1.0988106 -1.7007167] \t0\ttrue\n",
            "(1)\t 133\t [ 0.70335597 -1.4695197 ] \t0\tfalse\n",
            "(0)\t 134\t [ 0.82095706 -1.4491421 ] \t0\ttrue\n",
            "(0)\t 135\t [ 1.218645 -1.741291] \t0\ttrue\n",
            "(0)\t 136\t [ 1.797331  -1.7734345] \t0\ttrue\n",
            "(0)\t 137\t [ 1.40085   -1.2662405] \t0\ttrue\n",
            "(0)\t 138\t [ 1.8868957 -1.9193555] \t0\ttrue\n",
            "(0)\t 139\t [ 1.9871398 -1.9402467] \t0\ttrue\n",
            "(0)\t 140\t [ 1.7802473 -1.796584 ] \t0\ttrue\n",
            "(0)\t 141\t [ 1.9029013 -1.9054474] \t0\ttrue\n",
            "(0)\t 142\t [ 0.7243539 -1.492141 ] \t0\ttrue\n",
            "(1)\t 143\t [ 1.2412875 -1.7838764] \t0\tfalse\n",
            "(1)\t 144\t [ 1.3945373 -1.4348963] \t0\tfalse\n",
            "(1)\t 145\t [ 0.944819  -0.6292564] \t0\tfalse\n",
            "(0)\t 146\t [ 1.1591668 -1.6701006] \t0\ttrue\n",
            "(0)\t 147\t [-0.22709167  0.45846233] \t1\tfalse\n",
            "(1)\t 148\t [ 1.2025454 -1.7113473] \t0\tfalse\n",
            "(1)\t 149\t [ 1.2025454 -1.7113473] \t0\tfalse\n",
            "(1)\t 150\t [ 1.2382094 -1.7079537] \t0\tfalse\n",
            "(1)\t 151\t [ 1.2382094 -1.7079537] \t0\tfalse\n",
            "(1)\t 152\t [ 0.65494853 -1.418774  ] \t0\tfalse\n",
            "(1)\t 153\t [ 0.65494853 -1.418774  ] \t0\tfalse\n",
            "(0)\t 154\t [ 1.1341084 -1.7500951] \t0\ttrue\n",
            "(1)\t 155\t [ 0.56334525 -1.3866254 ] \t0\tfalse\n",
            "(1)\t 156\t [ 0.56334525 -1.3866254 ] \t0\tfalse\n",
            "(1)\t 157\t [ 1.3387381 -1.8317367] \t0\tfalse\n",
            "(0)\t 158\t [ 1.8404946 -1.9033239] \t0\ttrue\n",
            "(0)\t 159\t [ 0.8047134 -1.5428429] \t0\ttrue\n",
            "(1)\t 160\t [ 0.546449 -1.375027] \t0\tfalse\n",
            "(0)\t 161\t [ 1.4276489 -1.3102944] \t0\ttrue\n",
            "(1)\t 162\t [ 0.74752986 -0.4303519 ] \t0\tfalse\n",
            "(0)\t 163\t [ 1.169968  -1.7618074] \t0\ttrue\n",
            "(0)\t 164\t [ 0.8387152  -0.53409344] \t0\ttrue\n",
            "(0)\t 165\t [ 0.8508466 -1.5325668] \t0\ttrue\n",
            "(0)\t 166\t [-0.75279987  1.2199507 ] \t1\tfalse\n",
            "(0)\t 167\t [ 1.343495  -1.8355252] \t0\ttrue\n",
            "(1)\t 168\t [ 0.7503419 -1.5113285] \t0\tfalse\n",
            "(1)\t 169\t [ 1.1104238 -1.699332 ] \t0\tfalse\n",
            "(1)\t 170\t [ 1.1104238 -1.699332 ] \t0\tfalse\n",
            "(0)\t 171\t [ 0.48021042 -1.2908349 ] \t0\ttrue\n",
            "(1)\t 172\t [ 0.7504573 -1.4962338] \t0\tfalse\n",
            "(1)\t 173\t [-0.5235622   0.95261914] \t1\ttrue\n",
            "(1)\t 174\t [ 1.9839252 -1.964622 ] \t0\tfalse\n",
            "(0)\t 175\t [-1.4258856  1.9852496] \t1\tfalse\n",
            "(1)\t 176\t [ 0.7112905 -1.4836308] \t0\tfalse\n",
            "(0)\t 177\t [ 1.6865591 -1.8291036] \t0\ttrue\n",
            "(0)\t 178\t [ 1.4222809 -1.8493657] \t0\ttrue\n",
            "(0)\t 179\t [ 1.869681  -1.9693055] \t0\ttrue\n",
            "(0)\t 180\t [ 1.1625518 -1.6925529] \t0\ttrue\n",
            "(0)\t 181\t [ 0.94515353 -1.5946776 ] \t0\ttrue\n",
            "(0)\t 182\t [ 0.5337224 -1.3602326] \t0\ttrue\n",
            "(0)\t 183\t [ 0.57285726 -1.4011139 ] \t0\ttrue\n",
            "(0)\t 184\t [ 0.5448454 -1.3723537] \t0\ttrue\n",
            "(0)\t 185\t [ 1.8439181 -1.8523337] \t0\ttrue\n",
            "(0)\t 186\t [ 1.0687999 -1.6437994] \t0\ttrue\n",
            "(1)\t 187\t [ 0.5419414 -0.4429307] \t0\tfalse\n",
            "(1)\t 188\t [-1.151636   1.7002814] \t1\ttrue\n",
            "(0)\t 189\t [ 0.5420176 -1.3633604] \t0\ttrue\n",
            "(1)\t 190\t [ 0.4580625 -1.3204634] \t0\tfalse\n",
            "(0)\t 191\t [ 1.093708  -1.7084613] \t0\ttrue\n",
            "(0)\t 192\t [ 1.087655  -1.6913528] \t0\ttrue\n",
            "(1)\t 193\t [ 0.88991404 -0.76205003] \t0\tfalse\n",
            "(1)\t 194\t [ 0.9959323 -0.8704389] \t0\tfalse\n",
            "(0)\t 195\t [ 0.52522767 -0.38878608] \t0\ttrue\n",
            "(1)\t 196\t [ 0.91721857 -0.72914445] \t0\tfalse\n",
            "(1)\t 197\t [ 0.66755766 -1.4633018 ] \t0\tfalse\n",
            "(1)\t 198\t [ 0.47498026 -1.3375586 ] \t0\tfalse\n",
            "(1)\t 199\t [ 0.44689086 -1.3166214 ] \t0\tfalse\n",
            "(1)\t 200\t [ 0.44689086 -1.3166214 ] \t0\tfalse\n",
            "(0)\t 201\t [ 1.3743008 -1.1961374] \t0\ttrue\n",
            "(0)\t 202\t [-0.6220518  1.1034911] \t1\tfalse\n",
            "(1)\t 203\t [ 0.63445336 -1.4356922 ] \t0\tfalse\n",
            "(1)\t 204\t [ 0.63445336 -1.4356922 ] \t0\tfalse\n",
            "(1)\t 205\t [ 0.585195  -1.4032737] \t0\tfalse\n",
            "(1)\t 206\t [ 1.3947617 -1.3695434] \t0\tfalse\n",
            "(0)\t 207\t [ 1.2551537 -1.0926071] \t0\ttrue\n",
            "(0)\t 208\t [ 0.399502  -1.2900419] \t0\ttrue\n",
            "(1)\t 209\t [ 0.5558714 -1.3789328] \t0\tfalse\n",
            "(0)\t 210\t [ 1.8983574 -2.017647 ] \t0\ttrue\n",
            "(1)\t 211\t [ 1.4478167 -1.3683606] \t0\tfalse\n",
            "(0)\t 212\t [ 1.5537716 -1.5632716] \t0\ttrue\n",
            "(1)\t 213\t [ 1.3482679 -1.811359 ] \t0\tfalse\n",
            "(0)\t 214\t [ 1.4281147 -1.8552896] \t0\ttrue\n",
            "(1)\t 215\t [ 1.893906  -1.9538975] \t0\tfalse\n",
            "(1)\t 216\t [ 1.8306693 -1.7694178] \t0\tfalse\n",
            "(0)\t 217\t [0.30063322 0.00714759] \t0\ttrue\n",
            "(1)\t 218\t [ 0.5139448 -1.37086  ] \t0\tfalse\n",
            "(0)\t 219\t [-1.2240471  1.7901233] \t1\tfalse\n",
            "(1)\t 220\t [-0.7412178  1.3089582] \t1\ttrue\n",
            "(1)\t 221\t [ 1.2319776 -1.676657 ] \t0\tfalse\n",
            "(1)\t 222\t [ 1.2319776 -1.676657 ] \t0\tfalse\n",
            "(1)\t 223\t [-0.558215   1.0943521] \t1\ttrue\n",
            "(0)\t 224\t [ 0.49401286 -0.29134756] \t0\ttrue\n",
            "(0)\t 225\t [ 1.0866781 -1.1264504] \t0\ttrue\n",
            "(0)\t 226\t [-1.0727173  1.4816483] \t1\tfalse\n",
            "(1)\t 227\t [ 1.0678483 -1.6987593] \t0\tfalse\n",
            "(1)\t 228\t [ 1.0678483 -1.6987593] \t0\tfalse\n",
            "(1)\t 229\t [ 0.68474084 -1.4341124 ] \t0\tfalse\n",
            "(0)\t 230\t [ 1.9276289 -1.9015756] \t0\ttrue\n",
            "(1)\t 231\t [ 1.2306318 -1.3123797] \t0\tfalse\n",
            "(1)\t 232\t [ 1.2306318 -1.3123797] \t0\tfalse\n",
            "(0)\t 233\t [ 0.5699811 -1.4008886] \t0\ttrue\n",
            "(1)\t 234\t [ 0.7595038 -1.5107456] \t0\tfalse\n",
            "(0)\t 235\t [ 0.5776483 -1.4018704] \t0\ttrue\n",
            "(1)\t 236\t [ 1.019586  -1.6554039] \t0\tfalse\n",
            "(1)\t 237\t [-1.3726733  1.9509557] \t1\ttrue\n",
            "(0)\t 238\t [ 1.11696   -1.7392687] \t0\ttrue\n",
            "(1)\t 239\t [ 0.7431019 -1.4894184] \t0\tfalse\n",
            "(0)\t 240\t [ 0.5899145 -0.3986634] \t0\ttrue\n",
            "(0)\t 241\t [ 0.98137915 -0.8398009 ] \t0\ttrue\n",
            "(1)\t 242\t [ 1.6475419 -1.5954012] \t0\tfalse\n",
            "(1)\t 243\t [ 0.42479905 -1.2984637 ] \t0\tfalse\n",
            "(0)\t 244\t [ 0.7472588 -1.5352412] \t0\ttrue\n",
            "(1)\t 245\t [ 0.8940285 -1.5715176] \t0\tfalse\n",
            "(1)\t 246\t [ 1.1331013 -1.6992409] \t0\tfalse\n",
            "(1)\t 247\t [ 1.1331013 -1.6992409] \t0\tfalse\n",
            "(0)\t 248\t [ 0.67996734 -1.4800366 ] \t0\ttrue\n",
            "(1)\t 249\t [ 0.50472313 -1.3479298 ] \t0\tfalse\n",
            "(1)\t 250\t [ 0.50472313 -1.3479298 ] \t0\tfalse\n",
            "(0)\t 251\t [ 1.6921327 -1.5828005] \t0\ttrue\n",
            "(1)\t 252\t [ 1.12777   -1.6328778] \t0\tfalse\n",
            "(1)\t 253\t [ 0.51956993 -1.3752729 ] \t0\tfalse\n",
            "(1)\t 254\t [ 1.6857005 -1.7153324] \t0\tfalse\n",
            "(0)\t 255\t [ 0.94795924 -1.6315662 ] \t0\ttrue\n",
            "(0)\t 256\t [ 0.3859183 -1.2751521] \t0\ttrue\n",
            "(1)\t 257\t [ 0.45021507 -1.3163964 ] \t0\tfalse\n",
            "(0)\t 258\t [-0.30735862  0.6802328 ] \t1\tfalse\n",
            "(0)\t 259\t [ 1.9172374 -2.0485778] \t0\ttrue\n",
            "(0)\t 260\t [ 1.3169025 -1.8488553] \t0\ttrue\n",
            "(0)\t 261\t [ 2.0019739 -2.0759048] \t0\ttrue\n",
            "(0)\t 262\t [ 0.8305034 -1.5853429] \t0\ttrue\n",
            "(0)\t 263\t [ 1.9221917 -2.055317 ] \t0\ttrue\n",
            "(0)\t 264\t [ 0.45791036 -1.3239139 ] \t0\ttrue\n",
            "(0)\t 265\t [ 1.0996639 -1.6815498] \t0\ttrue\n",
            "(1)\t 266\t [ 1.2804929 -1.7918822] \t0\tfalse\n",
            "(0)\t 267\t [ 1.1718469 -1.7509562] \t0\ttrue\n",
            "(0)\t 268\t [-1.3268702  1.7487096] \t1\tfalse\n",
            "(0)\t 269\t [ 1.4005305 -1.2483615] \t0\ttrue\n",
            "(0)\t 270\t [ 0.89336926 -0.7940827 ] \t0\ttrue\n",
            "(1)\t 271\t [ 0.6448761  -0.25889897] \t0\tfalse\n",
            "(1)\t 272\t [ 1.7944576 -1.7338663] \t0\tfalse\n",
            "(1)\t 273\t [ 1.6942308 -1.816925 ] \t0\tfalse\n",
            "(1)\t 274\t [ 0.41485885 -0.23368372] \t0\tfalse\n",
            "(1)\t 275\t [ 0.45382106 -1.3114444 ] \t0\tfalse\n",
            "(1)\t 276\t [-1.288278   1.6742623] \t1\ttrue\n",
            "(1)\t 277\t [ 1.8624573 -1.8810287] \t0\tfalse\n",
            "(0)\t 278\t [ 1.2824723 -1.8211515] \t0\ttrue\n",
            "(0)\t 279\t [-0.21462914  0.6498633 ] \t1\tfalse\n",
            "(0)\t 280\t [ 0.92943317 -1.6441686 ] \t0\ttrue\n",
            "(1)\t 281\t [-1.3148161  1.7328305] \t1\ttrue\n",
            "(0)\t 282\t [ 0.42725766 -1.2949439 ] \t0\ttrue\n",
            "(1)\t 283\t [0.01401266 0.10693971] \t1\ttrue\n",
            "(1)\t 284\t [ 1.305886  -1.3464354] \t0\tfalse\n",
            "(1)\t 285\t [ 0.7902999 -1.5063165] \t0\tfalse\n",
            "(1)\t 286\t [ 0.7902999 -1.5063165] \t0\tfalse\n",
            "(1)\t 287\t [ 1.7833415 -1.7293669] \t0\tfalse\n",
            "(0)\t 288\t [ 0.5887857 -1.4022305] \t0\ttrue\n",
            "(0)\t 289\t [ 0.64664906 -1.4254051 ] \t0\ttrue\n",
            "(1)\t 290\t [ 0.37688315 -1.2740964 ] \t0\tfalse\n",
            "(1)\t 291\t [-0.27571574  0.66467285] \t1\ttrue\n",
            "(1)\t 292\t [-0.33256325  0.6098126 ] \t1\ttrue\n",
            "(1)\t 293\t [ 0.6716969 -1.4655094] \t0\tfalse\n",
            "(0)\t 294\t [ 1.6521435 -1.7670382] \t0\ttrue\n",
            "(0)\t 295\t [ 1.4526789 -1.3385627] \t0\ttrue\n",
            "(0)\t 296\t [ 1.245577  -1.1925552] \t0\ttrue\n",
            "(0)\t 297\t [-0.18027894  0.5151092 ] \t1\tfalse\n",
            "(0)\t 298\t [ 1.8650892 -1.8758702] \t0\ttrue\n",
            "(1)\t 299\t [ 0.45601302 -1.2977142 ] \t0\tfalse\n",
            "(0)\t 300\t [ 1.8623832 -1.8871939] \t0\ttrue\n",
            "(1)\t 301\t [-0.7660677  1.3889625] \t1\ttrue\n",
            "(1)\t 302\t [ 1.2988178 -1.7422695] \t0\tfalse\n",
            "(1)\t 303\t [ 1.2988178 -1.7422695] \t0\tfalse\n",
            "(1)\t 304\t [-1.3930038  2.0007837] \t1\ttrue\n",
            "(1)\t 305\t [ 0.39555958 -1.2817615 ] \t0\tfalse\n",
            "(1)\t 306\t [ 0.3938346 -1.2843925] \t0\tfalse\n",
            "(0)\t 307\t [ 1.9761894 -2.0529926] \t0\ttrue\n",
            "(1)\t 308\t [ 0.64391106 -1.4546646 ] \t0\tfalse\n",
            "(0)\t 309\t [ 1.1534306 -1.7103893] \t0\ttrue\n",
            "(0)\t 310\t [ 1.645073  -1.7480816] \t0\ttrue\n",
            "(0)\t 311\t [ 0.7356001 -1.1681337] \t0\ttrue\n",
            "(0)\t 312\t [0.19747059 0.12399185] \t0\ttrue\n",
            "(0)\t 313\t [ 1.8223537 -1.7502892] \t0\ttrue\n",
            "(0)\t 314\t [ 1.514756  -1.6773885] \t0\ttrue\n",
            "(0)\t 315\t [ 0.9250105 -1.6200949] \t0\ttrue\n",
            "(0)\t 316\t [ 1.8916215 -1.9498084] \t0\ttrue\n",
            "(0)\t 317\t [ 0.73978853 -1.5016108 ] \t0\ttrue\n",
            "(1)\t 318\t [-1.528461   2.0129583] \t1\ttrue\n",
            "(0)\t 319\t [ 0.7678016 -1.4806423] \t0\ttrue\n",
            "(1)\t 320\t [ 1.9821883 -1.992168 ] \t0\tfalse\n",
            "(1)\t 321\t [ 1.9355859 -1.977848 ] \t0\tfalse\n",
            "(0)\t 322\t [ 1.4138919 -1.2754664] \t0\ttrue\n",
            "(0)\t 323\t [ 1.2165846 -1.7956976] \t0\ttrue\n",
            "(0)\t 324\t [ 0.5869148 -1.4051604] \t0\ttrue\n",
            "(0)\t 325\t [ 0.72928196 -1.4975841 ] \t0\ttrue\n",
            "(1)\t 326\t [ 0.8835527 -1.6075952] \t0\tfalse\n",
            "(0)\t 327\t [ 1.433651  -1.3328711] \t0\ttrue\n",
            "(0)\t 328\t [ 1.8479707 -1.9716086] \t0\ttrue\n",
            "(0)\t 329\t [-1.1194092  1.6971259] \t1\tfalse\n",
            "(1)\t 330\t [-1.0427743  1.6327453] \t1\ttrue\n",
            "(0)\t 331\t [0.32296202 0.125661  ] \t0\ttrue\n",
            "(1)\t 332\t [ 0.32938072 -0.05511195] \t0\tfalse\n",
            "(1)\t 333\t [-0.791246   1.2849606] \t1\ttrue\n",
            "(1)\t 334\t [ 1.6946899 -1.7926968] \t0\tfalse\n",
            "(0)\t 335\t [-1.4862857  2.0136645] \t1\tfalse\n",
            "(0)\t 336\t [ 0.8605917 -1.5546309] \t0\ttrue\n",
            "(0)\t 337\t [ 1.0539298 -1.6776375] \t0\ttrue\n",
            "(1)\t 338\t [ 1.8284749 -1.9025702] \t0\tfalse\n",
            "(1)\t 339\t [-0.83029056  1.3365426 ] \t1\ttrue\n",
            "(1)\t 340\t [-1.3751885  1.8056432] \t1\ttrue\n",
            "(0)\t 341\t [-0.13821101  0.56515926] \t1\tfalse\n",
            "(1)\t 342\t [-1.3747188  1.9225047] \t1\ttrue\n",
            "(1)\t 343\t [ 0.48518384 -1.3357556 ] \t0\tfalse\n",
            "(0)\t 344\t [-1.2463423  1.8200185] \t1\tfalse\n",
            "(1)\t 345\t [ 0.3898112 -1.2789252] \t0\tfalse\n",
            "(1)\t 346\t [ 1.1329252 -1.7331524] \t0\tfalse\n",
            "(0)\t 347\t [ 1.2203599 -1.6859263] \t0\ttrue\n",
            "(0)\t 348\t [ 0.5863137 -0.2896529] \t0\ttrue\n",
            "(0)\t 349\t [ 0.27542442 -0.03961702] \t0\ttrue\n",
            "(0)\t 350\t [ 1.071179  -1.6418401] \t0\ttrue\n",
            "(0)\t 351\t [-0.58011025  0.90469754] \t1\tfalse\n",
            "(0)\t 352\t [ 1.0902377 -1.6782508] \t0\ttrue\n",
            "(1)\t 353\t [-0.18310097  0.30560333] \t1\ttrue\n",
            "(0)\t 354\t [ 1.7369885 -1.7518928] \t0\ttrue\n",
            "(0)\t 355\t [ 0.6848909 -1.4713368] \t0\ttrue\n",
            "(0)\t 356\t [ 1.8198622 -1.960871 ] \t0\ttrue\n",
            "(0)\t 357\t [ 1.8303466 -1.9712005] \t0\ttrue\n",
            "(0)\t 358\t [ 1.2978848 -1.6961136] \t0\ttrue\n",
            "(0)\t 359\t [ 1.8767802 -1.9242052] \t0\ttrue\n",
            "(0)\t 360\t [ 1.7932875 -1.8237144] \t0\ttrue\n",
            "(0)\t 361\t [ 0.63421583 -1.4112113 ] \t0\ttrue\n",
            "(0)\t 362\t [ 0.54404974 -1.3880657 ] \t0\ttrue\n",
            "(0)\t 363\t [ 0.390988  -1.2725577] \t0\ttrue\n",
            "(0)\t 364\t [ 0.40169775 -1.285448  ] \t0\ttrue\n",
            "(0)\t 365\t [-0.31860194  0.7309218 ] \t1\tfalse\n",
            "(0)\t 366\t [ 1.978121  -1.9623156] \t0\ttrue\n",
            "(1)\t 367\t [-0.8584911  1.4283589] \t1\ttrue\n",
            "(0)\t 368\t [ 1.756187  -1.8859116] \t0\ttrue\n",
            "(0)\t 369\t [ 1.0487279 -1.595915 ] \t0\ttrue\n",
            "(0)\t 370\t [-1.0121583  1.5515434] \t1\tfalse\n",
            "(0)\t 371\t [ 1.3732499 -1.4036804] \t0\ttrue\n",
            "(0)\t 372\t [-1.280747   1.8233231] \t1\tfalse\n",
            "(0)\t 373\t [ 0.618598   -0.66808367] \t0\ttrue\n",
            "(0)\t 374\t [-0.83273363  1.1392359 ] \t1\tfalse\n",
            "(0)\t 375\t [ 1.0825975 -1.674692 ] \t0\ttrue\n",
            "(0)\t 376\t [ 1.3026496 -1.8208691] \t0\ttrue\n",
            "(0)\t 377\t [ 1.858324  -1.9026617] \t0\ttrue\n",
            "(0)\t 378\t [ 0.47744226 -1.3297075 ] \t0\ttrue\n",
            "(0)\t 379\t [-0.32219002  0.8206872 ] \t1\tfalse\n",
            "(1)\t 380\t [ 1.9003069 -1.8752062] \t0\tfalse\n",
            "(0)\t 381\t [-0.5111297  1.1015769] \t1\tfalse\n",
            "(0)\t 382\t [ 0.51002157 -1.3549527 ] \t0\ttrue\n",
            "(0)\t 383\t [ 0.53651935 -1.366803  ] \t0\ttrue\n",
            "(1)\t 384\t [ 1.3527429 -1.3802543] \t0\tfalse\n",
            "(0)\t 385\t [ 1.4675915 -1.5712558] \t0\ttrue\n",
            "(0)\t 386\t [ 0.7159626 -1.4868088] \t0\ttrue\n",
            "(0)\t 387\t [ 1.65353   -1.6783463] \t0\ttrue\n",
            "(1)\t 388\t [ 0.8577863  -0.69909865] \t0\tfalse\n",
            "(0)\t 389\t [ 1.8996822 -2.0214977] \t0\ttrue\n",
            "(0)\t 390\t [ 1.9524031 -1.9628264] \t0\ttrue\n",
            "(0)\t 391\t [ 1.0305232 -1.6829833] \t0\ttrue\n",
            "(0)\t 392\t [ 1.149191  -1.7326751] \t0\ttrue\n",
            "(0)\t 393\t [-0.9991754  1.4318954] \t1\tfalse\n",
            "(0)\t 394\t [ 0.3515812  -0.17952536] \t0\ttrue\n",
            "(1)\t 395\t [ 0.9218632 -1.6147755] \t0\tfalse\n",
            "(0)\t 396\t [-0.8955427  1.4810797] \t1\tfalse\n",
            "(0)\t 397\t [ 0.7143193  -0.40742898] \t0\ttrue\n",
            "(0)\t 398\t [-1.2339092  1.7368644] \t1\tfalse\n",
            "(0)\t 399\t [ 1.9453512 -1.9187618] \t0\ttrue\n",
            "(0)\t 400\t [ 1.9845945 -1.9838792] \t0\ttrue\n",
            "(0)\t 401\t [ 1.3581698 -1.2667203] \t0\ttrue\n",
            "(0)\t 402\t [ 0.8201824  -0.37987894] \t0\ttrue\n",
            "(0)\t 403\t [0.32334605 0.06049366] \t0\ttrue\n",
            "(0)\t 404\t [0.10162722 0.291028  ] \t1\tfalse\n",
            "(0)\t 405\t [ 0.65747887 -0.6365504 ] \t0\ttrue\n",
            "(0)\t 406\t [ 1.8360783 -1.86881  ] \t0\ttrue\n",
            "(0)\t 407\t [ 0.36969376 -1.2695632 ] \t0\ttrue\n",
            "(1)\t 408\t [ 0.6596565  -0.48780656] \t0\tfalse\n",
            "(0)\t 409\t [ 1.8759754 -1.8568064] \t0\ttrue\n",
            "(0)\t 410\t [ 0.4538413 -1.3181806] \t0\ttrue\n",
            "(0)\t 411\t [ 1.3930808 -1.4982481] \t0\ttrue\n",
            "(0)\t 412\t [ 1.0853271 -1.5467676] \t0\ttrue\n",
            "(0)\t 413\t [0.24709113 0.01843104] \t0\ttrue\n",
            "(0)\t 414\t [-1.4539229  2.0539372] \t1\tfalse\n",
            "(0)\t 415\t [ 1.4722022 -1.3264202] \t0\ttrue\n",
            "(1)\t 416\t [ 0.47223818 -1.3213127 ] \t0\tfalse\n",
            "(0)\t 417\t [ 1.2098922 -1.7292312] \t0\ttrue\n",
            "(1)\t 418\t [ 0.7143769 -0.5163217] \t0\tfalse\n",
            "(0)\t 419\t [ 1.9033893 -1.9687927] \t0\ttrue\n",
            "(1)\t 420\t [ 1.0172871 -1.0133172] \t0\tfalse\n",
            "(0)\t 421\t [ 0.84189785 -1.4696441 ] \t0\ttrue\n",
            "(0)\t 422\t [ 0.43894017 -1.3102444 ] \t0\ttrue\n",
            "(1)\t 423\t [ 1.0945005 -1.7197589] \t0\tfalse\n",
            "(0)\t 424\t [ 1.7697167 -1.5417548] \t0\ttrue\n",
            "(0)\t 425\t [ 0.51173365 -1.359441  ] \t0\ttrue\n",
            "(1)\t 426\t [-1.2143776  1.7222968] \t1\ttrue\n",
            "(0)\t 427\t [ 1.5650986 -1.6473972] \t0\ttrue\n",
            "(0)\t 428\t [ 0.712866  -1.4931897] \t0\ttrue\n",
            "(0)\t 429\t [ 1.5551993 -1.6075873] \t0\ttrue\n",
            "(0)\t 430\t [ 1.9319623 -2.0787578] \t0\ttrue\n",
            "(1)\t 431\t [ 1.0451201 -0.9215532] \t0\tfalse\n",
            "(1)\t 432\t [ 0.91735095 -1.5287472 ] \t0\tfalse\n",
            "(1)\t 433\t [ 0.91735095 -1.5287472 ] \t0\tfalse\n",
            "(0)\t 434\t [ 1.174571  -0.9389922] \t0\ttrue\n",
            "(1)\t 435\t [ 1.1547837 -1.7099097] \t0\tfalse\n",
            "(1)\t 436\t [ 1.1547837 -1.7099097] \t0\tfalse\n",
            "(0)\t 437\t [ 0.9005985 -1.5967315] \t0\ttrue\n",
            "(0)\t 438\t [ 0.57882524 -0.22261347] \t0\ttrue\n",
            "(0)\t 439\t [-0.5195981   0.77836275] \t1\tfalse\n",
            "(0)\t 440\t [ 0.5651709 -1.4054705] \t0\ttrue\n",
            "(0)\t 441\t [ 0.9095606 -0.7343743] \t0\ttrue\n",
            "(1)\t 442\t [ 1.1582392 -1.7516192] \t0\tfalse\n",
            "(1)\t 443\t [ 1.3200963 -1.7442409] \t0\tfalse\n",
            "(0)\t 444\t [ 1.9430709 -1.9969587] \t0\ttrue\n",
            "(0)\t 445\t [ 1.8224477 -1.6974497] \t0\ttrue\n",
            "(1)\t 446\t [ 1.7870468 -1.8525286] \t0\tfalse\n",
            "(0)\t 447\t [ 1.9381196 -1.9707539] \t0\ttrue\n",
            "(0)\t 448\t [ 1.5422239 -1.4468873] \t0\ttrue\n",
            "(0)\t 449\t [ 0.41708934 -1.2875837 ] \t0\ttrue\n",
            "(1)\t 450\t [ 0.90435714 -1.574253  ] \t0\tfalse\n",
            "(1)\t 451\t [ 0.90435714 -1.574253  ] \t0\tfalse\n",
            "(1)\t 452\t [ 1.2597605 -1.7521814] \t0\tfalse\n",
            "(0)\t 453\t [ 1.419461  -1.3837596] \t0\ttrue\n",
            "(0)\t 454\t [ 1.8203884 -1.805165 ] \t0\ttrue\n",
            "(0)\t 455\t [ 0.5107631 -1.3647203] \t0\ttrue\n",
            "(1)\t 456\t [-0.71519226  1.2486562 ] \t1\ttrue\n",
            "(1)\t 457\t [0.23450865 0.13827665] \t0\tfalse\n",
            "(1)\t 458\t [ 1.7411362 -1.6193432] \t0\tfalse\n",
            "(0)\t 459\t [ 1.4737216 -1.5470033] \t0\ttrue\n",
            "(1)\t 460\t [ 1.3541481 -1.7890397] \t0\tfalse\n",
            "(1)\t 461\t [ 1.3541481 -1.7890397] \t0\tfalse\n",
            "(0)\t 462\t [ 0.7175523 -1.4936742] \t0\ttrue\n",
            "(1)\t 463\t [ 1.8673917 -1.8776985] \t0\tfalse\n",
            "(1)\t 464\t [ 1.6186978 -1.5821618] \t0\tfalse\n",
            "(1)\t 465\t [ 1.839419 -1.929753] \t0\tfalse\n",
            "(1)\t 466\t [-1.4862561  1.8485414] \t1\ttrue\n",
            "(0)\t 467\t [ 1.8587288 -1.9297851] \t0\ttrue\n",
            "(0)\t 468\t [ 1.5988523 -1.5288609] \t0\ttrue\n",
            "(1)\t 469\t [-0.0114499   0.22407083] \t1\ttrue\n",
            "(1)\t 470\t [ 0.674257  -0.3951915] \t0\tfalse\n",
            "(1)\t 471\t [ 1.2608576 -1.3701673] \t0\tfalse\n",
            "(1)\t 472\t [ 1.2608576 -1.3701673] \t0\tfalse\n",
            "(0)\t 473\t [ 0.6574898 -0.5018486] \t0\ttrue\n",
            "(1)\t 474\t [ 0.38211396 -1.2728524 ] \t0\tfalse\n",
            "(0)\t 475\t [ 0.7828227 -1.5372335] \t0\ttrue\n",
            "(0)\t 476\t [ 2.003318  -2.0152812] \t0\ttrue\n",
            "(0)\t 477\t [ 1.3432055 -1.746425 ] \t0\ttrue\n",
            "(0)\t 478\t [ 1.9483377 -1.906112 ] \t0\ttrue\n",
            "(1)\t 479\t [ 0.5246294 -1.3333111] \t0\tfalse\n",
            "(1)\t 480\t [ 1.5661383 -1.6306182] \t0\tfalse\n",
            "(1)\t 481\t [ 0.57472485 -0.15700546] \t0\tfalse\n",
            "(1)\t 482\t [ 0.9819069 -1.6157589] \t0\tfalse\n",
            "(0)\t 483\t [ 0.6631241 -1.4802996] \t0\ttrue\n",
            "(1)\t 484\t [-0.6460715  1.0916139] \t1\ttrue\n",
            "(0)\t 485\t [ 1.209022  -1.7288067] \t0\ttrue\n",
            "(1)\t 486\t [ 1.6723317 -1.843068 ] \t0\tfalse\n",
            "(1)\t 487\t [-0.23978181  0.74150974] \t1\ttrue\n",
            "(0)\t 488\t [ 0.62422997 -1.4306785 ] \t0\ttrue\n",
            "(0)\t 489\t [ 0.5877746 -1.4177624] \t0\ttrue\n",
            "(0)\t 490\t [ 0.4908497 -1.3522099] \t0\ttrue\n",
            "(0)\t 491\t [ 1.5429636 -1.5486594] \t0\ttrue\n",
            "(1)\t 492\t [ 1.1982478 -1.7743592] \t0\tfalse\n",
            "(1)\t 493\t [ 1.1982478 -1.7743592] \t0\tfalse\n",
            "(0)\t 494\t [ 1.7372411 -1.6982548] \t0\ttrue\n",
            "(0)\t 495\t [ 0.46116346 -1.3258109 ] \t0\ttrue\n",
            "(1)\t 496\t [ 1.8751937 -1.8147001] \t0\tfalse\n",
            "(1)\t 497\t [ 0.9296348  -0.53724146] \t0\tfalse\n",
            "(0)\t 498\t [ 0.6640068 -1.4388359] \t0\ttrue\n",
            "(0)\t 499\t [ 1.3713979 -1.8586161] \t0\ttrue\n",
            "(0)\t 500\t [ 0.43043038 -1.3068883 ] \t0\ttrue\n",
            "(1)\t 501\t [ 0.92651355 -1.6119986 ] \t0\tfalse\n",
            "(1)\t 502\t [ 0.92651355 -1.6119986 ] \t0\tfalse\n",
            "(0)\t 503\t [ 0.4782203 -1.3340998] \t0\ttrue\n",
            "(0)\t 504\t [ 0.9605005 -1.5471046] \t0\ttrue\n",
            "(1)\t 505\t [0.07458188 0.21486174] \t1\ttrue\n",
            "(1)\t 506\t [ 1.5382556 -1.3900716] \t0\tfalse\n",
            "(1)\t 507\t [-0.64118344  0.9286731 ] \t1\ttrue\n",
            "(0)\t 508\t [-1.3583494  1.590701 ] \t1\tfalse\n",
            "(0)\t 509\t [-0.914567   1.2177341] \t1\tfalse\n",
            "(0)\t 510\t [-0.10912807  0.3905854 ] \t1\tfalse\n",
            "(1)\t 511\t [-1.3070287  1.8760623] \t1\ttrue\n",
            "(1)\t 512\t [ 0.57509243 -1.3987842 ] \t0\tfalse\n",
            "(0)\t 513\t [ 1.3709105 -1.3215475] \t0\ttrue\n",
            "(0)\t 514\t [ 0.40391368 -0.26646483] \t0\ttrue\n",
            "(1)\t 515\t [-1.5799352  2.081283 ] \t1\ttrue\n",
            "(0)\t 516\t [ 1.218099  -1.7180197] \t0\ttrue\n",
            "(1)\t 517\t [ 0.39193362 -1.281019  ] \t0\tfalse\n",
            "(0)\t 518\t [ 0.48267296 -1.3437096 ] \t0\ttrue\n",
            "(0)\t 519\t [ 0.5137908 -1.3634244] \t0\ttrue\n",
            "(0)\t 520\t [ 1.9147012 -1.9632242] \t0\ttrue\n",
            "(0)\t 521\t [ 0.41893935 -1.2964312 ] \t0\ttrue\n",
            "(1)\t 522\t [ 0.5246237 -1.3586936] \t0\tfalse\n",
            "(1)\t 523\t [0.05140943 0.39235505] \t1\ttrue\n",
            "(1)\t 524\t [-1.1122211  1.6825205] \t1\ttrue\n",
            "(1)\t 525\t [ 1.9052657 -1.9943229] \t0\tfalse\n",
            "(1)\t 526\t [ 1.0128962  -0.80767703] \t0\tfalse\n",
            "(1)\t 527\t [-0.12953742  0.12063817] \t1\ttrue\n",
            "(0)\t 528\t [ 1.645642  -1.7267298] \t0\ttrue\n",
            "(1)\t 529\t [ 0.946058  -0.6885412] \t0\tfalse\n",
            "(1)\t 530\t [ 0.946058  -0.6885412] \t0\tfalse\n",
            "(1)\t 531\t [ 0.5595731  -0.40971598] \t0\tfalse\n",
            "(1)\t 532\t [-1.0352494  1.5700282] \t1\ttrue\n",
            "(1)\t 533\t [ 1.2711126 -1.740529 ] \t0\tfalse\n",
            "(1)\t 534\t [ 1.2711126 -1.740529 ] \t0\tfalse\n",
            "(1)\t 535\t [ 0.65695304 -1.3959771 ] \t0\tfalse\n",
            "(1)\t 536\t [ 0.65695304 -1.3959771 ] \t0\tfalse\n",
            "(1)\t 537\t [ 1.9124676 -1.868425 ] \t0\tfalse\n",
            "(1)\t 538\t [-0.46601358  0.9050866 ] \t1\ttrue\n",
            "(1)\t 539\t [-0.46601358  0.9050866 ] \t1\ttrue\n",
            "(1)\t 540\t [-0.10689763  0.1925459 ] \t1\ttrue\n",
            "(1)\t 541\t [-0.10689763  0.1925459 ] \t1\ttrue\n",
            "(0)\t 542\t [ 0.39374244 -1.2801208 ] \t0\ttrue\n",
            "(1)\t 543\t [ 0.9053802 -1.5599635] \t0\tfalse\n",
            "(1)\t 544\t [ 0.9053802 -1.5599635] \t0\tfalse\n",
            "(1)\t 545\t [ 1.2008137 -1.7777941] \t0\tfalse\n",
            "(1)\t 546\t [ 0.80828804 -1.5617265 ] \t0\tfalse\n",
            "(0)\t 547\t [-0.6512511  1.1500787] \t1\tfalse\n",
            "(1)\t 548\t [ 1.2707652 -1.8029021] \t0\tfalse\n",
            "(1)\t 549\t [ 1.2707652 -1.8029021] \t0\tfalse\n",
            "(0)\t 550\t [ 1.1503648 -1.7359781] \t0\ttrue\n",
            "(0)\t 551\t [ 1.5350813 -1.5016807] \t0\ttrue\n",
            "(0)\t 552\t [ 0.38612908 -1.2785302 ] \t0\ttrue\n",
            "(1)\t 553\t [-0.63745695  1.2544866 ] \t1\ttrue\n",
            "(0)\t 554\t [ 0.6364136 -1.439613 ] \t0\ttrue\n",
            "(0)\t 555\t [ 1.4709009 -1.3321059] \t0\ttrue\n",
            "(1)\t 556\t [-0.8145886  1.345659 ] \t1\ttrue\n",
            "(1)\t 557\t [-1.5682348  2.06793  ] \t1\ttrue\n",
            "(1)\t 558\t [ 1.1191792 -1.1121012] \t0\tfalse\n",
            "(0)\t 559\t [ 0.40393966 -1.2799901 ] \t0\ttrue\n",
            "(0)\t 560\t [-1.2477838  1.7734814] \t1\tfalse\n",
            "(0)\t 561\t [-1.3496164  1.9748156] \t1\tfalse\n",
            "(1)\t 562\t [ 1.3495423 -1.7789658] \t0\tfalse\n",
            "(1)\t 563\t [ 1.3495423 -1.7789658] \t0\tfalse\n",
            "(1)\t 564\t [ 1.1577814 -1.7359282] \t0\tfalse\n",
            "(0)\t 565\t [ 0.42321748 -1.2924219 ] \t0\ttrue\n",
            "(1)\t 566\t [ 1.5548252 -1.581069 ] \t0\tfalse\n",
            "(1)\t 567\t [ 1.5548252 -1.581069 ] \t0\tfalse\n",
            "(0)\t 568\t [ 1.0842426 -0.9939662] \t0\ttrue\n",
            "(0)\t 569\t [ 1.6626192 -1.6968307] \t0\ttrue\n",
            "(0)\t 570\t [ 1.9039217 -1.7892946] \t0\ttrue\n",
            "(0)\t 571\t [ 0.9200157 -1.5894451] \t0\ttrue\n",
            "(1)\t 572\t [-1.049777   1.4907494] \t1\ttrue\n",
            "(1)\t 573\t [-0.9484431  1.6170832] \t1\ttrue\n",
            "(0)\t 574\t [ 0.59616846 -1.3895019 ] \t0\ttrue\n",
            "(0)\t 575\t [ 0.49847478 -1.3533506 ] \t0\ttrue\n",
            "(1)\t 576\t [ 0.7556058 -1.5074494] \t0\tfalse\n",
            "(0)\t 577\t [ 0.5014291  -0.20149238] \t0\ttrue\n",
            "(0)\t 578\t [ 1.1888086 -1.7074085] \t0\ttrue\n",
            "(0)\t 579\t [ 1.3636894 -1.2559917] \t0\ttrue\n",
            "(0)\t 580\t [ 1.2350218 -1.7730381] \t0\ttrue\n",
            "(1)\t 581\t [ 0.67265505 -1.418339  ] \t0\tfalse\n",
            "(1)\t 582\t [ 0.4329026 -1.3015968] \t0\tfalse\n",
            "(1)\t 583\t [ 0.4329026 -1.3015968] \t0\tfalse\n",
            "(0)\t 584\t [ 1.8894382 -1.8373495] \t0\ttrue\n",
            "(1)\t 585\t [ 0.82815117 -1.5685383 ] \t0\tfalse\n",
            "(0)\t 586\t [ 1.1430261 -1.741956 ] \t0\ttrue\n",
            "(1)\t 587\t [ 0.8557947 -1.586145 ] \t0\tfalse\n",
            "(0)\t 588\t [ 0.90971273 -0.79267925] \t0\ttrue\n",
            "(1)\t 589\t [ 1.6008838 -1.5483701] \t0\tfalse\n",
            "(1)\t 590\t [ 1.6558472 -1.6645803] \t0\tfalse\n",
            "(1)\t 591\t [ 1.0213878  -0.71654147] \t0\tfalse\n",
            "(0)\t 592\t [ 1.3080522 -1.7683524] \t0\ttrue\n",
            "(1)\t 593\t [ 1.7290965 -1.7864171] \t0\tfalse\n",
            "(1)\t 594\t [ 1.7290965 -1.7864171] \t0\tfalse\n",
            "(0)\t 595\t [ 0.6476315 -1.4556869] \t0\ttrue\n",
            "(0)\t 596\t [ 0.6756621 -1.4875553] \t0\ttrue\n",
            "(1)\t 597\t [-1.3207928  1.8378702] \t1\ttrue\n",
            "(0)\t 598\t [ 1.0088166 -0.9201873] \t0\ttrue\n",
            "(0)\t 599\t [-0.83345443  1.338319  ] \t1\tfalse\n",
            "(0)\t 600\t [-0.8472147  1.2861984] \t1\tfalse\n",
            "(0)\t 601\t [ 0.21418893 -0.09370694] \t0\ttrue\n",
            "(0)\t 602\t [0.13352378 0.03581061] \t0\ttrue\n",
            "(1)\t 603\t [ 1.6895813 -1.7123462] \t0\tfalse\n",
            "(1)\t 604\t [ 0.5016478 -1.3502234] \t0\tfalse\n",
            "(1)\t 605\t [ 0.5016478 -1.3502234] \t0\tfalse\n",
            "(1)\t 606\t [ 1.2404621 -1.8030548] \t0\tfalse\n",
            "(0)\t 607\t [ 0.45195937 -1.3147421 ] \t0\ttrue\n",
            "(0)\t 608\t [ 1.0359323 -1.6619304] \t0\ttrue\n",
            "(1)\t 609\t [ 0.40760198 -1.2861048 ] \t0\tfalse\n",
            "(1)\t 610\t [ 0.40760198 -1.2861048 ] \t0\tfalse\n",
            "(0)\t 611\t [ 1.4446355 -1.3993028] \t0\ttrue\n",
            "(1)\t 612\t [ 0.5468157 -1.3890558] \t0\tfalse\n",
            "(1)\t 613\t [ 0.92312986 -1.6333175 ] \t0\tfalse\n",
            "(0)\t 614\t [ 1.071298  -1.6795969] \t0\ttrue\n",
            "(0)\t 615\t [ 0.85020673 -0.87654996] \t0\ttrue\n",
            "(0)\t 616\t [ 0.64248097 -0.6370787 ] \t0\ttrue\n",
            "(0)\t 617\t [-0.32032263  0.75415814] \t1\tfalse\n",
            "(0)\t 618\t [ 0.79831123 -0.6412842 ] \t0\ttrue\n",
            "(1)\t 619\t [ 1.1985216 -1.2391316] \t0\tfalse\n",
            "(0)\t 620\t [ 1.1211896 -1.6887819] \t0\ttrue\n",
            "(1)\t 621\t [ 0.5252695 -1.3661381] \t0\tfalse\n",
            "(1)\t 622\t [ 0.5252695 -1.3661381] \t0\tfalse\n",
            "(0)\t 623\t [ 0.40114412 -1.2920053 ] \t0\ttrue\n",
            "(0)\t 624\t [ 1.7191476 -1.7489322] \t0\ttrue\n",
            "(0)\t 625\t [ 0.40680662 -1.2955537 ] \t0\ttrue\n",
            "(0)\t 626\t [ 0.97607976 -0.8850167 ] \t0\ttrue\n",
            "(0)\t 627\t [ 1.3012142 -1.797843 ] \t0\ttrue\n",
            "(1)\t 628\t [-1.484554   1.9959795] \t1\ttrue\n",
            "(0)\t 629\t [ 0.7238813 -1.4763186] \t0\ttrue\n",
            "(1)\t 630\t [0.394084   0.01025029] \t0\tfalse\n",
            "(0)\t 631\t [-0.20397241  0.60778123] \t1\tfalse\n",
            "(1)\t 632\t [ 1.9279773 -1.8765218] \t0\tfalse\n",
            "(0)\t 633\t [ 0.868285  -1.5714039] \t0\ttrue\n",
            "(1)\t 634\t [ 0.6517205 -1.3869798] \t0\tfalse\n",
            "(1)\t 635\t [ 0.6704822 -1.454604 ] \t0\tfalse\n",
            "(1)\t 636\t [ 0.8042453 -1.5050988] \t0\tfalse\n",
            "(1)\t 637\t [ 0.66855145 -1.4388738 ] \t0\tfalse\n",
            "(1)\t 638\t [-1.4799919  1.874604 ] \t1\ttrue\n",
            "(0)\t 639\t [ 0.8115847 -0.7681706] \t0\ttrue\n",
            "(1)\t 640\t [-1.473926   1.9914825] \t1\ttrue\n",
            "(1)\t 641\t [ 1.74386   -1.9226116] \t0\tfalse\n",
            "(0)\t 642\t [ 1.9085451 -2.0057995] \t0\ttrue\n",
            "(1)\t 643\t [ 0.6357269 -1.4365648] \t0\tfalse\n",
            "(1)\t 644\t [-0.13131581  0.575948  ] \t1\ttrue\n",
            "(1)\t 645\t [ 0.57566786 -0.38940006] \t0\tfalse\n",
            "(0)\t 646\t [0.46625918 0.01439353] \t0\ttrue\n",
            "(1)\t 647\t [-0.9166406  1.5193177] \t1\ttrue\n",
            "(0)\t 648\t [ 1.7070872 -1.6903889] \t0\ttrue\n",
            "(1)\t 649\t [-1.2072687  1.7944889] \t1\ttrue\n",
            "(0)\t 650\t [-1.4613823  2.074613 ] \t1\tfalse\n",
            "(0)\t 651\t [ 1.9828389 -1.834865 ] \t0\ttrue\n",
            "(1)\t 652\t [-0.53795785  1.0404694 ] \t1\ttrue\n",
            "(1)\t 653\t [ 0.5575397 -1.3972702] \t0\tfalse\n",
            "(0)\t 654\t [-1.0518279  1.6581712] \t1\tfalse\n",
            "(1)\t 655\t [ 1.6297019 -1.4743401] \t0\tfalse\n",
            "(1)\t 656\t [-0.04611896  0.43190968] \t1\ttrue\n",
            "(0)\t 657\t [ 0.77121496 -1.5361495 ] \t0\ttrue\n",
            "(0)\t 658\t [ 1.140606  -1.6700114] \t0\ttrue\n",
            "(0)\t 659\t [ 0.8003114 -1.5428396] \t0\ttrue\n",
            "(0)\t 660\t [ 1.978481  -2.0135272] \t0\ttrue\n",
            "(0)\t 661\t [ 1.8695599 -1.960533 ] \t0\ttrue\n",
            "(1)\t 662\t [0.03388713 0.36271188] \t1\ttrue\n",
            "(1)\t 663\t [ 0.44163495 -1.3111957 ] \t0\tfalse\n",
            "(1)\t 664\t [ 0.44163495 -1.3111957 ] \t0\tfalse\n",
            "(0)\t 665\t [ 1.0140944 -1.6511061] \t0\ttrue\n",
            "(1)\t 666\t [ 0.5799759 -1.3646567] \t0\tfalse\n",
            "(0)\t 667\t [ 1.986381  -1.9074792] \t0\ttrue\n",
            "(1)\t 668\t [-0.9103498  1.3657112] \t1\ttrue\n",
            "(0)\t 669\t [ 1.909135  -1.8528543] \t0\ttrue\n",
            "(0)\t 670\t [ 1.6771495 -1.7345216] \t0\ttrue\n",
            "(0)\t 671\t [ 1.9857666 -2.0598528] \t0\ttrue\n",
            "(0)\t 672\t [ 1.7128091 -1.6148267] \t0\ttrue\n",
            "(1)\t 673\t [ 1.4911255 -1.4298959] \t0\tfalse\n",
            "(1)\t 674\t [ 1.0305437 -1.6801265] \t0\tfalse\n",
            "(1)\t 675\t [ 1.0305437 -1.6801265] \t0\tfalse\n",
            "(1)\t 676\t [ 0.5480699 -1.383903 ] \t0\tfalse\n",
            "(1)\t 677\t [ 1.1647981 -1.6391085] \t0\tfalse\n",
            "(1)\t 678\t [-1.3052539  1.8907615] \t1\ttrue\n",
            "(0)\t 679\t [-1.133437   1.7213582] \t1\tfalse\n",
            "(1)\t 680\t [ 1.3826176 -1.5228344] \t0\tfalse\n",
            "(0)\t 681\t [ 1.8740002 -1.8653845] \t0\ttrue\n",
            "(0)\t 682\t [ 0.9325413 -1.6496699] \t0\ttrue\n",
            "(0)\t 683\t [-0.11688217  0.33611986] \t1\tfalse\n",
            "(0)\t 684\t [-0.75338334  1.2797064 ] \t1\tfalse\n",
            "(1)\t 685\t [-0.49502087  0.92110735] \t1\ttrue\n",
            "(1)\t 686\t [ 0.72912425 -1.5093775 ] \t0\tfalse\n",
            "(1)\t 687\t [ 1.8920368 -1.8606249] \t0\tfalse\n",
            "(1)\t 688\t [ 1.141652  -1.7531403] \t0\tfalse\n",
            "(0)\t 689\t [ 1.8955958 -2.032796 ] \t0\ttrue\n",
            "(1)\t 690\t [ 0.6522874 -1.3878592] \t0\tfalse\n",
            "(1)\t 691\t [ 0.4864605  -0.29530096] \t0\tfalse\n",
            "(0)\t 692\t [ 1.9425924 -1.8771363] \t0\ttrue\n",
            "(0)\t 693\t [ 1.4522742 -1.5413088] \t0\ttrue\n",
            "(1)\t 694\t [ 1.2818878 -1.5050873] \t0\tfalse\n",
            "(1)\t 695\t [ 0.54544914 -1.3567107 ] \t0\tfalse\n",
            "(0)\t 696\t [ 1.0249732 -1.682501 ] \t0\ttrue\n",
            "(0)\t 697\t [ 0.40158683 -1.2872602 ] \t0\ttrue\n",
            "(0)\t 698\t [ 1.5934892 -1.5668633] \t0\ttrue\n",
            "(0)\t 699\t [ 0.8126403  -0.56317264] \t0\ttrue\n",
            "(0)\t 700\t [ 1.8320777 -1.9405841] \t0\ttrue\n",
            "(0)\t 701\t [ 1.2375805 -1.1014689] \t0\ttrue\n",
            "(0)\t 702\t [ 0.9402446 -1.633331 ] \t0\ttrue\n",
            "(0)\t 703\t [ 1.1046735 -1.7305764] \t0\ttrue\n",
            "(0)\t 704\t [ 0.59543574 -0.26822978] \t0\ttrue\n",
            "(1)\t 705\t [ 0.7749519 -1.5407661] \t0\tfalse\n",
            "(0)\t 706\t [ 1.3755174 -1.2230961] \t0\ttrue\n",
            "(0)\t 707\t [ 1.9529818 -1.9999934] \t0\ttrue\n",
            "(1)\t 708\t [ 0.6335062 -1.4350312] \t0\tfalse\n",
            "(0)\t 709\t [-0.8876436  1.5721543] \t1\tfalse\n",
            "(0)\t 710\t [ 1.7504958 -1.8019325] \t0\ttrue\n",
            "(1)\t 711\t [ 0.6925862 -1.4777839] \t0\tfalse\n",
            "(1)\t 712\t [ 0.34771556 -0.2839887 ] \t0\tfalse\n",
            "(1)\t 713\t [ 1.3772558 -1.3082858] \t0\tfalse\n",
            "(1)\t 714\t [ 0.44235188 -1.3070177 ] \t0\tfalse\n",
            "(1)\t 715\t [ 1.3466083 -1.3145611] \t0\tfalse\n",
            "(0)\t 716\t [ 0.8554607 -1.5071976] \t0\ttrue\n",
            "(0)\t 717\t [ 1.8577986 -1.8867264] \t0\ttrue\n",
            "(0)\t 718\t [ 0.9936    -0.8056382] \t0\ttrue\n",
            "(0)\t 719\t [ 1.9846792 -1.9046358] \t0\ttrue\n",
            "(0)\t 720\t [ 1.2339956 -1.7606616] \t0\ttrue\n",
            "(1)\t 721\t [ 0.50314456 -1.346227  ] \t0\tfalse\n",
            "(1)\t 722\t [-1.3972229  1.9585526] \t1\ttrue\n",
            "(0)\t 723\t [ 1.6054188 -1.7277015] \t0\ttrue\n",
            "(0)\t 724\t [ 0.70019245 -1.4903973 ] \t0\ttrue\n",
            "(1)\t 725\t [ 1.8927109 -1.9874022] \t0\tfalse\n",
            "(0)\t 726\t [ 0.9217115 -1.6312305] \t0\ttrue\n",
            "(0)\t 727\t [ 1.010333  -1.6295899] \t0\ttrue\n",
            "(1)\t 728\t [-1.5130336  2.0641468] \t1\ttrue\n",
            "(1)\t 729\t [ 0.3691486 -1.2693185] \t0\tfalse\n",
            "(1)\t 730\t [-1.1372029  1.7326927] \t1\ttrue\n",
            "(1)\t 731\t [ 0.87703913 -1.5869291 ] \t0\tfalse\n",
            "(1)\t 732\t [ 0.87703913 -1.5869291 ] \t0\tfalse\n",
            "(0)\t 733\t [ 1.8999515 -2.0673947] \t0\ttrue\n",
            "(1)\t 734\t [ 0.66074985 -1.4506949 ] \t0\tfalse\n",
            "(0)\t 735\t [0.04511661 0.4870676 ] \t1\tfalse\n",
            "(0)\t 736\t [ 0.94678825 -1.5493135 ] \t0\ttrue\n",
            "(0)\t 737\t [ 1.8503058 -1.822289 ] \t0\ttrue\n",
            "(1)\t 738\t [ 1.209503  -1.7519604] \t0\tfalse\n",
            "(1)\t 739\t [ 1.209503  -1.7519604] \t0\tfalse\n",
            "(0)\t 740\t [ 1.5753654 -1.5627323] \t0\ttrue\n",
            "(1)\t 741\t [ 0.71637744 -1.452821  ] \t0\tfalse\n",
            "(1)\t 742\t [ 0.71637744 -1.452821  ] \t0\tfalse\n",
            "(0)\t 743\t [ 0.6356657 -1.4048085] \t0\ttrue\n",
            "(0)\t 744\t [ 0.844416  -0.6346118] \t0\ttrue\n",
            "(1)\t 745\t [ 0.73833346 -0.3853028 ] \t0\tfalse\n",
            "(0)\t 746\t [ 1.7047836 -1.6958866] \t0\ttrue\n",
            "(0)\t 747\t [ 1.8088726 -1.7543775] \t0\ttrue\n",
            "(1)\t 748\t [ 1.2117621 -1.7658265] \t0\tfalse\n",
            "(0)\t 749\t [ 1.9836901 -1.9879547] \t0\ttrue\n",
            "(0)\t 750\t [-0.05491661  0.5633804 ] \t1\tfalse\n",
            "(1)\t 751\t [-0.55848056  1.0422646 ] \t1\ttrue\n",
            "(1)\t 752\t [ 1.1796103 -1.7089201] \t0\tfalse\n",
            "(1)\t 753\t [ 0.8108606 -1.573615 ] \t0\tfalse\n",
            "(0)\t 754\t [ 1.0149599 -1.686821 ] \t0\ttrue\n",
            "(0)\t 755\t [ 1.1583757 -1.0105939] \t0\ttrue\n",
            "(0)\t 756\t [ 1.3021345 -1.7429267] \t0\ttrue\n",
            "(0)\t 757\t [ 0.3377074  -0.08012577] \t0\ttrue\n",
            "(0)\t 758\t [-0.98496896  1.4957403 ] \t1\tfalse\n",
            "(0)\t 759\t [ 0.5472019 -1.3842533] \t0\ttrue\n",
            "(0)\t 760\t [ 0.6657145 -1.4377998] \t0\ttrue\n",
            "(0)\t 761\t [ 1.9422477 -1.9987484] \t0\ttrue\n",
            "(0)\t 762\t [ 1.8648618 -1.8502005] \t0\ttrue\n",
            "(1)\t 763\t [ 1.5145332 -1.5747932] \t0\tfalse\n",
            "(1)\t 764\t [ 0.5762891 -1.3878976] \t0\tfalse\n",
            "(0)\t 765\t [-1.5988989  2.065314 ] \t1\tfalse\n",
            "(0)\t 766\t [ 1.7623384 -1.9101013] \t0\ttrue\n",
            "(0)\t 767\t [ 0.52706677 -1.3768698 ] \t0\ttrue\n",
            "(1)\t 768\t [ 1.6871444 -1.6185178] \t0\tfalse\n",
            "(0)\t 769\t [ 0.34921247 -0.36012572] \t0\ttrue\n",
            "(0)\t 770\t [ 1.8842281 -1.8860977] \t0\ttrue\n",
            "(1)\t 771\t [ 1.9048079 -1.9777551] \t0\tfalse\n",
            "(1)\t 772\t [ 1.9048079 -1.9777551] \t0\tfalse\n",
            "(1)\t 773\t [ 1.3894285 -1.5142682] \t0\tfalse\n",
            "(1)\t 774\t [ 1.3894285 -1.5142682] \t0\tfalse\n",
            "(1)\t 775\t [ 0.6562541 -1.3790128] \t0\tfalse\n",
            "(1)\t 776\t [ 0.6562541 -1.3790128] \t0\tfalse\n",
            "(1)\t 777\t [ 0.6680111 -1.4782827] \t0\tfalse\n",
            "(0)\t 778\t [ 0.9428985 -1.6453847] \t0\ttrue\n",
            "(0)\t 779\t [-0.8750355  1.4265379] \t1\tfalse\n",
            "(0)\t 780\t [0.07895181 0.3335164 ] \t1\tfalse\n",
            "(0)\t 781\t [ 1.70099   -1.7435195] \t0\ttrue\n",
            "(0)\t 782\t [ 0.44518834 -1.3095535 ] \t0\ttrue\n",
            "(0)\t 783\t [ 0.8549414 -0.7117633] \t0\ttrue\n",
            "(0)\t 784\t [0.10952266 0.3237369 ] \t1\tfalse\n",
            "(0)\t 785\t [-0.13543865  0.3426579 ] \t1\tfalse\n",
            "(0)\t 786\t [ 0.76075137 -1.5175457 ] \t0\ttrue\n",
            "(1)\t 787\t [-1.1128901  1.7237993] \t1\ttrue\n",
            "(0)\t 788\t [ 1.8312842 -1.8770229] \t0\ttrue\n",
            "(0)\t 789\t [ 0.7497225 -1.4892   ] \t0\ttrue\n",
            "(1)\t 790\t [ 0.35781944 -1.2631285 ] \t0\tfalse\n",
            "(1)\t 791\t [ 0.31054002 -0.15999955] \t0\tfalse\n",
            "(1)\t 792\t [ 1.0270838 -1.6789101] \t0\tfalse\n",
            "(0)\t 793\t [-1.5999113  2.1166868] \t1\tfalse\n",
            "(1)\t 794\t [ 0.8493735 -0.703141 ] \t0\tfalse\n",
            "(1)\t 795\t [ 0.60774565 -1.3612984 ] \t0\tfalse\n",
            "(0)\t 796\t [ 1.2321998 -1.0635233] \t0\ttrue\n",
            "(0)\t 797\t [ 1.66118   -1.7022511] \t0\ttrue\n",
            "(0)\t 798\t [-1.199129   1.8428568] \t1\tfalse\n",
            "(0)\t 799\t [ 1.0342656 -1.0896927] \t0\ttrue\n",
            "(0)\t 800\t [ 0.5535949 -1.3824986] \t0\ttrue\n",
            "(1)\t 801\t [ 1.6089845 -1.4755948] \t0\tfalse\n",
            "(1)\t 802\t [ 1.6089845 -1.4755948] \t0\tfalse\n",
            "(0)\t 803\t [ 0.6991691 -1.4639966] \t0\ttrue\n",
            "(0)\t 804\t [-1.3848697  1.8847481] \t1\tfalse\n",
            "(0)\t 805\t [-0.34243304  0.85628533] \t1\tfalse\n",
            "(0)\t 806\t [ 1.0541319 -1.6647531] \t0\ttrue\n",
            "(1)\t 807\t [ 1.4075751 -1.4550936] \t0\tfalse\n",
            "(0)\t 808\t [ 1.7926198 -1.8326331] \t0\ttrue\n",
            "(1)\t 809\t [0.01724209 0.09814543] \t1\ttrue\n",
            "(1)\t 810\t [0.01724209 0.09814543] \t1\ttrue\n",
            "(1)\t 811\t [ 0.5915126 -1.3544786] \t0\tfalse\n",
            "(0)\t 812\t [ 1.688335  -1.6472589] \t0\ttrue\n",
            "(1)\t 813\t [ 0.7544494 -1.5180303] \t0\tfalse\n",
            "(1)\t 814\t [ 0.7544494 -1.5180303] \t0\tfalse\n",
            "(1)\t 815\t [-1.5836601  2.0469708] \t1\ttrue\n",
            "(0)\t 816\t [ 0.5489664 -1.3705448] \t0\ttrue\n",
            "(0)\t 817\t [ 0.91566765 -0.6717881 ] \t0\ttrue\n",
            "(0)\t 818\t [-0.27226642  0.63972   ] \t1\tfalse\n",
            "(0)\t 819\t [ 0.72378856 -1.5001473 ] \t0\ttrue\n",
            "(0)\t 820\t [ 0.7124945 -1.4700683] \t0\ttrue\n",
            "(0)\t 821\t [ 0.92276585 -1.6169642 ] \t0\ttrue\n",
            "(1)\t 822\t [ 1.820266  -1.8370782] \t0\tfalse\n",
            "(0)\t 823\t [ 0.43624353 -1.3027025 ] \t0\ttrue\n",
            "(0)\t 824\t [ 0.5150495 -1.3612196] \t0\ttrue\n",
            "(0)\t 825\t [ 1.8180399 -1.6552429] \t0\ttrue\n",
            "(0)\t 826\t [ 0.8900023  -0.63194436] \t0\ttrue\n",
            "(0)\t 827\t [ 0.92217684 -0.89098024] \t0\ttrue\n",
            "(0)\t 828\t [ 1.8240265 -1.9028968] \t0\ttrue\n",
            "(0)\t 829\t [ 0.64920056 -1.4481083 ] \t0\ttrue\n",
            "(0)\t 830\t [ 1.0615188 -1.6464132] \t0\ttrue\n",
            "(1)\t 831\t [ 1.9037561 -1.8473166] \t0\tfalse\n",
            "(0)\t 832\t [ 0.6028601 -1.4002185] \t0\ttrue\n",
            "(0)\t 833\t [ 1.8173473 -1.7236547] \t0\ttrue\n",
            "(1)\t 834\t [0.03161949 0.37095112] \t1\ttrue\n",
            "(1)\t 835\t [ 1.8984132 -1.8252678] \t0\tfalse\n",
            "(1)\t 836\t [ 1.2118436 -1.254149 ] \t0\tfalse\n",
            "(1)\t 837\t [-0.55111754  1.0383801 ] \t1\ttrue\n",
            "(0)\t 838\t [ 0.6970983 -1.4782484] \t0\ttrue\n",
            "(0)\t 839\t [ 1.9664271 -2.0294454] \t0\ttrue\n",
            "(0)\t 840\t [ 1.4242437 -1.3121611] \t0\ttrue\n",
            "(0)\t 841\t [-0.22753628  0.82042664] \t1\tfalse\n",
            "(0)\t 842\t [ 1.6204995 -1.671076 ] \t0\ttrue\n",
            "(1)\t 843\t [ 1.0043687 -1.6139235] \t0\tfalse\n",
            "(0)\t 844\t [-0.46275732  0.99489576] \t1\tfalse\n",
            "(1)\t 845\t [-0.3152201   0.73092604] \t1\ttrue\n",
            "(1)\t 846\t [ 0.42733097 -1.2806695 ] \t0\tfalse\n",
            "(1)\t 847\t [ 0.90030444 -1.5799884 ] \t0\tfalse\n",
            "(1)\t 848\t [ 0.90030444 -1.5799884 ] \t0\tfalse\n",
            "(0)\t 849\t [-0.5362158  1.0195941] \t1\tfalse\n",
            "(1)\t 850\t [-0.9473011  1.5365266] \t1\ttrue\n",
            "(1)\t 851\t [ 1.5675896 -1.5407205] \t0\tfalse\n",
            "(1)\t 852\t [ 1.8089429 -1.8813342] \t0\tfalse\n",
            "(0)\t 853\t [ 0.561582 -1.373754] \t0\ttrue\n",
            "(1)\t 854\t [ 0.58368367 -1.3999602 ] \t0\tfalse\n",
            "(0)\t 855\t [0.36469197 0.04096661] \t0\ttrue\n",
            "(0)\t 856\t [-1.140985   1.7679383] \t1\tfalse\n",
            "(0)\t 857\t [ 1.2694101 -1.3230996] \t0\ttrue\n",
            "(1)\t 858\t [ 1.0320293 -1.6683363] \t0\tfalse\n",
            "(0)\t 859\t [-1.1189368  1.7297316] \t1\tfalse\n",
            "(1)\t 860\t [ 1.2328417 -1.0546753] \t0\tfalse\n",
            "(0)\t 861\t [ 0.39113963 -1.2742323 ] \t0\ttrue\n",
            "(1)\t 862\t [ 1.2495704 -1.7954208] \t0\tfalse\n",
            "(1)\t 863\t [ 1.2495704 -1.7954208] \t0\tfalse\n",
            "(1)\t 864\t [ 0.39876086 -1.2754198 ] \t0\tfalse\n",
            "(0)\t 865\t [-0.18586934  0.3085407 ] \t1\tfalse\n",
            "(0)\t 866\t [ 0.91200775 -1.5769396 ] \t0\ttrue\n",
            "(0)\t 867\t [ 1.2205592 -1.7321519] \t0\ttrue\n",
            "(0)\t 868\t [0.08953973 0.13610598] \t1\tfalse\n",
            "(0)\t 869\t [0.16420001 0.01453486] \t0\ttrue\n",
            "(0)\t 870\t [ 1.159329 -1.724356] \t0\ttrue\n",
            "(0)\t 871\t [ 0.71560585 -1.4525708 ] \t0\ttrue\n",
            "(0)\t 872\t [ 1.0633247 -1.3468837] \t0\ttrue\n",
            "(1)\t 873\t [ 0.39572507 -1.2861056 ] \t0\tfalse\n",
            "(1)\t 874\t [ 0.610787  -1.4178559] \t0\tfalse\n",
            "(1)\t 875\t [-0.68711954  1.1683844 ] \t1\ttrue\n",
            "(1)\t 876\t [ 1.7189994 -1.6590159] \t0\tfalse\n",
            "(0)\t 877\t [ 1.0983188 -1.7140758] \t0\ttrue\n",
            "(1)\t 878\t [ 0.6756528 -1.4685692] \t0\tfalse\n",
            "(1)\t 879\t [-0.6894669  1.348208 ] \t1\ttrue\n",
            "(1)\t 880\t [ 1.0052296 -1.6829706] \t0\tfalse\n",
            "(0)\t 881\t [ 0.41429806 -1.2816079 ] \t0\ttrue\n",
            "(0)\t 882\t [ 0.4163987  -0.35258928] \t0\ttrue\n",
            "(1)\t 883\t [ 0.7756951 -1.4967977] \t0\tfalse\n",
            "(1)\t 884\t [ 0.38427627 -1.274284  ] \t0\tfalse\n",
            "(1)\t 885\t [-1.2854091  1.846471 ] \t1\ttrue\n",
            "(0)\t 886\t [ 1.1903424 -1.7708989] \t0\ttrue\n",
            "(0)\t 887\t [-1.0040952  1.4770771] \t1\tfalse\n",
            "(1)\t 888\t [-1.1892564  1.8293396] \t1\ttrue\n",
            "(0)\t 889\t [ 1.7943186 -1.7710056] \t0\ttrue\n",
            "(0)\t 890\t [ 1.6515238 -1.6578711] \t0\ttrue\n",
            "(0)\t 891\t [ 1.4526813 -1.4876952] \t0\ttrue\n",
            "(1)\t 892\t [ 0.9160088 -1.6257643] \t0\tfalse\n",
            "(1)\t 893\t [ 0.8996614 -1.6099912] \t0\tfalse\n",
            "(1)\t 894\t [ 1.8711166 -1.7980609] \t0\tfalse\n",
            "(1)\t 895\t [ 0.636339  -1.4226465] \t0\tfalse\n",
            "(0)\t 896\t [0.01593876 0.32205006] \t1\tfalse\n",
            "(1)\t 897\t [ 0.5403271 -1.3627723] \t0\tfalse\n",
            "(1)\t 898\t [ 0.4936107  -0.31782976] \t0\tfalse\n",
            "(1)\t 899\t [-1.05446    1.6190021] \t1\ttrue\n",
            "(1)\t 900\t [ 0.5162099 -1.3608961] \t0\tfalse\n",
            "Number of true predictions: 520\n",
            "Number of false predictions: 380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIgsWWCTKdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99c0cd40-eba9-4af5-f284-22d67b43ad40"
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 57.71365149833518 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print('True positives: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "outputId": "5dd0f9f2-c762-4a50-f49e-a385e23c3f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI2K1bqaR5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "outputId": "30ba0845-bf4e-4e97-f988-81c7f47a5f99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QowNXrYZSRMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "1b554f03-4beb-4354-da63-6e2dfb0d287d"
      },
      "source": [
        "print(flat_predictions)\n",
        "print(\"************\")\n",
        "print(flat_true_labels)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1\n",
            " 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0\n",
            " 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 1 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0\n",
            " 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 1 0 0 1 0]\n",
            "************\n",
            "[1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b84921a0-6c83-4d61-935c-c3742b370775"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-task1tamil'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task1tamil\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task1tamil/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1tamil/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1tamil/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    }
  ]
}
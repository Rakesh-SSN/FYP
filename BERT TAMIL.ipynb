{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9372bfac73774797a09309c69815e745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_07376b15a7984773b9f5106e770b6e64",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0b62ac5ad00e469a914e6b3aac3ba4db",
              "IPY_MODEL_75b349b22f1043b69a365a172d5394c0"
            ]
          }
        },
        "07376b15a7984773b9f5106e770b6e64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b62ac5ad00e469a914e6b3aac3ba4db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a05e1332a83c4276925210237e053bdd",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_386b2566bc4e4316900b3fa718c0001a"
          }
        },
        "75b349b22f1043b69a365a172d5394c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c8d31c3a200f4855b19f92c020aa4753",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 996k/996k [00:00&lt;00:00, 5.70MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_afcd52848b3d44e187de8aa5feafe785"
          }
        },
        "a05e1332a83c4276925210237e053bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "386b2566bc4e4316900b3fa718c0001a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8d31c3a200f4855b19f92c020aa4753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "afcd52848b3d44e187de8aa5feafe785": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a49ca599f8c4661b0734fb87b9b92af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_42fe52db9bd047158c528272cd17f160",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ce15c686d5a84ead889c840d7eab7e42",
              "IPY_MODEL_827a25cfab5d429391f22a3f460e20cb"
            ]
          }
        },
        "42fe52db9bd047158c528272cd17f160": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce15c686d5a84ead889c840d7eab7e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_885b83d4e0204a479d21c1acda6ef9bd",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 569,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 569,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bca6761a95224a0bab3bb5a65662766d"
          }
        },
        "827a25cfab5d429391f22a3f460e20cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_28705fd36f234067a623ef00eeb14721",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 569/569 [00:00&lt;00:00, 18.9kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8acb8bece7d74215bab3ed92c7121635"
          }
        },
        "885b83d4e0204a479d21c1acda6ef9bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bca6761a95224a0bab3bb5a65662766d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28705fd36f234067a623ef00eeb14721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8acb8bece7d74215bab3ed92c7121635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3702917040a45e7a7a2faae77b9ccf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f6098a852440401795a7b356cadbfd81",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f5a1302721344c2c8fb322ec7b26d375",
              "IPY_MODEL_06bb804c46dd4ee0a2c96c362d5ad224"
            ]
          }
        },
        "f6098a852440401795a7b356cadbfd81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f5a1302721344c2c8fb322ec7b26d375": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bf1f176dc88b4cfa958c606ac1e984b2",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ab0b9cdf62d4fbe91da85f12d9a26d2"
          }
        },
        "06bb804c46dd4ee0a2c96c362d5ad224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_46f0a1290e284e9a888510ad592edf6d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 714M/714M [00:17&lt;00:00, 40.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cfc56b1513a34b1f83c01e7c4af55baf"
          }
        },
        "bf1f176dc88b4cfa958c606ac1e984b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ab0b9cdf62d4fbe91da85f12d9a26d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46f0a1290e284e9a888510ad592edf6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cfc56b1513a34b1f83c01e7c4af55baf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT%20TAMIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "3c320932-d9e9-4ceb-9138-d9752b4f5b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 61.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=f7e289c2e6958f6eb8713766c3e0f081b3911d21c94f88532d3ba90ced4dc4bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "bc5594b3-a27d-48a5-d259-f540457305b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "ae565822-c465-4390-f0db-d13e21279d7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/tamil/task1tamil.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1405</th>\n",
              "      <td>TAM1406</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ஜிஷாவின் தாயாரிடம் மீண்டும் விசாரித்து புதிய அ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2047</th>\n",
              "      <td>TAM2048</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>பந்து பிககா கிராமத்தில் மூன்று கமாண்டோக்கள் இர...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2078</th>\n",
              "      <td>TAM2079</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>பிறகு மந்திரி அருண் ஜெட்லி மற்றும் அமித் ஷா ஆக...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2035</th>\n",
              "      <td>TAM2036</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>மோடி ஒரு நோக்கத்துடன் செயல்படுகிறார்.&lt;eol&gt;மோடி...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1982</th>\n",
              "      <td>TAM1983</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>முழு விவரங்களை ஐ.ஜி. பொன்மாணிக்கவேல் தலைமையில்...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1991</th>\n",
              "      <td>TAM1992</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>இந்தியா விண்ணில் செலுத்தியுள்ள செயற்கை கோள்களி...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>965</th>\n",
              "      <td>TAM0966</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>சபரிமலை அய்யப்பன் கோவிலில் மகரவிளக்கு பூஜையின்...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2173</th>\n",
              "      <td>TAM2174</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>அமெரிக்காவின் கார்னெல் பல்கலைக்கழக விஞ்ஞானி இவ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1302</th>\n",
              "      <td>TAM1303</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>பொறியியல் படிப்புக்கு பொது நுழைவுத் தேர்வு நடத...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>TAM0287</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>கம்யூனிஸ்டு தொகுதியை வைகோ போராடி வாங்கியது ஏன்...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "1405         TAM1406  ...  ஜிஷாவின் தாயாரிடம் மீண்டும் விசாரித்து புதிய அ...\n",
              "2047         TAM2048  ...  பந்து பிககா கிராமத்தில் மூன்று கமாண்டோக்கள் இர...\n",
              "2078         TAM2079  ...  பிறகு மந்திரி அருண் ஜெட்லி மற்றும் அமித் ஷா ஆக...\n",
              "2035         TAM2036  ...  மோடி ஒரு நோக்கத்துடன் செயல்படுகிறார்.<eol>மோடி...\n",
              "1982         TAM1983  ...  முழு விவரங்களை ஐ.ஜி. பொன்மாணிக்கவேல் தலைமையில்...\n",
              "1991         TAM1992  ...  இந்தியா விண்ணில் செலுத்தியுள்ள செயற்கை கோள்களி...\n",
              "965          TAM0966  ...  சபரிமலை அய்யப்பன் கோவிலில் மகரவிளக்கு பூஜையின்...\n",
              "2173         TAM2174  ...  அமெரிக்காவின் கார்னெல் பல்கலைக்கழக விஞ்ஞானி இவ...\n",
              "1302         TAM1303  ...  பொறியியல் படிப்புக்கு பொது நுழைவுத் தேர்வு நடத...\n",
              "286          TAM0287  ...  கம்யூனிஸ்டு தொகுதியை வைகோ போராடி வாங்கியது ஏன்...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "07e70b47-0645-49e8-d47c-bc34257734e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1872</th>\n",
              "      <td>தென் சென்னை இணை கமிஷனர் அன்பு, தி.நகர் துணை கம...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1945</th>\n",
              "      <td>தமிழக உள்ளாட்சி தேர்தல் விரைவில் நடைபெற உள்ளது...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2428</th>\n",
              "      <td>அமைச்சர்களின் செயல்பாடுகளில், அதிருப்தி அடைந்த...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1023</th>\n",
              "      <td>சித்திரைத் திங்கள் முதல் நாளில் விழா கொண்டாடுவ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1349</th>\n",
              "      <td>பவித்ரா 7 பேரை திருமணம் செய்து ஏமாற்றியது தெரி...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1872  தென் சென்னை இணை கமிஷனர் அன்பு, தி.நகர் துணை கம...      0\n",
              "1945  தமிழக உள்ளாட்சி தேர்தல் விரைவில் நடைபெற உள்ளது...      0\n",
              "2428  அமைச்சர்களின் செயல்பாடுகளில், அதிருப்தி அடைந்த...      0\n",
              "1023  சித்திரைத் திங்கள் முதல் நாளில் விழா கொண்டாடுவ...      0\n",
              "1349  பவித்ரா 7 பேரை திருமணம் செய்து ஏமாற்றியது தெரி...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "6473740e-8945-41de-eb72-a14c4ea23713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "9372bfac73774797a09309c69815e745",
            "07376b15a7984773b9f5106e770b6e64",
            "0b62ac5ad00e469a914e6b3aac3ba4db",
            "75b349b22f1043b69a365a172d5394c0",
            "a05e1332a83c4276925210237e053bdd",
            "386b2566bc4e4316900b3fa718c0001a",
            "c8d31c3a200f4855b19f92c020aa4753",
            "afcd52848b3d44e187de8aa5feafe785"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9372bfac73774797a09309c69815e745",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=995526, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "6a21286c-3e52-4fb1-a466-b3e549559970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Tokenized:  ['ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'பி', '##ர', '##ச', '##ார', '##ம', 'ச', '##ெ', '##ய', '##தா', '##ர', '.', '<', 'eo', '##l', '>', 'தி', '.', 'மு', '.', 'க', '.', ',', 'வ', '##ே', '##ட', '##பா', '##ள', '##ர', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'சி', '##ன', '##ன', 'ச', '##ே', '##ல', '##ம', 'பகுதி', '##ய', '##ில', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'ஓ', '##ட', '##டு', 'ச', '##ே', '##க', '##ரி', '##த', '##தா', '##ர', '.']\n",
            "Token IDs:  [1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "ffec5220-5662-4c09-8bc4-6118d774187a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Token IDs: [101, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "b6393cb2-1b2c-4d53-a815-122fcca40459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "bda6108f-88cd-4fb2-f96f-4b2323a116da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "5f3a947e-368c-4032-aae8-f1db25109514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "print(train_inputs)\n",
        "print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "#print(train_masks)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101   1154  35186 ...  55186  95512  31484]\n",
            " [   101   1170 111305 ...   1154 111312  15220]\n",
            " [   101   1163  14124 ...  28065    119    133]\n",
            " ...\n",
            " [   101   1163  27883 ...  27885   1165  18827]\n",
            " [   101  49189  66171 ...  86728  81773  20242]\n",
            " [   101   1142  46168 ...  37076  24171  19894]]\n",
            "[1 0 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "2d600f1e-d466-4de1-e06d-9e0b12b041d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7a49ca599f8c4661b0734fb87b9b92af",
            "42fe52db9bd047158c528272cd17f160",
            "ce15c686d5a84ead889c840d7eab7e42",
            "827a25cfab5d429391f22a3f460e20cb",
            "885b83d4e0204a479d21c1acda6ef9bd",
            "bca6761a95224a0bab3bb5a65662766d",
            "28705fd36f234067a623ef00eeb14721",
            "8acb8bece7d74215bab3ed92c7121635",
            "e3702917040a45e7a7a2faae77b9ccf5",
            "f6098a852440401795a7b356cadbfd81",
            "f5a1302721344c2c8fb322ec7b26d375",
            "06bb804c46dd4ee0a2c96c362d5ad224",
            "bf1f176dc88b4cfa958c606ac1e984b2",
            "5ab0b9cdf62d4fbe91da85f12d9a26d2",
            "46f0a1290e284e9a888510ad592edf6d",
            "cfc56b1513a34b1f83c01e7c4af55baf"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a49ca599f8c4661b0734fb87b9b92af",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=569, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3702917040a45e7a7a2faae77b9ccf5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=714314041, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "outputId": "bb1ea169-5c53-4890-c90c-8916e63c3f94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "outputId": "662727db-4d20-4e28-b1d0-6faeccdcb812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epcoh took: 0:00:27\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.88\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epcoh took: 0:00:27\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.88\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifs2QgT9HeUe",
        "colab_type": "code",
        "outputId": "a52d6449-2f7f-439c-bc87-4dc007e9470d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyIwwwBMRERERmREGeCIiIiIiM/L/m70otqgxSJ4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "outputId": "3d4c3f08-5189-40be-8d7a-4f2b4aaefbc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task1tamil-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "outputId": "3f01bfd9-3cda-4cc6-f91c-b10a8b7887a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 900 test sentences...\n",
            "    DONE.\n",
            "*************************\n",
            "29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/results.txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "outputId": "eb4ad53a-fc3f-4088-861d-403dd5f7b5ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] ):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j][0],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] ):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j][1],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    # elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "    #   #print(\" Semi Paraphrase     \",end=\"\")\n",
        "    #   #count_NP+=1      \n",
        "    #   print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "    #   x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(1)\t 1\t 0.3873154 \t1\ttrue\n",
            "(1)\t 2\t -0.043792788 \t0\tfalse\n",
            "(1)\t 3\t 1.5356576 \t0\tfalse\n",
            "(1)\t 4\t 2.249188 \t0\tfalse\n",
            "(0)\t 5\t 2.1640475 \t0\ttrue\n",
            "(1)\t 6\t 1.7655317 \t0\tfalse\n",
            "(0)\t 7\t 1.2790917 \t0\ttrue\n",
            "(0)\t 8\t 1.4735873 \t0\ttrue\n",
            "(0)\t 9\t 2.1737843 \t0\ttrue\n",
            "(0)\t 10\t 1.8903015 \t0\ttrue\n",
            "(1)\t 11\t 1.8317072 \t0\tfalse\n",
            "(0)\t 12\t 1.911345 \t0\ttrue\n",
            "(0)\t 13\t 1.1500262 \t0\ttrue\n",
            "(0)\t 14\t 2.2642171 \t0\ttrue\n",
            "(0)\t 15\t 1.8268254 \t0\ttrue\n",
            "(0)\t 16\t 1.1872232 \t0\ttrue\n",
            "(0)\t 17\t 2.259405 \t0\ttrue\n",
            "(0)\t 18\t 0.51758736 \t1\tfalse\n",
            "(0)\t 19\t 1.4412556 \t0\ttrue\n",
            "(0)\t 20\t 2.1588962 \t0\ttrue\n",
            "(0)\t 21\t 1.4168571 \t0\ttrue\n",
            "(1)\t 22\t 0.18709552 \t1\ttrue\n",
            "(0)\t 23\t 2.1464875 \t0\ttrue\n",
            "(0)\t 24\t 1.1448722 \t0\ttrue\n",
            "(0)\t 25\t 1.7274392 \t0\ttrue\n",
            "(0)\t 26\t 1.9023846 \t0\ttrue\n",
            "(0)\t 27\t 1.6064508 \t0\ttrue\n",
            "(0)\t 28\t 0.83906096 \t1\tfalse\n",
            "(0)\t 29\t 2.1382759 \t0\ttrue\n",
            "(0)\t 30\t 1.0423211 \t0\ttrue\n",
            "(0)\t 31\t 1.2260797 \t1\tfalse\n",
            "(0)\t 32\t 1.752402 \t0\ttrue\n",
            "(0)\t 33\t 1.9551508 \t0\ttrue\n",
            "(1)\t 34\t 2.0377414 \t0\tfalse\n",
            "(1)\t 35\t 2.0377414 \t0\tfalse\n",
            "(1)\t 36\t 2.1451416 \t0\tfalse\n",
            "(0)\t 37\t 0.1554908 \t0\ttrue\n",
            "(1)\t 38\t 2.010712 \t0\tfalse\n",
            "(0)\t 39\t 1.9956146 \t0\ttrue\n",
            "(0)\t 40\t 1.4989275 \t0\ttrue\n",
            "(1)\t 41\t 0.6980118 \t0\tfalse\n",
            "(0)\t 42\t 0.5449352 \t0\ttrue\n",
            "(1)\t 43\t 1.6268283 \t1\ttrue\n",
            "(1)\t 44\t 1.8260387 \t0\tfalse\n",
            "(1)\t 45\t 1.8260387 \t0\tfalse\n",
            "(0)\t 46\t 1.963589 \t0\ttrue\n",
            "(0)\t 47\t 2.0832953 \t0\ttrue\n",
            "(1)\t 48\t 2.1473541 \t0\tfalse\n",
            "(0)\t 49\t 0.74179584 \t1\tfalse\n",
            "(0)\t 50\t 1.8854327 \t0\ttrue\n",
            "(0)\t 51\t 0.38666144 \t0\ttrue\n",
            "(0)\t 52\t 1.0383903 \t0\ttrue\n",
            "(0)\t 53\t 2.2121255 \t0\ttrue\n",
            "(0)\t 54\t 1.5956588 \t0\ttrue\n",
            "(0)\t 55\t 2.1864429 \t0\ttrue\n",
            "(1)\t 56\t 1.2525202 \t0\tfalse\n",
            "(1)\t 57\t 1.9063989 \t1\ttrue\n",
            "(0)\t 58\t 1.835066 \t0\ttrue\n",
            "(0)\t 59\t 1.9684925 \t0\ttrue\n",
            "(1)\t 60\t 0.21005246 \t1\ttrue\n",
            "(0)\t 61\t 1.4455413 \t0\ttrue\n",
            "(1)\t 62\t 1.817899 \t0\tfalse\n",
            "(1)\t 63\t 1.7295145 \t1\ttrue\n",
            "(1)\t 64\t 0.92216784 \t0\tfalse\n",
            "(1)\t 65\t 0.92216784 \t0\tfalse\n",
            "(0)\t 66\t 1.5199561 \t0\ttrue\n",
            "(0)\t 67\t 1.6078422 \t0\ttrue\n",
            "(0)\t 68\t 1.9204831 \t0\ttrue\n",
            "(0)\t 69\t 2.1277988 \t0\ttrue\n",
            "(0)\t 70\t 1.8401897 \t0\ttrue\n",
            "(0)\t 71\t 1.8284242 \t0\ttrue\n",
            "(1)\t 72\t 1.4482816 \t1\ttrue\n",
            "(1)\t 73\t 1.4482816 \t1\ttrue\n",
            "(1)\t 74\t 1.4137958 \t1\ttrue\n",
            "(1)\t 75\t 1.4137958 \t1\ttrue\n",
            "(1)\t 76\t 1.9772065 \t0\tfalse\n",
            "(1)\t 77\t 1.9772065 \t0\tfalse\n",
            "(1)\t 78\t 1.3147417 \t1\ttrue\n",
            "(0)\t 79\t 1.6862031 \t0\ttrue\n",
            "(0)\t 80\t 1.8837391 \t0\ttrue\n",
            "(0)\t 81\t 2.2010558 \t0\ttrue\n",
            "(1)\t 82\t 1.7665579 \t1\ttrue\n",
            "(0)\t 83\t 1.8183832 \t0\ttrue\n",
            "(0)\t 84\t 2.0100138 \t0\ttrue\n",
            "(1)\t 85\t 2.0712416 \t0\tfalse\n",
            "(1)\t 86\t 2.0111423 \t0\tfalse\n",
            "(1)\t 87\t 2.106642 \t0\tfalse\n",
            "(0)\t 88\t 1.7855612 \t0\ttrue\n",
            "(0)\t 89\t 1.4079212 \t0\ttrue\n",
            "(1)\t 90\t 1.636173 \t1\ttrue\n",
            "(1)\t 91\t 1.6981062 \t1\ttrue\n",
            "(1)\t 92\t 1.7445498 \t1\ttrue\n",
            "(0)\t 93\t -0.06935156 \t0\ttrue\n",
            "(0)\t 94\t 0.80765575 \t0\ttrue\n",
            "(0)\t 95\t 2.1065805 \t0\ttrue\n",
            "(0)\t 96\t 1.5255092 \t0\ttrue\n",
            "(0)\t 97\t 1.5254432 \t0\ttrue\n",
            "(0)\t 98\t 2.1928935 \t0\ttrue\n",
            "(0)\t 99\t 2.0309095 \t0\ttrue\n",
            "(0)\t 100\t 1.8401855 \t0\ttrue\n",
            "(1)\t 101\t 1.7150832 \t1\ttrue\n",
            "(1)\t 102\t 2.1636941 \t0\tfalse\n",
            "(1)\t 103\t 1.9012502 \t0\tfalse\n",
            "(1)\t 104\t 1.4021137 \t0\tfalse\n",
            "(0)\t 105\t 2.0938487 \t0\ttrue\n",
            "(1)\t 106\t 1.8683211 \t1\ttrue\n",
            "(0)\t 107\t 2.0120108 \t0\ttrue\n",
            "(0)\t 108\t 1.8952407 \t0\ttrue\n",
            "(0)\t 109\t 2.2428086 \t0\ttrue\n",
            "(0)\t 110\t 1.8680309 \t0\ttrue\n",
            "(0)\t 111\t 1.9943075 \t0\ttrue\n",
            "(1)\t 112\t 0.08592177 \t0\tfalse\n",
            "(1)\t 113\t 1.4919895 \t0\tfalse\n",
            "(1)\t 114\t 2.1327565 \t0\tfalse\n",
            "(1)\t 115\t 1.7266304 \t0\tfalse\n",
            "(0)\t 116\t 2.187955 \t0\ttrue\n",
            "(0)\t 117\t 0.6496258 \t0\ttrue\n",
            "(0)\t 118\t 1.6535399 \t0\ttrue\n",
            "(0)\t 119\t 2.247067 \t0\ttrue\n",
            "(0)\t 120\t 2.2363183 \t0\ttrue\n",
            "(0)\t 121\t 1.6044725 \t0\ttrue\n",
            "(0)\t 122\t 1.7319168 \t0\ttrue\n",
            "(1)\t 123\t 1.3380911 \t1\ttrue\n",
            "(0)\t 124\t 2.168531 \t0\ttrue\n",
            "(0)\t 125\t 1.9255267 \t0\ttrue\n",
            "(0)\t 126\t 2.0827491 \t0\ttrue\n",
            "(1)\t 127\t 2.1030056 \t0\tfalse\n",
            "(1)\t 128\t 2.1030056 \t0\tfalse\n",
            "(0)\t 129\t 1.7419101 \t0\ttrue\n",
            "(0)\t 130\t 2.0997782 \t0\ttrue\n",
            "(0)\t 131\t 2.0114958 \t0\ttrue\n",
            "(0)\t 132\t 2.201215 \t0\ttrue\n",
            "(1)\t 133\t 2.0789678 \t0\tfalse\n",
            "(0)\t 134\t 1.5706258 \t0\ttrue\n",
            "(0)\t 135\t 2.1995628 \t0\ttrue\n",
            "(0)\t 136\t 1.193915 \t0\ttrue\n",
            "(0)\t 137\t 0.07323109 \t1\tfalse\n",
            "(0)\t 138\t 2.2559662 \t0\ttrue\n",
            "(0)\t 139\t 2.2381506 \t0\ttrue\n",
            "(0)\t 140\t 1.7473005 \t0\ttrue\n",
            "(0)\t 141\t 2.147394 \t0\ttrue\n",
            "(0)\t 142\t 1.1115687 \t0\ttrue\n",
            "(1)\t 143\t 2.1177392 \t0\tfalse\n",
            "(1)\t 144\t 0.18132868 \t0\tfalse\n",
            "(1)\t 145\t 1.624349 \t0\tfalse\n",
            "(0)\t 146\t 2.2481735 \t0\ttrue\n",
            "(0)\t 147\t 1.0576459 \t1\tfalse\n",
            "(1)\t 148\t 2.1772373 \t0\tfalse\n",
            "(1)\t 149\t 2.1772373 \t0\tfalse\n",
            "(1)\t 150\t 2.1462958 \t0\tfalse\n",
            "(1)\t 151\t 2.1462958 \t0\tfalse\n",
            "(1)\t 152\t 1.9282438 \t0\tfalse\n",
            "(1)\t 153\t 1.9282438 \t0\tfalse\n",
            "(0)\t 154\t 2.196818 \t0\ttrue\n",
            "(1)\t 155\t 1.3409851 \t0\tfalse\n",
            "(1)\t 156\t 1.3409851 \t0\tfalse\n",
            "(1)\t 157\t 2.2166288 \t0\tfalse\n",
            "(0)\t 158\t 2.079699 \t0\ttrue\n",
            "(0)\t 159\t 2.1414518 \t0\ttrue\n",
            "(1)\t 160\t 1.6453202 \t0\tfalse\n",
            "(0)\t 161\t 0.1324341 \t0\ttrue\n",
            "(1)\t 162\t 1.6227541 \t0\tfalse\n",
            "(0)\t 163\t 2.184943 \t0\ttrue\n",
            "(0)\t 164\t 1.6143572 \t0\ttrue\n",
            "(0)\t 165\t 2.045673 \t0\ttrue\n",
            "(0)\t 166\t 0.6380905 \t1\tfalse\n",
            "(0)\t 167\t 2.077226 \t0\ttrue\n",
            "(1)\t 168\t 1.3003883 \t0\tfalse\n",
            "(1)\t 169\t 1.9608614 \t0\tfalse\n",
            "(1)\t 170\t 1.9608614 \t0\tfalse\n",
            "(0)\t 171\t 0.19489965 \t0\ttrue\n",
            "(1)\t 172\t 1.2671205 \t0\tfalse\n",
            "(1)\t 173\t 1.190686 \t1\ttrue\n",
            "(1)\t 174\t 2.0806189 \t0\tfalse\n",
            "(0)\t 175\t 1.4966297 \t1\tfalse\n",
            "(1)\t 176\t 1.8584608 \t0\tfalse\n",
            "(0)\t 177\t 1.789118 \t0\ttrue\n",
            "(0)\t 178\t 2.2488086 \t0\ttrue\n",
            "(0)\t 179\t 2.1305857 \t0\ttrue\n",
            "(0)\t 180\t 2.1677544 \t0\ttrue\n",
            "(0)\t 181\t 1.8143972 \t0\ttrue\n",
            "(0)\t 182\t 2.0699077 \t0\ttrue\n",
            "(0)\t 183\t 1.9547461 \t0\ttrue\n",
            "(0)\t 184\t 1.0935136 \t0\ttrue\n",
            "(0)\t 185\t 2.1517463 \t0\ttrue\n",
            "(0)\t 186\t 2.140572 \t0\ttrue\n",
            "(1)\t 187\t 0.7973384 \t0\tfalse\n",
            "(1)\t 188\t 1.3156456 \t1\ttrue\n",
            "(0)\t 189\t 0.9415616 \t0\ttrue\n",
            "(1)\t 190\t 0.81652457 \t0\tfalse\n",
            "(0)\t 191\t 2.110176 \t0\ttrue\n",
            "(0)\t 192\t 2.1416552 \t0\ttrue\n",
            "(1)\t 193\t 0.21280864 \t1\ttrue\n",
            "(1)\t 194\t 1.798085 \t0\tfalse\n",
            "(0)\t 195\t 1.4574959 \t0\ttrue\n",
            "(1)\t 196\t 0.14916135 \t0\tfalse\n",
            "(1)\t 197\t 1.7955137 \t0\tfalse\n",
            "(1)\t 198\t 0.19656147 \t0\tfalse\n",
            "(1)\t 199\t 0.96793205 \t0\tfalse\n",
            "(1)\t 200\t 0.96793205 \t0\tfalse\n",
            "(0)\t 201\t 1.9651198 \t0\ttrue\n",
            "(0)\t 202\t 0.6868225 \t0\ttrue\n",
            "(1)\t 203\t 2.1761005 \t0\tfalse\n",
            "(1)\t 204\t 2.1761005 \t0\tfalse\n",
            "(1)\t 205\t 1.722266 \t0\tfalse\n",
            "(1)\t 206\t 2.023948 \t0\tfalse\n",
            "(0)\t 207\t -0.0067688012 \t0\ttrue\n",
            "(0)\t 208\t 1.5656635 \t0\ttrue\n",
            "(1)\t 209\t 1.2638159 \t0\tfalse\n",
            "(0)\t 210\t 2.1429398 \t0\ttrue\n",
            "(1)\t 211\t 1.3937865 \t0\tfalse\n",
            "(0)\t 212\t 1.4096304 \t0\ttrue\n",
            "(1)\t 213\t 2.0614717 \t0\tfalse\n",
            "(0)\t 214\t 2.1750252 \t0\ttrue\n",
            "(1)\t 215\t 2.201171 \t0\tfalse\n",
            "(1)\t 216\t 2.0623 \t0\tfalse\n",
            "(0)\t 217\t 1.2786148 \t1\tfalse\n",
            "(1)\t 218\t 1.1116356 \t0\tfalse\n",
            "(0)\t 219\t 0.9678069 \t1\tfalse\n",
            "(1)\t 220\t 0.6136758 \t1\ttrue\n",
            "(1)\t 221\t 1.7477366 \t0\tfalse\n",
            "(1)\t 222\t 1.7477366 \t0\tfalse\n",
            "(1)\t 223\t 1.1989428 \t1\ttrue\n",
            "(0)\t 224\t 0.31387055 \t0\ttrue\n",
            "(0)\t 225\t 0.2304633 \t0\ttrue\n",
            "(0)\t 226\t 1.3313785 \t1\tfalse\n",
            "(1)\t 227\t 2.0019453 \t0\tfalse\n",
            "(1)\t 228\t 2.0019453 \t0\tfalse\n",
            "(1)\t 229\t 1.9572934 \t0\tfalse\n",
            "(0)\t 230\t 2.184319 \t0\ttrue\n",
            "(1)\t 231\t 1.5000304 \t0\tfalse\n",
            "(1)\t 232\t 1.5000304 \t0\tfalse\n",
            "(0)\t 233\t 1.9501119 \t0\ttrue\n",
            "(1)\t 234\t 1.3485539 \t0\tfalse\n",
            "(0)\t 235\t 1.6697193 \t0\ttrue\n",
            "(1)\t 236\t 2.2351105 \t0\tfalse\n",
            "(1)\t 237\t 1.7340158 \t1\ttrue\n",
            "(0)\t 238\t 2.1487772 \t0\ttrue\n",
            "(1)\t 239\t 1.8227681 \t0\tfalse\n",
            "(0)\t 240\t 1.0146832 \t0\ttrue\n",
            "(0)\t 241\t 0.69870776 \t0\ttrue\n",
            "(1)\t 242\t 1.8352916 \t0\tfalse\n",
            "(1)\t 243\t 1.1193899 \t0\tfalse\n",
            "(0)\t 244\t 1.8806156 \t0\ttrue\n",
            "(1)\t 245\t 1.9151188 \t0\tfalse\n",
            "(1)\t 246\t 2.2113488 \t0\tfalse\n",
            "(1)\t 247\t 2.2113488 \t0\tfalse\n",
            "(0)\t 248\t 2.0020769 \t0\ttrue\n",
            "(1)\t 249\t 0.89140755 \t0\tfalse\n",
            "(1)\t 250\t 0.89140755 \t0\tfalse\n",
            "(0)\t 251\t 2.1023014 \t0\ttrue\n",
            "(1)\t 252\t 2.028936 \t0\tfalse\n",
            "(1)\t 253\t 1.8372742 \t0\tfalse\n",
            "(1)\t 254\t 1.4940895 \t0\tfalse\n",
            "(0)\t 255\t 1.5673685 \t0\ttrue\n",
            "(0)\t 256\t -0.11276502 \t1\tfalse\n",
            "(1)\t 257\t 0.13923731 \t1\ttrue\n",
            "(0)\t 258\t 0.7201941 \t1\tfalse\n",
            "(0)\t 259\t 2.0645742 \t0\ttrue\n",
            "(0)\t 260\t 2.2381647 \t0\ttrue\n",
            "(0)\t 261\t 2.2823367 \t0\ttrue\n",
            "(0)\t 262\t 2.0041356 \t0\ttrue\n",
            "(0)\t 263\t 2.097002 \t0\ttrue\n",
            "(0)\t 264\t 2.013037 \t0\ttrue\n",
            "(0)\t 265\t 2.1601572 \t0\ttrue\n",
            "(1)\t 266\t 2.1903613 \t0\tfalse\n",
            "(0)\t 267\t 2.1789834 \t0\ttrue\n",
            "(0)\t 268\t 1.0398362 \t1\tfalse\n",
            "(0)\t 269\t 1.1571211 \t0\ttrue\n",
            "(0)\t 270\t -0.014990986 \t0\ttrue\n",
            "(1)\t 271\t 1.6189617 \t0\tfalse\n",
            "(1)\t 272\t 0.1640226 \t0\tfalse\n",
            "(1)\t 273\t 1.4811862 \t0\tfalse\n",
            "(1)\t 274\t 0.83817345 \t1\ttrue\n",
            "(1)\t 275\t 1.7014908 \t0\tfalse\n",
            "(1)\t 276\t 1.4432846 \t1\ttrue\n",
            "(1)\t 277\t 2.0224426 \t0\tfalse\n",
            "(0)\t 278\t 2.0381494 \t0\ttrue\n",
            "(0)\t 279\t 0.62911576 \t0\ttrue\n",
            "(0)\t 280\t 1.9315857 \t0\ttrue\n",
            "(1)\t 281\t 1.4913797 \t1\ttrue\n",
            "(0)\t 282\t 0.59593314 \t0\ttrue\n",
            "(1)\t 283\t 0.6824804 \t1\ttrue\n",
            "(1)\t 284\t 0.1623714 \t0\tfalse\n",
            "(1)\t 285\t 1.462215 \t0\tfalse\n",
            "(1)\t 286\t 1.462215 \t0\tfalse\n",
            "(1)\t 287\t 1.968269 \t0\tfalse\n",
            "(0)\t 288\t 1.8962076 \t0\ttrue\n",
            "(0)\t 289\t 2.0664716 \t0\ttrue\n",
            "(1)\t 290\t 1.0148696 \t0\tfalse\n",
            "(1)\t 291\t 0.3265114 \t0\tfalse\n",
            "(1)\t 292\t -0.021069815 \t0\tfalse\n",
            "(1)\t 293\t 1.1746023 \t0\tfalse\n",
            "(0)\t 294\t 2.084489 \t0\ttrue\n",
            "(0)\t 295\t 1.8471359 \t0\ttrue\n",
            "(0)\t 296\t 1.8548278 \t0\ttrue\n",
            "(0)\t 297\t 0.11433992 \t1\tfalse\n",
            "(0)\t 298\t 1.7339399 \t0\ttrue\n",
            "(1)\t 299\t 0.03142584 \t0\tfalse\n",
            "(0)\t 300\t 1.9012648 \t0\ttrue\n",
            "(1)\t 301\t 1.5846046 \t1\ttrue\n",
            "(1)\t 302\t 2.2023675 \t0\tfalse\n",
            "(1)\t 303\t 2.2023675 \t0\tfalse\n",
            "(1)\t 304\t 1.5349144 \t1\ttrue\n",
            "(1)\t 305\t 0.44440395 \t1\ttrue\n",
            "(1)\t 306\t 0.015492694 \t1\ttrue\n",
            "(0)\t 307\t 2.1785493 \t0\ttrue\n",
            "(1)\t 308\t 2.0767114 \t0\tfalse\n",
            "(0)\t 309\t 2.1669104 \t0\ttrue\n",
            "(0)\t 310\t 1.7901206 \t0\ttrue\n",
            "(0)\t 311\t 0.22051504 \t0\ttrue\n",
            "(0)\t 312\t 0.73291403 \t0\ttrue\n",
            "(0)\t 313\t 2.0813613 \t0\ttrue\n",
            "(0)\t 314\t 1.7809579 \t0\ttrue\n",
            "(0)\t 315\t 2.0613425 \t0\ttrue\n",
            "(0)\t 316\t 2.1184418 \t0\ttrue\n",
            "(0)\t 317\t 2.142546 \t0\ttrue\n",
            "(1)\t 318\t 1.8087614 \t1\ttrue\n",
            "(0)\t 319\t 1.9667078 \t0\ttrue\n",
            "(1)\t 320\t 2.1165175 \t0\tfalse\n",
            "(1)\t 321\t 2.127797 \t0\tfalse\n",
            "(0)\t 322\t 0.58244085 \t0\ttrue\n",
            "(0)\t 323\t 2.182231 \t0\ttrue\n",
            "(0)\t 324\t 1.708182 \t0\ttrue\n",
            "(0)\t 325\t 1.8195479 \t0\ttrue\n",
            "(1)\t 326\t 1.742489 \t0\tfalse\n",
            "(0)\t 327\t 1.1734688 \t0\ttrue\n",
            "(0)\t 328\t 1.994345 \t0\ttrue\n",
            "(0)\t 329\t 1.3511224 \t1\tfalse\n",
            "(1)\t 330\t 1.4621731 \t1\ttrue\n",
            "(0)\t 331\t -0.018898021 \t1\tfalse\n",
            "(1)\t 332\t 1.1823392 \t1\ttrue\n",
            "(1)\t 333\t 0.61868304 \t1\ttrue\n",
            "(1)\t 334\t 1.8907971 \t0\tfalse\n",
            "(0)\t 335\t 1.28648 \t1\tfalse\n",
            "(0)\t 336\t 2.1098607 \t0\ttrue\n",
            "(0)\t 337\t 2.1301303 \t0\ttrue\n",
            "(1)\t 338\t 1.9046155 \t0\tfalse\n",
            "(1)\t 339\t 1.5632416 \t1\ttrue\n",
            "(1)\t 340\t 1.8091203 \t1\ttrue\n",
            "(0)\t 341\t 1.3260404 \t1\tfalse\n",
            "(1)\t 342\t 0.5394807 \t1\ttrue\n",
            "(1)\t 343\t 1.6350893 \t0\tfalse\n",
            "(0)\t 344\t 1.5413872 \t1\tfalse\n",
            "(1)\t 345\t 1.5610291 \t0\tfalse\n",
            "(1)\t 346\t 2.0090342 \t0\tfalse\n",
            "(0)\t 347\t 1.8063146 \t0\ttrue\n",
            "(0)\t 348\t 0.6250166 \t0\ttrue\n",
            "(0)\t 349\t 0.052543186 \t0\ttrue\n",
            "(0)\t 350\t 1.5171754 \t0\ttrue\n",
            "(0)\t 351\t 0.47374025 \t1\tfalse\n",
            "(0)\t 352\t 2.0415387 \t0\ttrue\n",
            "(1)\t 353\t 1.0298531 \t1\ttrue\n",
            "(0)\t 354\t 1.7227304 \t0\ttrue\n",
            "(0)\t 355\t 1.653322 \t0\ttrue\n",
            "(0)\t 356\t 2.1897175 \t0\ttrue\n",
            "(0)\t 357\t 1.8797557 \t0\ttrue\n",
            "(0)\t 358\t 2.1726277 \t0\ttrue\n",
            "(0)\t 359\t 2.1941772 \t0\ttrue\n",
            "(0)\t 360\t 1.9697696 \t0\ttrue\n",
            "(0)\t 361\t 2.0051892 \t0\ttrue\n",
            "(0)\t 362\t 2.0398734 \t0\ttrue\n",
            "(0)\t 363\t 2.0215168 \t0\ttrue\n",
            "(0)\t 364\t 1.0084951 \t0\ttrue\n",
            "(0)\t 365\t 0.70961756 \t1\tfalse\n",
            "(0)\t 366\t 2.1604383 \t0\ttrue\n",
            "(1)\t 367\t 0.98431253 \t1\ttrue\n",
            "(0)\t 368\t 1.7832733 \t0\ttrue\n",
            "(0)\t 369\t 1.9790411 \t0\ttrue\n",
            "(0)\t 370\t 0.03529921 \t1\tfalse\n",
            "(0)\t 371\t 1.7616353 \t0\ttrue\n",
            "(0)\t 372\t 1.9101249 \t1\tfalse\n",
            "(0)\t 373\t 1.2829659 \t0\ttrue\n",
            "(0)\t 374\t 0.4142599 \t1\tfalse\n",
            "(0)\t 375\t 2.0862265 \t0\ttrue\n",
            "(0)\t 376\t 2.123833 \t0\ttrue\n",
            "(0)\t 377\t 2.1794279 \t0\ttrue\n",
            "(0)\t 378\t 0.5959289 \t0\ttrue\n",
            "(0)\t 379\t 0.5978702 \t1\tfalse\n",
            "(1)\t 380\t 2.128312 \t0\tfalse\n",
            "(0)\t 381\t 1.310992 \t1\tfalse\n",
            "(0)\t 382\t 1.734759 \t0\ttrue\n",
            "(0)\t 383\t 2.0133877 \t0\ttrue\n",
            "(1)\t 384\t 1.9174502 \t0\tfalse\n",
            "(0)\t 385\t 2.058503 \t0\ttrue\n",
            "(0)\t 386\t 1.4221491 \t0\ttrue\n",
            "(0)\t 387\t 1.5700744 \t0\ttrue\n",
            "(1)\t 388\t 1.4312141 \t1\ttrue\n",
            "(0)\t 389\t 2.1407773 \t0\ttrue\n",
            "(0)\t 390\t 2.1707187 \t0\ttrue\n",
            "(0)\t 391\t 1.9790617 \t0\ttrue\n",
            "(0)\t 392\t 2.0603952 \t0\ttrue\n",
            "(0)\t 393\t 1.8841336 \t1\tfalse\n",
            "(0)\t 394\t 1.2483876 \t0\ttrue\n",
            "(1)\t 395\t 1.92382 \t0\tfalse\n",
            "(0)\t 396\t 0.3725925 \t1\tfalse\n",
            "(0)\t 397\t 1.1439177 \t0\ttrue\n",
            "(0)\t 398\t 1.3095361 \t1\tfalse\n",
            "(0)\t 399\t 2.1417477 \t0\ttrue\n",
            "(0)\t 400\t 2.2005222 \t0\ttrue\n",
            "(0)\t 401\t 2.1301837 \t0\ttrue\n",
            "(0)\t 402\t 1.3259611 \t0\ttrue\n",
            "(0)\t 403\t 0.468976 \t0\ttrue\n",
            "(0)\t 404\t 0.23708586 \t1\tfalse\n",
            "(0)\t 405\t 1.0260532 \t0\ttrue\n",
            "(0)\t 406\t 2.016458 \t0\ttrue\n",
            "(0)\t 407\t 0.17510392 \t1\tfalse\n",
            "(1)\t 408\t 0.86463135 \t0\tfalse\n",
            "(0)\t 409\t 2.0954607 \t0\ttrue\n",
            "(0)\t 410\t 1.104506 \t0\ttrue\n",
            "(0)\t 411\t 1.323236 \t0\ttrue\n",
            "(0)\t 412\t 1.9525379 \t0\ttrue\n",
            "(0)\t 413\t 1.1612989 \t1\tfalse\n",
            "(0)\t 414\t 1.7377558 \t1\tfalse\n",
            "(0)\t 415\t 1.8924197 \t0\ttrue\n",
            "(1)\t 416\t 1.3633845 \t0\tfalse\n",
            "(0)\t 417\t 2.232832 \t0\ttrue\n",
            "(1)\t 418\t 0.8948314 \t0\tfalse\n",
            "(0)\t 419\t 1.9473594 \t0\ttrue\n",
            "(1)\t 420\t 1.7587388 \t1\ttrue\n",
            "(0)\t 421\t 1.5257688 \t0\ttrue\n",
            "(0)\t 422\t 1.1014831 \t0\ttrue\n",
            "(1)\t 423\t 2.1459734 \t0\tfalse\n",
            "(0)\t 424\t 2.1179278 \t0\ttrue\n",
            "(0)\t 425\t 1.1736579 \t0\ttrue\n",
            "(1)\t 426\t 1.5998137 \t1\ttrue\n",
            "(0)\t 427\t 2.0688195 \t0\ttrue\n",
            "(0)\t 428\t 1.8521717 \t0\ttrue\n",
            "(0)\t 429\t 1.7581878 \t0\ttrue\n",
            "(0)\t 430\t 2.234761 \t0\ttrue\n",
            "(1)\t 431\t 0.6419225 \t1\ttrue\n",
            "(1)\t 432\t 2.0461698 \t0\tfalse\n",
            "(1)\t 433\t 2.0461698 \t0\tfalse\n",
            "(0)\t 434\t 0.74586457 \t0\ttrue\n",
            "(1)\t 435\t 2.1850271 \t0\tfalse\n",
            "(1)\t 436\t 2.1850271 \t0\tfalse\n",
            "(0)\t 437\t 2.1092973 \t0\ttrue\n",
            "(0)\t 438\t 0.017138265 \t0\ttrue\n",
            "(0)\t 439\t 0.51257694 \t0\ttrue\n",
            "(0)\t 440\t 1.6431054 \t0\ttrue\n",
            "(0)\t 441\t 1.3505827 \t0\ttrue\n",
            "(1)\t 442\t 1.8765212 \t0\tfalse\n",
            "(1)\t 443\t 2.0860538 \t0\tfalse\n",
            "(0)\t 444\t 2.0275242 \t0\ttrue\n",
            "(0)\t 445\t 1.8684295 \t0\ttrue\n",
            "(1)\t 446\t 2.0153713 \t0\tfalse\n",
            "(0)\t 447\t 2.1850944 \t0\ttrue\n",
            "(0)\t 448\t 1.2474252 \t0\ttrue\n",
            "(0)\t 449\t 1.0421335 \t0\ttrue\n",
            "(1)\t 450\t 1.7559195 \t0\tfalse\n",
            "(1)\t 451\t 1.7559195 \t0\tfalse\n",
            "(1)\t 452\t 2.1712399 \t0\tfalse\n",
            "(0)\t 453\t 1.5423485 \t0\ttrue\n",
            "(0)\t 454\t 2.1374655 \t0\ttrue\n",
            "(0)\t 455\t 1.577014 \t0\ttrue\n",
            "(1)\t 456\t 0.46523058 \t1\ttrue\n",
            "(1)\t 457\t 1.4721859 \t0\tfalse\n",
            "(1)\t 458\t 1.803205 \t0\tfalse\n",
            "(0)\t 459\t 1.7243083 \t0\ttrue\n",
            "(1)\t 460\t 2.135493 \t0\tfalse\n",
            "(1)\t 461\t 2.135493 \t0\tfalse\n",
            "(0)\t 462\t 2.014103 \t0\ttrue\n",
            "(1)\t 463\t 2.0309289 \t0\tfalse\n",
            "(1)\t 464\t 0.75644803 \t0\tfalse\n",
            "(1)\t 465\t 2.0740905 \t0\tfalse\n",
            "(1)\t 466\t 1.6089514 \t1\ttrue\n",
            "(0)\t 467\t 1.9432923 \t0\ttrue\n",
            "(0)\t 468\t 1.4223802 \t0\ttrue\n",
            "(1)\t 469\t 0.24591374 \t0\tfalse\n",
            "(1)\t 470\t 0.36808765 \t1\ttrue\n",
            "(1)\t 471\t 1.5878444 \t0\tfalse\n",
            "(1)\t 472\t 1.5878444 \t0\tfalse\n",
            "(0)\t 473\t 0.23461227 \t0\ttrue\n",
            "(1)\t 474\t 0.3323691 \t0\tfalse\n",
            "(0)\t 475\t 1.579958 \t0\ttrue\n",
            "(0)\t 476\t 2.240713 \t0\ttrue\n",
            "(0)\t 477\t 2.2494237 \t0\ttrue\n",
            "(0)\t 478\t 2.1054354 \t0\ttrue\n",
            "(1)\t 479\t 1.5049187 \t0\tfalse\n",
            "(1)\t 480\t 1.7614418 \t0\tfalse\n",
            "(1)\t 481\t 0.681325 \t0\tfalse\n",
            "(1)\t 482\t 1.9974703 \t0\tfalse\n",
            "(0)\t 483\t 2.0180862 \t0\ttrue\n",
            "(1)\t 484\t 0.3258173 \t1\ttrue\n",
            "(0)\t 485\t 2.0396745 \t0\ttrue\n",
            "(1)\t 486\t 2.0292742 \t0\tfalse\n",
            "(1)\t 487\t 1.1938363 \t0\tfalse\n",
            "(0)\t 488\t 1.9399238 \t0\ttrue\n",
            "(0)\t 489\t 1.5421252 \t0\ttrue\n",
            "(0)\t 490\t 1.5890119 \t0\ttrue\n",
            "(0)\t 491\t 1.8519676 \t0\ttrue\n",
            "(1)\t 492\t 2.1730235 \t0\tfalse\n",
            "(1)\t 493\t 2.1730235 \t0\tfalse\n",
            "(0)\t 494\t 1.8699625 \t0\ttrue\n",
            "(0)\t 495\t 1.8886634 \t0\ttrue\n",
            "(1)\t 496\t 1.6724055 \t0\tfalse\n",
            "(1)\t 497\t 0.63822925 \t1\ttrue\n",
            "(0)\t 498\t 1.5901605 \t0\ttrue\n",
            "(0)\t 499\t 2.1268594 \t0\ttrue\n",
            "(0)\t 500\t 0.7126911 \t0\ttrue\n",
            "(1)\t 501\t 1.9680141 \t0\tfalse\n",
            "(1)\t 502\t 1.9680141 \t0\tfalse\n",
            "(0)\t 503\t 1.788687 \t0\ttrue\n",
            "(0)\t 504\t 2.190711 \t0\ttrue\n",
            "(1)\t 505\t 0.52527 \t1\ttrue\n",
            "(1)\t 506\t 1.9267726 \t0\tfalse\n",
            "(1)\t 507\t 0.21215981 \t0\tfalse\n",
            "(0)\t 508\t 1.2538568 \t1\tfalse\n",
            "(0)\t 509\t 1.0317117 \t1\tfalse\n",
            "(0)\t 510\t 0.19590491 \t1\tfalse\n",
            "(1)\t 511\t 1.410782 \t1\ttrue\n",
            "(1)\t 512\t 1.5620025 \t0\tfalse\n",
            "(0)\t 513\t 1.9702823 \t0\ttrue\n",
            "(0)\t 514\t 0.11503833 \t0\ttrue\n",
            "(1)\t 515\t 1.9674833 \t1\ttrue\n",
            "(0)\t 516\t 1.9023352 \t0\ttrue\n",
            "(1)\t 517\t 0.5905411 \t0\tfalse\n",
            "(0)\t 518\t 1.9396524 \t0\ttrue\n",
            "(0)\t 519\t 1.7516333 \t0\ttrue\n",
            "(0)\t 520\t 2.1331284 \t0\ttrue\n",
            "(0)\t 521\t 0.37927312 \t0\ttrue\n",
            "(1)\t 522\t 0.9232395 \t0\tfalse\n",
            "(1)\t 523\t 0.8392649 \t0\tfalse\n",
            "(1)\t 524\t 0.6484454 \t1\ttrue\n",
            "(1)\t 525\t 2.1271422 \t0\tfalse\n",
            "(1)\t 526\t 1.1007967 \t1\ttrue\n",
            "(1)\t 527\t 1.5407996 \t1\ttrue\n",
            "(0)\t 528\t 1.9051657 \t0\ttrue\n",
            "(1)\t 529\t 0.8310652 \t0\tfalse\n",
            "(1)\t 530\t 0.8310652 \t0\tfalse\n",
            "(1)\t 531\t 0.9611519 \t0\tfalse\n",
            "(1)\t 532\t 1.2494613 \t1\ttrue\n",
            "(1)\t 533\t 2.2183752 \t0\tfalse\n",
            "(1)\t 534\t 2.2183752 \t0\tfalse\n",
            "(1)\t 535\t 1.8068966 \t0\tfalse\n",
            "(1)\t 536\t 1.8068966 \t0\tfalse\n",
            "(1)\t 537\t 2.0376673 \t0\tfalse\n",
            "(1)\t 538\t 0.7193085 \t1\ttrue\n",
            "(1)\t 539\t 0.7193085 \t1\ttrue\n",
            "(1)\t 540\t 1.5001154 \t1\ttrue\n",
            "(1)\t 541\t 1.5001154 \t1\ttrue\n",
            "(0)\t 542\t 1.2827841 \t0\ttrue\n",
            "(1)\t 543\t 2.1633186 \t0\tfalse\n",
            "(1)\t 544\t 2.1633186 \t0\tfalse\n",
            "(1)\t 545\t 1.8393449 \t0\tfalse\n",
            "(1)\t 546\t 1.8785635 \t0\tfalse\n",
            "(0)\t 547\t 0.31768548 \t1\tfalse\n",
            "(1)\t 548\t 2.1149936 \t0\tfalse\n",
            "(1)\t 549\t 2.1149936 \t0\tfalse\n",
            "(0)\t 550\t 2.2440116 \t0\ttrue\n",
            "(0)\t 551\t 1.9112508 \t0\ttrue\n",
            "(0)\t 552\t 1.7090697 \t0\ttrue\n",
            "(1)\t 553\t 0.76013786 \t1\ttrue\n",
            "(0)\t 554\t 1.9955686 \t0\ttrue\n",
            "(0)\t 555\t 1.9997752 \t0\ttrue\n",
            "(1)\t 556\t 1.2899675 \t1\ttrue\n",
            "(1)\t 557\t 1.8345051 \t1\ttrue\n",
            "(1)\t 558\t 0.99502295 \t0\tfalse\n",
            "(0)\t 559\t 0.25765172 \t0\ttrue\n",
            "(0)\t 560\t 1.1453557 \t1\tfalse\n",
            "(0)\t 561\t 1.467184 \t1\tfalse\n",
            "(1)\t 562\t 2.0140803 \t0\tfalse\n",
            "(1)\t 563\t 2.0140803 \t0\tfalse\n",
            "(1)\t 564\t 2.0310223 \t0\tfalse\n",
            "(0)\t 565\t 0.89124054 \t0\ttrue\n",
            "(1)\t 566\t 1.2920128 \t0\tfalse\n",
            "(1)\t 567\t 1.2920128 \t0\tfalse\n",
            "(0)\t 568\t 0.7480379 \t0\ttrue\n",
            "(0)\t 569\t 1.4269222 \t0\ttrue\n",
            "(0)\t 570\t 1.9614339 \t0\ttrue\n",
            "(0)\t 571\t 1.9095626 \t0\ttrue\n",
            "(1)\t 572\t 0.37897485 \t0\tfalse\n",
            "(1)\t 573\t 0.29209906 \t1\ttrue\n",
            "(0)\t 574\t 1.5027976 \t0\ttrue\n",
            "(0)\t 575\t 1.1749061 \t0\ttrue\n",
            "(1)\t 576\t 2.1720576 \t0\tfalse\n",
            "(0)\t 577\t 0.4631173 \t1\tfalse\n",
            "(0)\t 578\t 2.1670318 \t0\ttrue\n",
            "(0)\t 579\t 0.07803033 \t0\ttrue\n",
            "(0)\t 580\t 2.136271 \t0\ttrue\n",
            "(1)\t 581\t 1.6802061 \t0\tfalse\n",
            "(1)\t 582\t 1.2782559 \t0\tfalse\n",
            "(1)\t 583\t 1.2782559 \t0\tfalse\n",
            "(0)\t 584\t 1.7185179 \t0\ttrue\n",
            "(1)\t 585\t 1.5683368 \t0\tfalse\n",
            "(0)\t 586\t 2.1827967 \t0\ttrue\n",
            "(1)\t 587\t 2.0706813 \t0\tfalse\n",
            "(0)\t 588\t 1.9051814 \t0\ttrue\n",
            "(1)\t 589\t 0.5132045 \t0\tfalse\n",
            "(1)\t 590\t 1.1039948 \t0\tfalse\n",
            "(1)\t 591\t 0.96138155 \t0\tfalse\n",
            "(0)\t 592\t 2.1977234 \t0\ttrue\n",
            "(1)\t 593\t 2.1198573 \t0\tfalse\n",
            "(1)\t 594\t 2.1198573 \t0\tfalse\n",
            "(0)\t 595\t 1.983492 \t0\ttrue\n",
            "(0)\t 596\t 1.4873492 \t0\ttrue\n",
            "(1)\t 597\t 1.6817796 \t1\ttrue\n",
            "(0)\t 598\t 0.77012724 \t0\ttrue\n",
            "(0)\t 599\t 0.91953886 \t1\tfalse\n",
            "(0)\t 600\t 1.4994826 \t1\tfalse\n",
            "(0)\t 601\t 0.61783725 \t0\ttrue\n",
            "(0)\t 602\t 0.36298743 \t1\tfalse\n",
            "(1)\t 603\t 0.7840843 \t0\tfalse\n",
            "(1)\t 604\t 1.4373386 \t0\tfalse\n",
            "(1)\t 605\t 1.4373386 \t0\tfalse\n",
            "(1)\t 606\t 2.1060438 \t0\tfalse\n",
            "(0)\t 607\t 1.7842338 \t0\ttrue\n",
            "(0)\t 608\t 1.8917081 \t0\ttrue\n",
            "(1)\t 609\t 0.09876245 \t0\tfalse\n",
            "(1)\t 610\t 0.09876245 \t0\tfalse\n",
            "(0)\t 611\t 1.7295544 \t0\ttrue\n",
            "(1)\t 612\t 1.2702909 \t0\tfalse\n",
            "(1)\t 613\t 2.1141438 \t0\tfalse\n",
            "(0)\t 614\t 1.6957356 \t0\ttrue\n",
            "(0)\t 615\t 0.38528752 \t0\ttrue\n",
            "(0)\t 616\t 0.9824522 \t0\ttrue\n",
            "(0)\t 617\t 0.83983016 \t0\ttrue\n",
            "(0)\t 618\t 1.0636379 \t0\ttrue\n",
            "(1)\t 619\t 1.586451 \t0\tfalse\n",
            "(0)\t 620\t 1.5920987 \t0\ttrue\n",
            "(1)\t 621\t 1.3208414 \t0\tfalse\n",
            "(1)\t 622\t 1.3208414 \t0\tfalse\n",
            "(0)\t 623\t 0.23170945 \t0\ttrue\n",
            "(0)\t 624\t 1.9467487 \t0\ttrue\n",
            "(0)\t 625\t 0.5278533 \t0\ttrue\n",
            "(0)\t 626\t 0.579308 \t0\ttrue\n",
            "(0)\t 627\t 2.1914835 \t0\ttrue\n",
            "(1)\t 628\t 1.8832216 \t1\ttrue\n",
            "(0)\t 629\t 1.8318293 \t0\ttrue\n",
            "(1)\t 630\t 0.78954285 \t1\ttrue\n",
            "(0)\t 631\t -0.008072047 \t1\tfalse\n",
            "(1)\t 632\t 1.9411417 \t0\tfalse\n",
            "(0)\t 633\t 2.0323665 \t0\ttrue\n",
            "(1)\t 634\t 1.2985668 \t0\tfalse\n",
            "(1)\t 635\t 1.6680218 \t0\tfalse\n",
            "(1)\t 636\t 1.9433658 \t0\tfalse\n",
            "(1)\t 637\t 1.8013126 \t0\tfalse\n",
            "(1)\t 638\t 1.7183577 \t1\ttrue\n",
            "(0)\t 639\t 0.91324466 \t0\ttrue\n",
            "(1)\t 640\t 1.9165797 \t1\ttrue\n",
            "(1)\t 641\t 1.7908432 \t0\tfalse\n",
            "(0)\t 642\t 2.2143972 \t0\ttrue\n",
            "(1)\t 643\t 1.4715573 \t0\tfalse\n",
            "(1)\t 644\t 0.57092255 \t1\ttrue\n",
            "(1)\t 645\t 0.58135825 \t1\ttrue\n",
            "(0)\t 646\t 0.33372158 \t1\tfalse\n",
            "(1)\t 647\t 0.5516506 \t1\ttrue\n",
            "(0)\t 648\t 2.0674121 \t0\ttrue\n",
            "(1)\t 649\t 1.7363596 \t1\ttrue\n",
            "(0)\t 650\t 1.4393729 \t1\tfalse\n",
            "(0)\t 651\t 1.9018664 \t0\ttrue\n",
            "(1)\t 652\t 1.235489 \t1\ttrue\n",
            "(1)\t 653\t 1.8179784 \t0\tfalse\n",
            "(0)\t 654\t 1.5107542 \t1\tfalse\n",
            "(1)\t 655\t 1.8407328 \t0\tfalse\n",
            "(1)\t 656\t 0.5188512 \t0\tfalse\n",
            "(0)\t 657\t 1.8532676 \t0\ttrue\n",
            "(0)\t 658\t 2.2029843 \t0\ttrue\n",
            "(0)\t 659\t 1.5822082 \t0\ttrue\n",
            "(0)\t 660\t 2.2569954 \t0\ttrue\n",
            "(0)\t 661\t 1.9910201 \t0\ttrue\n",
            "(1)\t 662\t 0.47434232 \t1\ttrue\n",
            "(1)\t 663\t 1.1128818 \t0\tfalse\n",
            "(1)\t 664\t 1.1128818 \t0\tfalse\n",
            "(0)\t 665\t 2.1539707 \t0\ttrue\n",
            "(1)\t 666\t 1.8380344 \t0\tfalse\n",
            "(0)\t 667\t 2.164436 \t0\ttrue\n",
            "(1)\t 668\t 1.2606757 \t1\ttrue\n",
            "(0)\t 669\t 1.9908752 \t0\ttrue\n",
            "(0)\t 670\t 2.0043132 \t0\ttrue\n",
            "(0)\t 671\t 2.1089923 \t0\ttrue\n",
            "(0)\t 672\t 2.0369303 \t0\ttrue\n",
            "(1)\t 673\t 1.2715187 \t0\tfalse\n",
            "(1)\t 674\t 2.1122315 \t0\tfalse\n",
            "(1)\t 675\t 2.1122315 \t0\tfalse\n",
            "(1)\t 676\t 1.4975266 \t0\tfalse\n",
            "(1)\t 677\t 1.7627306 \t0\tfalse\n",
            "(1)\t 678\t 0.9058702 \t1\ttrue\n",
            "(0)\t 679\t 1.7494882 \t1\tfalse\n",
            "(1)\t 680\t 0.77965635 \t1\ttrue\n",
            "(0)\t 681\t 2.0721602 \t0\ttrue\n",
            "(0)\t 682\t 2.171209 \t0\ttrue\n",
            "(0)\t 683\t 0.31157026 \t0\ttrue\n",
            "(0)\t 684\t 1.656243 \t1\tfalse\n",
            "(1)\t 685\t 1.5363886 \t1\ttrue\n",
            "(1)\t 686\t 1.723141 \t0\tfalse\n",
            "(1)\t 687\t 1.9354471 \t0\tfalse\n",
            "(1)\t 688\t 1.9199129 \t0\tfalse\n",
            "(0)\t 689\t 2.1501596 \t0\ttrue\n",
            "(1)\t 690\t 0.6222571 \t0\tfalse\n",
            "(1)\t 691\t -0.0018985108 \t1\ttrue\n",
            "(0)\t 692\t 1.9421889 \t0\ttrue\n",
            "(0)\t 693\t 0.22189452 \t0\ttrue\n",
            "(1)\t 694\t 2.0651116 \t0\tfalse\n",
            "(1)\t 695\t 1.9867713 \t0\tfalse\n",
            "(0)\t 696\t 2.2200472 \t0\ttrue\n",
            "(0)\t 697\t 1.2834303 \t0\ttrue\n",
            "(0)\t 698\t 1.0539777 \t0\ttrue\n",
            "(0)\t 699\t 0.80170524 \t0\ttrue\n",
            "(0)\t 700\t 2.1075675 \t0\ttrue\n",
            "(0)\t 701\t 1.38095 \t0\ttrue\n",
            "(0)\t 702\t 1.9822857 \t0\ttrue\n",
            "(0)\t 703\t 2.131158 \t0\ttrue\n",
            "(0)\t 704\t 0.8170709 \t0\ttrue\n",
            "(1)\t 705\t 1.584041 \t0\tfalse\n",
            "(0)\t 706\t 0.03916613 \t1\tfalse\n",
            "(0)\t 707\t 2.1719182 \t0\ttrue\n",
            "(1)\t 708\t 1.8425912 \t0\tfalse\n",
            "(0)\t 709\t 1.3044623 \t1\tfalse\n",
            "(0)\t 710\t 1.8340173 \t0\ttrue\n",
            "(1)\t 711\t 2.037467 \t0\tfalse\n",
            "(1)\t 712\t 0.10876584 \t0\tfalse\n",
            "(1)\t 713\t 1.2516767 \t0\tfalse\n",
            "(1)\t 714\t 1.8290567 \t0\tfalse\n",
            "(1)\t 715\t 0.22126041 \t0\tfalse\n",
            "(0)\t 716\t 1.6022577 \t0\ttrue\n",
            "(0)\t 717\t 2.1007397 \t0\ttrue\n",
            "(0)\t 718\t 1.0566964 \t0\ttrue\n",
            "(0)\t 719\t 2.1721146 \t0\ttrue\n",
            "(0)\t 720\t 2.047179 \t0\ttrue\n",
            "(1)\t 721\t 1.2790966 \t0\tfalse\n",
            "(1)\t 722\t 1.7453797 \t1\ttrue\n",
            "(0)\t 723\t 1.8543098 \t0\ttrue\n",
            "(0)\t 724\t 1.8684429 \t0\ttrue\n",
            "(1)\t 725\t 2.0612106 \t0\tfalse\n",
            "(0)\t 726\t 1.7906947 \t0\ttrue\n",
            "(0)\t 727\t 2.0088274 \t0\ttrue\n",
            "(1)\t 728\t 1.8515795 \t1\ttrue\n",
            "(1)\t 729\t 0.6642834 \t0\tfalse\n",
            "(1)\t 730\t 1.8176147 \t1\ttrue\n",
            "(1)\t 731\t 1.4045838 \t0\tfalse\n",
            "(1)\t 732\t 1.4045838 \t0\tfalse\n",
            "(0)\t 733\t 2.1944826 \t0\ttrue\n",
            "(1)\t 734\t 1.8223745 \t0\tfalse\n",
            "(0)\t 735\t 0.9689026 \t0\ttrue\n",
            "(0)\t 736\t 2.1600986 \t0\ttrue\n",
            "(0)\t 737\t 2.1281133 \t0\ttrue\n",
            "(1)\t 738\t 2.2114193 \t0\tfalse\n",
            "(1)\t 739\t 2.2114193 \t0\tfalse\n",
            "(0)\t 740\t 1.9040027 \t0\ttrue\n",
            "(1)\t 741\t 0.26185554 \t0\tfalse\n",
            "(1)\t 742\t 0.26185554 \t0\tfalse\n",
            "(0)\t 743\t 1.9991807 \t0\ttrue\n",
            "(0)\t 744\t 0.7974652 \t0\ttrue\n",
            "(1)\t 745\t 0.5432376 \t0\tfalse\n",
            "(0)\t 746\t 1.8373973 \t0\ttrue\n",
            "(0)\t 747\t 1.9782875 \t0\ttrue\n",
            "(1)\t 748\t 2.177738 \t0\tfalse\n",
            "(0)\t 749\t 2.0760362 \t0\ttrue\n",
            "(0)\t 750\t 0.5656361 \t0\ttrue\n",
            "(1)\t 751\t 1.3545266 \t1\ttrue\n",
            "(1)\t 752\t 2.1043315 \t0\tfalse\n",
            "(1)\t 753\t 1.8297628 \t0\tfalse\n",
            "(0)\t 754\t 2.2220454 \t0\ttrue\n",
            "(0)\t 755\t 1.097105 \t0\ttrue\n",
            "(0)\t 756\t 2.1677625 \t0\ttrue\n",
            "(0)\t 757\t 0.7899513 \t1\tfalse\n",
            "(0)\t 758\t 1.04079 \t1\tfalse\n",
            "(0)\t 759\t 1.8870368 \t0\ttrue\n",
            "(0)\t 760\t 1.749822 \t0\ttrue\n",
            "(0)\t 761\t 2.0856817 \t0\ttrue\n",
            "(0)\t 762\t 1.9934014 \t0\ttrue\n",
            "(1)\t 763\t 0.8270106 \t0\tfalse\n",
            "(1)\t 764\t 1.4022945 \t0\tfalse\n",
            "(0)\t 765\t 1.8845837 \t1\tfalse\n",
            "(0)\t 766\t 2.1118937 \t0\ttrue\n",
            "(0)\t 767\t 1.6068344 \t0\ttrue\n",
            "(1)\t 768\t 2.0680177 \t0\tfalse\n",
            "(0)\t 769\t 0.21732625 \t0\ttrue\n",
            "(0)\t 770\t 2.1999638 \t0\ttrue\n",
            "(1)\t 771\t 1.8018289 \t0\tfalse\n",
            "(1)\t 772\t 1.8018289 \t0\tfalse\n",
            "(1)\t 773\t 1.429527 \t0\tfalse\n",
            "(1)\t 774\t 1.429527 \t0\tfalse\n",
            "(1)\t 775\t 2.1959531 \t0\tfalse\n",
            "(1)\t 776\t 2.1959531 \t0\tfalse\n",
            "(1)\t 777\t 1.8824033 \t0\tfalse\n",
            "(0)\t 778\t 1.974896 \t0\ttrue\n",
            "(0)\t 779\t 0.41588706 \t1\tfalse\n",
            "(0)\t 780\t 0.4285201 \t0\ttrue\n",
            "(0)\t 781\t 2.0059927 \t0\ttrue\n",
            "(0)\t 782\t 1.6985404 \t0\ttrue\n",
            "(0)\t 783\t 0.2630393 \t0\ttrue\n",
            "(0)\t 784\t -0.034108654 \t0\ttrue\n",
            "(0)\t 785\t 0.2565502 \t0\ttrue\n",
            "(0)\t 786\t 1.9606441 \t0\ttrue\n",
            "(1)\t 787\t 1.2934222 \t1\ttrue\n",
            "(0)\t 788\t 1.6133149 \t0\ttrue\n",
            "(0)\t 789\t 1.6004763 \t0\ttrue\n",
            "(1)\t 790\t 0.5143835 \t0\tfalse\n",
            "(1)\t 791\t 1.026789 \t0\tfalse\n",
            "(1)\t 792\t 2.0206838 \t0\tfalse\n",
            "(0)\t 793\t 1.7467744 \t1\tfalse\n",
            "(1)\t 794\t 0.4207286 \t0\tfalse\n",
            "(1)\t 795\t 0.8164678 \t0\tfalse\n",
            "(0)\t 796\t 2.048063 \t0\ttrue\n",
            "(0)\t 797\t 2.1407506 \t0\ttrue\n",
            "(0)\t 798\t 1.4105787 \t1\tfalse\n",
            "(0)\t 799\t 0.29523912 \t0\ttrue\n",
            "(0)\t 800\t 1.2798346 \t0\ttrue\n",
            "(1)\t 801\t 1.6470298 \t0\tfalse\n",
            "(1)\t 802\t 1.6470298 \t0\tfalse\n",
            "(0)\t 803\t 1.1985581 \t0\ttrue\n",
            "(0)\t 804\t 1.7033278 \t1\tfalse\n",
            "(0)\t 805\t 0.94096607 \t1\tfalse\n",
            "(0)\t 806\t 1.2672373 \t0\ttrue\n",
            "(1)\t 807\t 0.65277344 \t0\tfalse\n",
            "(0)\t 808\t 1.6070925 \t0\ttrue\n",
            "(1)\t 809\t 1.2246513 \t1\ttrue\n",
            "(1)\t 810\t 1.2246513 \t1\ttrue\n",
            "(1)\t 811\t 1.212807 \t0\tfalse\n",
            "(0)\t 812\t 1.4386792 \t0\ttrue\n",
            "(1)\t 813\t 1.6519549 \t0\tfalse\n",
            "(1)\t 814\t 1.6519549 \t0\tfalse\n",
            "(1)\t 815\t 1.7470152 \t1\ttrue\n",
            "(0)\t 816\t 1.4427407 \t0\ttrue\n",
            "(0)\t 817\t 1.436117 \t0\ttrue\n",
            "(0)\t 818\t 0.9105251 \t1\tfalse\n",
            "(0)\t 819\t 2.1021988 \t0\ttrue\n",
            "(0)\t 820\t 2.0921197 \t0\ttrue\n",
            "(0)\t 821\t 2.045811 \t0\ttrue\n",
            "(1)\t 822\t 2.1148686 \t0\tfalse\n",
            "(0)\t 823\t 1.5045608 \t0\ttrue\n",
            "(0)\t 824\t 1.708215 \t0\ttrue\n",
            "(0)\t 825\t 1.5351698 \t0\ttrue\n",
            "(0)\t 826\t 0.22927535 \t1\tfalse\n",
            "(0)\t 827\t 0.3752765 \t1\tfalse\n",
            "(0)\t 828\t 1.8380613 \t0\ttrue\n",
            "(0)\t 829\t 1.6572182 \t0\ttrue\n",
            "(0)\t 830\t 1.7202635 \t0\ttrue\n",
            "(1)\t 831\t 1.899965 \t0\tfalse\n",
            "(0)\t 832\t 1.8294241 \t0\ttrue\n",
            "(0)\t 833\t 1.1553673 \t0\ttrue\n",
            "(1)\t 834\t 0.5282489 \t1\ttrue\n",
            "(1)\t 835\t 1.2209519 \t0\tfalse\n",
            "(1)\t 836\t 0.18924035 \t0\tfalse\n",
            "(1)\t 837\t 0.13644542 \t1\ttrue\n",
            "(0)\t 838\t 1.6015252 \t0\ttrue\n",
            "(0)\t 839\t 2.1066444 \t0\ttrue\n",
            "(0)\t 840\t 1.2356274 \t0\ttrue\n",
            "(0)\t 841\t 0.075573295 \t1\tfalse\n",
            "(0)\t 842\t 2.071158 \t0\ttrue\n",
            "(1)\t 843\t 2.1613305 \t0\tfalse\n",
            "(0)\t 844\t 0.698142 \t1\tfalse\n",
            "(1)\t 845\t 0.8112063 \t1\ttrue\n",
            "(1)\t 846\t 0.17646149 \t0\tfalse\n",
            "(1)\t 847\t 1.8719156 \t0\tfalse\n",
            "(1)\t 848\t 1.8719156 \t0\tfalse\n",
            "(0)\t 849\t 0.85575134 \t0\ttrue\n",
            "(1)\t 850\t 0.40113655 \t1\ttrue\n",
            "(1)\t 851\t 1.2648622 \t0\tfalse\n",
            "(1)\t 852\t 1.3083702 \t0\tfalse\n",
            "(0)\t 853\t 1.6409659 \t0\ttrue\n",
            "(1)\t 854\t 1.1235756 \t0\tfalse\n",
            "(0)\t 855\t 0.6406885 \t1\tfalse\n",
            "(0)\t 856\t 1.4834666 \t1\tfalse\n",
            "(0)\t 857\t 1.7292824 \t0\ttrue\n",
            "(1)\t 858\t 2.0571573 \t0\tfalse\n",
            "(0)\t 859\t 1.9244725 \t1\tfalse\n",
            "(1)\t 860\t 1.6446115 \t0\tfalse\n",
            "(0)\t 861\t 0.23296437 \t0\ttrue\n",
            "(1)\t 862\t 2.0704079 \t0\tfalse\n",
            "(1)\t 863\t 2.0704079 \t0\tfalse\n",
            "(1)\t 864\t 0.647726 \t0\tfalse\n",
            "(0)\t 865\t 1.2324371 \t1\tfalse\n",
            "(0)\t 866\t 1.6453629 \t0\ttrue\n",
            "(0)\t 867\t 2.043794 \t0\ttrue\n",
            "(0)\t 868\t 0.30253825 \t1\tfalse\n",
            "(0)\t 869\t 0.45744753 \t0\ttrue\n",
            "(0)\t 870\t 2.1804268 \t0\ttrue\n",
            "(0)\t 871\t 1.7521172 \t0\ttrue\n",
            "(0)\t 872\t 1.807687 \t0\ttrue\n",
            "(1)\t 873\t 0.55653334 \t0\tfalse\n",
            "(1)\t 874\t 1.9121003 \t0\tfalse\n",
            "(1)\t 875\t 1.1445099 \t1\ttrue\n",
            "(1)\t 876\t 1.9784096 \t0\tfalse\n",
            "(0)\t 877\t 2.2130573 \t0\ttrue\n",
            "(1)\t 878\t 1.1031055 \t0\tfalse\n",
            "(1)\t 879\t 0.9727029 \t1\ttrue\n",
            "(1)\t 880\t 2.098336 \t0\tfalse\n",
            "(0)\t 881\t 0.36364582 \t1\tfalse\n",
            "(0)\t 882\t 0.23807204 \t1\tfalse\n",
            "(1)\t 883\t 1.9937289 \t0\tfalse\n",
            "(1)\t 884\t 1.1055845 \t0\tfalse\n",
            "(1)\t 885\t 1.4422916 \t1\ttrue\n",
            "(0)\t 886\t 2.1689372 \t0\ttrue\n",
            "(0)\t 887\t 1.4671416 \t1\tfalse\n",
            "(1)\t 888\t 1.3365793 \t1\ttrue\n",
            "(0)\t 889\t 1.6779814 \t0\ttrue\n",
            "(0)\t 890\t 0.90328515 \t0\ttrue\n",
            "(0)\t 891\t 2.0428843 \t0\ttrue\n",
            "(1)\t 892\t 1.724161 \t0\tfalse\n",
            "(1)\t 893\t 2.004342 \t0\tfalse\n",
            "(1)\t 894\t 2.0771136 \t0\tfalse\n",
            "(1)\t 895\t 1.9402385 \t0\tfalse\n",
            "(0)\t 896\t 0.29907095 \t1\tfalse\n",
            "(1)\t 897\t 1.8179032 \t0\tfalse\n",
            "(1)\t 898\t 1.5394996 \t0\tfalse\n",
            "(1)\t 899\t 1.2295932 \t1\ttrue\n",
            "(1)\t 900\t 0.7704172 \t0\tfalse\n",
            "Number of true predictions: 525\n",
            "Number of false predictions: 375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNC2ARoXOfyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "## left -P\n",
        "#right - NP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "outputId": "7a264b1b-c952-4d32-e9ac-e645f943a8c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('True positives: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True positives: 400 of 900 (44.44%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "outputId": "d2c0e0ca-30e1-48bf-a06d-924a0e07dd4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        "  print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n",
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0]\n",
            "[0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0]\n",
            "[0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0]\n",
            "[0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            "[1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            "[0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0]\n",
            "[1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0]\n",
            "[0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0]\n",
            "[0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0]\n",
            "[0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0]\n",
            "[0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1]\n",
            "[0 0 0 1 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0]\n",
            "[0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0]\n",
            "[0 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0]\n",
            "[0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0]\n",
            "[1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1]\n",
            "[0 0 1 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "outputId": "4b607744-dc76-4dbe-9f91-96b328e6b3dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e033f829-34d7-4896-b4a5-d6bfcea742e3"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1pCMRfZM2YG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b5d2d755-2898-4bcf-ae35-517f62abf5eb"
      },
      "source": [
        "!ls -l --block-size=K \"/content/drive/My Drive/FYP/bert/model-colab\""
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 695776K\n",
            "-rw------- 1 root root      2K Feb 28 05:14 config.json\n",
            "drwx------ 2 root root      4K Feb 16 13:29 malayalam\n",
            "drwx------ 2 root root      4K Feb 16 08:55 model-colab\n",
            "-rw------- 1 root root 694793K Feb 28 05:14 pytorch_model.bin\n",
            "-rw------- 1 root root      1K Feb 28 05:14 special_tokens_map.json\n",
            "-rw------- 1 root root      1K Feb 28 05:14 tokenizer_config.json\n",
            "-rw------- 1 root root    973K Feb 28 05:14 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
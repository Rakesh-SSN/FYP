{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_TASK_2_PUNJABI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT_TASK_2_PUNJABI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "6d7eaff0-c71a-4b58-9b20-eddff7c539b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "2feae638-663d-4144-cf5c-f6cd66e611e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "f26365d6-8572-4a64-9a94-552cef9bde4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/punjabi/task2punjabi.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,200\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1451</th>\n",
              "      <td>PUN1452</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ਮੋਬਾਈਲ ਇੰਟਰਨੈੱਟ ਸੇਵਾਵਾਂ ਲਗਾਤਾਰ ਬੰਦ ਹਨ। ਵੱਖਵਾਦੀ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1334</th>\n",
              "      <td>PUN1335</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ਸਵੇਰ ਦਾ ਪੇਪਰ 9.30 ਵਜੇ ਸ਼ੁਰੂ ਹੋਇਆ ਸੀ ਅਤੇ 11.30 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1761</th>\n",
              "      <td>PUN1762</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ਵਿਜੈ ਰੁਪਾਨੀ ਹੋਣਗੇ ਗੁਜਰਾਤ ਦੇ ਮੁੱਖ ਮੰਤਰੀ&lt;eol&gt;ਦਿੱ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1735</th>\n",
              "      <td>PUN1736</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ਸ਼੍ਰੋਮਣੀ ਅਕਾਲੀ ਦਲ (ਬ) ਵੱਲੋਂ ਐਨ ਆਰ ਆਈ ਵਿੰਗ, ਇਟਲ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1576</th>\n",
              "      <td>PUN1577</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ਚਿੱਤਰਕਾਰ ਐਸ.ਐਚ. ਰਜ਼ਾ ਦਾ ਦਿਹਾਂਤ&lt;eol&gt;ਚੰਡੀਗੜ੍ਹ ਯੂ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1110</th>\n",
              "      <td>PUN1111</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>ਮੌਕੇ ਤੋਂ ਇਕੱਤਰ ਕੀਤੀ ਜਾਣਕਾਰੀ ਅਨੁਸਾਰ ਜ਼ੀਰਕਪੁਰ-ਪਟਿ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>PUN1595</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ਬੀ.ਐਸ.ਐਨ.ਐਲ. ਦੇ ਡਾਟਾ ਪਲੈਨ ਹੁਣ ਜ਼ਰੂਰਤ ਮੁਤਾਬਿਕ&lt;e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>PUN0531</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>ਸ੍ਰੀ ਕੇਜਰੀਵਾਲ ਪੰਜਾਬ ਦਾ ਮੁੱਖ ਮੰਤਰੀ ਬਣਨ ਤੋਂ ਬਾਅਦ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651</th>\n",
              "      <td>PUN0652</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>ਅਧਿਕਾਰੀਆਂ ਨੇ ਕਿਹਾ ਕਿ ਵਿਜੈ ਮਾਲਿਆ ਅਤੇ ਉਸ ਦੇ ਪਰਿਵ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>819</th>\n",
              "      <td>PUN0820</td>\n",
              "      <td>2</td>\n",
              "      <td>a</td>\n",
              "      <td>ਬਠਿੰਡਾ-ਬਾਦਲ ਸੜਕ ਮਾਰਗ ’ਤੇ ਵੱਡੇ ਭਾਰੀ ਦਰਖ਼ਤ ਵੀ ਕੁਹ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "1451         PUN1452  ...  ਮੋਬਾਈਲ ਇੰਟਰਨੈੱਟ ਸੇਵਾਵਾਂ ਲਗਾਤਾਰ ਬੰਦ ਹਨ। ਵੱਖਵਾਦੀ...\n",
              "1334         PUN1335  ...   ਸਵੇਰ ਦਾ ਪੇਪਰ 9.30 ਵਜੇ ਸ਼ੁਰੂ ਹੋਇਆ ਸੀ ਅਤੇ 11.30 ...\n",
              "1761         PUN1762  ...  ਵਿਜੈ ਰੁਪਾਨੀ ਹੋਣਗੇ ਗੁਜਰਾਤ ਦੇ ਮੁੱਖ ਮੰਤਰੀ<eol>ਦਿੱ...\n",
              "1735         PUN1736  ...  ਸ਼੍ਰੋਮਣੀ ਅਕਾਲੀ ਦਲ (ਬ) ਵੱਲੋਂ ਐਨ ਆਰ ਆਈ ਵਿੰਗ, ਇਟਲ...\n",
              "1576         PUN1577  ...  ਚਿੱਤਰਕਾਰ ਐਸ.ਐਚ. ਰਜ਼ਾ ਦਾ ਦਿਹਾਂਤ<eol>ਚੰਡੀਗੜ੍ਹ ਯੂ...\n",
              "1110         PUN1111  ...  ਮੌਕੇ ਤੋਂ ਇਕੱਤਰ ਕੀਤੀ ਜਾਣਕਾਰੀ ਅਨੁਸਾਰ ਜ਼ੀਰਕਪੁਰ-ਪਟਿ...\n",
              "1594         PUN1595  ...  ਬੀ.ਐਸ.ਐਨ.ਐਲ. ਦੇ ਡਾਟਾ ਪਲੈਨ ਹੁਣ ਜ਼ਰੂਰਤ ਮੁਤਾਬਿਕ<e...\n",
              "530          PUN0531  ...  ਸ੍ਰੀ ਕੇਜਰੀਵਾਲ ਪੰਜਾਬ ਦਾ ਮੁੱਖ ਮੰਤਰੀ ਬਣਨ ਤੋਂ ਬਾਅਦ...\n",
              "651          PUN0652  ...  ਅਧਿਕਾਰੀਆਂ ਨੇ ਕਿਹਾ ਕਿ ਵਿਜੈ ਮਾਲਿਆ ਅਤੇ ਉਸ ਦੇ ਪਰਿਵ...\n",
              "819          PUN0820  ...  ਬਠਿੰਡਾ-ਬਾਦਲ ਸੜਕ ਮਾਰਗ ’ਤੇ ਵੱਡੇ ਭਾਰੀ ਦਰਖ਼ਤ ਵੀ ਕੁਹ...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "bb201ffe-d94b-4dfd-8ae9-f3c082d0e396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]\n",
        "print(df.label)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       1\n",
            "1       1\n",
            "2       1\n",
            "3       1\n",
            "4       1\n",
            "       ..\n",
            "2195    0\n",
            "2196    0\n",
            "2197    0\n",
            "2198    0\n",
            "2199    0\n",
            "Name: label, Length: 2200, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "b4fb7989-38b7-4db6-af62-5ba0adb9458c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "525652d2-577c-4539-cd67-5dc49ddd125a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  ਰੀਓ ਉਲੰਪਿਕ ਜਾਵੇਗਾ ਨਰਸਿੰਘ<eol>ਨਰਸਿੰਘ ਜਾਵੇਗਾ ਰੀਓ ਉਲਪਿੰਕ\n",
            "Tokenized:  ['ਰ', '##ੀ', '##ਓ', 'ਉ', '##ਲ', '##ਪ', '##ਿਕ', 'ਜਾ', '##ਵ', '##ਗ', '##ਾ', 'ਨ', '##ਰ', '##ਸ', '##ਿ', '##ਘ', '<', 'eo', '##l', '>', 'ਨ', '##ਰ', '##ਸ', '##ਿ', '##ਘ', 'ਜਾ', '##ਵ', '##ਗ', '##ਾ', 'ਰ', '##ੀ', '##ਓ', 'ਉ', '##ਲ', '##ਪ', '##ਿਕ']\n",
            "Token IDs:  [1041, 13094, 111248, 1011, 15423, 31948, 26674, 62130, 32143, 48874, 13768, 1034, 13475, 19836, 45378, 111250, 133, 13934, 10161, 135, 1034, 13475, 19836, 45378, 111250, 62130, 32143, 48874, 13768, 1041, 13094, 111248, 1011, 15423, 31948, 26674]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "928a10ba-940e-4b40-bd1a-7d89e251dddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  ਰੀਓ ਉਲੰਪਿਕ ਜਾਵੇਗਾ ਨਰਸਿੰਘ<eol>ਨਰਸਿੰਘ ਜਾਵੇਗਾ ਰੀਓ ਉਲਪਿੰਕ\n",
            "Token IDs: [101, 1041, 13094, 111248, 1011, 15423, 31948, 26674, 62130, 32143, 48874, 13768, 1034, 13475, 19836, 45378, 111250, 133, 13934, 10161, 135, 1034, 13475, 19836, 45378, 111250, 62130, 32143, 48874, 13768, 1041, 13094, 111248, 1011, 15423, 31948, 26674, 102]\n",
            "Original:  ਰੀਓ ਉਲੰਪਿਕ ਜਾਵੇਗਾ ਨਰਸਿੰਘ<eol>ਨਰਸਿੰਘ ਜਾਵੇਗਾ ਰੀਓ ਉਲਪਿੰਕ\n",
            "Token IDs: [101, 1041, 13094, 111248, 1011, 15423, 31948, 26674, 62130, 32143, 48874, 13768, 1034, 13475, 19836, 45378, 111250, 133, 13934, 10161, 135, 1034, 13475, 19836, 45378, 111250, 62130, 32143, 48874, 13768, 1041, 13094, 111248, 1011, 15423, 31948, 26674, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "379b0e29-c3ad-4202-9876-15a8c9cea317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  283\n",
            "Max sentence length:  283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "ccdd132d-0094-4d13-d2c0-86ae2ffaef5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "334f41e7-9d4c-4c73-e3f0-dc9ec3618741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 20% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "# print(train_inputs)\n",
        "# print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "\n",
        "print(labels)\n",
        "#print(train_masks)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 ... 0 0 0]\n",
            "[1 1 1 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "0a2bcc62-7711-4916-945a-7b91ac9a68cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 3, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8df18091-3cd2-4a93-d92b-ebba3fb01c48"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (3, 768)\n",
            "classifier.bias                                                 (3,)\n",
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (3, 768)\n",
            "classifier.bias                                                 (3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "adc0a4c5-20a9-409b-f403-007240ffddbc"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     55.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training loss: 0.88\n",
            "  Training epcoh took: 0:00:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     55.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training loss: 0.66\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     55.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training loss: 0.60\n",
            "  Training epcoh took: 0:00:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     55.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epcoh took: 0:00:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     55.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training loss: 0.90\n",
            "  Training epcoh took: 0:00:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     55.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     55.    Elapsed: 0:00:17.\n",
            "\n",
            "  Average training loss: 0.64\n",
            "  Training epcoh took: 0:00:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.67\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     55.    Elapsed: 0:00:17.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "48118dc7-e002-4b4a-9274-48e3da173354"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task2punjabi-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "#print(len(labels))\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "#print(len(prediction_labels))\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 750\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c5c02d51-532d-47de-9b91-409bad2249b1"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  #print(outputs)\n",
        "  # print(logits)\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 750 test sentences...\n",
            "    DONE.\n",
            "*************************\n",
            "24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "99e82515-530e-42d4-b374-93689cd3d04d"
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] and predictions[i][j][0]>predictions[i][j][2]):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] and predictions[i][j][1]>predictions[i][j][2]):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "      #print(\" Semi Paraphrase     \",end=\"\")\n",
        "      #count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "      x=2\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)\n",
        "  \n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(0)\t 1\t [ 0.39383778 -0.8610478   0.5250055 ] \t2\tfalse\n",
            "(1)\t 2\t [ 0.39383778 -0.8610478   0.5250055 ] \t2\tfalse\n",
            "(0)\t 3\t [ 2.6725204 -1.6996533 -0.8029047] \t0\ttrue\n",
            "(1)\t 4\t [-2.4261615   2.9653232  -0.88414335] \t1\ttrue\n",
            "(0)\t 5\t [ 1.5865573 -1.6643666 -0.0906079] \t0\ttrue\n",
            "(1)\t 6\t [-2.167908    0.47147217  1.2999414 ] \t2\tfalse\n",
            "(0)\t 7\t [ 2.6217906 -1.5122868 -0.9503421] \t0\ttrue\n",
            "(1)\t 8\t [-2.13611    3.0636902 -1.2166771] \t1\ttrue\n",
            "(0)\t 9\t [ 1.9656068  -1.7430431  -0.10993354] \t0\ttrue\n",
            "(1)\t 10\t [-1.7736821   0.00329567  1.5185819 ] \t2\tfalse\n",
            "(0)\t 11\t [ 1.6448426  -1.6411881   0.11821485] \t0\ttrue\n",
            "(1)\t 12\t [-2.541482   1.477618   0.5995183] \t1\ttrue\n",
            "(0)\t 13\t [ 2.4763062  -1.6923548  -0.56689847] \t0\ttrue\n",
            "(1)\t 14\t [-2.3023934  1.6217672  0.094087 ] \t1\ttrue\n",
            "(2)\t 15\t [-1.2300369  -0.28488177  1.1128881 ] \t2\ttrue\n",
            "(0)\t 16\t [ 2.1268482  -1.7996606  -0.13250239] \t0\ttrue\n",
            "(1)\t 17\t [-1.8151836   0.03271659  1.4978063 ] \t2\tfalse\n",
            "(2)\t 18\t [-1.3451625 -0.505891   1.6243416] \t2\ttrue\n",
            "(0)\t 19\t [ 2.6029396 -1.7519302 -0.7271434] \t0\ttrue\n",
            "(1)\t 20\t [-1.9073554   0.26152888  1.0906427 ] \t2\tfalse\n",
            "(2)\t 21\t [ 0.3119547 -0.5537541  0.2921884] \t0\tfalse\n",
            "(2)\t 22\t [ 0.16544624 -0.78220665  0.6879275 ] \t2\ttrue\n",
            "(0)\t 23\t [ 1.8037897 -1.9113144 -0.0329918] \t0\ttrue\n",
            "(1)\t 24\t [-1.8172138 -0.0955098  1.5177861] \t2\tfalse\n",
            "(0)\t 25\t [ 0.24311629 -0.5163883   0.3894725 ] \t2\tfalse\n",
            "(1)\t 26\t [ 0.24311629 -0.5163883   0.3894725 ] \t2\tfalse\n",
            "(0)\t 27\t [ 0.3091645  -0.52942985  0.32163408] \t2\tfalse\n",
            "(1)\t 28\t [ 0.3091645  -0.52942985  0.32163408] \t2\tfalse\n",
            "(0)\t 29\t [ 2.6553695 -1.4852389 -0.965467 ] \t0\ttrue\n",
            "(1)\t 30\t [-2.029722   3.1064956 -1.4117086] \t1\ttrue\n",
            "(2)\t 31\t [ 1.5822674  -1.5075241  -0.08290265] \t0\tfalse\n",
            "(2)\t 32\t [ 0.38556302 -0.41248     0.22718315] \t0\tfalse\n",
            "(2)\t 33\t [-2.1026971  0.7947341  0.736522 ] \t1\tfalse\n",
            "(0)\t 34\t [ 1.7650423  -1.7284545  -0.00792058] \t0\ttrue\n",
            "(1)\t 35\t [-2.3354094   1.5322726   0.34382683] \t1\ttrue\n",
            "(0)\t 36\t [ 0.28475475 -0.72366494  0.5368877 ] \t2\tfalse\n",
            "(2)\t 37\t [ 0.28475475 -0.72366494  0.5368877 ] \t2\ttrue\n",
            "(0)\t 38\t [ 2.3251526  -1.9724109  -0.28422368] \t0\ttrue\n",
            "(1)\t 39\t [-2.4775715  2.099228  -0.0795438] \t1\ttrue\n",
            "(2)\t 40\t [-1.4961973  -0.44479123  1.6710608 ] \t2\ttrue\n",
            "(2)\t 41\t [ 0.4285757  -0.85036063  0.5462784 ] \t2\ttrue\n",
            "(0)\t 42\t [ 1.816397   -1.9246061   0.04907065] \t0\ttrue\n",
            "(1)\t 43\t [-1.3901579 -0.3175306  1.333953 ] \t2\tfalse\n",
            "(2)\t 44\t [-1.3598669  -0.47011572  1.5320065 ] \t2\ttrue\n",
            "(0)\t 45\t [ 2.5991542  -1.8753587  -0.50811565] \t0\ttrue\n",
            "(1)\t 46\t [-2.2968528  2.8449903 -0.8783032] \t1\ttrue\n",
            "(2)\t 47\t [-1.1438375 -0.5639056  1.3222115] \t2\ttrue\n",
            "(0)\t 48\t [-0.2956294  0.3435294 -0.1460184] \t1\tfalse\n",
            "(2)\t 49\t [-0.2956294  0.3435294 -0.1460184] \t1\tfalse\n",
            "(2)\t 50\t [ 0.09845681 -0.9758536   0.9096983 ] \t2\ttrue\n",
            "(2)\t 51\t [-1.922856    0.04462454  1.5414889 ] \t2\ttrue\n",
            "(0)\t 52\t [ 0.29289493 -0.9412162   0.7385395 ] \t2\tfalse\n",
            "(1)\t 53\t [ 0.29289493 -0.9412162   0.7385395 ] \t2\tfalse\n",
            "(2)\t 54\t [-1.3375405  -0.64060456  1.6668403 ] \t2\ttrue\n",
            "(2)\t 55\t [ 0.28016117 -0.7708244   0.5718863 ] \t2\ttrue\n",
            "(0)\t 56\t [ 2.7065365 -1.8818728 -0.6087   ] \t0\ttrue\n",
            "(1)\t 57\t [-2.4672277   1.535255    0.60783273] \t1\ttrue\n",
            "(2)\t 58\t [ 0.3128621  -0.8588718   0.66417205] \t2\ttrue\n",
            "(0)\t 59\t [ 1.774226   -1.6663616   0.00849796] \t0\ttrue\n",
            "(1)\t 60\t [-2.2565467   1.0138844   0.94589055] \t1\ttrue\n",
            "(2)\t 61\t [ 0.2540506 -0.8028888  0.6624733] \t2\ttrue\n",
            "(0)\t 62\t [ 2.7336628 -1.6264918 -0.9996929] \t0\ttrue\n",
            "(1)\t 63\t [-2.1456084  3.037982  -1.2272065] \t1\ttrue\n",
            "(0)\t 64\t [ 2.3588827  -1.9022093  -0.51998013] \t0\ttrue\n",
            "(1)\t 65\t [-2.1032102   0.72739416  0.96597344] \t2\tfalse\n",
            "(0)\t 66\t [ 1.8102734  -1.785583   -0.00805634] \t0\ttrue\n",
            "(1)\t 67\t [-1.8562256  -0.08823694  1.5426126 ] \t2\tfalse\n",
            "(2)\t 68\t [ 0.22227654 -0.5030206   0.37971032] \t2\ttrue\n",
            "(0)\t 69\t [ 0.9261475  -0.9245685   0.10277711] \t0\ttrue\n",
            "(1)\t 70\t [-1.6741154  -0.10001652  1.1579323 ] \t2\tfalse\n",
            "(0)\t 71\t [ 0.45342925 -1.2505907   0.5140544 ] \t2\tfalse\n",
            "(1)\t 72\t [ 0.45342925 -1.2505907   0.5140544 ] \t2\tfalse\n",
            "(2)\t 73\t [-0.51111674 -0.35416245  0.64472574] \t2\ttrue\n",
            "(0)\t 74\t [ 0.35011122 -0.45116985  0.23665585] \t0\ttrue\n",
            "(1)\t 75\t [ 0.35011122 -0.45116985  0.23665585] \t0\tfalse\n",
            "(0)\t 76\t [ 0.22437286 -0.13574924  0.03438982] \t0\ttrue\n",
            "(1)\t 77\t [ 0.22437286 -0.13574924  0.03438982] \t0\tfalse\n",
            "(2)\t 78\t [-1.4733272 -0.476938   1.6300743] \t2\ttrue\n",
            "(2)\t 79\t [-1.92971     0.28203046  1.29475   ] \t2\ttrue\n",
            "(2)\t 80\t [-2.2016153  0.7029338  1.0923045] \t2\ttrue\n",
            "(0)\t 81\t [ 0.31990612 -0.791335    0.23045756] \t0\ttrue\n",
            "(1)\t 82\t [ 0.31990612 -0.791335    0.23045756] \t0\tfalse\n",
            "(2)\t 83\t [ 0.36791006 -0.5115606   0.27032822] \t0\tfalse\n",
            "(2)\t 84\t [ 1.6061434  -1.634867    0.00733093] \t0\tfalse\n",
            "(2)\t 85\t [-2.2311246  0.8645713  0.9439983] \t2\ttrue\n",
            "(0)\t 86\t [ 2.6984727  -1.7262564  -0.70879996] \t0\ttrue\n",
            "(1)\t 87\t [-1.850932   3.11915   -1.3773606] \t1\ttrue\n",
            "(2)\t 88\t [ 0.37686783 -0.5503614   0.33011827] \t0\tfalse\n",
            "(2)\t 89\t [ 0.26706895 -0.407091    0.2584969 ] \t0\tfalse\n",
            "(2)\t 90\t [-1.800815   0.1648391  1.2326124] \t2\ttrue\n",
            "(0)\t 91\t [ 0.24083388 -0.8643251   0.40733886] \t2\tfalse\n",
            "(1)\t 92\t [ 0.24083388 -0.8643251   0.40733886] \t2\tfalse\n",
            "(0)\t 93\t [ 2.7850235  -1.7624707  -0.78995705] \t0\ttrue\n",
            "(1)\t 94\t [-2.5844643   2.3403296  -0.04056754] \t1\ttrue\n",
            "(2)\t 95\t [-0.8738152  -0.43060887  0.9886821 ] \t2\ttrue\n",
            "(0)\t 96\t [ 1.6531477  -1.6751502  -0.12423142] \t0\ttrue\n",
            "(2)\t 97\t [-2.2911491  0.9452767  0.9666412] \t2\ttrue\n",
            "(0)\t 98\t [-0.00594653 -0.05178149 -0.01216778] \t0\ttrue\n",
            "(1)\t 99\t [-2.1129522   0.46123093  1.2212424 ] \t2\tfalse\n",
            "(0)\t 100\t [ 1.9692508 -1.8343524  0.0191975] \t0\ttrue\n",
            "(1)\t 101\t [-1.5239166  -0.43459788  1.5872532 ] \t2\tfalse\n",
            "(0)\t 102\t [-0.6214724  1.3200498 -0.9801209] \t1\tfalse\n",
            "(1)\t 103\t [-2.3207521   1.6381428   0.20514613] \t1\ttrue\n",
            "(0)\t 104\t [ 0.33622655 -0.5516842   0.34080654] \t2\tfalse\n",
            "(1)\t 105\t [ 0.33622655 -0.5516842   0.34080654] \t2\tfalse\n",
            "(2)\t 106\t [-2.5044672   1.3450196   0.62773645] \t1\tfalse\n",
            "(2)\t 107\t [-1.6663507   0.17151515  1.0383765 ] \t2\ttrue\n",
            "(0)\t 108\t [ 1.5115381  -1.6087759   0.27330083] \t0\ttrue\n",
            "(1)\t 109\t [-2.5968187   2.2225547  -0.08914722] \t1\ttrue\n",
            "(2)\t 110\t [-2.229257   0.9420771  1.011308 ] \t2\ttrue\n",
            "(2)\t 111\t [-1.8768365   0.19508623  1.3310037 ] \t2\ttrue\n",
            "(0)\t 112\t [ 2.6000326  -1.9371929  -0.47689366] \t0\ttrue\n",
            "(1)\t 113\t [-2.6024258   2.3610942  -0.21060212] \t1\ttrue\n",
            "(0)\t 114\t [ 2.628758  -1.8746681 -0.5242405] \t0\ttrue\n",
            "(1)\t 115\t [-2.4094691  2.9220848 -0.822908 ] \t1\ttrue\n",
            "(2)\t 116\t [-1.358487    1.6578162  -0.35557052] \t1\tfalse\n",
            "(2)\t 117\t [ 0.37330255 -0.6784661   0.48294076] \t2\ttrue\n",
            "(2)\t 118\t [ 0.66724247 -0.90119874  0.38957578] \t0\tfalse\n",
            "(2)\t 119\t [-1.279763   -0.44568115  1.2331191 ] \t2\ttrue\n",
            "(2)\t 120\t [-1.7023215  -0.27531245  1.4695611 ] \t2\ttrue\n",
            "(2)\t 121\t [-0.2681356  -0.6373423   0.85136706] \t2\ttrue\n",
            "(2)\t 122\t [-1.5524449 -0.4097309  1.6007304] \t2\ttrue\n",
            "(0)\t 123\t [ 0.21510126 -1.1633894   0.96320134] \t2\tfalse\n",
            "(1)\t 124\t [-1.5224758 -0.4098397  1.6273277] \t2\tfalse\n",
            "(0)\t 125\t [ 2.4187233  -2.0915687  -0.11293648] \t0\ttrue\n",
            "(1)\t 126\t [-2.5522547   2.0344572   0.09575413] \t1\ttrue\n",
            "(0)\t 127\t [ 1.7983088  -1.7459579   0.12191016] \t0\ttrue\n",
            "(1)\t 128\t [-2.648211    2.296677   -0.03528198] \t1\ttrue\n",
            "(0)\t 129\t [ 1.7795668  -1.7876464   0.11145737] \t0\ttrue\n",
            "(1)\t 130\t [-2.4870737   1.3123665   0.84947276] \t1\ttrue\n",
            "(0)\t 131\t [ 2.256329   -1.9526381  -0.25001007] \t0\ttrue\n",
            "(1)\t 132\t [-2.3527699  1.079811   0.8620199] \t1\ttrue\n",
            "(0)\t 133\t [ 2.66627   -1.631684  -0.8463598] \t0\ttrue\n",
            "(1)\t 134\t [-2.507028    2.2562134  -0.25941765] \t1\ttrue\n",
            "(2)\t 135\t [ 0.14027885 -0.49603188  0.38825804] \t2\ttrue\n",
            "(2)\t 136\t [-1.4085721  -0.54928553  1.6506606 ] \t2\ttrue\n",
            "(2)\t 137\t [ 0.3080006 -0.9790467  0.399839 ] \t2\ttrue\n",
            "(2)\t 138\t [-1.8825657   0.19954997  1.3046532 ] \t2\ttrue\n",
            "(2)\t 139\t [-2.4687462   1.26253     0.81794214] \t1\tfalse\n",
            "(0)\t 140\t [ 2.2965355 -1.7297145 -0.4774692] \t0\ttrue\n",
            "(1)\t 141\t [-1.4037138 -0.5483201  1.5611675] \t2\tfalse\n",
            "(2)\t 142\t [ 0.33936647 -0.62288564  0.39461395] \t2\ttrue\n",
            "(2)\t 143\t [ 0.34407705 -0.68969154  0.41222924] \t2\ttrue\n",
            "(2)\t 144\t [ 0.34757033 -0.60585076  0.3670468 ] \t2\ttrue\n",
            "(0)\t 145\t [ 2.6291556 -1.6571147 -0.91497  ] \t0\ttrue\n",
            "(1)\t 146\t [-2.0753777  2.484926  -1.0141289] \t1\ttrue\n",
            "(2)\t 147\t [ 0.5625554  -0.5012039   0.00088202] \t0\tfalse\n",
            "(2)\t 148\t [ 0.41338423 -0.83659667  0.12339164] \t0\tfalse\n",
            "(2)\t 149\t [ 0.2670363  -0.84426254  0.6707763 ] \t2\ttrue\n",
            "(2)\t 150\t [-1.6954123  -0.16329919  1.4932903 ] \t2\ttrue\n",
            "(0)\t 151\t [-1.511542  -0.4173866  1.5080953] \t2\tfalse\n",
            "(1)\t 152\t [-1.2931166  -0.67548436  1.5206738 ] \t2\tfalse\n",
            "(2)\t 153\t [-0.6614145  1.108738  -0.9564203] \t1\tfalse\n",
            "(0)\t 154\t [ 0.5154031  -0.88351405  0.15876128] \t0\ttrue\n",
            "(1)\t 155\t [ 0.5154031  -0.88351405  0.15876128] \t0\tfalse\n",
            "(0)\t 156\t [ 2.4206073 -1.5251817 -0.8587105] \t0\ttrue\n",
            "(1)\t 157\t [-2.257449   2.976525  -1.1430917] \t1\ttrue\n",
            "(0)\t 158\t [ 2.6438708 -1.8226763 -0.7129797] \t0\ttrue\n",
            "(1)\t 159\t [-2.1495562   1.0908225   0.41208446] \t1\ttrue\n",
            "(0)\t 160\t [ 1.453586  -1.631207   0.3411191] \t0\ttrue\n",
            "(1)\t 161\t [-2.5628786  1.4298499  0.5523354] \t1\ttrue\n",
            "(2)\t 162\t [ 0.44279957 -0.46670002  0.17074573] \t0\tfalse\n",
            "(2)\t 163\t [-0.00768612 -0.30396023  0.3727471 ] \t2\ttrue\n",
            "(2)\t 164\t [-1.6714803  -0.25526488  1.611424  ] \t2\ttrue\n",
            "(2)\t 165\t [ 0.69274175 -0.9892531   0.10460423] \t0\tfalse\n",
            "(2)\t 166\t [-1.6205083   0.09409357  0.8803545 ] \t2\ttrue\n",
            "(2)\t 167\t [-2.427823    1.2664053   0.76813376] \t1\tfalse\n",
            "(0)\t 168\t [ 2.5849564  -1.8628093  -0.64244413] \t0\ttrue\n",
            "(1)\t 169\t [-2.3184643   1.9711422  -0.14223967] \t1\ttrue\n",
            "(2)\t 170\t [-0.09673741 -0.64485765  0.5935425 ] \t2\ttrue\n",
            "(2)\t 171\t [ 0.29011017 -0.41874477  0.27644813] \t0\tfalse\n",
            "(0)\t 172\t [ 2.6325274 -1.5627033 -0.8496916] \t0\ttrue\n",
            "(1)\t 173\t [-0.55809397  0.97149163 -1.1119471 ] \t1\ttrue\n",
            "(2)\t 174\t [-1.5565171  -0.39788243  1.6069747 ] \t2\ttrue\n",
            "(2)\t 175\t [-1.1997454 -0.5275034  1.277193 ] \t2\ttrue\n",
            "(2)\t 176\t [-0.14548768 -0.94310355  1.0732706 ] \t2\ttrue\n",
            "(0)\t 177\t [ 2.4862597  -1.7576922  -0.54560935] \t0\ttrue\n",
            "(1)\t 178\t [-1.4080206  -0.07216766  0.9784912 ] \t2\tfalse\n",
            "(0)\t 179\t [-0.24628866 -0.21085338  0.1593244 ] \t2\tfalse\n",
            "(1)\t 180\t [-1.7487873  -0.11633208  1.4078114 ] \t2\tfalse\n",
            "(2)\t 181\t [ 1.1569002  -0.6640579  -0.24281351] \t0\tfalse\n",
            "(0)\t 182\t [ 0.871469  -1.4527885  0.537005 ] \t0\ttrue\n",
            "(1)\t 183\t [-1.7305232   0.09475256  1.0947291 ] \t2\tfalse\n",
            "(0)\t 184\t [ 1.8694482  -0.865471   -0.84004056] \t0\ttrue\n",
            "(1)\t 185\t [-2.2644017   1.7462629  -0.17426033] \t1\ttrue\n",
            "(2)\t 186\t [ 0.19643836 -0.6772004   0.41883516] \t2\ttrue\n",
            "(2)\t 187\t [ 0.30855116 -0.56333196  0.33836186] \t2\ttrue\n",
            "(2)\t 188\t [-1.466144   -0.38654196  1.539249  ] \t2\ttrue\n",
            "(2)\t 189\t [ 0.34031478 -0.41952568  0.11672238] \t0\tfalse\n",
            "(2)\t 190\t [-0.8830534  -0.14725755  0.6928548 ] \t2\ttrue\n",
            "(0)\t 191\t [ 1.9182118  -1.8239208   0.26968297] \t0\ttrue\n",
            "(1)\t 192\t [-1.7899343  3.1900253 -1.5899487] \t1\ttrue\n",
            "(0)\t 193\t [ 2.7106366 -1.4310267 -1.0369172] \t0\ttrue\n",
            "(1)\t 194\t [-1.8310454  3.1480553 -1.5309341] \t1\ttrue\n",
            "(2)\t 195\t [ 0.43844032 -0.7057953   0.42049098] \t0\tfalse\n",
            "(0)\t 196\t [ 2.3892052  -1.7991016  -0.35271657] \t0\ttrue\n",
            "(1)\t 197\t [-2.4930494  2.6083264 -0.412013 ] \t1\ttrue\n",
            "(0)\t 198\t [ 1.2818022  -1.6068887   0.44826686] \t0\ttrue\n",
            "(1)\t 199\t [-2.417905   2.811519  -0.7454196] \t1\ttrue\n",
            "(0)\t 200\t [ 2.7205527  -1.6012633  -0.99018514] \t0\ttrue\n",
            "(1)\t 201\t [-1.7500604  3.0628803 -1.5419046] \t1\ttrue\n",
            "(0)\t 202\t [ 1.9554421  -1.6692263  -0.11505186] \t0\ttrue\n",
            "(1)\t 203\t [-2.5268066  1.6194915  0.5800414] \t1\ttrue\n",
            "(2)\t 204\t [-1.5112956 -0.2769668  1.4911985] \t2\ttrue\n",
            "(0)\t 205\t [ 2.2597532  -1.5393293  -0.37651414] \t0\ttrue\n",
            "(1)\t 206\t [-1.9878042   0.23573676  1.4250365 ] \t2\tfalse\n",
            "(2)\t 207\t [ 0.42917684 -0.73410434  0.45266473] \t2\ttrue\n",
            "(0)\t 208\t [ 0.2727814   0.66745675 -1.1113756 ] \t1\tfalse\n",
            "(1)\t 209\t [ 0.2727814   0.66745675 -1.1113756 ] \t1\ttrue\n",
            "(2)\t 210\t [-0.00916374  0.07716658 -0.04098822] \t1\tfalse\n",
            "(1)\t 211\t [-1.7769022   0.9296494   0.37938362] \t1\ttrue\n",
            "(0)\t 212\t [ 1.6375517  -1.8226744   0.22409214] \t0\ttrue\n",
            "(1)\t 213\t [-1.9714619   0.18931349  1.2387468 ] \t2\tfalse\n",
            "(2)\t 214\t [ 0.43528414 -0.88691634  0.21171996] \t0\tfalse\n",
            "(2)\t 215\t [-2.1415498   0.39846197  1.2803907 ] \t2\ttrue\n",
            "(0)\t 216\t [ 2.2471857  -1.6901188  -0.26953977] \t0\ttrue\n",
            "(1)\t 217\t [-1.6719099   0.03278967  1.124471  ] \t2\tfalse\n",
            "(0)\t 218\t [ 2.3206851  -1.7830813  -0.40033597] \t0\ttrue\n",
            "(1)\t 219\t [-1.8461733   0.17330517  1.1662132 ] \t2\tfalse\n",
            "(0)\t 220\t [ 0.36217085 -0.62708265  0.32563284] \t0\ttrue\n",
            "(1)\t 221\t [ 0.36217085 -0.62708265  0.32563284] \t0\tfalse\n",
            "(2)\t 222\t [ 0.05630009 -0.63566005  0.36562997] \t2\ttrue\n",
            "(0)\t 223\t [ 2.6678147 -1.7114948 -0.8242085] \t0\ttrue\n",
            "(1)\t 224\t [-2.2206674  2.881215  -1.10064  ] \t1\ttrue\n",
            "(2)\t 225\t [-1.7005054  0.6241407  0.5786901] \t1\tfalse\n",
            "(2)\t 226\t [ 0.29143822 -0.5735966   0.40640944] \t2\ttrue\n",
            "(0)\t 227\t [ 2.6413057  -1.7965945  -0.69189876] \t0\ttrue\n",
            "(1)\t 228\t [-2.0618508  3.0843172 -1.2530909] \t1\ttrue\n",
            "(0)\t 229\t [ 0.44374037 -0.52968854  0.22579779] \t0\ttrue\n",
            "(1)\t 230\t [ 0.44374037 -0.52968854  0.22579779] \t0\tfalse\n",
            "(2)\t 231\t [ 0.10308293 -0.7138504   0.72927487] \t2\ttrue\n",
            "(0)\t 232\t [ 0.34985116 -0.7748413   0.55504394] \t2\tfalse\n",
            "(1)\t 233\t [ 0.34985116 -0.7748413   0.55504394] \t2\tfalse\n",
            "(0)\t 234\t [ 0.38841268 -0.6844221   0.38390923] \t0\ttrue\n",
            "(1)\t 235\t [ 0.38841268 -0.6844221   0.38390923] \t0\tfalse\n",
            "(2)\t 236\t [-1.8137887   0.24777429  0.9997541 ] \t2\ttrue\n",
            "(2)\t 237\t [-2.2948472   1.8313899  -0.06084393] \t1\tfalse\n",
            "(2)\t 238\t [-2.0070527   0.03875631  1.4720671 ] \t2\ttrue\n",
            "(2)\t 239\t [-0.23025258  0.06761865  0.08954667] \t2\ttrue\n",
            "(2)\t 240\t [-1.8110374  -0.06357374  1.5051397 ] \t2\ttrue\n",
            "(0)\t 241\t [ 1.6397816  -1.6560271  -0.04102459] \t0\ttrue\n",
            "(1)\t 242\t [ 1.1108391  -1.2399321   0.18231164] \t0\tfalse\n",
            "(2)\t 243\t [-1.6380217  -0.12811838  1.4791551 ] \t2\ttrue\n",
            "(2)\t 244\t [ 2.401616   -1.6938971  -0.70953023] \t0\tfalse\n",
            "(2)\t 245\t [ 0.40947488 -1.035212    0.734109  ] \t2\ttrue\n",
            "(2)\t 246\t [ 0.27989325 -0.39235654  0.14956093] \t0\tfalse\n",
            "(2)\t 247\t [ 0.27605477 -0.72500986  0.54936594] \t2\ttrue\n",
            "(2)\t 248\t [ 0.17672308 -0.6557214   0.5287356 ] \t2\ttrue\n",
            "(2)\t 249\t [-1.5697615   0.02262871  1.1113225 ] \t2\ttrue\n",
            "(0)\t 250\t [ 2.5400763  -1.9416931  -0.56902444] \t0\ttrue\n",
            "(1)\t 251\t [-2.0950317   0.44432428  1.2234797 ] \t2\tfalse\n",
            "(0)\t 252\t [ 1.9952474  -1.7644523  -0.16286545] \t0\ttrue\n",
            "(1)\t 253\t [-2.2019627  3.1562822 -1.132693 ] \t1\ttrue\n",
            "(2)\t 254\t [ 0.24061573 -0.80522054  0.6457966 ] \t2\ttrue\n",
            "(2)\t 255\t [ 0.14293514 -0.74074763  0.6423167 ] \t2\ttrue\n",
            "(2)\t 256\t [ 0.40053454 -0.6184032   0.3290882 ] \t0\tfalse\n",
            "(0)\t 257\t [ 2.1143906  -1.9391485  -0.16733345] \t0\ttrue\n",
            "(1)\t 258\t [-1.484067   -0.50817055  1.5551656 ] \t2\tfalse\n",
            "(2)\t 259\t [ 0.43273324 -0.5360108   0.25424823] \t0\tfalse\n",
            "(2)\t 260\t [-1.9119009   0.35287702  1.3255645 ] \t2\ttrue\n",
            "(2)\t 261\t [ 0.64622456 -0.439317    0.16510029] \t0\tfalse\n",
            "(2)\t 262\t [-2.6164145   2.1802516   0.04956398] \t1\tfalse\n",
            "(2)\t 263\t [ 0.08148365 -0.59358615  0.53678274] \t2\ttrue\n",
            "(2)\t 264\t [ 0.24096248 -0.4803801   0.07060432] \t0\tfalse\n",
            "(2)\t 265\t [ 0.32960436 -1.3283035   0.796959  ] \t2\ttrue\n",
            "(2)\t 266\t [ 0.13948455 -0.91762316  0.8525266 ] \t2\ttrue\n",
            "(2)\t 267\t [-0.19006298  0.03798639  0.10325453] \t2\ttrue\n",
            "(2)\t 268\t [ 0.35751078 -0.8779193   0.644086  ] \t2\ttrue\n",
            "(2)\t 269\t [ 0.39221004 -0.66617405  0.4140222 ] \t2\ttrue\n",
            "(2)\t 270\t [ 0.26165727 -0.5480543   0.39958343] \t2\ttrue\n",
            "(0)\t 271\t [ 1.7231326  -1.5471365   0.02102953] \t0\ttrue\n",
            "(1)\t 272\t [-2.1490557   0.51499975  1.2542782 ] \t2\tfalse\n",
            "(2)\t 273\t [ 0.20076421 -0.9699787   0.76501137] \t2\ttrue\n",
            "(2)\t 274\t [-2.0207794   0.18829976  1.3340135 ] \t2\ttrue\n",
            "(2)\t 275\t [-2.3082695   1.2549162   0.55452377] \t1\tfalse\n",
            "(0)\t 276\t [ 2.6401863  -2.0179284  -0.37069523] \t0\ttrue\n",
            "(1)\t 277\t [-2.5522091   2.5164328  -0.38158694] \t1\ttrue\n",
            "(2)\t 278\t [-1.521818  -0.3986231  1.5326905] \t2\ttrue\n",
            "(2)\t 279\t [ 0.3044841  -0.82647246  0.68149656] \t2\ttrue\n",
            "(2)\t 280\t [ 0.80595094 -1.0084636   0.13231741] \t0\tfalse\n",
            "(2)\t 281\t [ 0.3737647  -0.7876558   0.52149457] \t2\ttrue\n",
            "(2)\t 282\t [-1.404982   -0.46260452  1.4696044 ] \t2\ttrue\n",
            "(2)\t 283\t [ 0.43034437 -0.7054288   0.4320961 ] \t2\ttrue\n",
            "(2)\t 284\t [ 0.3168808  -0.5317423  -0.14315374] \t0\tfalse\n",
            "(0)\t 285\t [ 0.59125865 -0.90351003  0.00908697] \t0\ttrue\n",
            "(2)\t 286\t [ 0.59125865 -0.90351003  0.00908697] \t0\tfalse\n",
            "(2)\t 287\t [ 0.26097664 -0.79435825  0.5993667 ] \t2\ttrue\n",
            "(0)\t 288\t [ 0.22766024 -0.6184957   0.41636133] \t2\tfalse\n",
            "(2)\t 289\t [ 0.22766024 -0.6184957   0.41636133] \t2\ttrue\n",
            "(0)\t 290\t [ 2.2245967  -1.9451888  -0.19938351] \t0\ttrue\n",
            "(1)\t 291\t [-2.014263    0.23934874  1.2729743 ] \t2\tfalse\n",
            "(1)\t 292\t [-1.4951146  1.9129045 -1.0212333] \t1\ttrue\n",
            "(0)\t 293\t [ 2.0777295  -1.7220528  -0.20879403] \t0\ttrue\n",
            "(1)\t 294\t [-2.6625643   2.3081708  -0.08389327] \t1\ttrue\n",
            "(2)\t 295\t [ 0.29206362 -0.8130078   0.6479723 ] \t2\ttrue\n",
            "(2)\t 296\t [-2.2857132  0.8690209  0.9933804] \t2\ttrue\n",
            "(2)\t 297\t [ 0.05177666 -0.6948641   0.69779074] \t2\ttrue\n",
            "(2)\t 298\t [ 0.3916076  -0.6371062   0.05914469] \t0\tfalse\n",
            "(2)\t 299\t [-2.2728922   1.153075    0.61425275] \t1\tfalse\n",
            "(2)\t 300\t [-2.3483427  1.0993984  0.8471416] \t1\tfalse\n",
            "(2)\t 301\t [ 0.3232172  -0.58318293  0.42760333] \t2\ttrue\n",
            "(2)\t 302\t [-1.6646394   0.01786269  1.2186333 ] \t2\ttrue\n",
            "(0)\t 303\t [ 2.5531342 -1.2077807 -1.1140513] \t0\ttrue\n",
            "(1)\t 304\t [-1.9770498  2.6138744 -1.1310005] \t1\ttrue\n",
            "(2)\t 305\t [ 0.5269359  -0.651153    0.09131548] \t0\tfalse\n",
            "(0)\t 306\t [ 2.1113691  -1.7085437  -0.40185797] \t0\ttrue\n",
            "(1)\t 307\t [-1.5074265 -0.3294404  1.526209 ] \t2\tfalse\n",
            "(2)\t 308\t [ 0.42366824 -0.6423304   0.39897996] \t0\tfalse\n",
            "(2)\t 309\t [ 0.33860144 -0.49003372  0.2691339 ] \t0\tfalse\n",
            "(0)\t 310\t [ 0.16740692  0.529991   -0.90330386] \t1\tfalse\n",
            "(1)\t 311\t [ 0.16740692  0.529991   -0.90330386] \t1\ttrue\n",
            "(1)\t 312\t [-1.8113962  3.1414125 -1.6023898] \t1\ttrue\n",
            "(0)\t 313\t [ 0.3852075 -0.6913842  0.3396549] \t0\ttrue\n",
            "(1)\t 314\t [ 0.3852075 -0.6913842  0.3396549] \t0\tfalse\n",
            "(0)\t 315\t [ 2.7653368  -1.687373   -0.94454294] \t0\ttrue\n",
            "(1)\t 316\t [-1.9699392  3.1203635 -1.3878701] \t1\ttrue\n",
            "(0)\t 317\t [ 1.9960824  -1.5731009  -0.35321227] \t0\ttrue\n",
            "(1)\t 318\t [-1.7082667  -0.17665128  1.46327   ] \t2\tfalse\n",
            "(2)\t 319\t [-1.5493766  -0.23402573  1.4433067 ] \t2\ttrue\n",
            "(2)\t 320\t [ 0.34274533 -0.7345964   0.54145616] \t2\ttrue\n",
            "(2)\t 321\t [-1.6031331 -0.3660419  1.5224369] \t2\ttrue\n",
            "(2)\t 322\t [ 0.38507426 -1.0051565   0.7334132 ] \t2\ttrue\n",
            "(0)\t 323\t [ 1.520952   -1.4032519   0.04699508] \t0\ttrue\n",
            "(1)\t 324\t [-2.47125     2.8016052  -0.70924264] \t1\ttrue\n",
            "(0)\t 325\t [ 2.6504912 -1.6529187 -0.873091 ] \t0\ttrue\n",
            "(1)\t 326\t [-2.5023692   2.2419386  -0.27427274] \t1\ttrue\n",
            "(1)\t 327\t [-2.3578532   0.78785497  1.041205  ] \t2\tfalse\n",
            "(2)\t 328\t [-1.5274377  -0.34855863  1.5011796 ] \t2\ttrue\n",
            "(0)\t 329\t [ 2.3682363  -1.859995   -0.41652563] \t0\ttrue\n",
            "(1)\t 330\t [-1.6928393   0.10292306  1.1032393 ] \t2\tfalse\n",
            "(2)\t 331\t [ 0.36150107 -0.79779166  0.56237113] \t2\ttrue\n",
            "(2)\t 332\t [-0.6829702  -0.20550512  0.6731007 ] \t2\ttrue\n",
            "(2)\t 333\t [ 0.3011259  -0.4310961   0.14047042] \t0\tfalse\n",
            "(2)\t 334\t [ 0.41294444 -0.27383444 -0.03947009] \t0\tfalse\n",
            "(0)\t 335\t [ 1.9315436  -1.785456   -0.00927323] \t0\ttrue\n",
            "(1)\t 336\t [-2.4886942   1.437276    0.71614385] \t1\ttrue\n",
            "(0)\t 337\t [ 0.41645223 -0.5127613   0.21684621] \t0\ttrue\n",
            "(2)\t 338\t [ 0.41645223 -0.5127613   0.21684621] \t0\tfalse\n",
            "(0)\t 339\t [ 0.02525319 -0.47140533  0.5063697 ] \t2\tfalse\n",
            "(1)\t 340\t [ 0.02525319 -0.47140533  0.5063697 ] \t2\tfalse\n",
            "(2)\t 341\t [ 0.28634351 -0.47660178  0.31378087] \t2\ttrue\n",
            "(2)\t 342\t [ 0.3760553  -0.6181387   0.39039433] \t2\ttrue\n",
            "(0)\t 343\t [ 0.28137472 -0.7357238   0.2637605 ] \t0\ttrue\n",
            "(1)\t 344\t [ 0.28137472 -0.7357238   0.2637605 ] \t0\tfalse\n",
            "(0)\t 345\t [ 0.16234848 -0.36170965  0.16440961] \t2\tfalse\n",
            "(1)\t 346\t [ 0.16234848 -0.36170965  0.16440961] \t2\tfalse\n",
            "(1)\t 347\t [-1.8003362  3.1111894 -1.5254377] \t1\ttrue\n",
            "(0)\t 348\t [ 2.5304294  -2.0485861  -0.23589791] \t0\ttrue\n",
            "(1)\t 349\t [-2.522816    2.5310788  -0.40139556] \t1\ttrue\n",
            "(0)\t 350\t [ 0.31900668 -0.478457    0.3102155 ] \t0\ttrue\n",
            "(1)\t 351\t [ 0.31900668 -0.478457    0.3102155 ] \t0\tfalse\n",
            "(0)\t 352\t [ 2.4684172 -1.8274574 -0.5417215] \t0\ttrue\n",
            "(1)\t 353\t [-1.9620687   1.2220614   0.19795763] \t1\ttrue\n",
            "(0)\t 354\t [ 0.45782056 -0.659033    0.3329644 ] \t0\ttrue\n",
            "(1)\t 355\t [ 0.45782056 -0.659033    0.3329644 ] \t0\tfalse\n",
            "(0)\t 356\t [ 2.7741494 -1.532343  -1.0532817] \t0\ttrue\n",
            "(1)\t 357\t [-1.7356719  3.0829887 -1.500626 ] \t1\ttrue\n",
            "(2)\t 358\t [ 0.22905564 -0.98526317  0.8554565 ] \t2\ttrue\n",
            "(0)\t 359\t [ 2.0040057  -1.903981   -0.00404902] \t0\ttrue\n",
            "(1)\t 360\t [-1.5883497  -0.44732007  1.4981345 ] \t2\tfalse\n",
            "(0)\t 361\t [ 0.30279472 -0.56431305  0.38833416] \t2\tfalse\n",
            "(1)\t 362\t [ 0.30279472 -0.56431305  0.38833416] \t2\tfalse\n",
            "(2)\t 363\t [ 0.31737256 -0.6189936   0.45833218] \t2\ttrue\n",
            "(2)\t 364\t [-1.8068006  0.1341495  1.1972994] \t2\ttrue\n",
            "(1)\t 365\t [-2.5423882   1.7162341   0.42080644] \t1\ttrue\n",
            "(0)\t 366\t [ 0.13376307 -0.53446484  0.5053475 ] \t2\tfalse\n",
            "(1)\t 367\t [ 0.13376307 -0.53446484  0.5053475 ] \t2\tfalse\n",
            "(0)\t 368\t [ 2.7251015  -1.659932   -0.80060947] \t0\ttrue\n",
            "(1)\t 369\t [-1.922031   3.1873496 -1.4958109] \t1\ttrue\n",
            "(2)\t 370\t [-0.12016852 -0.8273166   0.91818523] \t2\ttrue\n",
            "(2)\t 371\t [-1.6825713 -0.3356927  1.5257865] \t2\ttrue\n",
            "(2)\t 372\t [-1.5250722 -0.4325418  1.6232085] \t2\ttrue\n",
            "(0)\t 373\t [ 2.5051866  -1.9420844  -0.49128032] \t0\ttrue\n",
            "(1)\t 374\t [-1.969494    1.5353265  -0.25458884] \t1\ttrue\n",
            "(0)\t 375\t [ 2.0809996  -1.8419907  -0.17032553] \t0\ttrue\n",
            "(1)\t 376\t [-2.126489    0.49201545  1.2600882 ] \t2\tfalse\n",
            "(2)\t 377\t [-1.2539291 -0.6290501  1.523098 ] \t2\ttrue\n",
            "(0)\t 378\t [ 0.40590307 -0.6181445   0.36666647] \t0\ttrue\n",
            "(1)\t 379\t [ 0.40590307 -0.6181445   0.36666647] \t0\tfalse\n",
            "(0)\t 380\t [ 2.2488422  -1.6222185  -0.38743076] \t0\ttrue\n",
            "(1)\t 381\t [-2.3874993   2.0086179  -0.24972926] \t1\ttrue\n",
            "(2)\t 382\t [ 0.67538947 -0.86714894  0.32678136] \t0\tfalse\n",
            "(2)\t 383\t [-1.3850912 -0.542857   1.4947865] \t2\ttrue\n",
            "(0)\t 384\t [ 1.5160959  -1.633524    0.25806308] \t0\ttrue\n",
            "(1)\t 385\t [-2.3969707  1.056593   0.935097 ] \t1\ttrue\n",
            "(0)\t 386\t [ 2.4259608 -1.4485704 -0.8539808] \t0\ttrue\n",
            "(1)\t 387\t [-1.8785323  3.1723707 -1.5224022] \t1\ttrue\n",
            "(0)\t 388\t [ 1.3681631  -1.803381    0.48985964] \t0\ttrue\n",
            "(1)\t 389\t [-2.414966   1.3507653  0.4257149] \t1\ttrue\n",
            "(0)\t 390\t [ 2.3882833  -1.9877781  -0.39394408] \t0\ttrue\n",
            "(1)\t 391\t [-2.087283   0.5502024  1.1033915] \t2\tfalse\n",
            "(2)\t 392\t [ 0.1511083  -0.8246652   0.77077967] \t2\ttrue\n",
            "(0)\t 393\t [ 0.12617701  0.01957267 -0.6353106 ] \t0\ttrue\n",
            "(1)\t 394\t [ 0.12617701  0.01957267 -0.6353106 ] \t0\tfalse\n",
            "(0)\t 395\t [ 2.2575061 -0.7509391 -1.266904 ] \t0\ttrue\n",
            "(1)\t 396\t [-1.8853221  3.1263738 -1.4432926] \t1\ttrue\n",
            "(0)\t 397\t [ 2.85644   -1.5559456 -0.9802016] \t0\ttrue\n",
            "(1)\t 398\t [-1.8341393  3.1162431 -1.4695275] \t1\ttrue\n",
            "(2)\t 399\t [ 0.40192646 -0.78065836  0.50098777] \t2\ttrue\n",
            "(2)\t 400\t [ 0.26009616 -0.63256836  0.44122967] \t2\ttrue\n",
            "(2)\t 401\t [-1.8226782   0.11670257  1.2923747 ] \t2\ttrue\n",
            "(0)\t 402\t [ 2.7021453  -1.499741   -0.80609596] \t0\ttrue\n",
            "(1)\t 403\t [-2.2293115  2.314145  -0.7594633] \t1\ttrue\n",
            "(0)\t 404\t [ 0.9447507  -0.21617576 -1.0164462 ] \t0\ttrue\n",
            "(1)\t 405\t [-1.7108356   1.4619948  -0.43230638] \t1\ttrue\n",
            "(0)\t 406\t [ 2.6399007 -1.4857463 -0.8704397] \t0\ttrue\n",
            "(1)\t 407\t [-2.1934826  3.0968516 -1.1969928] \t1\ttrue\n",
            "(0)\t 408\t [ 2.7866979 -1.7129067 -0.9016998] \t0\ttrue\n",
            "(1)\t 409\t [-1.8199164  3.1280355 -1.4786888] \t1\ttrue\n",
            "(0)\t 410\t [-0.35695678  0.15180549  0.05669109] \t1\tfalse\n",
            "(1)\t 411\t [-0.35695678  0.15180549  0.05669109] \t1\ttrue\n",
            "(2)\t 412\t [ 0.01968808 -0.22847274  0.12126447] \t2\ttrue\n",
            "(2)\t 413\t [ 0.24988854 -0.9061623   0.75253814] \t2\ttrue\n",
            "(2)\t 414\t [-1.7284082   0.14149372  1.0984895 ] \t2\ttrue\n",
            "(0)\t 415\t [ 2.476411  -1.8257198 -0.4749791] \t0\ttrue\n",
            "(1)\t 416\t [-2.2644236   1.1652721   0.43071935] \t1\ttrue\n",
            "(2)\t 417\t [ 0.34800813 -0.6183357   0.38477194] \t2\ttrue\n",
            "(0)\t 418\t [ 2.0432215  -1.9695972  -0.04363258] \t0\ttrue\n",
            "(1)\t 419\t [-2.408083   2.8055887 -0.7833149] \t1\ttrue\n",
            "(2)\t 420\t [ 0.6287089  -0.8985304   0.13439779] \t0\tfalse\n",
            "(2)\t 421\t [-1.6797677  -0.15698831  1.4592391 ] \t2\ttrue\n",
            "(2)\t 422\t [-1.6871977  -0.06377573  1.3822014 ] \t2\ttrue\n",
            "(0)\t 423\t [ 2.7979457  -1.7280982  -0.97311425] \t0\ttrue\n",
            "(1)\t 424\t [-2.0327759  3.1645715 -1.4210663] \t1\ttrue\n",
            "(0)\t 425\t [ 1.1235267  -0.8977469  -0.14928874] \t0\ttrue\n",
            "(1)\t 426\t [-2.4531426   1.2331872   0.87463546] \t1\ttrue\n",
            "(2)\t 427\t [ 0.41164553 -0.57843095  0.31601742] \t0\tfalse\n",
            "(0)\t 428\t [ 2.7737203 -1.3304814 -1.1229421] \t0\ttrue\n",
            "(1)\t 429\t [-1.9400696  3.1302543 -1.3352762] \t1\ttrue\n",
            "(0)\t 430\t [ 2.0437012  -1.4679886  -0.33519953] \t0\ttrue\n",
            "(1)\t 431\t [-2.5345478  2.3917859 -0.2941448] \t1\ttrue\n",
            "(0)\t 432\t [ 1.7174842  -1.7988992   0.08207847] \t0\ttrue\n",
            "(1)\t 433\t [-1.5321137  -0.40707678  1.5402391 ] \t2\tfalse\n",
            "(1)\t 434\t [-2.6351004   1.7797503   0.26991054] \t1\ttrue\n",
            "(2)\t 435\t [ 0.20850484 -0.6551049   0.48671672] \t2\ttrue\n",
            "(0)\t 436\t [ 2.761959  -1.6520188 -0.9553473] \t0\ttrue\n",
            "(1)\t 437\t [-1.9169558  3.0988045 -1.4438473] \t1\ttrue\n",
            "(2)\t 438\t [ 0.49339643 -0.41388407  0.1348113 ] \t0\tfalse\n",
            "(2)\t 439\t [ 0.2511867  -0.67938864  0.5661849 ] \t2\ttrue\n",
            "(0)\t 440\t [ 2.786948  -1.5761425 -0.9324861] \t0\ttrue\n",
            "(1)\t 441\t [-2.1335487  3.0800922 -1.2099886] \t1\ttrue\n",
            "(2)\t 442\t [-1.4311547  -0.57214963  1.5364882 ] \t2\ttrue\n",
            "(0)\t 443\t [ 2.4642835 -1.1959327 -1.0625705] \t0\ttrue\n",
            "(1)\t 444\t [-1.8498205  3.1567578 -1.4847664] \t1\ttrue\n",
            "(1)\t 445\t [-1.9529347   0.21745133  1.2656356 ] \t2\tfalse\n",
            "(2)\t 446\t [ 0.33299464 -0.6588246   0.41694137] \t2\ttrue\n",
            "(0)\t 447\t [ 2.1131995 -1.8471402 -0.1348906] \t0\ttrue\n",
            "(1)\t 448\t [-1.9081178  -0.01699683  1.4716095 ] \t2\tfalse\n",
            "(2)\t 449\t [-1.5186979  -0.32075793  1.3274839 ] \t2\ttrue\n",
            "(2)\t 450\t [ 0.21723092 -0.4407992   0.30645117] \t2\ttrue\n",
            "(2)\t 451\t [ 0.6721421  -0.8291462  -0.03494726] \t0\tfalse\n",
            "(0)\t 452\t [ 2.393204  -1.632864  -0.4240994] \t0\ttrue\n",
            "(1)\t 453\t [-2.5328684  2.359662  -0.295406 ] \t1\ttrue\n",
            "(0)\t 454\t [ 2.7756944 -1.6685653 -0.9454717] \t0\ttrue\n",
            "(1)\t 455\t [-1.965161   3.0600548 -1.4405171] \t1\ttrue\n",
            "(0)\t 456\t [ 1.8101497  -1.7095081   0.10935396] \t0\ttrue\n",
            "(1)\t 457\t [-2.561981   1.3891689  0.6885078] \t1\ttrue\n",
            "(0)\t 458\t [ 0.87375766 -0.73613644  0.11719254] \t0\ttrue\n",
            "(2)\t 459\t [ 0.87375766 -0.73613644  0.11719254] \t0\tfalse\n",
            "(0)\t 460\t [ 2.7932103 -1.5830009 -1.0317705] \t0\ttrue\n",
            "(1)\t 461\t [-1.7741184  3.1181953 -1.5081842] \t1\ttrue\n",
            "(2)\t 462\t [-0.01551015 -0.44973415  0.50104916] \t2\ttrue\n",
            "(0)\t 463\t [ 2.7795787  -1.6453518  -0.94743836] \t0\ttrue\n",
            "(1)\t 464\t [-1.8359715  3.1086833 -1.5076216] \t1\ttrue\n",
            "(0)\t 465\t [ 1.7459476  -1.7947131   0.11330466] \t0\ttrue\n",
            "(1)\t 466\t [-2.535889    2.1546633  -0.05883252] \t1\ttrue\n",
            "(0)\t 467\t [ 2.3235345  -0.69625264 -1.3650044 ] \t0\ttrue\n",
            "(1)\t 468\t [-1.8832265  3.1408925 -1.4589932] \t1\ttrue\n",
            "(0)\t 469\t [ 2.8240333  -1.7861696  -0.87804073] \t0\ttrue\n",
            "(1)\t 470\t [-1.7556765  3.1453702 -1.5938723] \t1\ttrue\n",
            "(0)\t 471\t [ 0.33572426 -0.7741993   0.5765955 ] \t2\tfalse\n",
            "(1)\t 472\t [ 0.33572426 -0.7741993   0.5765955 ] \t2\tfalse\n",
            "(0)\t 473\t [ 2.4140806  -1.9938384  -0.31521106] \t0\ttrue\n",
            "(1)\t 474\t [-2.5324545   1.4676669   0.39695212] \t1\ttrue\n",
            "(2)\t 475\t [-0.15601677 -0.29392377  0.42948747] \t2\ttrue\n",
            "(2)\t 476\t [ 0.20061035 -0.44901985  0.34293744] \t2\ttrue\n",
            "(0)\t 477\t [ 0.6877329  -0.91160125  0.07806004] \t0\ttrue\n",
            "(1)\t 478\t [ 0.6877329  -0.91160125  0.07806004] \t0\tfalse\n",
            "(0)\t 479\t [ 2.4411643 -1.6161891 -0.6956497] \t0\ttrue\n",
            "(1)\t 480\t [-2.4485142   2.6730263  -0.59107935] \t1\ttrue\n",
            "(0)\t 481\t [ 0.3396401  -0.9393584   0.71338135] \t2\tfalse\n",
            "(2)\t 482\t [ 0.3396401  -0.9393584   0.71338135] \t2\ttrue\n",
            "(0)\t 483\t [ 1.9879388 -1.6170546 -0.3248603] \t0\ttrue\n",
            "(1)\t 484\t [-2.1764183   0.43100798  1.1959621 ] \t2\tfalse\n",
            "(2)\t 485\t [-1.4020418 -0.4680875  1.4774489] \t2\ttrue\n",
            "(0)\t 486\t [ 2.7659106  -1.704653   -0.83389133] \t0\ttrue\n",
            "(1)\t 487\t [-1.7667441  3.1382484 -1.5449812] \t1\ttrue\n",
            "(0)\t 488\t [ 2.61389   -1.0482012 -1.2777026] \t0\ttrue\n",
            "(1)\t 489\t [-1.8775693  3.1437516 -1.41642  ] \t1\ttrue\n",
            "(0)\t 490\t [ 0.5242924  -0.4903052   0.11932924] \t0\ttrue\n",
            "(1)\t 491\t [ 0.5242924  -0.4903052   0.11932924] \t0\tfalse\n",
            "(2)\t 492\t [ 0.24279866 -0.34163558  0.19795744] \t0\tfalse\n",
            "(0)\t 493\t [ 2.8561504 -1.5715724 -1.0299122] \t0\ttrue\n",
            "(1)\t 494\t [-1.978068   3.124501  -1.4327521] \t1\ttrue\n",
            "(0)\t 495\t [ 2.6989067 -1.601037  -0.8903918] \t0\ttrue\n",
            "(1)\t 496\t [-2.0227106  3.160406  -1.283235 ] \t1\ttrue\n",
            "(0)\t 497\t [-2.5721824  1.3383753  0.5753117] \t1\tfalse\n",
            "(1)\t 498\t [-2.2462332   1.1865001   0.45853144] \t1\ttrue\n",
            "(0)\t 499\t [ 1.9352683  -1.923713   -0.06100815] \t0\ttrue\n",
            "(1)\t 500\t [-1.7779293  -0.07152694  1.3410604 ] \t2\tfalse\n",
            "(2)\t 501\t [ 0.47664958 -0.7722352   0.35266772] \t0\tfalse\n",
            "(0)\t 502\t [ 2.7796001 -1.52496   -1.0436798] \t0\ttrue\n",
            "(1)\t 503\t [-1.9529623  3.1148038 -1.4475121] \t1\ttrue\n",
            "(2)\t 504\t [-2.2515078   1.4275438   0.46030715] \t1\tfalse\n",
            "(0)\t 505\t [ 1.8999695  -1.7847161  -0.03822007] \t0\ttrue\n",
            "(1)\t 506\t [-2.476996    1.6082916   0.49625945] \t1\ttrue\n",
            "(0)\t 507\t [ 1.8666111  -1.7265047  -0.22742559] \t0\ttrue\n",
            "(1)\t 508\t [-1.5892814  -0.26954415  1.3307732 ] \t2\tfalse\n",
            "(0)\t 509\t [ 2.420979  -1.9463673 -0.4100271] \t0\ttrue\n",
            "(1)\t 510\t [-1.631518  -0.2044162  1.2559919] \t2\tfalse\n",
            "(0)\t 511\t [ 2.5030093  -1.8849045  -0.51097137] \t0\ttrue\n",
            "(1)\t 512\t [-2.4837282   2.6739407  -0.51731205] \t1\ttrue\n",
            "(0)\t 513\t [ 2.4226975  -1.6730233  -0.33958763] \t0\ttrue\n",
            "(1)\t 514\t [-1.7929695  -0.15863581  1.496342  ] \t2\tfalse\n",
            "(2)\t 515\t [ 0.08520439 -0.5463826   0.5347868 ] \t2\ttrue\n",
            "(2)\t 516\t [-0.74208367  0.35838556  0.23530804] \t1\tfalse\n",
            "(2)\t 517\t [ 0.31435734 -0.35827613  0.111239  ] \t0\tfalse\n",
            "(0)\t 518\t [ 0.42497084 -0.3964277   0.15909232] \t0\ttrue\n",
            "(1)\t 519\t [ 0.42497084 -0.3964277   0.15909232] \t0\tfalse\n",
            "(0)\t 520\t [ 1.9077915  -1.4560256  -0.30248988] \t0\ttrue\n",
            "(1)\t 521\t [-1.6860605   0.37941647  0.6583863 ] \t2\tfalse\n",
            "(0)\t 522\t [ 2.0532417e+00 -1.9020301e+00 -7.1458583e-04] \t0\ttrue\n",
            "(1)\t 523\t [-2.3514016  1.048551   0.9731743] \t1\ttrue\n",
            "(0)\t 524\t [-0.8308684 -0.9373048  1.3788558] \t2\tfalse\n",
            "(1)\t 525\t [-1.3655578  -0.27343103  1.249586  ] \t2\tfalse\n",
            "(0)\t 526\t [ 2.0115747 -1.7619787 -0.1454369] \t0\ttrue\n",
            "(1)\t 527\t [-2.5231335   1.7626876   0.43325496] \t1\ttrue\n",
            "(2)\t 528\t [ 0.32462564 -0.6614573   0.44709656] \t2\ttrue\n",
            "(2)\t 529\t [ 0.43791187 -0.81687796  0.17938098] \t0\tfalse\n",
            "(0)\t 530\t [ 1.8178498  -1.6503114  -0.18038541] \t0\ttrue\n",
            "(1)\t 531\t [-1.2610193  -0.44109282  1.2692167 ] \t2\tfalse\n",
            "(2)\t 532\t [-1.3044497 -0.6782793  1.6010536] \t2\ttrue\n",
            "(0)\t 533\t [ 1.1367468 -0.628613  -0.3551852] \t0\ttrue\n",
            "(1)\t 534\t [-1.9868596   0.16569316  1.3047476 ] \t2\tfalse\n",
            "(0)\t 535\t [ 2.2039754  -1.8162022  -0.20209566] \t0\ttrue\n",
            "(1)\t 536\t [-1.8527517   0.04631874  1.5078259 ] \t2\tfalse\n",
            "(2)\t 537\t [-1.9974862   0.48385084  1.2738978 ] \t2\ttrue\n",
            "(0)\t 538\t [ 2.7483711 -1.5162064 -1.1701066] \t0\ttrue\n",
            "(1)\t 539\t [-1.6676718  3.0936775 -1.6329864] \t1\ttrue\n",
            "(0)\t 540\t [ 2.0877776  -1.8730265  -0.11748444] \t0\ttrue\n",
            "(1)\t 541\t [-2.5641735  2.6626155 -0.6237787] \t1\ttrue\n",
            "(2)\t 542\t [ 0.4716613  -0.866611    0.09309639] \t0\tfalse\n",
            "(0)\t 543\t [ 1.5922619  -1.6465827   0.15041177] \t0\ttrue\n",
            "(1)\t 544\t [-1.4831245 -0.3661642  1.4628174] \t2\tfalse\n",
            "(0)\t 545\t [ 2.7537634 -1.5147808 -1.1481755] \t0\ttrue\n",
            "(1)\t 546\t [-1.796681   3.125854  -1.5236186] \t1\ttrue\n",
            "(0)\t 547\t [ 2.716725  -1.8411932 -0.843983 ] \t0\ttrue\n",
            "(1)\t 548\t [-1.9215728  3.10542   -1.3663831] \t1\ttrue\n",
            "(0)\t 549\t [ 1.2246042  -1.251716    0.13615508] \t0\ttrue\n",
            "(1)\t 550\t [-1.81993     0.10164462  1.1336839 ] \t2\tfalse\n",
            "(0)\t 551\t [ 0.24797487 -0.5268977   0.36004156] \t2\tfalse\n",
            "(1)\t 552\t [ 0.24797487 -0.5268977   0.36004156] \t2\tfalse\n",
            "(2)\t 553\t [ 0.2744148  -0.6005491   0.45270902] \t2\ttrue\n",
            "(2)\t 554\t [ 0.31186265 -0.5839694   0.45066655] \t2\ttrue\n",
            "(2)\t 555\t [ 0.9466854 -0.5160789 -0.6800429] \t0\tfalse\n",
            "(0)\t 556\t [ 1.8127611  -1.9260093   0.22776316] \t0\ttrue\n",
            "(1)\t 557\t [-2.4313648  1.4870347  0.6409211] \t1\ttrue\n",
            "(0)\t 558\t [ 2.541035  -1.1213347 -0.961375 ] \t0\ttrue\n",
            "(1)\t 559\t [-1.8278595  3.0970914 -1.4756532] \t1\ttrue\n",
            "(2)\t 560\t [ 0.07687383 -1.2290596   0.7482617 ] \t2\ttrue\n",
            "(2)\t 561\t [-2.0893385  0.4574209  1.202671 ] \t2\ttrue\n",
            "(2)\t 562\t [ 0.10406403 -0.46368557  0.4196313 ] \t2\ttrue\n",
            "(0)\t 563\t [ 0.31662273 -0.735275    0.57731515] \t2\tfalse\n",
            "(1)\t 564\t [ 0.31662273 -0.735275    0.57731515] \t2\tfalse\n",
            "(0)\t 565\t [ 0.249185   -0.76937634  0.60100996] \t2\tfalse\n",
            "(1)\t 566\t [ 0.249185   -0.76937634  0.60100996] \t2\tfalse\n",
            "(2)\t 567\t [-2.4209325  1.1297629  0.7558514] \t1\tfalse\n",
            "(2)\t 568\t [ 0.44997892 -0.6176583   0.35522604] \t0\tfalse\n",
            "(2)\t 569\t [ 0.22262844 -0.8789757   0.2868424 ] \t2\ttrue\n",
            "(2)\t 570\t [ 0.30367306 -0.2920121   0.16202414] \t0\tfalse\n",
            "(0)\t 571\t [ 0.28528437 -0.6307516   0.4948239 ] \t2\tfalse\n",
            "(1)\t 572\t [ 0.28528437 -0.6307516   0.4948239 ] \t2\tfalse\n",
            "(0)\t 573\t [ 2.7970994 -1.3658454 -1.2576225] \t0\ttrue\n",
            "(1)\t 574\t [-1.7775006  3.1258352 -1.5272379] \t1\ttrue\n",
            "(2)\t 575\t [ 0.45694873 -0.5286609   0.26767412] \t0\tfalse\n",
            "(0)\t 576\t [ 0.5551256  -0.9723514   0.20598109] \t0\ttrue\n",
            "(1)\t 577\t [ 0.5551256  -0.9723514   0.20598109] \t0\tfalse\n",
            "(2)\t 578\t [-1.7132996  0.621813   0.8176607] \t2\ttrue\n",
            "(0)\t 579\t [-0.05698833 -0.15898919  0.34348592] \t2\tfalse\n",
            "(1)\t 580\t [-0.05698833 -0.15898919  0.34348592] \t2\tfalse\n",
            "(2)\t 581\t [ 0.3843161  -0.88862175  0.26208672] \t0\tfalse\n",
            "(0)\t 582\t [ 0.33335844 -0.6958976   0.11749546] \t0\ttrue\n",
            "(1)\t 583\t [ 0.33335844 -0.6958976   0.11749546] \t0\tfalse\n",
            "(0)\t 584\t [ 0.26014802 -0.21387061  0.05714171] \t0\ttrue\n",
            "(1)\t 585\t [ 0.26014802 -0.21387061  0.05714171] \t0\tfalse\n",
            "(0)\t 586\t [ 0.20263642 -0.3444748   0.26939183] \t2\tfalse\n",
            "(1)\t 587\t [ 0.20263642 -0.3444748   0.26939183] \t2\tfalse\n",
            "(2)\t 588\t [ 0.11106901 -0.57493895  0.57688195] \t2\ttrue\n",
            "(2)\t 589\t [-1.6200202 -0.2277592  1.4395918] \t2\ttrue\n",
            "(2)\t 590\t [ 0.3672023  -0.5037706   0.26253566] \t0\tfalse\n",
            "(2)\t 591\t [ 0.38817856 -0.7407373   0.47489232] \t2\ttrue\n",
            "(0)\t 592\t [ 2.106004   -1.8814275  -0.28945667] \t0\ttrue\n",
            "(1)\t 593\t [-1.5231425 -0.422584   1.5200498] \t2\tfalse\n",
            "(2)\t 594\t [ 0.18789065 -0.7740961   0.68717444] \t2\ttrue\n",
            "(0)\t 595\t [ 2.1277425  -1.5593363  -0.18730107] \t0\ttrue\n",
            "(1)\t 596\t [-1.5192889 -0.3256708  1.4812623] \t2\tfalse\n",
            "(2)\t 597\t [ 0.24437498 -0.31817633  0.21875228] \t0\tfalse\n",
            "(0)\t 598\t [ 2.4668713 -1.6381747 -0.6463104] \t0\ttrue\n",
            "(1)\t 599\t [-2.2187562  3.0366607 -1.0946556] \t1\ttrue\n",
            "(0)\t 600\t [ 0.04347147 -0.920374    0.5842071 ] \t2\tfalse\n",
            "(1)\t 601\t [ 0.04347147 -0.920374    0.5842071 ] \t2\tfalse\n",
            "(0)\t 602\t [ 2.6953943 -1.6643949 -0.8745496] \t0\ttrue\n",
            "(1)\t 603\t [-2.4679198   2.6159756  -0.50749016] \t1\ttrue\n",
            "(2)\t 604\t [ 0.29255235 -0.854687    0.40911612] \t2\ttrue\n",
            "(2)\t 605\t [-1.6848774  -0.20333561  1.3756886 ] \t2\ttrue\n",
            "(2)\t 606\t [ 0.39016077 -0.541964    0.3054706 ] \t0\tfalse\n",
            "(2)\t 607\t [ 0.29246292 -1.1538063   0.5759693 ] \t2\ttrue\n",
            "(0)\t 608\t [ 0.13350871 -0.31398413  0.28895077] \t2\tfalse\n",
            "(1)\t 609\t [ 0.13350871 -0.31398413  0.28895077] \t2\tfalse\n",
            "(2)\t 610\t [ 0.30222902 -0.80907524  0.6293609 ] \t2\ttrue\n",
            "(2)\t 611\t [ 0.5957795  -0.704668    0.28296006] \t0\tfalse\n",
            "(2)\t 612\t [ 0.7765786  -0.6875028   0.16688533] \t0\tfalse\n",
            "(0)\t 613\t [ 2.687065  -1.3496593 -1.0623884] \t0\ttrue\n",
            "(1)\t 614\t [-2.0365179  3.1408286 -1.3191189] \t1\ttrue\n",
            "(2)\t 615\t [ 0.3322415  -0.7816553   0.63528466] \t2\ttrue\n",
            "(0)\t 616\t [ 2.718343   -1.8095423  -0.77516925] \t0\ttrue\n",
            "(1)\t 617\t [-1.9870819  3.0971258 -1.2325296] \t1\ttrue\n",
            "(0)\t 618\t [ 2.4880726  -1.8038105  -0.41775557] \t0\ttrue\n",
            "(1)\t 619\t [-2.5072618  1.5795146  0.5537419] \t1\ttrue\n",
            "(2)\t 620\t [ 0.28858468 -0.6483309   0.50129926] \t2\ttrue\n",
            "(2)\t 621\t [ 0.28898558 -0.80865943  0.57573164] \t2\ttrue\n",
            "(0)\t 622\t [ 2.658536  -1.7628053 -0.8955911] \t0\ttrue\n",
            "(1)\t 623\t [-1.9411438  3.1345732 -1.3739741] \t1\ttrue\n",
            "(0)\t 624\t [ 1.9190195  -1.8022811  -0.13643439] \t0\ttrue\n",
            "(1)\t 625\t [-2.3995671   1.3333604   0.48488533] \t1\ttrue\n",
            "(0)\t 626\t [ 2.4661717  -1.9969319  -0.43116516] \t0\ttrue\n",
            "(1)\t 627\t [-1.9429778  0.5330284  1.0467626] \t2\tfalse\n",
            "(0)\t 628\t [ 2.5615082  -1.8892119  -0.59419656] \t0\ttrue\n",
            "(1)\t 629\t [-2.234788   1.1257885  0.5512091] \t1\ttrue\n",
            "(0)\t 630\t [ 2.3965003 -1.2390928 -1.0494257] \t0\ttrue\n",
            "(1)\t 631\t [-2.106683   3.1446466 -1.2271805] \t1\ttrue\n",
            "(0)\t 632\t [ 2.3730102 -1.68265   -0.441064 ] \t0\ttrue\n",
            "(1)\t 633\t [-2.3117924   1.7909117   0.03645096] \t1\ttrue\n",
            "(2)\t 634\t [ 0.7010304  -0.627137    0.05766336] \t0\tfalse\n",
            "(2)\t 635\t [ 0.28044817 -0.4070103   0.20080018] \t0\tfalse\n",
            "(2)\t 636\t [ 0.18678285 -0.72374684  0.53448653] \t2\ttrue\n",
            "(2)\t 637\t [-1.6087571  -0.15753582  1.2809172 ] \t2\ttrue\n",
            "(2)\t 638\t [ 0.14643402 -0.80970883  0.74149925] \t2\ttrue\n",
            "(2)\t 639\t [ 0.6592088 -1.110596   0.3534097] \t0\tfalse\n",
            "(0)\t 640\t [ 1.7790524  -1.8326532   0.19011134] \t0\ttrue\n",
            "(1)\t 641\t [-2.382264   0.8845431  1.0094078] \t2\tfalse\n",
            "(2)\t 642\t [-1.1540198 -0.5678876  1.3131535] \t2\ttrue\n",
            "(2)\t 643\t [ 0.25171432 -0.93813425  0.767283  ] \t2\ttrue\n",
            "(0)\t 644\t [ 1.915752   -1.9109132   0.12759535] \t0\ttrue\n",
            "(1)\t 645\t [-1.9668533  -0.02577686  1.565972  ] \t2\tfalse\n",
            "(2)\t 646\t [-0.6643117   0.492234    0.11953363] \t1\tfalse\n",
            "(0)\t 647\t [ 1.9629328  -1.7082008  -0.17690118] \t0\ttrue\n",
            "(1)\t 648\t [-2.5180888   2.1198645   0.12633197] \t1\ttrue\n",
            "(0)\t 649\t [ 0.2908267  -0.5238764   0.33595636] \t2\tfalse\n",
            "(1)\t 650\t [ 0.2908267  -0.5238764   0.33595636] \t2\tfalse\n",
            "(2)\t 651\t [ 0.39420775 -0.6070447   0.35752714] \t0\tfalse\n",
            "(2)\t 652\t [ 0.21748099 -0.4055196   0.21757555] \t2\ttrue\n",
            "(0)\t 653\t [ 2.753553  -1.4624494 -1.143769 ] \t0\ttrue\n",
            "(1)\t 654\t [-1.7381971  3.1107605 -1.5233517] \t1\ttrue\n",
            "(1)\t 655\t [-2.6281292   1.9814575   0.23411375] \t1\ttrue\n",
            "(2)\t 656\t [ 0.27904335 -0.49702457  0.34766114] \t2\ttrue\n",
            "(2)\t 657\t [-1.551732   -0.02506808  0.9739288 ] \t2\ttrue\n",
            "(0)\t 658\t [ 0.70869476 -0.85650957  0.06409025] \t0\ttrue\n",
            "(1)\t 659\t [ 0.70869476 -0.85650957  0.06409025] \t0\tfalse\n",
            "(2)\t 660\t [-2.1102338   0.47529376  1.2234288 ] \t2\ttrue\n",
            "(0)\t 661\t [ 2.753946  -1.3925632 -1.2708817] \t0\ttrue\n",
            "(1)\t 662\t [-1.788159   3.1306086 -1.531653 ] \t1\ttrue\n",
            "(0)\t 663\t [ 0.62580085 -0.4858768  -0.01563415] \t0\ttrue\n",
            "(1)\t 664\t [ 0.62580085 -0.4858768  -0.01563415] \t0\tfalse\n",
            "(0)\t 665\t [ 2.8435447 -1.5844219 -1.0763341] \t0\ttrue\n",
            "(1)\t 666\t [-1.8827868  3.18284   -1.4966711] \t1\ttrue\n",
            "(2)\t 667\t [ 0.2406533  -0.80558014  0.67700595] \t2\ttrue\n",
            "(0)\t 668\t [ 0.43851024 -0.7167945   0.11882957] \t0\ttrue\n",
            "(1)\t 669\t [ 0.43851024 -0.7167945   0.11882957] \t0\tfalse\n",
            "(0)\t 670\t [ 2.729822  -1.7119428 -0.9600691] \t0\ttrue\n",
            "(1)\t 671\t [-2.1501358  2.758523  -0.9809905] \t1\ttrue\n",
            "(0)\t 672\t [ 2.6758013 -1.5259367 -0.869313 ] \t0\ttrue\n",
            "(1)\t 673\t [-2.3447278   2.3993793  -0.48289314] \t1\ttrue\n",
            "(0)\t 674\t [ 2.7561917 -1.7534221 -0.896297 ] \t0\ttrue\n",
            "(1)\t 675\t [-2.0307243  3.1651592 -1.2308997] \t1\ttrue\n",
            "(0)\t 676\t [ 2.7971504 -1.6893395 -0.8783083] \t0\ttrue\n",
            "(1)\t 677\t [-1.8044947  3.1761808 -1.6054738] \t1\ttrue\n",
            "(2)\t 678\t [ 0.28298151 -0.54225326  0.34079346] \t2\ttrue\n",
            "(0)\t 679\t [ 2.4259455  -1.8489374  -0.49904805] \t0\ttrue\n",
            "(1)\t 680\t [-2.581068    1.8502784   0.31206983] \t1\ttrue\n",
            "(0)\t 681\t [ 2.7249877 -1.6713617 -0.8060261] \t0\ttrue\n",
            "(1)\t 682\t [-1.8020797  3.1210158 -1.5570909] \t1\ttrue\n",
            "(0)\t 683\t [ 2.5986614 -1.9286796 -0.590978 ] \t0\ttrue\n",
            "(1)\t 684\t [-2.2855656  1.0019931  0.9839954] \t1\ttrue\n",
            "(2)\t 685\t [-1.423244   -0.42611372  1.4578224 ] \t2\ttrue\n",
            "(0)\t 686\t [ 2.7123983  -1.8693877  -0.71819293] \t0\ttrue\n",
            "(1)\t 687\t [-2.0141928  3.1343226 -1.3828442] \t1\ttrue\n",
            "(0)\t 688\t [ 2.5436215 -1.4307739 -0.9040258] \t0\ttrue\n",
            "(1)\t 689\t [-2.4035027   2.7042441  -0.75824934] \t1\ttrue\n",
            "(2)\t 690\t [-1.5826343  0.2750619  0.9071045] \t2\ttrue\n",
            "(0)\t 691\t [ 0.2380525 -0.6930012  0.5758151] \t2\tfalse\n",
            "(1)\t 692\t [ 0.2380525 -0.6930012  0.5758151] \t2\tfalse\n",
            "(0)\t 693\t [ 2.1609924  -1.815958   -0.20234126] \t0\ttrue\n",
            "(1)\t 694\t [-2.5620053   2.6934395  -0.50415176] \t1\ttrue\n",
            "(2)\t 695\t [-0.41264766  0.0983393   0.02574779] \t1\tfalse\n",
            "(2)\t 696\t [ 0.53542113 -0.65573776  0.31014436] \t0\tfalse\n",
            "(2)\t 697\t [ 0.4033915  -0.6479294   0.36013973] \t0\tfalse\n",
            "(0)\t 698\t [ 0.19377993 -0.57608306  0.48071346] \t2\tfalse\n",
            "(1)\t 699\t [ 0.19377993 -0.57608306  0.48071346] \t2\tfalse\n",
            "(0)\t 700\t [ 1.830464   -1.5367076  -0.17648546] \t0\ttrue\n",
            "(1)\t 701\t [-2.5434053  2.4790735 -0.3710131] \t1\ttrue\n",
            "(0)\t 702\t [ 2.0483348  -1.9080673  -0.07314675] \t0\ttrue\n",
            "(1)\t 703\t [-2.0844874   0.21378915  1.338974  ] \t2\tfalse\n",
            "(0)\t 704\t [ 2.8010147 -1.4605589 -1.17873  ] \t0\ttrue\n",
            "(1)\t 705\t [-1.7956842  3.1738687 -1.5871718] \t1\ttrue\n",
            "(0)\t 706\t [ 2.5845902  -1.8200768  -0.41384563] \t0\ttrue\n",
            "(1)\t 707\t [-2.2447772  3.1697085 -1.1527672] \t1\ttrue\n",
            "(0)\t 708\t [ 0.3324981  -0.67428535  0.48905146] \t2\tfalse\n",
            "(1)\t 709\t [ 0.3324981  -0.67428535  0.48905146] \t2\tfalse\n",
            "(0)\t 710\t [ 1.974013  -1.6253929 -0.1692875] \t0\ttrue\n",
            "(1)\t 711\t [-2.327342    2.9650974  -0.95343316] \t1\ttrue\n",
            "(0)\t 712\t [ 2.165311   -1.9686506  -0.11438452] \t0\ttrue\n",
            "(1)\t 713\t [-2.5887632   2.4001718  -0.15083775] \t1\ttrue\n",
            "(0)\t 714\t [ 2.1022723  -1.8032368  -0.20781909] \t0\ttrue\n",
            "(1)\t 715\t [-2.3178356  2.9459534 -0.8046128] \t1\ttrue\n",
            "(2)\t 716\t [-1.758197   -0.23797195  1.4746326 ] \t2\ttrue\n",
            "(2)\t 717\t [ 0.2045216 -0.8971511  0.7492685] \t2\ttrue\n",
            "(2)\t 718\t [-1.3350278 -0.3127936  1.0963186] \t2\ttrue\n",
            "(0)\t 719\t [ 2.759258  -1.4277064 -1.0464665] \t0\ttrue\n",
            "(1)\t 720\t [-1.8610305  3.1682332 -1.5277755] \t1\ttrue\n",
            "(0)\t 721\t [ 2.788793  -1.6526455 -1.0222352] \t0\ttrue\n",
            "(1)\t 722\t [-1.8750827  3.1608567 -1.398713 ] \t1\ttrue\n",
            "(0)\t 723\t [ 2.2272391  -1.8401989  -0.36184096] \t0\ttrue\n",
            "(1)\t 724\t [-2.0893059   0.35822868  1.1699458 ] \t2\tfalse\n",
            "(0)\t 725\t [ 0.2977341 -0.4488239  0.2756038] \t0\ttrue\n",
            "(2)\t 726\t [ 0.2977341 -0.4488239  0.2756038] \t0\tfalse\n",
            "(0)\t 727\t [ 2.6733255 -1.7383476 -1.0173321] \t0\ttrue\n",
            "(1)\t 728\t [-2.1304471  3.1194122 -1.2341894] \t1\ttrue\n",
            "(0)\t 729\t [ 0.5492826  -1.269765    0.46686405] \t0\ttrue\n",
            "(1)\t 730\t [-1.6247051  -0.32285777  1.5862684 ] \t2\tfalse\n",
            "(0)\t 731\t [ 2.7442079  -1.76284    -0.78833514] \t0\ttrue\n",
            "(1)\t 732\t [-2.3139067  3.0931373 -1.1264315] \t1\ttrue\n",
            "(0)\t 733\t [ 2.1635408 -1.8945649 -0.1112147] \t0\ttrue\n",
            "(1)\t 734\t [-1.5545598  -0.17386828  1.2524664 ] \t2\tfalse\n",
            "(0)\t 735\t [ 2.4335415 -1.5269566 -0.6577968] \t0\ttrue\n",
            "(1)\t 736\t [-2.531809   2.5981529 -0.4263501] \t1\ttrue\n",
            "(2)\t 737\t [ 0.6922563  -0.937991    0.37592652] \t0\tfalse\n",
            "(2)\t 738\t [ 0.35127482 -0.7045288   0.41802457] \t2\ttrue\n",
            "(0)\t 739\t [ 2.6443014  -1.5789373  -0.94293046] \t0\ttrue\n",
            "(1)\t 740\t [-1.8233359  3.1100981 -1.4932017] \t1\ttrue\n",
            "(0)\t 741\t [ 2.3768737  -1.4428067  -0.82492906] \t0\ttrue\n",
            "(1)\t 742\t [-2.5406213  2.856806  -0.6900066] \t1\ttrue\n",
            "(2)\t 743\t [-2.3772395  1.0669571  0.9279036] \t1\tfalse\n",
            "(2)\t 744\t [ 0.15543409 -1.1285532   1.0106304 ] \t2\ttrue\n",
            "(0)\t 745\t [ 2.2952902 -1.7589495 -0.5782348] \t0\ttrue\n",
            "(1)\t 746\t [-2.4625304  2.8857293 -0.8823807] \t1\ttrue\n",
            "(0)\t 747\t [ 2.0799375  -1.8877077  -0.11927376] \t0\ttrue\n",
            "(1)\t 748\t [-1.959344   0.1901813  1.4332721] \t2\tfalse\n",
            "(0)\t 749\t [ 0.57578593 -0.8201344   0.33232683] \t0\ttrue\n",
            "(1)\t 750\t [ 0.57578593 -0.8201344   0.33232683] \t0\tfalse\n",
            "Number of true predictions: 524\n",
            "Number of false predictions: 226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "754e319a-a292-427b-a6cd-737e0489b134"
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 69.77363515312916 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgrPoBuDDz_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33068609-8a10-4bc9-b2e7-e54f87d50855"
      },
      "source": [
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "scores = [3.0, 1.0, 0.2]\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    predictions[i][j]=softmax(predictions[i][j])\n",
        "    print(predictions[i][j])\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.41232565 0.11755753 0.4701168 ]\n",
            "[0.41232565 0.11755753 0.4701168 ]\n",
            "[0.9582469  0.01209669 0.02965647]\n",
            "[0.00444044 0.9748048  0.02075466]\n",
            "[0.81589925 0.03160664 0.15249409]\n",
            "[0.02124395 0.29751134 0.68124473]\n",
            "[0.9577505  0.0153407  0.02690883]\n",
            "[0.0054129  0.9810122  0.01357483]\n",
            "[0.8695658  0.02131357 0.1091206 ]\n",
            "[0.02957209 0.1748291  0.79559886]\n",
            "[0.7970228  0.02981026 0.17316686]\n",
            "[0.01253478 0.6975733  0.28989184]\n",
            "[0.94059676 0.01455382 0.04484938]\n",
            "[0.01597571 0.8085399  0.17548434]\n",
            "[0.0715059  0.18399988 0.7444942 ]\n",
            "[0.8895764  0.01753566 0.09288803]\n",
            "[0.0287244  0.18229882 0.7889768 ]\n",
            "[0.04386554 0.10153471 0.8545998 ]\n",
            "[0.9536214  0.01224842 0.03413024]\n",
            "[0.03356393 0.29363468 0.67280143]\n",
            "[0.41646174 0.17522761 0.40831065]\n",
            "[0.32532427 0.1261119  0.5485639 ]\n",
            "[0.8448213  0.02057385 0.13460487]\n",
            "[0.02884135 0.16134006 0.8098186 ]\n",
            "[0.38087887 0.17821254 0.4409086 ]\n",
            "[0.38087887 0.17821254 0.4409086 ]\n",
            "[0.40902048 0.17682682 0.41415274]\n",
            "[0.40902048 0.17682682 0.41415274]\n",
            "[0.959073   0.01526189 0.02566507]\n",
            "[0.0057828  0.98348874 0.01072848]\n",
            "[0.80993307 0.03686122 0.15320571]\n",
            "[0.43407774 0.19542578 0.37049642]\n",
            "[0.02760149 0.5003466  0.4720519 ]\n",
            "[0.83317816 0.02532395 0.14149787]\n",
            "[0.01577154 0.7543747  0.22985378]\n",
            "[0.37713617 0.1375774  0.48528644]\n",
            "[0.37713617 0.1375774  0.48528644]\n",
            "[0.91980916 0.01251093 0.06767994]\n",
            "[0.00915715 0.8901009  0.10074203]\n",
            "[0.03622667 0.10366886 0.8601045 ]\n",
            "[0.41610447 0.11581568 0.46807992]\n",
            "[0.83715683 0.01986597 0.14297718]\n",
            "[0.05217598 0.15251297 0.795311  ]\n",
            "[0.04659491 0.11343642 0.83996874]\n",
            "[0.94686323 0.01079024 0.04234646]\n",
            "[0.0056766  0.9708727  0.02345074]\n",
            "[0.06867309 0.12264442 0.80868244]\n",
            "[0.24653196 0.46715006 0.28631788]\n",
            "[0.24653196 0.46715006 0.28631788]\n",
            "[0.2783783  0.09507544 0.6265462 ]\n",
            "[0.02493258 0.17833365 0.79673374]\n",
            "[0.35055864 0.10204531 0.54739606]\n",
            "[0.35055864 0.10204531 0.54739606]\n",
            "[0.0431382  0.08660389 0.87025785]\n",
            "[0.37197843 0.13004106 0.49798056]\n",
            "[0.95557123 0.00971722 0.03471156]\n",
            "[0.01292239 0.70729244 0.27978516]\n",
            "[0.3661988  0.11345911 0.52034205]\n",
            "[0.8311805  0.02663589 0.14218357]\n",
            "[0.01926223 0.50703347 0.47370425]\n",
            "[0.35063598 0.12185225 0.52751184]\n",
            "[0.9646096  0.01232425 0.02306619]\n",
            "[0.00549973 0.98072195 0.01377838]\n",
            "[0.9343127  0.01318024 0.0525071 ]\n",
            "[0.02532926 0.4294755  0.5451952 ]\n",
            "[0.84052414 0.02306161 0.13641421]\n",
            "[0.02718251 0.15926385 0.8135536 ]\n",
            "[0.37669268 0.18238732 0.44092   ]\n",
            "[0.6265372  0.09844442 0.27501836]\n",
            "[0.04384698 0.21162267 0.7445304 ]\n",
            "[0.44554323 0.08106687 0.47338995]\n",
            "[0.44554323 0.08106687 0.47338995]\n",
            "[0.18703331 0.21881816 0.5941485 ]\n",
            "[0.42707705 0.19165242 0.3812705 ]\n",
            "[0.42707705 0.19165242 0.3812705 ]\n",
            "[0.39610797 0.2763214  0.32757065]\n",
            "[0.39610797 0.2763214  0.32757065]\n",
            "[0.03848808 0.10424436 0.85726756]\n",
            "[0.02835143 0.25889382 0.7127548 ]\n",
            "[0.02164254 0.39512804 0.58322936]\n",
            "[0.4457151  0.14670722 0.40757763]\n",
            "[0.4457151  0.14670722 0.40757763]\n",
            "[0.4306575  0.17872396 0.39061847]\n",
            "[0.8056325  0.03151985 0.16284767]\n",
            "[0.02126195 0.46994466 0.50879335]\n",
            "[0.9568381  0.01146048 0.03170142]\n",
            "[0.00681922 0.98223096 0.01094974]\n",
            "[0.42553645 0.16836284 0.4061007 ]\n",
            "[0.39983222 0.20374823 0.3964195 ]\n",
            "[0.03459265 0.24697706 0.7184303 ]\n",
            "[0.39803728 0.13181332 0.47014946]\n",
            "[0.39803728 0.13181332 0.47014946]\n",
            "[0.96282554 0.0101999  0.0269745 ]\n",
            "[0.00660545 0.9093126  0.08408193]\n",
            "[0.11114207 0.17312531 0.71573263]\n",
            "[0.8299204  0.02975603 0.14032352]\n",
            "[0.01907136 0.48522526 0.4957034 ]\n",
            "[0.33909824 0.3239065  0.33699518]\n",
            "[0.02370995 0.31108874 0.6652013 ]\n",
            "[0.85869575 0.0191406  0.12216362]\n",
            "[0.03785078 0.11250192 0.84964734]\n",
            "[0.11536725 0.8040348  0.08059785]\n",
            "[0.01517416 0.79511666 0.18970914]\n",
            "[0.4138893  0.17032148 0.41578925]\n",
            "[0.4138893  0.17032148 0.41578925]\n",
            "[0.01410569 0.6625293  0.32336497]\n",
            "[0.04497747 0.28259864 0.6724239 ]\n",
            "[0.7496038  0.03308999 0.2173062 ]\n",
            "[0.00729056 0.90320826 0.08950112]\n",
            "[0.01984622 0.47311944 0.5070343 ]\n",
            "[0.02970382 0.23585059 0.7344456 ]\n",
            "[0.9462494  0.01012776 0.0436228 ]\n",
            "[0.00645036 0.9230251  0.0705245 ]\n",
            "[0.9489514  0.01050584 0.04054284]\n",
            "[0.00470265 0.9723159  0.02298146]\n",
            "[0.04142176 0.84565336 0.11292487]\n",
            "[0.40564764 0.14170058 0.45265177]\n",
            "[0.50866795 0.10599101 0.38534105]\n",
            "[0.06392576 0.14720178 0.7888725 ]\n",
            "[0.03446069 0.14357083 0.82196844]\n",
            "[0.21032207 0.14539197 0.644286  ]\n",
            "[0.03630351 0.11382114 0.84987533]\n",
            "[0.29718208 0.07487758 0.62794036]\n",
            "[0.03653121 0.11114141 0.8523274 ]\n",
            "[0.9169919  0.01008255 0.07292553]\n",
            "[0.00882636 0.86649346 0.12468012]\n",
            "[0.8224116  0.02375931 0.15382913]\n",
            "[0.0064477 0.9056125 0.0879398]\n",
            "[0.82180226 0.02320312 0.1549946 ]\n",
            "[0.0135505  0.60538447 0.381065  ]\n",
            "[0.9120526  0.01355466 0.07439276]\n",
            "[0.01758876 0.5444853  0.43792593]\n",
            "[0.9583917  0.01303063 0.0285777 ]\n",
            "[0.0078376  0.9179788  0.07418361]\n",
            "[0.35578644 0.1882969  0.45591658]\n",
            "[0.04053067 0.09571217 0.86375713]\n",
            "[0.42153674 0.11637987 0.46208337]\n",
            "[0.03008207 0.24130097 0.72861695]\n",
            "[0.0143913 0.6005823 0.3850264]\n",
            "[0.92570937 0.01651568 0.05777497]\n",
            "[0.04396628 0.10342191 0.8526118 ]\n",
            "[0.41003206 0.15664518 0.43332276]\n",
            "[0.4121685  0.14659353 0.44123802]\n",
            "[0.41578525 0.16025203 0.42396265]\n",
            "[0.95909464 0.01319344 0.02771196]\n",
            "[0.01005    0.96090573 0.02904428]\n",
            "[0.5220815  0.18019946 0.29771906]\n",
            "[0.4914538  0.14080656 0.3677397 ]\n",
            "[0.3537887  0.11644291 0.5297684 ]\n",
            "[0.03346173 0.15485908 0.8116791 ]\n",
            "[0.04086551 0.12205132 0.83708316]\n",
            "[0.05120976 0.09497012 0.8538201 ]\n",
            "[0.13129777 0.7709474  0.09775475]\n",
            "[0.5136404  0.12679939 0.3595602 ]\n",
            "[0.5136404  0.12679939 0.3595602 ]\n",
            "[0.94608283 0.01829342 0.03562374]\n",
            "[0.00521964 0.9788729  0.01590741]\n",
            "[0.9557198  0.01097827 0.03330189]\n",
            "[0.02531617 0.6466608  0.32802305]\n",
            "[0.7275444  0.03327752 0.23917805]\n",
            "[0.01286325 0.6972215  0.28991526]\n",
            "[0.46199217 0.18605612 0.35195175]\n",
            "[0.31186634 0.2318987  0.45623496]\n",
            "[0.03147172 0.12971047 0.83881783]\n",
            "[0.57426274 0.10681434 0.31892294]\n",
            "[0.05334052 0.29627797 0.6503815 ]\n",
            "[0.01523274 0.6125775  0.37218976]\n",
            "[0.95114434 0.01113285 0.03772282]\n",
            "[0.01208449 0.88141537 0.10650023]\n",
            "[0.27993098 0.16181001 0.558259  ]\n",
            "[0.4034473  0.19857979 0.39797288]\n",
            "[0.9562     0.01440732 0.02939275]\n",
            "[0.16152494 0.74564165 0.0928334 ]\n",
            "[0.03592132 0.11443026 0.8496485 ]\n",
            "[0.06727944 0.13177508 0.80094546]\n",
            "[0.20689432 0.09318551 0.6999202 ]\n",
            "[0.941109   0.01350564 0.04538538]\n",
            "[0.06378068 0.24257314 0.69364625]\n",
            "[0.28278202 0.29298213 0.4242358 ]\n",
            "[0.03377582 0.1728113  0.79341286]\n",
            "[0.7099559  0.11492097 0.17512311]\n",
            "[0.5513963  0.05395748 0.39464626]\n",
            "[0.041546   0.25777242 0.7006816 ]\n",
            "[0.88380593 0.05735835 0.05883569]\n",
            "[0.01555946 0.8586258  0.12581468]\n",
            "[0.3750239  0.15654619 0.46842995]\n",
            "[0.4084262  0.17078885 0.42078495]\n",
            "[0.04142904 0.12194671 0.83662426]\n",
            "[0.44103727 0.20629123 0.35267147]\n",
            "[0.1262264  0.26345244 0.61032116]\n",
            "[0.822344   0.01949243 0.1581636 ]\n",
            "[0.00677094 0.98495907 0.00826993]\n",
            "[0.96202713 0.01529275 0.02268012]\n",
            "[0.00677077 0.98408854 0.0091406 ]\n",
            "[0.43465447 0.13842322 0.42692232]\n",
            "[0.92625326 0.01405307 0.05969366]\n",
            "[0.00577166 0.9479814  0.04624695]\n",
            "[0.6710691  0.03734433 0.29158652]\n",
            "[0.00518106 0.9672278  0.02759116]\n",
            "[0.96363693 0.01279301 0.02357005]\n",
            "[0.00797929 0.98219496 0.00982573]\n",
            "[0.86746716 0.02312488 0.10940798]\n",
            "[0.011554   0.7302084  0.25823757]\n",
            "[0.04069712 0.13983819 0.81946474]\n",
            "[0.91406053 0.02046701 0.06547245]\n",
            "[0.0246358  0.22763458 0.7477296 ]\n",
            "[0.42804095 0.13374537 0.43821374]\n",
            "[0.36570597 0.5426719  0.09162211]\n",
            "[0.36570597 0.5426719  0.09162211]\n",
            "[0.32692102 0.35639828 0.31668073]\n",
            "[0.04062311 0.6084342  0.3509427 ]\n",
            "[0.7844843  0.02465058 0.19086514]\n",
            "[0.02901737 0.25180876 0.71917385]\n",
            "[0.48397753 0.12900329 0.3870192 ]\n",
            "[0.02256897 0.28617054 0.6912605 ]\n",
            "[0.9089056  0.0177243  0.07337005]\n",
            "[0.04369679 0.24032073 0.71598244]\n",
            "[0.92394304 0.01525464 0.0608024 ]\n",
            "[0.03463772 0.26097417 0.7043881 ]\n",
            "[0.42808667 0.1591858  0.41272753]\n",
            "[0.42808667 0.1591858  0.41272753]\n",
            "[0.34927112 0.17484297 0.475886  ]\n",
            "[0.95879734 0.01201757 0.02918505]\n",
            "[0.00593839 0.97586083 0.01820079]\n",
            "[0.04763748 0.4870008  0.4653618 ]\n",
            "[0.39325517 0.16557494 0.44116983]\n",
            "[0.95465446 0.01128472 0.03406072]\n",
            "[0.00571373 0.98145825 0.01282801]\n",
            "[0.45830417 0.17314063 0.36855522]\n",
            "[0.45830417 0.17314063 0.36855522]\n",
            "[0.3019088 0.1333786 0.5647126]\n",
            "[0.3917706  0.12722842 0.481001  ]\n",
            "[0.3917706  0.12722842 0.481001  ]\n",
            "[0.4277994  0.14632346 0.42587715]\n",
            "[0.4277994  0.14632346 0.42587715]\n",
            "[0.03917404 0.30783913 0.6529869 ]\n",
            "[0.01383479 0.8569875  0.12917775]\n",
            "[0.02429155 0.1879052  0.7878033 ]\n",
            "[0.26854002 0.36172026 0.36973968]\n",
            "[0.02915891 0.16737288 0.80346817]\n",
            "[0.81748646 0.03027813 0.15223543]\n",
            "[0.67094696 0.06393832 0.2651147 ]\n",
            "[0.03557602 0.16102746 0.8033965 ]\n",
            "[0.9423321  0.01568719 0.04198075]\n",
            "[0.38177496 0.09003013 0.5281949 ]\n",
            "[0.41869694 0.21376932 0.3675337 ]\n",
            "[0.37288424 0.1370305  0.49008518]\n",
            "[0.35002843 0.152257   0.49771464]\n",
            "[0.04874149 0.23958805 0.7116704 ]\n",
            "[0.9470105  0.01071389 0.04227557]\n",
            "[0.02421852 0.30688515 0.6688963 ]\n",
            "[0.8780913  0.02045139 0.10145728]\n",
            "[0.00462395 0.98190534 0.01347074]\n",
            "[0.3507576  0.12325547 0.5259869 ]\n",
            "[0.32669488 0.13500935 0.5382958 ]\n",
            "[0.43629548 0.15749317 0.4062113 ]\n",
            "[0.8932808  0.01550809 0.09121114]\n",
            "[0.04074522 0.10811929 0.85113543]\n",
            "[0.45124397 0.17127392 0.3774821 ]\n",
            "[0.02770231 0.26674518 0.7055526 ]\n",
            "[0.5112979  0.17267495 0.31602713]\n",
            "[0.00732665 0.8873013  0.105372  ]\n",
            "[0.32406867 0.16499011 0.5109412 ]\n",
            "[0.42928365 0.20867445 0.36204183]\n",
            "[0.35889834 0.06838348 0.5727182 ]\n",
            "[0.2951901  0.10256656 0.60224336]\n",
            "[0.27800822 0.3492193  0.37277249]\n",
            "[0.38130563 0.11084925 0.5078451 ]\n",
            "[0.42210695 0.14647791 0.43141517]\n",
            "[0.38567197 0.17161879 0.4427093 ]\n",
            "[0.8194734  0.03113687 0.14938976]\n",
            "[0.02201744 0.31604674 0.6619358 ]\n",
            "[0.32591763 0.10107893 0.5730035 ]\n",
            "[0.02580845 0.23504598 0.7391456 ]\n",
            "[0.01859228 0.65585005 0.3255577 ]\n",
            "[0.94452566 0.00895819 0.04651624]\n",
            "[0.00592691 0.9421313  0.05194185]\n",
            "[0.03954848 0.12159821 0.83885336]\n",
            "[0.35962856 0.11606091 0.5243105 ]\n",
            "[0.59780514 0.09740235 0.30479252]\n",
            "[0.4044913  0.12662245 0.46888623]\n",
            "[0.04698328 0.1205626  0.8324541 ]\n",
            "[0.43049136 0.13826251 0.4312461 ]\n",
            "[0.48561    0.20784296 0.30654708]\n",
            "[0.5608575  0.12580061 0.31334195]\n",
            "[0.5608575  0.12580061 0.31334195]\n",
            "[0.36353546 0.12653792 0.5099267 ]\n",
            "[0.37925604 0.16272402 0.4580199 ]\n",
            "[0.37925604 0.16272402 0.4580199 ]\n",
            "[0.90577745 0.01399931 0.08022325]\n",
            "[0.02681622 0.25534573 0.71783805]\n",
            "[0.03047706 0.92057025 0.04895275]\n",
            "[0.8896839  0.01990725 0.09040889]\n",
            "[0.00631662 0.91043264 0.08325064]\n",
            "[0.36249426 0.12005347 0.51745224]\n",
            "[0.01960842 0.45975477 0.52063686]\n",
            "[0.29569378 0.14014585 0.56416035]\n",
            "[0.48201558 0.17230439 0.34568003]\n",
            "[0.020123   0.61882997 0.36104706]\n",
            "[0.01758974 0.55283356 0.42957667]\n",
            "[0.3977717  0.16069016 0.44153813]\n",
            "[0.04123428 0.2217991  0.7369666 ]\n",
            "[0.9534609  0.02217984 0.02435928]\n",
            "[0.00981201 0.9673218  0.02286612]\n",
            "[0.51157963 0.15749826 0.33092207]\n",
            "[0.9066708  0.01988303 0.07344615]\n",
            "[0.03996731 0.12980694 0.8302257 ]\n",
            "[0.4310349  0.14844142 0.42052367]\n",
            "[0.42202368 0.18427446 0.39370185]\n",
            "[0.35973772 0.5169568  0.12330544]\n",
            "[0.35973772 0.5169568  0.12330544]\n",
            "[0.00695388 0.9844758  0.00857034]\n",
            "[0.43549752 0.14839792 0.4161045 ]\n",
            "[0.43549752 0.14839792 0.4161045 ]\n",
            "[0.96513224 0.01124086 0.02362689]\n",
            "[0.00605222 0.98311585 0.01083188]\n",
            "[0.88998437 0.02507874 0.08493695]\n",
            "[0.03393311 0.15696247 0.8091044 ]\n",
            "[0.04054299 0.15106556 0.8083914 ]\n",
            "[0.39057484 0.13299054 0.47643462]\n",
            "[0.03673979 0.12658975 0.83667046]\n",
            "[0.37513173 0.09341449 0.5314537 ]\n",
            "[0.77959037 0.0418698  0.17853981]\n",
            "[0.0049555  0.96618307 0.02886141]\n",
            "[0.9587581  0.01296468 0.02827722]\n",
            "[0.00798656 0.9178808  0.07413267]\n",
            "[0.01845971 0.42893222 0.5526081 ]\n",
            "[0.04012943 0.13044989 0.8294207 ]\n",
            "[0.92909014 0.01354442 0.05736554]\n",
            "[0.04272709 0.25739083 0.69988215]\n",
            "[0.3942947  0.1236934  0.48201182]\n",
            "[0.15401442 0.24826865 0.5977169 ]\n",
            "[0.4287384  0.2061543  0.36510736]\n",
            "[0.46744594 0.23521611 0.2973379 ]\n",
            "[0.85624254 0.02081251 0.12294497]\n",
            "[0.01309694 0.6640439  0.32285914]\n",
            "[0.45168802 0.1783554  0.3699566 ]\n",
            "[0.45168802 0.1783554  0.3699566 ]\n",
            "[0.30993906 0.18861675 0.50144416]\n",
            "[0.30993906 0.18861675 0.50144416]\n",
            "[0.40094492 0.18695703 0.4120981 ]\n",
            "[0.41938147 0.1551802  0.4254383 ]\n",
            "[0.4265879  0.1542724  0.41913968]\n",
            "[0.4265879  0.1542724  0.41913968]\n",
            "[0.38547882 0.22824694 0.38627416]\n",
            "[0.38547882 0.22824694 0.38627416]\n",
            "[0.00723783 0.9832343  0.00952786]\n",
            "[0.93182963 0.00956523 0.05860515]\n",
            "[0.00602503 0.9437083  0.05026669]\n",
            "[0.4095478  0.18448903 0.40596318]\n",
            "[0.4095478  0.18448903 0.40596318]\n",
            "[0.9408139  0.01281826 0.04636787]\n",
            "[0.02957036 0.7140141  0.25641546]\n",
            "[0.45250264 0.14810775 0.39938962]\n",
            "[0.45250264 0.14810775 0.39938962]\n",
            "[0.96595377 0.01302178 0.02102442]\n",
            "[0.00793247 0.9820332  0.01003431]\n",
            "[0.31567878 0.09372887 0.5905923 ]\n",
            "[0.866303   0.01739614 0.11630081]\n",
            "[0.03841731 0.12024575 0.84133697]\n",
            "[0.3984906  0.1674318  0.43407762]\n",
            "[0.3984906  0.1674318  0.43407762]\n",
            "[0.39317018 0.15414232 0.45268753]\n",
            "[0.03554492 0.24758331 0.71687174]\n",
            "[0.0109803  0.77644414 0.21257548]\n",
            "[0.33753604 0.17302644 0.4894375 ]\n",
            "[0.33753604 0.17302644 0.4894375 ]\n",
            "[0.9597911  0.01196137 0.02824749]\n",
            "[0.00594887 0.9849407  0.00911044]\n",
            "[0.23160952 0.11419471 0.6541958 ]\n",
            "[0.03380225 0.12998328 0.8362144 ]\n",
            "[0.03665987 0.10931274 0.8540274 ]\n",
            "[0.94190896 0.01103021 0.04706086]\n",
            "[0.02510569 0.8354034  0.1394909 ]\n",
            "[0.8888561  0.01758322 0.09356068]\n",
            "[0.02258367 0.3097395  0.66767687]\n",
            "[0.05280056 0.09863251 0.8485669 ]\n",
            "[0.4309116  0.15475687 0.4143315 ]\n",
            "[0.4309116  0.15475687 0.4143315 ]\n",
            "[0.915362   0.01907271 0.06556531]\n",
            "[0.01103561 0.8953769  0.09358758]\n",
            "[0.520967   0.11140235 0.36763063]\n",
            "[0.04731793 0.10985082 0.8428312 ]\n",
            "[0.75353354 0.03230279 0.21416372]\n",
            "[0.0164992  0.5215866  0.46191424]\n",
            "[0.94482726 0.01961846 0.03555428]\n",
            "[0.00630552 0.98469144 0.00900298]\n",
            "[0.6861412  0.02877593 0.28508288]\n",
            "[0.01630717 0.70439297 0.27929986]\n",
            "[0.93068683 0.01170319 0.05760997]\n",
            "[0.02545642 0.35583007 0.6187135 ]\n",
            "[0.30909818 0.11649931 0.57440245]\n",
            "[0.42268065 0.3799397  0.19737971]\n",
            "[0.42268065 0.3799397  0.19737971]\n",
            "[0.92692363 0.04576071 0.02731568]\n",
            "[0.00654814 0.9832638  0.01018801]\n",
            "[0.96740603 0.01173097 0.02086299]\n",
            "[0.00696045 0.9830169  0.01002273]\n",
            "[0.41483113 0.12713973 0.45802915]\n",
            "[0.38341528 0.15703271 0.45955205]\n",
            "[0.03279864 0.2280963  0.73910505]\n",
            "[0.9570139  0.01432393 0.02866213]\n",
            "[0.01006403 0.9461718  0.04376419]\n",
            "[0.68781185 0.21541984 0.0967682 ]\n",
            "[0.03512924 0.83870924 0.12616152]\n",
            "[0.95598656 0.01544209 0.02857132]\n",
            "[0.00494759 0.9816506  0.01340183]\n",
            "[0.96513414 0.01072591 0.02413998]\n",
            "[0.00697872 0.9832045  0.00981678]\n",
            "[0.23948908 0.3983257  0.36218518]\n",
            "[0.23948908 0.3983257  0.36218518]\n",
            "[0.34636247 0.2702439  0.38339362]\n",
            "[0.3369474  0.10604633 0.5570063 ]\n",
            "[0.04101617 0.26609892 0.6928849 ]\n",
            "[0.9382565  0.01270369 0.04903984]\n",
            "[0.0214248  0.66132456 0.3172506 ]\n",
            "[0.4135787  0.15735492 0.42906636]\n",
            "[0.87553465 0.01583173 0.10863361]\n",
            "[0.00526746 0.9679883  0.02674417]\n",
            "[0.54730785 0.11883929 0.33385286]\n",
            "[0.03488467 0.15994453 0.80517083]\n",
            "[0.03623271 0.18371472 0.78005254]\n",
            "[0.9672573  0.01046902 0.02227366]\n",
            "[0.00544557 0.9845151  0.01003934]\n",
            "[0.70795023 0.09379394 0.19825584]\n",
            "[0.01454028 0.5801301  0.40532964]\n",
            "[0.43852913 0.16293474 0.39853612]\n",
            "[0.9644943  0.01591723 0.01958848]\n",
            "[0.00617068 0.9825316  0.01129773]\n",
            "[0.8908696  0.02658929 0.08254118]\n",
            "[0.00674444 0.9298773  0.06337832]\n",
            "[0.81660646 0.02425867 0.15913478]\n",
            "[0.03895146 0.11998338 0.84106517]\n",
            "[0.00981018 0.81100255 0.17918725]\n",
            "[0.36464363 0.15374738 0.48160896]\n",
            "[0.9648726  0.01168164 0.02344576]\n",
            "[0.00651994 0.9830157  0.01046434]\n",
            "[0.47567302 0.19199131 0.33233568]\n",
            "[0.36171815 0.14263515 0.49564674]\n",
            "[0.96433264 0.01228459 0.02338284]\n",
            "[0.00533959 0.98121405 0.01344638]\n",
            "[0.0438465  0.10351327 0.8526402 ]\n",
            "[0.94775575 0.02438286 0.02786142]\n",
            "[0.00658617 0.98392594 0.00948799]\n",
            "[0.02877364 0.25210506 0.71912134]\n",
            "[0.40675667 0.1508666  0.44237673]\n",
            "[0.8891597  0.0169444  0.09389591]\n",
            "[0.02703466 0.17915308 0.7938123 ]\n",
            "[0.04643564 0.15385449 0.79970986]\n",
            "[0.38296688 0.19832723 0.41870588]\n",
            "[0.5827777  0.12986787 0.28735444]\n",
            "[0.92797846 0.01655918 0.05546239]\n",
            "[0.00696088 0.92781895 0.06522015]\n",
            "[0.96529615 0.01133818 0.02336569]\n",
            "[0.00645607 0.982634   0.01090984]\n",
            "[0.82498187 0.02442736 0.15059073]\n",
            "[0.01269066 0.6598527  0.3274566 ]\n",
            "[0.599095   0.11976436 0.28114066]\n",
            "[0.599095   0.11976436 0.28114066]\n",
            "[0.9667517  0.01215488 0.02109342]\n",
            "[0.00737647 0.98299986 0.00962371]\n",
            "[0.3008408  0.19487476 0.50428444]\n",
            "[0.9652141  0.01155848 0.02322741]\n",
            "[0.00700226 0.9832739  0.00972386]\n",
            "[0.81672215 0.02368017 0.15959765]\n",
            "[0.00820887 0.894055   0.09773616]\n",
            "[0.9312547  0.04545604 0.02328929]\n",
            "[0.00646978 0.9836416  0.00988853]\n",
            "[0.9665365  0.00961685 0.02384668]\n",
            "[0.00732032 0.98407364 0.00860599]\n",
            "[0.38432842 0.12666856 0.489003  ]\n",
            "[0.38432842 0.12666856 0.489003  ]\n",
            "[0.9281208  0.01130498 0.06057427]\n",
            "[0.0134551  0.73471236 0.25183254]\n",
            "[0.27269682 0.23756795 0.48973522]\n",
            "[0.37380487 0.19521543 0.4309797 ]\n",
            "[0.5728822  0.11573996 0.31137788]\n",
            "[0.5728822  0.11573996 0.31137788]\n",
            "[0.9427597  0.01630477 0.04093549]\n",
            "[0.00571427 0.95767283 0.03661289]\n",
            "[0.36610228 0.10189208 0.5320056 ]\n",
            "[0.36610228 0.10189208 0.5320056 ]\n",
            "[0.88796437 0.02414164 0.08789409]\n",
            "[0.02287706 0.31030655 0.6668164 ]\n",
            "[0.04683894 0.11918411 0.8339769 ]\n",
            "[0.96267706 0.01101386 0.02630913]\n",
            "[0.00728809 0.98361444 0.00909754]\n",
            "[0.9559393  0.02454733 0.01951341]\n",
            "[0.0064852  0.98322994 0.01028484]\n",
            "[0.49272016 0.17863484 0.32864502]\n",
            "[0.49272016 0.17863484 0.32864502]\n",
            "[0.39784044 0.22176467 0.38039485]\n",
            "[0.9685533  0.01156612 0.01988053]\n",
            "[0.00598197 0.9836981  0.01031982]\n",
            "[0.96044225 0.01303255 0.0265252 ]\n",
            "[0.00551474 0.9829328  0.01155247]\n",
            "[0.01347628 0.6728274  0.3136963 ]\n",
            "[0.02131653 0.65998435 0.31869912]\n",
            "[0.8643568  0.01822883 0.11741437]\n",
            "[0.03432579 0.18910438 0.7765698 ]\n",
            "[0.4607827  0.13216376 0.40705353]\n",
            "[0.9658451  0.0130455  0.02110951]\n",
            "[0.00619284 0.9835411  0.01026606]\n",
            "[0.01796449 0.7115519  0.27048367]\n",
            "[0.85538113 0.0214744  0.1231444 ]\n",
            "[0.01249773 0.7431032  0.24439913]\n",
            "[0.86903524 0.02390931 0.10705545]\n",
            "[0.04294658 0.16072494 0.7963285 ]\n",
            "[0.93315065 0.01183688 0.05501251]\n",
            "[0.04326164 0.1802542  0.77648413]\n",
            "[0.9420432  0.01170642 0.04625042]\n",
            "[0.00549741 0.95522356 0.03927911]\n",
            "[0.9261051  0.01541386 0.05848103]\n",
            "[0.03034862 0.15556827 0.8140831 ]\n",
            "[0.3226426  0.17156433 0.5057931 ]\n",
            "[0.15008049 0.45107833 0.39884123]\n",
            "[0.429822   0.21936515 0.35081282]\n",
            "[0.45323756 0.19934121 0.3474212 ]\n",
            "[0.45323756 0.19934121 0.3474212 ]\n",
            "[0.8739176  0.0302401  0.09584235]\n",
            "[0.05176904 0.4084091  0.53982186]\n",
            "[0.87155074 0.01669321 0.11175598]\n",
            "[0.01702131 0.510004   0.4729747 ]\n",
            "[0.09080809 0.08163939 0.82755256]\n",
            "[0.05665791 0.1688749  0.7744672 ]\n",
            "[0.8782403  0.02017344 0.10158628]\n",
            "[0.01076536 0.7822341  0.20700055]\n",
            "[0.39946932 0.14901605 0.45151466]\n",
            "[0.486069   0.13859566 0.3753353 ]\n",
            "[0.8570826  0.02671895 0.1161984 ]\n",
            "[0.06318396 0.14344862 0.7933674 ]\n",
            "[0.04729279 0.08845803 0.86424917]\n",
            "[0.71630013 0.12257702 0.16112289]\n",
            "[0.02740264 0.23584905 0.73674834]\n",
            "[0.9024304  0.01619842 0.08137112]\n",
            "[0.02740814 0.1830777  0.7895141 ]\n",
            "[0.02544204 0.30421662 0.6703413 ]\n",
            "[0.9671843  0.01359649 0.01921923]\n",
            "[0.00840768 0.98288786 0.00870442]\n",
            "[0.8855283  0.01686736 0.09760439]\n",
            "[0.00515051 0.9589942  0.03585528]\n",
            "[0.5135732  0.13470945 0.3517173 ]\n",
            "[0.78388405 0.03073544 0.18538058]\n",
            "[0.04331981 0.13236593 0.8243143 ]\n",
            "[0.96692634 0.01353905 0.01953452]\n",
            "[0.00716002 0.98343176 0.00940814]\n",
            "[0.9625542  0.01009128 0.02735453]\n",
            "[0.00644263 0.9823325  0.01122481]\n",
            "[0.70383364 0.05915859 0.23700775]\n",
            "[0.03702767 0.25296217 0.7100102 ]\n",
            "[0.3876946  0.17863512 0.4336703 ]\n",
            "[0.3876946  0.17863512 0.4336703 ]\n",
            "[0.38284048 0.15959741 0.45756212]\n",
            "[0.39105785 0.15965632 0.4492859 ]\n",
            "[0.7001983  0.16216257 0.13763914]\n",
            "[0.8138474 0.019356  0.1667966]\n",
            "[0.01371535 0.6901539  0.29613075]\n",
            "[0.94715244 0.02431492 0.02853268]\n",
            "[0.00713751 0.9827115  0.01015099]\n",
            "[0.30980173 0.08393134 0.6062669 ]\n",
            "[0.02459267 0.31394184 0.6614655 ]\n",
            "[0.34038657 0.19293095 0.46668252]\n",
            "[0.37777153 0.13194588 0.49028262]\n",
            "[0.37777153 0.13194588 0.49028262]\n",
            "[0.3593537  0.12976767 0.5108786 ]\n",
            "[0.3593537  0.12976767 0.5108786 ]\n",
            "[0.01672043 0.58249855 0.400781  ]\n",
            "[0.44377035 0.1525771  0.4036525 ]\n",
            "[0.41689977 0.13855144 0.4445488 ]\n",
            "[0.41337502 0.22784606 0.35877892]\n",
            "[0.37976432 0.15194443 0.46829125]\n",
            "[0.37976432 0.15194443 0.46829125]\n",
            "[0.9681461  0.01506596 0.01678795]\n",
            "[0.00729806 0.9833287  0.00937335]\n",
            "[0.45438623 0.16958223 0.37603152]\n",
            "[0.5201902  0.11292428 0.36688557]\n",
            "[0.5201902  0.11292428 0.36688557]\n",
            "[0.04184767 0.43231255 0.52583975]\n",
            "[0.29450214 0.26594388 0.43955398]\n",
            "[0.29450214 0.26594388 0.43955398]\n",
            "[0.46190375 0.12933666 0.4087596 ]\n",
            "[0.46229556 0.16516557 0.3725389 ]\n",
            "[0.46229556 0.16516557 0.3725389 ]\n",
            "[0.41004297 0.25524995 0.33470702]\n",
            "[0.41004297 0.25524995 0.33470702]\n",
            "[0.37769303 0.21854033 0.40376666]\n",
            "[0.37769303 0.21854033 0.40376666]\n",
            "[0.32290447 0.162609   0.51448655]\n",
            "[0.03796044 0.1527504  0.80928916]\n",
            "[0.43118897 0.18047163 0.38833943]\n",
            "[0.41425565 0.13396353 0.4517808 ]\n",
            "[0.90116227 0.01671412 0.08212355]\n",
            "[0.04003534 0.12034001 0.8396247 ]\n",
            "[0.33006856 0.12613004 0.54380137]\n",
            "[0.88983256 0.0222859  0.08788153]\n",
            "[0.04099099 0.13522908 0.82377994]\n",
            "[0.39301124 0.22391957 0.38306916]\n",
            "[0.9425528  0.015542   0.04190522]\n",
            "[0.00511042 0.97916245 0.01572704]\n",
            "[0.32271668 0.12309168 0.55419165]\n",
            "[0.32271668 0.12309168 0.55419165]\n",
            "[0.9606714  0.01227842 0.02705004]\n",
            "[0.00589957 0.9521994  0.04190096]\n",
            "[0.40964445 0.13006707 0.46028847]\n",
            "[0.03739806 0.16454114 0.79806083]\n",
            "[0.43242994 0.1702547  0.3973153 ]\n",
            "[0.39013362 0.09185581 0.51801056]\n",
            "[0.35620108 0.22769399 0.41610494]\n",
            "[0.35620108 0.22769399 0.41610494]\n",
            "[0.36817306 0.12117659 0.51065034]\n",
            "[0.49905384 0.1359472  0.36499897]\n",
            "[0.5634413  0.13031852 0.3062402 ]\n",
            "[0.9604433  0.01695683 0.02259981]\n",
            "[0.00554751 0.9830851  0.01136738]\n",
            "[0.37282133 0.12238874 0.50478995]\n",
            "[0.96043265 0.01037603 0.02919133]\n",
            "[0.00607612 0.9810021  0.01292184]\n",
            "[0.93599427 0.0128036  0.05120213]\n",
            "[0.01221052 0.72710747 0.26068205]\n",
            "[0.38039213 0.14905079 0.4705571 ]\n",
            "[0.37513006 0.12516436 0.4997055 ]\n",
            "[0.9609615  0.01154893 0.02748959]\n",
            "[0.00614061 0.98303187 0.01082755]\n",
            "[0.8678769  0.02100476 0.11111838]\n",
            "[0.0164758  0.68871003 0.29481417]\n",
            "[0.9374738  0.01080582 0.05172037]\n",
            "[0.03051182 0.36289743 0.60659075]\n",
            "[0.948516   0.01106934 0.04041471]\n",
            "[0.02172885 0.6259166  0.3523545 ]\n",
            "[0.9449621  0.02491703 0.03012088]\n",
            "[0.00514855 0.98244494 0.01240646]\n",
            "[0.92826    0.01608121 0.05565867]\n",
            "[0.01389452 0.8406693  0.14543621]\n",
            "[0.55850863 0.14798379 0.2935075 ]\n",
            "[0.41215122 0.20725125 0.3805975 ]\n",
            "[0.35484603 0.14275849 0.5023955 ]\n",
            "[0.04300008 0.18353827 0.7734617 ]\n",
            "[0.3127422  0.12020954 0.56704825]\n",
            "[0.5244111  0.08934196 0.38624692]\n",
            "[0.81225044 0.02193543 0.16581415]\n",
            "[0.01756141 0.4605911  0.5218475 ]\n",
            "[0.06855842 0.12320118 0.8082405 ]\n",
            "[0.3356991  0.10214227 0.5621586 ]\n",
            "[0.8410073  0.01831893 0.14067379]\n",
            "[0.02370412 0.16512872 0.8111671 ]\n",
            "[0.1570152  0.49914086 0.34384397]\n",
            "[0.8747973  0.0222615  0.10294123]\n",
            "[0.00844538 0.8726834  0.11887117]\n",
            "[0.40177837 0.17789571 0.42032588]\n",
            "[0.40177837 0.17789571 0.42032588]\n",
            "[0.42892626 0.15759566 0.41347805]\n",
            "[0.39425534 0.21145204 0.39429262]\n",
            "[0.9661334  0.01425773 0.01960883]\n",
            "[0.00770137 0.9827514  0.00954716]\n",
            "[0.00840736 0.8444576  0.14713503]\n",
            "[0.39506346 0.18181297 0.42312354]\n",
            "[0.05524267 0.25427073 0.69048655]\n",
            "[0.5767294  0.12056257 0.30270803]\n",
            "[0.5767294  0.12056257 0.30270803]\n",
            "[0.02363444 0.31363577 0.6627298 ]\n",
            "[0.96741176 0.01530401 0.01728426]\n",
            "[0.00718772 0.98352283 0.00928945]\n",
            "[0.5389258  0.17731012 0.283764  ]\n",
            "[0.5389258  0.17731012 0.283764  ]\n",
            "[0.96919686 0.01157099 0.01923225]\n",
            "[0.00621307 0.9846459  0.00914101]\n",
            "[0.34502834 0.12119406 0.5337776 ]\n",
            "[0.4898737  0.15429138 0.35583493]\n",
            "[0.4898737  0.15429138 0.35583493]\n",
            "[0.9645528  0.01135774 0.02408944]\n",
            "[0.00715938 0.96979284 0.02304777]\n",
            "[0.9580059  0.01434091 0.02765315]\n",
            "[0.00817389 0.93922263 0.05260337]\n",
            "[0.96438605 0.01061086 0.02500318]\n",
            "[0.00544209 0.9824485  0.01210948]\n",
            "[0.96469414 0.01086255 0.02444321]\n",
            "[0.00676622 0.9849776  0.0082562 ]\n",
            "[0.40037674 0.1754179  0.42420542]\n",
            "[0.9366991  0.01303293 0.05026799]\n",
            "[0.00969985 0.8152191  0.17508096]\n",
            "[0.9600639  0.01183014 0.02810608]\n",
            "[0.00715794 0.983697   0.00914504]\n",
            "[0.95057386 0.01027512 0.03915101]\n",
            "[0.01849208 0.49517003 0.48633784]\n",
            "[0.04641713 0.12581326 0.8277696 ]\n",
            "[0.95913655 0.00981829 0.03104514]\n",
            "[0.00571246 0.9835473  0.01074025]\n",
            "[0.95182705 0.01788546 0.03028753]\n",
            "[0.00583159 0.96394724 0.03022118]\n",
            "[0.05136877 0.32922015 0.6194111 ]\n",
            "[0.3576607  0.14096776 0.5013715 ]\n",
            "[0.3576607  0.14096776 0.5013715 ]\n",
            "[0.8985948  0.0168421  0.08456314]\n",
            "[0.00498913 0.95595026 0.03906062]\n",
            "[0.23712677 0.39527506 0.36759815]\n",
            "[0.47570023 0.14455052 0.37974924]\n",
            "[0.43343592 0.15147538 0.41508868]\n",
            "[0.357729  0.1656559 0.4766151]\n",
            "[0.357729  0.1656559 0.4766151]\n",
            "[0.85551614 0.02950422 0.1149797 ]\n",
            "[0.00618941 0.93947214 0.05433842]\n",
            "[0.8779742  0.01679724 0.10522865]\n",
            "[0.02401974 0.23916475 0.7368155 ]\n",
            "[0.9682505  0.01365242 0.018097  ]\n",
            "[0.00684021 0.98473376 0.00842607]\n",
            "[0.9415441  0.01150583 0.04695011]\n",
            "[0.00437412 0.98258984 0.01303602]\n",
            "[0.39449826 0.14414667 0.46135512]\n",
            "[0.39449826 0.14414667 0.46135512]\n",
            "[0.87366223 0.02388589 0.10245189]\n",
            "[0.00490729 0.9757052  0.01938753]\n",
            "[0.89418674 0.01432424 0.09148904]\n",
            "[0.00628025 0.92181545 0.07190427]\n",
            "[0.8933493  0.01798375 0.08866688]\n",
            "[0.00503136 0.9721195  0.02284912]\n",
            "[0.03233678 0.1478843  0.819779  ]\n",
            "[0.32717302 0.10872443 0.56410253]\n",
            "[0.06599099 0.18341519 0.75059384]\n",
            "[0.9639154  0.01464412 0.02144044]\n",
            "[0.00644264 0.9845666  0.00899072]\n",
            "[0.9672075  0.01139272 0.02139987]\n",
            "[0.00639165 0.98331636 0.01029194]\n",
            "[0.9155746  0.01567573 0.06874966]\n",
            "[0.02591354 0.29955566 0.6745308 ]\n",
            "[0.4078124  0.19330114 0.3988865 ]\n",
            "[0.4078124  0.19330114 0.3988865 ]\n",
            "[0.96423596 0.01170087 0.02406307]\n",
            "[0.00515491 0.9822134  0.01263167]\n",
            "[0.48006144 0.07785642 0.44208208]\n",
            "[0.03392211 0.12470042 0.84137744]\n",
            "[0.96129674 0.01060404 0.02809918]\n",
            "[0.00440043 0.9811715  0.01442811]\n",
            "[0.89277434 0.01542869 0.09179689]\n",
            "[0.04642902 0.1846785  0.7688925 ]\n",
            "[0.93941283 0.01789922 0.04268795]\n",
            "[0.00561099 0.9483178  0.04607114]\n",
            "[0.5195614  0.10177227 0.37866628]\n",
            "[0.41374618 0.14394756 0.44230622]\n",
            "[0.9593929  0.01405617 0.02655104]\n",
            "[0.00707981 0.98307115 0.00984911]\n",
            "[0.941067   0.02064212 0.03829087]\n",
            "[0.0043821  0.9677315  0.02788648]\n",
            "[0.01678682 0.52573144 0.45748174]\n",
            "[0.2755757  0.07631543 0.6481089 ]\n",
            "[0.9312305  0.0161556  0.05261397]\n",
            "[0.00462753 0.97290254 0.02246984]\n",
            "[0.8851057  0.01674437 0.09814991]\n",
            "[0.02542947 0.21820483 0.7563657 ]\n",
            "[0.4922431  0.12188189 0.38587505]\n",
            "[0.4922431  0.12188189 0.38587505]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/task2punjabi-results.txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f05e0a31-24d6-4ecd-b731-89f4d1932d93"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "                 \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e634a874-b493-489a-cce5-eec23c2096f3"
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f3a6b961-4982-4e1d-cb0b-ce2f5f479406"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-task2malayalam'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task2malayalam\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task2malayalam/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task2malayalam/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task2malayalam/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    }
  ]
}
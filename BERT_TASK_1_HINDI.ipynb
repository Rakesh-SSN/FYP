{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT TASK 1 HINDI",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT_TASK_1_HINDI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "be6aac8d-bf8a-486c-c2db-984c2a77d9f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "98aaf687-745f-4302-f875-420d82be5f72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "68ee463f-4e39-4dbe-8584-16c71ce9b15a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/hindi/task1hindi.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1447</th>\n",
              "      <td>HIN1448</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>केंद्रीय मंत्री रविशंकर प्रसाद ने कहा कि एंटनी...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114</th>\n",
              "      <td>HIN1115</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>सूत्रों ने पहले ही इस बात के संकेत दिए थे कि ए...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1064</th>\n",
              "      <td>HIN1065</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>किन-किन ने ली शपथ?&lt;eol&gt;कौन-कौन बना मंत्री?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2287</th>\n",
              "      <td>HIN2288</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>मुझे भारत के गेंदबाज पसंद हैं और  इशांत शर्मा ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1537</th>\n",
              "      <td>HIN1538</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>पहले पड़ाव में वे बेल्जियम की राजधानी ब्रसेल्स...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>668</th>\n",
              "      <td>HIN0669</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>सेटलमेंट के लिए माल्या का छ्ह हज़ार आठ सौ अठ्सठ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1583</th>\n",
              "      <td>HIN1584</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>कोलकाता: मृतकों की संख्या चौबीस  हुई|&lt;eol&gt;वहीं...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2404</th>\n",
              "      <td>HIN2405</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>जम्मू-कश्मीर में सरकार गठन पर अगले कदम को लेकर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>HIN0498</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>सेना और एनडीआरएफ की मौजूदगी में राहत और बचाव क...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2480</th>\n",
              "      <td>HIN2481</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>प्रवर्तन निदेशालय ने माल्या के पासपोर्ट को रद ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "1447         HIN1448  ...  केंद्रीय मंत्री रविशंकर प्रसाद ने कहा कि एंटनी...\n",
              "1114         HIN1115  ...  सूत्रों ने पहले ही इस बात के संकेत दिए थे कि ए...\n",
              "1064         HIN1065  ...         किन-किन ने ली शपथ?<eol>कौन-कौन बना मंत्री?\n",
              "2287         HIN2288  ...  मुझे भारत के गेंदबाज पसंद हैं और  इशांत शर्मा ...\n",
              "1537         HIN1538  ...  पहले पड़ाव में वे बेल्जियम की राजधानी ब्रसेल्स...\n",
              "668          HIN0669  ...  सेटलमेंट के लिए माल्या का छ्ह हज़ार आठ सौ अठ्सठ...\n",
              "1583         HIN1584  ...  कोलकाता: मृतकों की संख्या चौबीस  हुई|<eol>वहीं...\n",
              "2404         HIN2405  ...  जम्मू-कश्मीर में सरकार गठन पर अगले कदम को लेकर...\n",
              "497          HIN0498  ...  सेना और एनडीआरएफ की मौजूदगी में राहत और बचाव क...\n",
              "2480         HIN2481  ...  प्रवर्तन निदेशालय ने माल्या के पासपोर्ट को रद ...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1447</th>\n",
              "      <td>HIN1448</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>केंद्रीय मंत्री रविशंकर प्रसाद ने कहा कि एंटनी...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114</th>\n",
              "      <td>HIN1115</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>सूत्रों ने पहले ही इस बात के संकेत दिए थे कि ए...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1064</th>\n",
              "      <td>HIN1065</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>किन-किन ने ली शपथ?&lt;eol&gt;कौन-कौन बना मंत्री?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2287</th>\n",
              "      <td>HIN2288</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>मुझे भारत के गेंदबाज पसंद हैं और  इशांत शर्मा ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1537</th>\n",
              "      <td>HIN1538</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>पहले पड़ाव में वे बेल्जियम की राजधानी ब्रसेल्स...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>668</th>\n",
              "      <td>HIN0669</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>सेटलमेंट के लिए माल्या का छ्ह हज़ार आठ सौ अठ्सठ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1583</th>\n",
              "      <td>HIN1584</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>कोलकाता: मृतकों की संख्या चौबीस  हुई|&lt;eol&gt;वहीं...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2404</th>\n",
              "      <td>HIN2405</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>जम्मू-कश्मीर में सरकार गठन पर अगले कदम को लेकर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>HIN0498</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>सेना और एनडीआरएफ की मौजूदगी में राहत और बचाव क...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2480</th>\n",
              "      <td>HIN2481</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>प्रवर्तन निदेशालय ने माल्या के पासपोर्ट को रद ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "1447         HIN1448  ...  केंद्रीय मंत्री रविशंकर प्रसाद ने कहा कि एंटनी...\n",
              "1114         HIN1115  ...  सूत्रों ने पहले ही इस बात के संकेत दिए थे कि ए...\n",
              "1064         HIN1065  ...         किन-किन ने ली शपथ?<eol>कौन-कौन बना मंत्री?\n",
              "2287         HIN2288  ...  मुझे भारत के गेंदबाज पसंद हैं और  इशांत शर्मा ...\n",
              "1537         HIN1538  ...  पहले पड़ाव में वे बेल्जियम की राजधानी ब्रसेल्स...\n",
              "668          HIN0669  ...  सेटलमेंट के लिए माल्या का छ्ह हज़ार आठ सौ अठ्सठ...\n",
              "1583         HIN1584  ...  कोलकाता: मृतकों की संख्या चौबीस  हुई|<eol>वहीं...\n",
              "2404         HIN2405  ...  जम्मू-कश्मीर में सरकार गठन पर अगले कदम को लेकर...\n",
              "497          HIN0498  ...  सेना और एनडीआरएफ की मौजूदगी में राहत और बचाव क...\n",
              "2480         HIN2481  ...  प्रवर्तन निदेशालय ने माल्या के पासपोर्ट को रद ...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "9aa10ae0-7c6d-4b88-e05a-7702dae13a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1673</th>\n",
              "      <td>माता वैष्णो देवी भी लड़कियों को मेडल मिलते देख...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1802</th>\n",
              "      <td>इसलिए उससे सेना को निपटना चाहिए।’’&lt;eol&gt;तीन दिव...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1351</th>\n",
              "      <td>सीबीआई यह भी पता लगाने की कोशिश में है कि इटली...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1714</th>\n",
              "      <td>उत्तराखंडः हाईकोर्ट  का सख्त कमेंट- कभी-कभी प्...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>आम आदमी पार्टी की सरकार को केंद्र पर ब्लेम लगा...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1673  माता वैष्णो देवी भी लड़कियों को मेडल मिलते देख...      0\n",
              "1802  इसलिए उससे सेना को निपटना चाहिए।’’<eol>तीन दिव...      0\n",
              "1351  सीबीआई यह भी पता लगाने की कोशिश में है कि इटली...      0\n",
              "1714  उत्तराखंडः हाईकोर्ट  का सख्त कमेंट- कभी-कभी प्...      0\n",
              "1001  आम आदमी पार्टी की सरकार को केंद्र पर ब्लेम लगा...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1673</th>\n",
              "      <td>माता वैष्णो देवी भी लड़कियों को मेडल मिलते देख...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1802</th>\n",
              "      <td>इसलिए उससे सेना को निपटना चाहिए।’’&lt;eol&gt;तीन दिव...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1351</th>\n",
              "      <td>सीबीआई यह भी पता लगाने की कोशिश में है कि इटली...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1714</th>\n",
              "      <td>उत्तराखंडः हाईकोर्ट  का सख्त कमेंट- कभी-कभी प्...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>आम आदमी पार्टी की सरकार को केंद्र पर ब्लेम लगा...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1673  माता वैष्णो देवी भी लड़कियों को मेडल मिलते देख...      0\n",
              "1802  इसलिए उससे सेना को निपटना चाहिए।’’<eol>तीन दिव...      0\n",
              "1351  सीबीआई यह भी पता लगाने की कोशिश में है कि इटली...      0\n",
              "1714  उत्तराखंडः हाईकोर्ट  का सख्त कमेंट- कभी-कभी प्...      0\n",
              "1001  आम आदमी पार्टी की सरकार को केंद्र पर ब्लेम लगा...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "c6917f7c-5c41-40c3-c0b5-f412e244af58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n",
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "51531081-a29c-412c-f82e-2af0789d42e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  भारतीय मुस्लिमों की वजह से नहीं पनप सकता आईएस|<eol>भारत में कभी वर्चस्व कायम नहीं कर सकता आईएस|\n",
            "Tokenized:  ['भारतीय', 'म', '##स', '##ल', '##िम', '##ो', 'की', 'व', '##ज', '##ह', 'स', 'न', '##ही', 'प', '##न', '##प', 'सकता', 'आई', '##ए', '##स', '|', '<', 'eo', '##l', '>', 'भारत', 'म', 'कभी', 'व', '##र', '##च', '##स', '##व', 'का', '##यम', 'न', '##ही', 'कर', 'सकता', 'आई', '##ए', '##स', '|']\n",
            "Token IDs:  [18725, 889, 13432, 11714, 50419, 13718, 10826, 895, 17413, 17110, 898, 884, 24667, 885, 11453, 18187, 26886, 44881, 22599, 13432, 196, 133, 13934, 10161, 135, 14311, 889, 50058, 895, 11549, 16940, 13432, 15070, 11081, 87136, 884, 24667, 16192, 26886, 44881, 22599, 13432, 196]\n",
            " Original:  भारतीय मुस्लिमों की वजह से नहीं पनप सकता आईएस|<eol>भारत में कभी वर्चस्व कायम नहीं कर सकता आईएस|\n",
            "Tokenized:  ['भारतीय', 'म', '##स', '##ल', '##िम', '##ो', 'की', 'व', '##ज', '##ह', 'स', 'न', '##ही', 'प', '##न', '##प', 'सकता', 'आई', '##ए', '##स', '|', '<', 'eo', '##l', '>', 'भारत', 'म', 'कभी', 'व', '##र', '##च', '##स', '##व', 'का', '##यम', 'न', '##ही', 'कर', 'सकता', 'आई', '##ए', '##स', '|']\n",
            "Token IDs:  [18725, 889, 13432, 11714, 50419, 13718, 10826, 895, 17413, 17110, 898, 884, 24667, 885, 11453, 18187, 26886, 44881, 22599, 13432, 196, 133, 13934, 10161, 135, 14311, 889, 50058, 895, 11549, 16940, 13432, 15070, 11081, 87136, 884, 24667, 16192, 26886, 44881, 22599, 13432, 196]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "1dc13ed6-9bd5-46e8-ca06-4e3fa669e691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  भारतीय मुस्लिमों की वजह से नहीं पनप सकता आईएस|<eol>भारत में कभी वर्चस्व कायम नहीं कर सकता आईएस|\n",
            "Token IDs: [101, 18725, 889, 13432, 11714, 50419, 13718, 10826, 895, 17413, 17110, 898, 884, 24667, 885, 11453, 18187, 26886, 44881, 22599, 13432, 196, 133, 13934, 10161, 135, 14311, 889, 50058, 895, 11549, 16940, 13432, 15070, 11081, 87136, 884, 24667, 16192, 26886, 44881, 22599, 13432, 196, 102]\n",
            "Original:  भारतीय मुस्लिमों की वजह से नहीं पनप सकता आईएस|<eol>भारत में कभी वर्चस्व कायम नहीं कर सकता आईएस|\n",
            "Token IDs: [101, 18725, 889, 13432, 11714, 50419, 13718, 10826, 895, 17413, 17110, 898, 884, 24667, 885, 11453, 18187, 26886, 44881, 22599, 13432, 196, 133, 13934, 10161, 135, 14311, 889, 50058, 895, 11549, 16940, 13432, 15070, 11081, 87136, 884, 24667, 16192, 26886, 44881, 22599, 13432, 196, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "914e576d-b631-4cc6-9ad7-f5c8632f17ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  212\n",
            "Max sentence length:  212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "940968ba-608d-487c-bffa-900ec1011238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "59983ac1-00a7-4b3c-d0d2-e3a58501a161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "print(train_inputs)\n",
        "print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "#print(train_masks)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101    881  11549 ...      0      0      0]\n",
            " [   101    882  27391 ...      0      0      0]\n",
            " [   101    899  15552 ...  10826    889  78530]\n",
            " ...\n",
            " [   101    882  27391 ...  14265    886  13432]\n",
            " [   101    882  27391 ...    884 100915  16380]\n",
            " [   101    889  18351 ...      0      0      0]]\n",
            "[1 0 1 ... 1 0 0]\n",
            "[[   101    881  11549 ...      0      0      0]\n",
            " [   101    882  27391 ...      0      0      0]\n",
            " [   101    899  15552 ...  10826    889  78530]\n",
            " ...\n",
            " [   101    882  27391 ...  14265    886  13432]\n",
            " [   101    882  27391 ...    884 100915  16380]\n",
            " [   101    889  18351 ...      0      0      0]]\n",
            "[1 0 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "55e1b527-97ca-4ae4-9abe-342724ad37e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "outputId": "f2f18d72-f607-43ab-92b4-7d0b469261b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n",
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "outputId": "c7808155-3755-478c-ed28-019d72bddda1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epcoh took: 0:00:26\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.26\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "outputId": "ebbbc887-86cc-42fa-cd57-574ad7fc6933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task1hindi-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "outputId": "e5d906cd-f1a5-4759-ca81-0ce0b0a99571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "#print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 900 test sentences...\n",
            "    DONE.\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "outputId": "889e01b1-8169-4156-a815-037904a82389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] ):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] ):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    # elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "    #   #print(\" Semi Paraphrase     \",end=\"\")\n",
        "    #   #count_NP+=1      \n",
        "    #   print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "    #   x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(1)\t 1\t [-1.6758636  2.046024 ] \t1\ttrue\n",
            "(0)\t 2\t [ 2.13875   -2.0825372] \t0\ttrue\n",
            "(1)\t 3\t [-1.8004187  2.027262 ] \t1\ttrue\n",
            "(0)\t 4\t [ 1.8269557 -1.9381888] \t0\ttrue\n",
            "(0)\t 5\t [ 2.1909826 -2.063403 ] \t0\ttrue\n",
            "(1)\t 6\t [-1.3945733  1.2508321] \t1\ttrue\n",
            "(0)\t 7\t [ 2.1287696 -2.1769288] \t0\ttrue\n",
            "(0)\t 8\t [ 1.8862541 -1.8272736] \t0\ttrue\n",
            "(1)\t 9\t [-1.5068591  1.1041815] \t1\ttrue\n",
            "(0)\t 10\t [ 2.0352836 -2.082721 ] \t0\ttrue\n",
            "(0)\t 11\t [ 0.6615194 -1.3502717] \t0\ttrue\n",
            "(1)\t 12\t [-0.06145397 -0.2787489 ] \t0\tfalse\n",
            "(0)\t 13\t [ 1.9604257 -2.0373886] \t0\ttrue\n",
            "(0)\t 14\t [ 1.0013927 -1.7328866] \t0\ttrue\n",
            "(1)\t 15\t [-1.596306   2.0558975] \t1\ttrue\n",
            "(0)\t 16\t [-1.8307753  1.664124 ] \t1\tfalse\n",
            "(0)\t 17\t [-0.09304757 -0.728441  ] \t0\ttrue\n",
            "(0)\t 18\t [ 1.9984021 -1.934967 ] \t0\ttrue\n",
            "(1)\t 19\t [-1.7868441  1.8670609] \t1\ttrue\n",
            "(1)\t 20\t [-1.7678653  1.8206753] \t1\ttrue\n",
            "(0)\t 21\t [ 0.45392498 -1.160796  ] \t0\ttrue\n",
            "(0)\t 22\t [ 2.181983 -2.135288] \t0\ttrue\n",
            "(0)\t 23\t [-1.041771   0.7578257] \t1\tfalse\n",
            "(1)\t 24\t [-1.7906796  1.853015 ] \t1\ttrue\n",
            "(0)\t 25\t [ 1.794228  -1.9864727] \t0\ttrue\n",
            "(0)\t 26\t [ 2.0346518 -2.0489562] \t0\ttrue\n",
            "(1)\t 27\t [-1.7720349  1.8666452] \t1\ttrue\n",
            "(0)\t 28\t [ 1.4653133 -1.533331 ] \t0\ttrue\n",
            "(0)\t 29\t [ 1.3901092 -1.7787532] \t0\ttrue\n",
            "(1)\t 30\t [-1.6613472  2.003858 ] \t1\ttrue\n",
            "(1)\t 31\t [-1.6600356  1.9140685] \t1\ttrue\n",
            "(0)\t 32\t [ 2.0492313 -2.070082 ] \t0\ttrue\n",
            "(1)\t 33\t [-1.6659678  1.9663812] \t1\ttrue\n",
            "(0)\t 34\t [-0.3299118 -0.2977021] \t1\tfalse\n",
            "(0)\t 35\t [ 1.4432901 -1.7423812] \t0\ttrue\n",
            "(0)\t 36\t [ 1.8384337 -1.8782417] \t0\ttrue\n",
            "(0)\t 37\t [ 0.4012136 -1.0444071] \t0\ttrue\n",
            "(1)\t 38\t [-1.7549344  1.8187711] \t1\ttrue\n",
            "(0)\t 39\t [ 2.114118  -1.9278098] \t0\ttrue\n",
            "(1)\t 40\t [-1.6334348  1.7627769] \t1\ttrue\n",
            "(0)\t 41\t [ 1.0314858 -1.5694199] \t0\ttrue\n",
            "(1)\t 42\t [-1.6435724  1.9835166] \t1\ttrue\n",
            "(0)\t 43\t [ 1.4894114 -1.631882 ] \t0\ttrue\n",
            "(0)\t 44\t [ 0.2626468  -0.95096433] \t0\ttrue\n",
            "(0)\t 45\t [-0.15459955 -0.47513992] \t0\ttrue\n",
            "(0)\t 46\t [-0.8173737   0.19259001] \t1\tfalse\n",
            "(1)\t 47\t [-1.5053     1.9951181] \t1\ttrue\n",
            "(1)\t 48\t [-1.7320235  1.7443831] \t1\ttrue\n",
            "(0)\t 49\t [-0.9558816   0.28643623] \t1\tfalse\n",
            "(0)\t 50\t [ 1.9836395 -1.9593555] \t0\ttrue\n",
            "(1)\t 51\t [-1.891338   2.0438933] \t1\ttrue\n",
            "(0)\t 52\t [ 2.1997013 -2.0596404] \t0\ttrue\n",
            "(1)\t 53\t [ 0.62999815 -1.2350736 ] \t0\tfalse\n",
            "(1)\t 54\t [-1.652383   2.0146005] \t1\ttrue\n",
            "(0)\t 55\t [ 1.5899652 -1.9619039] \t0\ttrue\n",
            "(0)\t 56\t [ 0.0031887  -0.66168094] \t0\ttrue\n",
            "(0)\t 57\t [ 1.7616102 -1.8932203] \t0\ttrue\n",
            "(0)\t 58\t [-0.02864945 -0.7547408 ] \t0\ttrue\n",
            "(1)\t 59\t [-1.7519828  1.9129827] \t1\ttrue\n",
            "(0)\t 60\t [ 0.12199801 -0.972583  ] \t0\ttrue\n",
            "(0)\t 61\t [ 1.9878596 -1.982774 ] \t0\ttrue\n",
            "(1)\t 62\t [-1.6858779  1.6591221] \t1\ttrue\n",
            "(0)\t 63\t [-1.0085542   0.40562877] \t1\tfalse\n",
            "(1)\t 64\t [-1.6601226  2.0175095] \t1\ttrue\n",
            "(0)\t 65\t [ 1.6822078 -1.6925764] \t0\ttrue\n",
            "(0)\t 66\t [-0.02740681 -0.7908186 ] \t0\ttrue\n",
            "(1)\t 67\t [-1.809059   1.8325783] \t1\ttrue\n",
            "(1)\t 68\t [-1.6171937  1.9587982] \t1\ttrue\n",
            "(0)\t 69\t [-1.6606721  1.4892924] \t1\tfalse\n",
            "(0)\t 70\t [ 2.0291302 -2.0546339] \t0\ttrue\n",
            "(1)\t 71\t [-1.473991   1.9829636] \t1\ttrue\n",
            "(1)\t 72\t [-1.5021464  1.9693136] \t1\ttrue\n",
            "(0)\t 73\t [ 2.0433223 -2.0969634] \t0\ttrue\n",
            "(0)\t 74\t [ 1.7881349 -1.9570795] \t0\ttrue\n",
            "(1)\t 75\t [-1.4769971  1.9795225] \t1\ttrue\n",
            "(0)\t 76\t [ 2.0202813 -2.0424159] \t0\ttrue\n",
            "(1)\t 77\t [-1.7017108  2.0151005] \t1\ttrue\n",
            "(0)\t 78\t [-0.29273686 -0.49012193] \t0\ttrue\n",
            "(1)\t 79\t [-1.6850209  1.8894254] \t1\ttrue\n",
            "(0)\t 80\t [ 1.9480519 -1.9587077] \t0\ttrue\n",
            "(1)\t 81\t [-1.8304176  1.855658 ] \t1\ttrue\n",
            "(0)\t 82\t [-1.2325064  0.7199671] \t1\tfalse\n",
            "(0)\t 83\t [ 1.1173899 -1.6396624] \t0\ttrue\n",
            "(1)\t 84\t [-1.7340708  1.7282839] \t1\ttrue\n",
            "(0)\t 85\t [ 0.9145211 -1.5445131] \t0\ttrue\n",
            "(1)\t 86\t [-0.6631876  -0.08851279] \t1\ttrue\n",
            "(1)\t 87\t [-1.808244   1.8125312] \t1\ttrue\n",
            "(0)\t 88\t [ 1.974055  -2.0546947] \t0\ttrue\n",
            "(1)\t 89\t [-1.3850282  0.7357727] \t1\ttrue\n",
            "(0)\t 90\t [-1.6535449  1.6540053] \t1\tfalse\n",
            "(0)\t 91\t [ 2.253138  -2.0104446] \t0\ttrue\n",
            "(1)\t 92\t [-1.6993517  2.0006292] \t1\ttrue\n",
            "(1)\t 93\t [-1.6910611  1.8946913] \t1\ttrue\n",
            "(0)\t 94\t [-0.5205675 -0.3141836] \t1\tfalse\n",
            "(0)\t 95\t [ 1.1244481 -1.7184567] \t0\ttrue\n",
            "(1)\t 96\t [-1.8561435  1.8564335] \t1\ttrue\n",
            "(0)\t 97\t [ 1.1433469 -1.7607727] \t0\ttrue\n",
            "(1)\t 98\t [-1.8014648  1.8111708] \t1\ttrue\n",
            "(0)\t 99\t [-0.39276925 -0.1832155 ] \t1\tfalse\n",
            "(1)\t 100\t [-0.99020505  0.3737881 ] \t1\ttrue\n",
            "(0)\t 101\t [-0.21680169 -0.58546066] \t0\ttrue\n",
            "(1)\t 102\t [-0.05628867 -0.7536584 ] \t0\tfalse\n",
            "(0)\t 103\t [-0.05628867 -0.7536584 ] \t0\ttrue\n",
            "(0)\t 104\t [ 0.75303954 -1.4192662 ] \t0\ttrue\n",
            "(1)\t 105\t [-1.6750218  1.6760322] \t1\ttrue\n",
            "(1)\t 106\t [-1.5556062  2.0563018] \t1\ttrue\n",
            "(0)\t 107\t [ 1.7053789 -1.9755703] \t0\ttrue\n",
            "(0)\t 108\t [ 1.2456576 -1.8277638] \t0\ttrue\n",
            "(1)\t 109\t [-1.3981123  1.3049834] \t1\ttrue\n",
            "(0)\t 110\t [ 2.178496 -1.960459] \t0\ttrue\n",
            "(1)\t 111\t [-1.6295472  2.0018075] \t1\ttrue\n",
            "(1)\t 112\t [ 0.32938096 -1.2006938 ] \t0\tfalse\n",
            "(0)\t 113\t [ 0.32938096 -1.2006938 ] \t0\ttrue\n",
            "(0)\t 114\t [ 1.5860411 -1.9621466] \t0\ttrue\n",
            "(1)\t 115\t [-0.86145526  0.13693891] \t1\ttrue\n",
            "(1)\t 116\t [-1.7202168  1.913716 ] \t1\ttrue\n",
            "(0)\t 117\t [ 2.0968714 -1.9942814] \t0\ttrue\n",
            "(0)\t 118\t [ 2.118747  -2.0749626] \t0\ttrue\n",
            "(1)\t 119\t [-1.7594109  2.031532 ] \t1\ttrue\n",
            "(0)\t 120\t [ 1.793795  -1.9762679] \t0\ttrue\n",
            "(1)\t 121\t [-1.5218904  1.9970719] \t1\ttrue\n",
            "(1)\t 122\t [-1.573526   1.4992777] \t1\ttrue\n",
            "(0)\t 123\t [ 2.2574036 -2.1024234] \t0\ttrue\n",
            "(0)\t 124\t [ 1.5652304 -1.8491666] \t0\ttrue\n",
            "(1)\t 125\t [-1.748489   1.7243671] \t1\ttrue\n",
            "(1)\t 126\t [-1.497159   1.9991466] \t1\ttrue\n",
            "(0)\t 127\t [ 0.9618002 -1.444011 ] \t0\ttrue\n",
            "(0)\t 128\t [ 1.1752546 -1.7499659] \t0\ttrue\n",
            "(0)\t 129\t [ 2.1317172 -2.077323 ] \t0\ttrue\n",
            "(0)\t 130\t [ 2.155742  -2.1460586] \t0\ttrue\n",
            "(1)\t 131\t [-1.6931067  1.6114185] \t1\ttrue\n",
            "(0)\t 132\t [ 0.861062  -1.5354186] \t0\ttrue\n",
            "(1)\t 133\t [-1.7930273  1.7859921] \t1\ttrue\n",
            "(0)\t 134\t [ 1.9402771 -1.9806995] \t0\ttrue\n",
            "(0)\t 135\t [ 1.3449363 -1.7882887] \t0\ttrue\n",
            "(1)\t 136\t [-1.7410228  1.9659975] \t1\ttrue\n",
            "(1)\t 137\t [-1.7887762  1.7724441] \t1\ttrue\n",
            "(0)\t 138\t [ 1.7838409 -1.906487 ] \t0\ttrue\n",
            "(1)\t 139\t [-1.6867558  1.7641425] \t1\ttrue\n",
            "(0)\t 140\t [ 0.6570796 -1.2985905] \t0\ttrue\n",
            "(0)\t 141\t [ 1.2639297 -1.6557021] \t0\ttrue\n",
            "(1)\t 142\t [ 0.22059444 -1.0741633 ] \t0\tfalse\n",
            "(0)\t 143\t [-0.32972932 -0.48417947] \t0\ttrue\n",
            "(1)\t 144\t [ 1.7778829 -1.9703915] \t0\tfalse\n",
            "(0)\t 145\t [ 1.6536405 -1.9653778] \t0\ttrue\n",
            "(1)\t 146\t [-1.7409338  1.7012639] \t1\ttrue\n",
            "(0)\t 147\t [ 0.59340733 -1.2302635 ] \t0\ttrue\n",
            "(1)\t 148\t [-1.6640605  2.016825 ] \t1\ttrue\n",
            "(1)\t 149\t [-1.7634097  2.0564342] \t1\ttrue\n",
            "(0)\t 150\t [ 2.1139362 -2.0338662] \t0\ttrue\n",
            "(0)\t 151\t [ 2.0347984 -2.1060174] \t0\ttrue\n",
            "(1)\t 152\t [-1.600723   1.9922388] \t1\ttrue\n",
            "(0)\t 153\t [-0.05958579 -0.5003955 ] \t0\ttrue\n",
            "(0)\t 154\t [ 2.114424  -2.0339377] \t0\ttrue\n",
            "(1)\t 155\t [-1.628004   1.9393358] \t1\ttrue\n",
            "(0)\t 156\t [ 2.0594378 -1.9515202] \t0\ttrue\n",
            "(0)\t 157\t [-0.02207332 -0.73444045] \t0\ttrue\n",
            "(1)\t 158\t [-0.02207332 -0.73444045] \t0\tfalse\n",
            "(1)\t 159\t [-1.5078187  2.017996 ] \t1\ttrue\n",
            "(0)\t 160\t [-1.3544865  1.0256796] \t1\tfalse\n",
            "(0)\t 161\t [ 1.1133039 -1.6948898] \t0\ttrue\n",
            "(0)\t 162\t [-1.3988531  1.0080047] \t1\tfalse\n",
            "(1)\t 163\t [-1.5040841  1.9406255] \t1\ttrue\n",
            "(1)\t 164\t [-1.3878818  1.9128176] \t1\ttrue\n",
            "(0)\t 165\t [ 1.9817872 -2.0402713] \t0\ttrue\n",
            "(1)\t 166\t [-1.531816   1.9241693] \t1\ttrue\n",
            "(0)\t 167\t [ 2.0414917 -2.1605384] \t0\ttrue\n",
            "(1)\t 168\t [-1.675412   1.9694066] \t1\ttrue\n",
            "(0)\t 169\t [ 1.5158159 -1.9081312] \t0\ttrue\n",
            "(0)\t 170\t [ 0.12824984 -0.88043004] \t0\ttrue\n",
            "(1)\t 171\t [ 0.12824984 -0.88043004] \t0\tfalse\n",
            "(0)\t 172\t [ 1.6493752 -1.7601113] \t0\ttrue\n",
            "(0)\t 173\t [ 1.9776814 -1.9967215] \t0\ttrue\n",
            "(1)\t 174\t [-1.6780071  2.0330646] \t1\ttrue\n",
            "(0)\t 175\t [ 1.648282  -1.9586129] \t0\ttrue\n",
            "(0)\t 176\t [ 2.1334488 -1.9346337] \t0\ttrue\n",
            "(1)\t 177\t [-1.7254055  1.6648731] \t1\ttrue\n",
            "(1)\t 178\t [-1.9095185  1.9888809] \t1\ttrue\n",
            "(0)\t 179\t [ 1.892191 -2.092762] \t0\ttrue\n",
            "(1)\t 180\t [-0.56984854 -0.11675105] \t1\ttrue\n",
            "(0)\t 181\t [ 1.6448127 -1.7973208] \t0\ttrue\n",
            "(0)\t 182\t [ 2.1589444 -2.0546935] \t0\ttrue\n",
            "(1)\t 183\t [-1.5020863  1.9662893] \t1\ttrue\n",
            "(1)\t 184\t [ 0.45369253 -1.1701941 ] \t0\tfalse\n",
            "(0)\t 185\t [-1.4104757  1.1883856] \t1\tfalse\n",
            "(0)\t 186\t [ 0.3345581 -1.0579149] \t0\ttrue\n",
            "(1)\t 187\t [-1.8064412  1.9907153] \t1\ttrue\n",
            "(1)\t 188\t [-1.476502   1.9541427] \t1\ttrue\n",
            "(0)\t 189\t [ 2.0007272 -1.9399151] \t0\ttrue\n",
            "(0)\t 190\t [ 2.1266909 -2.121271 ] \t0\ttrue\n",
            "(1)\t 191\t [-1.8225737  2.016505 ] \t1\ttrue\n",
            "(1)\t 192\t [-1.7794348  1.8533092] \t1\ttrue\n",
            "(0)\t 193\t [ 0.88418025 -1.57936   ] \t0\ttrue\n",
            "(1)\t 194\t [-1.6822474  1.5712727] \t1\ttrue\n",
            "(0)\t 195\t [-0.23002037 -0.5488283 ] \t0\ttrue\n",
            "(1)\t 196\t [-1.7221001  1.7790786] \t1\ttrue\n",
            "(0)\t 197\t [ 0.96203524 -1.6062338 ] \t0\ttrue\n",
            "(1)\t 198\t [-1.5481551  1.975028 ] \t1\ttrue\n",
            "(0)\t 199\t [ 1.5496023 -1.6718087] \t0\ttrue\n",
            "(0)\t 200\t [-0.20472068 -0.5217138 ] \t0\ttrue\n",
            "(0)\t 201\t [-0.5391581 -0.330439 ] \t1\tfalse\n",
            "(1)\t 202\t [-1.6946428  1.8401363] \t1\ttrue\n",
            "(1)\t 203\t [ 2.0182073 -2.0689838] \t0\tfalse\n",
            "(0)\t 204\t [ 1.9488336 -1.9938745] \t0\ttrue\n",
            "(0)\t 205\t [ 2.0173469 -2.0643482] \t0\ttrue\n",
            "(1)\t 206\t [-1.5722896  1.9954782] \t1\ttrue\n",
            "(1)\t 207\t [-1.71438    1.8456722] \t1\ttrue\n",
            "(0)\t 208\t [ 0.43319792 -1.1848401 ] \t0\ttrue\n",
            "(1)\t 209\t [-1.561014  2.005552] \t1\ttrue\n",
            "(0)\t 210\t [ 1.9674047 -2.0409446] \t0\ttrue\n",
            "(0)\t 211\t [ 0.75942975 -1.0930042 ] \t0\ttrue\n",
            "(1)\t 212\t [-1.5601728  2.0187016] \t1\ttrue\n",
            "(1)\t 213\t [-1.801989   1.8513755] \t1\ttrue\n",
            "(0)\t 214\t [ 2.0610476 -2.0461268] \t0\ttrue\n",
            "(0)\t 215\t [-0.8816928   0.11259965] \t1\tfalse\n",
            "(1)\t 216\t [-1.6448908  1.7227149] \t1\ttrue\n",
            "(0)\t 217\t [-1.8178449  1.8418661] \t1\tfalse\n",
            "(1)\t 218\t [-1.802188  1.763978] \t1\ttrue\n",
            "(0)\t 219\t [-1.7547953  1.7779005] \t1\tfalse\n",
            "(1)\t 220\t [-1.7789727  1.714406 ] \t1\ttrue\n",
            "(1)\t 221\t [-1.7863052  1.888548 ] \t1\ttrue\n",
            "(0)\t 222\t [ 1.9249679 -2.0518935] \t0\ttrue\n",
            "(1)\t 223\t [-1.4623578  2.0164144] \t1\ttrue\n",
            "(0)\t 224\t [ 2.065953  -2.1241093] \t0\ttrue\n",
            "(0)\t 225\t [ 2.065953  -2.1241093] \t0\ttrue\n",
            "(1)\t 226\t [-1.4346051  1.0457641] \t1\ttrue\n",
            "(0)\t 227\t [-1.890708   1.8544565] \t1\tfalse\n",
            "(1)\t 228\t [-1.8966572  1.9587306] \t1\ttrue\n",
            "(1)\t 229\t [ 0.19876331 -0.9070594 ] \t0\tfalse\n",
            "(0)\t 230\t [ 0.19876331 -0.9070594 ] \t0\ttrue\n",
            "(0)\t 231\t [ 1.7559431 -1.9854333] \t0\ttrue\n",
            "(1)\t 232\t [-1.6327698  1.3754505] \t1\ttrue\n",
            "(1)\t 233\t [-1.5508691  1.0513871] \t1\ttrue\n",
            "(0)\t 234\t [-0.86533105  0.06404629] \t1\tfalse\n",
            "(1)\t 235\t [-1.9645865  1.779686 ] \t1\ttrue\n",
            "(0)\t 236\t [ 2.2117946 -2.1833034] \t0\ttrue\n",
            "(0)\t 237\t [-0.3604221 -0.5164163] \t0\ttrue\n",
            "(1)\t 238\t [-1.586807   1.5402131] \t1\ttrue\n",
            "(1)\t 239\t [-1.7754027  1.8767219] \t1\ttrue\n",
            "(0)\t 240\t [ 2.0841153 -2.1344502] \t0\ttrue\n",
            "(0)\t 241\t [ 1.3378946 -1.6031209] \t0\ttrue\n",
            "(1)\t 242\t [-1.7472341  1.8495   ] \t1\ttrue\n",
            "(1)\t 243\t [ 0.3351753 -1.0313418] \t0\tfalse\n",
            "(0)\t 244\t [ 2.2699533 -1.9991885] \t0\ttrue\n",
            "(1)\t 245\t [-1.7242303  1.8962879] \t1\ttrue\n",
            "(0)\t 246\t [-0.9116061   0.36527827] \t1\tfalse\n",
            "(1)\t 247\t [-1.631699  1.972141] \t1\ttrue\n",
            "(0)\t 248\t [ 1.304013  -1.2321131] \t0\ttrue\n",
            "(0)\t 249\t [-0.6198087  -0.03772908] \t1\tfalse\n",
            "(1)\t 250\t [ 1.2028971 -1.6413009] \t0\tfalse\n",
            "(0)\t 251\t [ 2.2325397 -2.1497257] \t0\ttrue\n",
            "(1)\t 252\t [-1.6408257  2.0001724] \t1\ttrue\n",
            "(0)\t 253\t [ 0.25985765 -0.9858419 ] \t0\ttrue\n",
            "(1)\t 254\t [ 0.36998743 -1.0597314 ] \t0\tfalse\n",
            "(0)\t 255\t [ 1.1395253 -1.7090036] \t0\ttrue\n",
            "(1)\t 256\t [-1.6982783  1.6804628] \t1\ttrue\n",
            "(0)\t 257\t [ 1.6927309 -2.002781 ] \t0\ttrue\n",
            "(1)\t 258\t [ 0.36157462 -1.0536752 ] \t0\tfalse\n",
            "(1)\t 259\t [-1.4553689  1.1281372] \t1\ttrue\n",
            "(0)\t 260\t [ 1.2406069 -1.7823993] \t0\ttrue\n",
            "(1)\t 261\t [-1.6906655  1.8960736] \t1\ttrue\n",
            "(0)\t 262\t [ 1.3145763 -1.751296 ] \t0\ttrue\n",
            "(1)\t 263\t [-0.6447142  -0.19365749] \t1\ttrue\n",
            "(0)\t 264\t [-0.6447142  -0.19365749] \t1\tfalse\n",
            "(1)\t 265\t [ 0.28906   -1.1017869] \t0\tfalse\n",
            "(0)\t 266\t [ 1.991781  -1.9598454] \t0\ttrue\n",
            "(1)\t 267\t [-1.6758991  1.9843485] \t1\ttrue\n",
            "(0)\t 268\t [ 2.0694337 -2.001285 ] \t0\ttrue\n",
            "(0)\t 269\t [ 1.7238085 -1.8557057] \t0\ttrue\n",
            "(1)\t 270\t [-1.3255134  1.1167893] \t1\ttrue\n",
            "(0)\t 271\t [ 1.4408485 -1.8434048] \t0\ttrue\n",
            "(1)\t 272\t [-1.6470373  1.5753758] \t1\ttrue\n",
            "(1)\t 273\t [-1.7226753  1.801862 ] \t1\ttrue\n",
            "(0)\t 274\t [-1.2897356  0.7987283] \t1\tfalse\n",
            "(0)\t 275\t [ 1.4950824 -1.7942586] \t0\ttrue\n",
            "(1)\t 276\t [-1.7701305  1.8742182] \t1\ttrue\n",
            "(0)\t 277\t [-0.8739191   0.12620687] \t1\tfalse\n",
            "(1)\t 278\t [-1.650189   1.6548282] \t1\ttrue\n",
            "(1)\t 279\t [-1.4829234  1.455569 ] \t1\ttrue\n",
            "(0)\t 280\t [ 2.0084126 -1.9897931] \t0\ttrue\n",
            "(1)\t 281\t [-1.7684131  1.8371978] \t1\ttrue\n",
            "(0)\t 282\t [ 1.9634103 -2.0812845] \t0\ttrue\n",
            "(0)\t 283\t [ 1.5805708 -1.8734841] \t0\ttrue\n",
            "(1)\t 284\t [-1.1676536   0.77188987] \t1\ttrue\n",
            "(0)\t 285\t [ 1.4147685 -1.7181802] \t0\ttrue\n",
            "(0)\t 286\t [-1.6005036  1.2667732] \t1\tfalse\n",
            "(1)\t 287\t [-1.8333415  1.8160373] \t1\ttrue\n",
            "(0)\t 288\t [ 1.0253403 -1.688883 ] \t0\ttrue\n",
            "(1)\t 289\t [-1.849939   1.9320803] \t1\ttrue\n",
            "(1)\t 290\t [-1.7911596  1.9805131] \t1\ttrue\n",
            "(0)\t 291\t [-0.2561139 -0.511834 ] \t0\ttrue\n",
            "(0)\t 292\t [ 0.27575767 -1.0001585 ] \t0\ttrue\n",
            "(1)\t 293\t [ 0.27575767 -1.0001585 ] \t0\tfalse\n",
            "(0)\t 294\t [ 1.9456606 -2.0546417] \t0\ttrue\n",
            "(1)\t 295\t [-1.6875942  2.0473616] \t1\ttrue\n",
            "(0)\t 296\t [ 2.0839646 -1.9867241] \t0\ttrue\n",
            "(0)\t 297\t [ 1.7629218 -1.9801621] \t0\ttrue\n",
            "(1)\t 298\t [-1.7767957  1.8819599] \t1\ttrue\n",
            "(0)\t 299\t [ 1.8704554 -1.8540981] \t0\ttrue\n",
            "(1)\t 300\t [-1.6828022  1.8098533] \t1\ttrue\n",
            "(0)\t 301\t [ 0.53609234 -1.1350151 ] \t0\ttrue\n",
            "(1)\t 302\t [-1.7581127  1.8756547] \t1\ttrue\n",
            "(1)\t 303\t [-1.6266752  2.0486887] \t1\ttrue\n",
            "(0)\t 304\t [ 1.9359199 -2.0993254] \t0\ttrue\n",
            "(1)\t 305\t [-1.8238891  1.7802312] \t1\ttrue\n",
            "(0)\t 306\t [-0.9757109  0.6693021] \t1\tfalse\n",
            "(0)\t 307\t [ 0.5314632 -1.1393138] \t0\ttrue\n",
            "(1)\t 308\t [-1.4933281  2.0579755] \t1\ttrue\n",
            "(0)\t 309\t [ 0.38130406 -1.0454777 ] \t0\ttrue\n",
            "(0)\t 310\t [-1.1110249   0.43475512] \t1\tfalse\n",
            "(1)\t 311\t [-1.7211256  1.5992775] \t1\ttrue\n",
            "(1)\t 312\t [-1.889835   1.7904204] \t1\ttrue\n",
            "(0)\t 313\t [-0.08513352 -0.40295348] \t0\ttrue\n",
            "(1)\t 314\t [-1.156635    0.60568047] \t1\ttrue\n",
            "(0)\t 315\t [ 1.9314998 -2.1077046] \t0\ttrue\n",
            "(1)\t 316\t [-1.719598   1.9651766] \t1\ttrue\n",
            "(0)\t 317\t [ 0.42607597 -1.1224666 ] \t0\ttrue\n",
            "(0)\t 318\t [ 2.1437807 -1.9095455] \t0\ttrue\n",
            "(1)\t 319\t [-1.5239397  2.0072465] \t1\ttrue\n",
            "(0)\t 320\t [-1.7077993  1.6460178] \t1\tfalse\n",
            "(1)\t 321\t [-1.7937653  1.8826103] \t1\ttrue\n",
            "(1)\t 322\t [-1.7454119  1.8398938] \t1\ttrue\n",
            "(0)\t 323\t [ 1.965777  -2.0764325] \t0\ttrue\n",
            "(0)\t 324\t [ 1.5831203 -1.9152023] \t0\ttrue\n",
            "(1)\t 325\t [-1.9016076  1.8256264] \t1\ttrue\n",
            "(0)\t 326\t [ 2.150885  -2.0572236] \t0\ttrue\n",
            "(0)\t 327\t [-0.1577321 -0.5818044] \t0\ttrue\n",
            "(1)\t 328\t [-0.1577321 -0.5818044] \t0\tfalse\n",
            "(1)\t 329\t [-1.8564061  2.0150385] \t1\ttrue\n",
            "(0)\t 330\t [ 0.48580912 -1.1650537 ] \t0\ttrue\n",
            "(0)\t 331\t [ 2.154665 -2.156925] \t0\ttrue\n",
            "(1)\t 332\t [-1.5764459   0.86180377] \t1\ttrue\n",
            "(0)\t 333\t [-1.5041951  1.9622737] \t1\tfalse\n",
            "(1)\t 334\t [-1.76207    1.7752934] \t1\ttrue\n",
            "(0)\t 335\t [ 0.1992608  -0.89392316] \t0\ttrue\n",
            "(1)\t 336\t [-1.7961292  1.8049319] \t1\ttrue\n",
            "(0)\t 337\t [ 1.7186145 -2.0585985] \t0\ttrue\n",
            "(0)\t 338\t [ 2.217675  -1.9876525] \t0\ttrue\n",
            "(0)\t 339\t [ 0.75684285 -1.3870555 ] \t0\ttrue\n",
            "(1)\t 340\t [-1.8446563  1.8377767] \t1\ttrue\n",
            "(1)\t 341\t [-1.7138029  1.6822587] \t1\ttrue\n",
            "(0)\t 342\t [ 2.1240087 -2.1536052] \t0\ttrue\n",
            "(0)\t 343\t [-0.723395    0.21103477] \t1\tfalse\n",
            "(1)\t 344\t [-1.4623805  1.9518789] \t1\ttrue\n",
            "(1)\t 345\t [-1.6888463  2.0059392] \t1\ttrue\n",
            "(0)\t 346\t [ 1.2573365 -1.6502684] \t0\ttrue\n",
            "(1)\t 347\t [-1.3061812   0.77983963] \t1\ttrue\n",
            "(0)\t 348\t [ 1.9467139 -2.0934734] \t0\ttrue\n",
            "(0)\t 349\t [ 0.1558081  -0.92965525] \t0\ttrue\n",
            "(1)\t 350\t [ 0.1558081  -0.92965525] \t0\tfalse\n",
            "(1)\t 351\t [ 1.1891334 -1.7278641] \t0\tfalse\n",
            "(0)\t 352\t [ 1.1891334 -1.7278641] \t0\ttrue\n",
            "(0)\t 353\t [ 0.6262544 -1.3370368] \t0\ttrue\n",
            "(1)\t 354\t [-1.7622895  1.8803002] \t1\ttrue\n",
            "(1)\t 355\t [-1.4803997  1.5380242] \t1\ttrue\n",
            "(0)\t 356\t [ 2.1699357 -2.1159236] \t0\ttrue\n",
            "(0)\t 357\t [ 0.9750392 -1.5409701] \t0\ttrue\n",
            "(1)\t 358\t [-1.7545986  1.8300974] \t1\ttrue\n",
            "(1)\t 359\t [-1.5442286  1.3665521] \t1\ttrue\n",
            "(0)\t 360\t [ 1.927207  -2.0696983] \t0\ttrue\n",
            "(1)\t 361\t [-1.6666754  1.9738173] \t1\ttrue\n",
            "(0)\t 362\t [-0.38005033 -0.40447563] \t0\ttrue\n",
            "(1)\t 363\t [-1.7062438  1.7053108] \t1\ttrue\n",
            "(0)\t 364\t [ 2.0613987 -2.0206609] \t0\ttrue\n",
            "(1)\t 365\t [-1.7000451  1.8683131] \t1\ttrue\n",
            "(0)\t 366\t [ 1.260597  -1.7548603] \t0\ttrue\n",
            "(0)\t 367\t [-1.4256549  0.9635778] \t1\tfalse\n",
            "(1)\t 368\t [-1.5986137  1.4074016] \t1\ttrue\n",
            "(1)\t 369\t [-1.5929708  2.0316124] \t1\ttrue\n",
            "(0)\t 370\t [ 2.1851459 -2.1003907] \t0\ttrue\n",
            "(0)\t 371\t [ 1.510997  -1.8672614] \t0\ttrue\n",
            "(1)\t 372\t [-1.8527086  1.8855141] \t1\ttrue\n",
            "(1)\t 373\t [-1.8074092  1.8071871] \t1\ttrue\n",
            "(0)\t 374\t [ 1.9773514 -2.022501 ] \t0\ttrue\n",
            "(0)\t 375\t [ 1.3263378 -1.6688914] \t0\ttrue\n",
            "(1)\t 376\t [-1.837156   1.9030122] \t1\ttrue\n",
            "(1)\t 377\t [-1.5806568  1.8231096] \t1\ttrue\n",
            "(0)\t 378\t [ 1.7413046 -1.9580264] \t0\ttrue\n",
            "(1)\t 379\t [ 2.0903933 -1.9788072] \t0\tfalse\n",
            "(0)\t 380\t [ 1.3090633 -1.7415751] \t0\ttrue\n",
            "(0)\t 381\t [ 2.0702658 -2.0360644] \t0\ttrue\n",
            "(1)\t 382\t [ 0.42888287 -1.1078519 ] \t0\tfalse\n",
            "(0)\t 383\t [ 0.42888287 -1.1078519 ] \t0\ttrue\n",
            "(0)\t 384\t [ 1.7612953 -1.8761061] \t0\ttrue\n",
            "(0)\t 385\t [ 2.2485778 -2.1191833] \t0\ttrue\n",
            "(0)\t 386\t [ 1.2749071 -1.7574601] \t0\ttrue\n",
            "(1)\t 387\t [-1.6593105  2.0306542] \t1\ttrue\n",
            "(0)\t 388\t [ 1.9323856 -2.0636861] \t0\ttrue\n",
            "(1)\t 389\t [-0.22341593 -0.60504645] \t0\tfalse\n",
            "(0)\t 390\t [-0.22341593 -0.60504645] \t0\ttrue\n",
            "(0)\t 391\t [ 0.68776   -1.4509901] \t0\ttrue\n",
            "(0)\t 392\t [ 0.57453585 -0.85996693] \t0\ttrue\n",
            "(1)\t 393\t [-1.6165961  1.9855149] \t1\ttrue\n",
            "(0)\t 394\t [-0.12696806 -0.6903717 ] \t0\ttrue\n",
            "(1)\t 395\t [ 0.88303334 -1.4991401 ] \t0\tfalse\n",
            "(0)\t 396\t [ 2.1429348 -2.007024 ] \t0\ttrue\n",
            "(1)\t 397\t [-1.7924901  1.8528132] \t1\ttrue\n",
            "(0)\t 398\t [-1.6858715  1.6890404] \t1\tfalse\n",
            "(1)\t 399\t [-1.5282941  1.243654 ] \t1\ttrue\n",
            "(0)\t 400\t [ 2.269704 -2.037007] \t0\ttrue\n",
            "(0)\t 401\t [ 2.053837 -2.115927] \t0\ttrue\n",
            "(1)\t 402\t [-1.6087955  2.0254002] \t1\ttrue\n",
            "(1)\t 403\t [-0.9139042   0.34496334] \t1\ttrue\n",
            "(0)\t 404\t [-0.0734193 -0.6767993] \t0\ttrue\n",
            "(0)\t 405\t [ 1.6677358 -1.6713102] \t0\ttrue\n",
            "(1)\t 406\t [-1.5272764  1.999582 ] \t1\ttrue\n",
            "(1)\t 407\t [-1.7476997  1.9003215] \t1\ttrue\n",
            "(0)\t 408\t [ 1.6942607 -1.9689188] \t0\ttrue\n",
            "(0)\t 409\t [ 0.40082565 -1.1092744 ] \t0\ttrue\n",
            "(0)\t 410\t [ 2.2465682 -2.1782277] \t0\ttrue\n",
            "(1)\t 411\t [-1.6736763  2.044894 ] \t1\ttrue\n",
            "(0)\t 412\t [ 1.5601122 -1.9239297] \t0\ttrue\n",
            "(0)\t 413\t [ 1.9752142 -2.0164104] \t0\ttrue\n",
            "(1)\t 414\t [-1.8944503  1.9333292] \t1\ttrue\n",
            "(1)\t 415\t [-1.5709893  1.9793905] \t1\ttrue\n",
            "(0)\t 416\t [ 2.1426601 -2.006621 ] \t0\ttrue\n",
            "(1)\t 417\t [-1.6225443  2.0242202] \t1\ttrue\n",
            "(0)\t 418\t [ 0.69816643 -1.3987556 ] \t0\ttrue\n",
            "(0)\t 419\t [ 0.6741804 -1.3207319] \t0\ttrue\n",
            "(1)\t 420\t [-1.5261929  1.223822 ] \t1\ttrue\n",
            "(1)\t 421\t [-1.6392304  1.855311 ] \t1\ttrue\n",
            "(0)\t 422\t [ 0.7055564 -1.3573895] \t0\ttrue\n",
            "(1)\t 423\t [-1.0677493  0.7106282] \t1\ttrue\n",
            "(0)\t 424\t [ 0.1694645 -0.7629435] \t0\ttrue\n",
            "(0)\t 425\t [ 0.9478245 -1.4553113] \t0\ttrue\n",
            "(1)\t 426\t [-1.1727438  0.6091586] \t1\ttrue\n",
            "(0)\t 427\t [-0.5660551  -0.31534845] \t1\tfalse\n",
            "(1)\t 428\t [-1.7619479  1.8634235] \t1\ttrue\n",
            "(0)\t 429\t [ 0.99242634 -1.6089618 ] \t0\ttrue\n",
            "(1)\t 430\t [-0.2813942 -0.5289586] \t0\tfalse\n",
            "(0)\t 431\t [ 0.23211776 -0.9773454 ] \t0\ttrue\n",
            "(0)\t 432\t [ 0.32815003 -1.1281793 ] \t0\ttrue\n",
            "(1)\t 433\t [-1.4680324  1.0169694] \t1\ttrue\n",
            "(1)\t 434\t [-0.93961     0.25689808] \t1\ttrue\n",
            "(0)\t 435\t [ 0.6663516 -1.255663 ] \t0\ttrue\n",
            "(0)\t 436\t [ 1.8617363 -2.0421486] \t0\ttrue\n",
            "(0)\t 437\t [-0.46730438 -0.38420013] \t1\tfalse\n",
            "(1)\t 438\t [-0.7996248  0.3535789] \t1\ttrue\n",
            "(0)\t 439\t [-1.5777022  2.0109308] \t1\tfalse\n",
            "(1)\t 440\t [-1.9370631  1.9256284] \t1\ttrue\n",
            "(1)\t 441\t [-1.5843079  2.004451 ] \t1\ttrue\n",
            "(0)\t 442\t [ 2.1139421 -1.9862958] \t0\ttrue\n",
            "(0)\t 443\t [ 1.0067347 -1.6782056] \t0\ttrue\n",
            "(1)\t 444\t [-1.8249112  1.8382673] \t1\ttrue\n",
            "(0)\t 445\t [ 0.7547241 -1.3557284] \t0\ttrue\n",
            "(0)\t 446\t [ 2.153585  -2.0167086] \t0\ttrue\n",
            "(0)\t 447\t [ 0.28542632 -0.87970567] \t0\ttrue\n",
            "(0)\t 448\t [-0.20116413 -0.592202  ] \t0\ttrue\n",
            "(0)\t 449\t [ 1.3220677 -1.8453108] \t0\ttrue\n",
            "(1)\t 450\t [-1.7328999  1.8170996] \t1\ttrue\n",
            "(1)\t 451\t [ 0.6985353 -1.4545779] \t0\tfalse\n",
            "(0)\t 452\t [ 0.6985353 -1.4545779] \t0\ttrue\n",
            "(1)\t 453\t [-1.947185   1.9836607] \t1\ttrue\n",
            "(0)\t 454\t [ 0.41410434 -1.0358504 ] \t0\ttrue\n",
            "(0)\t 455\t [ 0.2602872 -1.1361892] \t0\ttrue\n",
            "(0)\t 456\t [ 0.8845635 -1.5998765] \t0\ttrue\n",
            "(1)\t 457\t [-1.5624871  1.9855058] \t1\ttrue\n",
            "(1)\t 458\t [-1.5655416  2.0249572] \t1\ttrue\n",
            "(0)\t 459\t [ 2.016404  -2.0219998] \t0\ttrue\n",
            "(0)\t 460\t [-0.5245025 -0.2717537] \t1\tfalse\n",
            "(1)\t 461\t [-1.6934705  1.8359123] \t1\ttrue\n",
            "(0)\t 462\t [ 2.104686  -1.9362202] \t0\ttrue\n",
            "(0)\t 463\t [ 1.6422001 -1.9968144] \t0\ttrue\n",
            "(1)\t 464\t [-1.7792649  1.853469 ] \t1\ttrue\n",
            "(0)\t 465\t [ 2.1865802 -2.1759279] \t0\ttrue\n",
            "(1)\t 466\t [-1.6743433  1.9114912] \t1\ttrue\n",
            "(0)\t 467\t [ 1.3273703 -1.553123 ] \t0\ttrue\n",
            "(0)\t 468\t [ 0.9960047 -1.512702 ] \t0\ttrue\n",
            "(0)\t 469\t [ 0.8458703 -1.4978908] \t0\ttrue\n",
            "(0)\t 470\t [-0.35210097 -0.20000741] \t1\tfalse\n",
            "(1)\t 471\t [-1.8217756  2.023759 ] \t1\ttrue\n",
            "(0)\t 472\t [ 2.1131778 -1.9700003] \t0\ttrue\n",
            "(1)\t 473\t [-1.7402749  1.9903502] \t1\ttrue\n",
            "(0)\t 474\t [ 1.9195433 -1.9725189] \t0\ttrue\n",
            "(1)\t 475\t [-1.4196348  0.900007 ] \t1\ttrue\n",
            "(0)\t 476\t [ 2.082871  -2.0909405] \t0\ttrue\n",
            "(0)\t 477\t [ 1.7545328 -1.9766768] \t0\ttrue\n",
            "(1)\t 478\t [-1.5165169  1.7272083] \t1\ttrue\n",
            "(0)\t 479\t [ 2.170431  -2.0229526] \t0\ttrue\n",
            "(1)\t 480\t [-1.7873225  1.8542317] \t1\ttrue\n",
            "(1)\t 481\t [-1.5928335  1.265824 ] \t1\ttrue\n",
            "(0)\t 482\t [-1.6354517  1.5560662] \t1\tfalse\n",
            "(1)\t 483\t [-1.5324128  1.9970837] \t1\ttrue\n",
            "(0)\t 484\t [ 1.0999585 -1.5798106] \t0\ttrue\n",
            "(1)\t 485\t [-1.7772077  1.8617634] \t1\ttrue\n",
            "(0)\t 486\t [ 2.1388183 -2.0787623] \t0\ttrue\n",
            "(0)\t 487\t [ 2.2306736 -1.9649127] \t0\ttrue\n",
            "(1)\t 488\t [-1.5879335  2.0005643] \t1\ttrue\n",
            "(0)\t 489\t [-0.90496194  0.08725738] \t1\tfalse\n",
            "(1)\t 490\t [-0.957211    0.15735154] \t1\ttrue\n",
            "(1)\t 491\t [-1.7470177  1.6786851] \t1\ttrue\n",
            "(0)\t 492\t [ 1.1544888 -1.6831819] \t0\ttrue\n",
            "(0)\t 493\t [ 0.61817956 -1.3260729 ] \t0\ttrue\n",
            "(1)\t 494\t [-1.590596   1.9607646] \t1\ttrue\n",
            "(0)\t 495\t [ 1.4748015 -1.8535064] \t0\ttrue\n",
            "(0)\t 496\t [-0.79018575  0.0389932 ] \t1\tfalse\n",
            "(1)\t 497\t [-1.3818208  1.2138832] \t1\ttrue\n",
            "(1)\t 498\t [-1.7686132  1.8644607] \t1\ttrue\n",
            "(0)\t 499\t [ 1.2015805 -1.5265481] \t0\ttrue\n",
            "(1)\t 500\t [-1.7838671  1.850486 ] \t1\ttrue\n",
            "(0)\t 501\t [-0.5941909  -0.06724151] \t1\tfalse\n",
            "(1)\t 502\t [-1.8088819  1.8222135] \t1\ttrue\n",
            "(0)\t 503\t [ 1.709651  -2.0390098] \t0\ttrue\n",
            "(0)\t 504\t [-0.59955484 -0.3051907 ] \t1\tfalse\n",
            "(1)\t 505\t [-0.59955484 -0.3051907 ] \t1\ttrue\n",
            "(0)\t 506\t [ 2.0093398 -2.006121 ] \t0\ttrue\n",
            "(0)\t 507\t [ 1.2080449 -1.6291418] \t0\ttrue\n",
            "(1)\t 508\t [-1.8067544  2.0204806] \t1\ttrue\n",
            "(0)\t 509\t [ 1.1258447 -1.6495329] \t0\ttrue\n",
            "(0)\t 510\t [ 1.8999707 -1.9861393] \t0\ttrue\n",
            "(0)\t 511\t [ 1.9199154 -1.9228688] \t0\ttrue\n",
            "(0)\t 512\t [ 1.1183028 -1.5412102] \t0\ttrue\n",
            "(1)\t 513\t [ 1.8480966 -1.947568 ] \t0\tfalse\n",
            "(0)\t 514\t [-0.95940125  0.09138416] \t1\tfalse\n",
            "(0)\t 515\t [ 0.6313393 -1.3824772] \t0\ttrue\n",
            "(0)\t 516\t [ 2.0583034 -2.098112 ] \t0\ttrue\n",
            "(1)\t 517\t [-1.715013   2.0052657] \t1\ttrue\n",
            "(1)\t 518\t [-1.6613489  2.0195343] \t1\ttrue\n",
            "(0)\t 519\t [ 0.90570587 -1.4796251 ] \t0\ttrue\n",
            "(0)\t 520\t [-1.3375634  1.9999791] \t1\tfalse\n",
            "(1)\t 521\t [-1.4132192  1.9417967] \t1\ttrue\n",
            "(0)\t 522\t [-0.2418056 -0.2225878] \t1\tfalse\n",
            "(1)\t 523\t [-1.1056728   0.38165846] \t1\ttrue\n",
            "(0)\t 524\t [ 1.3305321 -1.7444546] \t0\ttrue\n",
            "(1)\t 525\t [-1.960059   1.9779713] \t1\ttrue\n",
            "(0)\t 526\t [ 1.5242534 -1.921812 ] \t0\ttrue\n",
            "(0)\t 527\t [-1.1151937  0.3756683] \t1\tfalse\n",
            "(1)\t 528\t [-1.7082483  1.8711038] \t1\ttrue\n",
            "(0)\t 529\t [ 0.08716956 -0.6624061 ] \t0\ttrue\n",
            "(1)\t 530\t [-1.746442   1.8112011] \t1\ttrue\n",
            "(0)\t 531\t [-1.6066124  1.475187 ] \t1\tfalse\n",
            "(1)\t 532\t [-1.7027897  2.041005 ] \t1\ttrue\n",
            "(0)\t 533\t [ 1.8892742 -2.0145028] \t0\ttrue\n",
            "(1)\t 534\t [-1.6105819  1.9354298] \t1\ttrue\n",
            "(0)\t 535\t [ 1.5270243 -1.9137244] \t0\ttrue\n",
            "(0)\t 536\t [ 2.004497  -2.0802476] \t0\ttrue\n",
            "(1)\t 537\t [-1.6928949  1.9819291] \t1\ttrue\n",
            "(1)\t 538\t [-0.75989527 -0.01397037] \t1\ttrue\n",
            "(0)\t 539\t [ 0.40115297 -1.1914527 ] \t0\ttrue\n",
            "(0)\t 540\t [ 1.69071   -1.8657658] \t0\ttrue\n",
            "(1)\t 541\t [-1.7561048  1.9014395] \t1\ttrue\n",
            "(0)\t 542\t [ 2.2016854 -2.1536596] \t0\ttrue\n",
            "(1)\t 543\t [-1.6757343  1.9291022] \t1\ttrue\n",
            "(0)\t 544\t [ 2.0932295 -2.0327928] \t0\ttrue\n",
            "(0)\t 545\t [ 0.3612383 -1.1057202] \t0\ttrue\n",
            "(0)\t 546\t [-0.8150321  0.1748942] \t1\tfalse\n",
            "(1)\t 547\t [-1.8252618  1.8458493] \t1\ttrue\n",
            "(1)\t 548\t [ 1.5796937 -1.9066067] \t0\tfalse\n",
            "(0)\t 549\t [ 1.5796937 -1.9066067] \t0\ttrue\n",
            "(0)\t 550\t [ 1.7562742 -2.0391688] \t0\ttrue\n",
            "(1)\t 551\t [-1.7480419  1.8169543] \t1\ttrue\n",
            "(0)\t 552\t [ 1.170814  -1.6330557] \t0\ttrue\n",
            "(1)\t 553\t [-1.289035    0.68943983] \t1\ttrue\n",
            "(1)\t 554\t [-1.7746288  1.9903544] \t1\ttrue\n",
            "(0)\t 555\t [ 2.0775847 -2.1105423] \t0\ttrue\n",
            "(0)\t 556\t [ 0.58977944 -1.2858616 ] \t0\ttrue\n",
            "(1)\t 557\t [-1.687168   1.9854718] \t1\ttrue\n",
            "(0)\t 558\t [ 2.0984316 -2.065666 ] \t0\ttrue\n",
            "(0)\t 559\t [ 1.652458  -1.9002801] \t0\ttrue\n",
            "(1)\t 560\t [-1.6289917  2.0049846] \t1\ttrue\n",
            "(1)\t 561\t [-1.5689844  1.5908278] \t1\ttrue\n",
            "(0)\t 562\t [-0.90364134  0.48077926] \t1\tfalse\n",
            "(0)\t 563\t [-0.436063   -0.37095624] \t1\tfalse\n",
            "(1)\t 564\t [-0.436063   -0.37095624] \t1\ttrue\n",
            "(0)\t 565\t [ 1.5967276 -1.9069903] \t0\ttrue\n",
            "(1)\t 566\t [-1.6791121  1.9966573] \t1\ttrue\n",
            "(1)\t 567\t [-1.483624   1.9925747] \t1\ttrue\n",
            "(0)\t 568\t [ 2.2023365 -2.1561441] \t0\ttrue\n",
            "(0)\t 569\t [ 2.0097177 -2.0539908] \t0\ttrue\n",
            "(1)\t 570\t [-1.7849388  1.8765814] \t1\ttrue\n",
            "(0)\t 571\t [ 0.45312205 -1.1439801 ] \t0\ttrue\n",
            "(1)\t 572\t [-1.4610188  1.1248254] \t1\ttrue\n",
            "(0)\t 573\t [ 2.2050798 -2.0331144] \t0\ttrue\n",
            "(1)\t 574\t [-1.8011743  1.8152349] \t1\ttrue\n",
            "(1)\t 575\t [-1.5563458  1.9625508] \t1\ttrue\n",
            "(0)\t 576\t [ 0.67456174 -1.2728893 ] \t0\ttrue\n",
            "(1)\t 577\t [-1.6121104  1.3390306] \t1\ttrue\n",
            "(0)\t 578\t [-1.6121104  1.3390306] \t1\tfalse\n",
            "(1)\t 579\t [-1.7337157  1.4381019] \t1\ttrue\n",
            "(0)\t 580\t [ 2.075248  -2.0917041] \t0\ttrue\n",
            "(0)\t 581\t [-0.00761191 -0.8118967 ] \t0\ttrue\n",
            "(1)\t 582\t [-0.00761191 -0.8118967 ] \t0\tfalse\n",
            "(0)\t 583\t [ 0.9384611 -1.6384466] \t0\ttrue\n",
            "(1)\t 584\t [ 0.9384611 -1.6384466] \t0\tfalse\n",
            "(1)\t 585\t [-1.4200766  1.9648342] \t1\ttrue\n",
            "(0)\t 586\t [ 1.3316683 -1.5046515] \t0\ttrue\n",
            "(1)\t 587\t [-1.7711574  1.8404322] \t1\ttrue\n",
            "(0)\t 588\t [-1.1890779   0.68920517] \t1\tfalse\n",
            "(1)\t 589\t [-1.6079113  2.0112152] \t1\ttrue\n",
            "(0)\t 590\t [-1.6079113  2.0112152] \t1\tfalse\n",
            "(0)\t 591\t [-1.4866467  1.0959234] \t1\tfalse\n",
            "(1)\t 592\t [-1.6760652  1.6651442] \t1\ttrue\n",
            "(0)\t 593\t [ 1.9511648 -1.9074244] \t0\ttrue\n",
            "(1)\t 594\t [-1.7220776  1.9796629] \t1\ttrue\n",
            "(1)\t 595\t [-1.6304522  1.4622371] \t1\ttrue\n",
            "(0)\t 596\t [-0.6298632  -0.21135385] \t1\tfalse\n",
            "(1)\t 597\t [-1.1144747  0.4065103] \t1\ttrue\n",
            "(0)\t 598\t [ 1.4906054 -1.9055839] \t0\ttrue\n",
            "(1)\t 599\t [-1.6710347  1.6044393] \t1\ttrue\n",
            "(0)\t 600\t [ 1.2133399 -1.7151549] \t0\ttrue\n",
            "(0)\t 601\t [-0.2384686  -0.54836863] \t0\ttrue\n",
            "(1)\t 602\t [-1.8633765  1.9949222] \t1\ttrue\n",
            "(0)\t 603\t [ 0.888494  -1.5250248] \t0\ttrue\n",
            "(1)\t 604\t [-1.4595621   0.96843433] \t1\ttrue\n",
            "(1)\t 605\t [-1.5555512  2.0112054] \t1\ttrue\n",
            "(0)\t 606\t [ 0.6885867 -1.3086852] \t0\ttrue\n",
            "(1)\t 607\t [-1.6048349  1.4363471] \t1\ttrue\n",
            "(0)\t 608\t [ 0.1423486 -1.0083467] \t0\ttrue\n",
            "(1)\t 609\t [-1.7204388  1.8558333] \t1\ttrue\n",
            "(0)\t 610\t [ 2.027139  -1.9309658] \t0\ttrue\n",
            "(0)\t 611\t [ 2.022067  -2.0716262] \t0\ttrue\n",
            "(0)\t 612\t [-0.90232825  0.59541905] \t1\tfalse\n",
            "(0)\t 613\t [ 0.99884176 -1.5917425 ] \t0\ttrue\n",
            "(1)\t 614\t [-1.6445582  2.0104864] \t1\ttrue\n",
            "(0)\t 615\t [ 2.2289817 -2.1020787] \t0\ttrue\n",
            "(1)\t 616\t [-1.3993472  0.959717 ] \t1\ttrue\n",
            "(0)\t 617\t [ 0.21769483 -0.908785  ] \t0\ttrue\n",
            "(0)\t 618\t [ 1.8813497 -1.9138503] \t0\ttrue\n",
            "(0)\t 619\t [-1.0746182   0.52781606] \t1\tfalse\n",
            "(1)\t 620\t [-1.5596142  1.9987555] \t1\ttrue\n",
            "(0)\t 621\t [ 1.0771831 -1.6956661] \t0\ttrue\n",
            "(1)\t 622\t [-1.7917112  1.8186994] \t1\ttrue\n",
            "(1)\t 623\t [-1.8088738  1.7797904] \t1\ttrue\n",
            "(0)\t 624\t [-0.04796644 -0.7832703 ] \t0\ttrue\n",
            "(0)\t 625\t [-0.76644295 -0.05983905] \t1\tfalse\n",
            "(0)\t 626\t [ 2.2382677 -2.1888924] \t0\ttrue\n",
            "(1)\t 627\t [-1.6308053  2.0114589] \t1\ttrue\n",
            "(1)\t 628\t [-1.7555242  1.9460982] \t1\ttrue\n",
            "(0)\t 629\t [ 1.9919331 -2.0561545] \t0\ttrue\n",
            "(1)\t 630\t [-1.5715618  2.042611 ] \t1\ttrue\n",
            "(0)\t 631\t [ 2.110868  -2.0663028] \t0\ttrue\n",
            "(0)\t 632\t [ 1.8811525 -2.0156229] \t0\ttrue\n",
            "(1)\t 633\t [-1.461254   1.3683243] \t1\ttrue\n",
            "(0)\t 634\t [ 0.38465127 -1.0812798 ] \t0\ttrue\n",
            "(1)\t 635\t [-0.3912778  -0.43080875] \t0\tfalse\n",
            "(0)\t 636\t [ 0.34606656 -0.9876758 ] \t0\ttrue\n",
            "(1)\t 637\t [-1.8840411  1.598127 ] \t1\ttrue\n",
            "(1)\t 638\t [-1.9202099  2.0307808] \t1\ttrue\n",
            "(0)\t 639\t [ 2.181341  -2.0763159] \t0\ttrue\n",
            "(0)\t 640\t [ 0.5173761 -1.2884482] \t0\ttrue\n",
            "(1)\t 641\t [-1.6698787  1.6360232] \t1\ttrue\n",
            "(0)\t 642\t [ 0.6633884 -1.4011561] \t0\ttrue\n",
            "(1)\t 643\t [ 0.6633884 -1.4011561] \t0\tfalse\n",
            "(1)\t 644\t [-1.7207412  1.7387754] \t1\ttrue\n",
            "(0)\t 645\t [ 1.5427159 -1.9553449] \t0\ttrue\n",
            "(0)\t 646\t [-0.13410258 -0.66205513] \t0\ttrue\n",
            "(1)\t 647\t [-0.873169    0.07047722] \t1\ttrue\n",
            "(0)\t 648\t [-0.44607645 -0.24836276] \t1\tfalse\n",
            "(1)\t 649\t [-1.7769849  1.7926977] \t1\ttrue\n",
            "(1)\t 650\t [-1.6868304  2.0076253] \t1\ttrue\n",
            "(0)\t 651\t [ 2.2486994 -2.0746977] \t0\ttrue\n",
            "(0)\t 652\t [ 1.5651948 -1.7973261] \t0\ttrue\n",
            "(1)\t 653\t [-1.4955299  1.9675982] \t1\ttrue\n",
            "(1)\t 654\t [-1.6464407  1.993807 ] \t1\ttrue\n",
            "(0)\t 655\t [ 1.7222825 -1.9774071] \t0\ttrue\n",
            "(1)\t 656\t [-1.6986053  2.043012 ] \t1\ttrue\n",
            "(0)\t 657\t [-0.82527506  0.10452337] \t1\tfalse\n",
            "(1)\t 658\t [ 1.8913869 -2.008325 ] \t0\tfalse\n",
            "(0)\t 659\t [ 1.9462425 -2.048969 ] \t0\ttrue\n",
            "(1)\t 660\t [ 1.2440064 -1.8210493] \t0\tfalse\n",
            "(0)\t 661\t [ 1.4411125 -1.8868735] \t0\ttrue\n",
            "(0)\t 662\t [ 1.6555543 -2.0075908] \t0\ttrue\n",
            "(1)\t 663\t [ 0.4262311 -1.1423872] \t0\tfalse\n",
            "(1)\t 664\t [-1.6180001  2.0182428] \t1\ttrue\n",
            "(0)\t 665\t [-1.1132584   0.46401876] \t1\tfalse\n",
            "(0)\t 666\t [ 0.11958433 -0.76094043] \t0\ttrue\n",
            "(1)\t 667\t [-1.763858   2.0796764] \t1\ttrue\n",
            "(1)\t 668\t [-1.7605408  1.7262328] \t1\ttrue\n",
            "(0)\t 669\t [ 1.3036615 -1.8659368] \t0\ttrue\n",
            "(0)\t 670\t [ 1.835681  -1.8415905] \t0\ttrue\n",
            "(0)\t 671\t [ 2.0035098 -1.9258174] \t0\ttrue\n",
            "(1)\t 672\t [-1.6041633  2.0361197] \t1\ttrue\n",
            "(0)\t 673\t [-1.1082633  0.7719109] \t1\tfalse\n",
            "(1)\t 674\t [-1.7186387  2.0230148] \t1\ttrue\n",
            "(0)\t 675\t [ 2.2235227 -2.1403854] \t0\ttrue\n",
            "(1)\t 676\t [-1.7869843  1.7966177] \t1\ttrue\n",
            "(0)\t 677\t [-0.4106002  -0.45400757] \t0\ttrue\n",
            "(0)\t 678\t [ 1.7486534 -1.8618509] \t0\ttrue\n",
            "(0)\t 679\t [ 1.803387  -1.9014386] \t0\ttrue\n",
            "(1)\t 680\t [-1.4676647  2.0394175] \t1\ttrue\n",
            "(0)\t 681\t [ 2.144487  -1.9987353] \t0\ttrue\n",
            "(1)\t 682\t [-1.5493182  2.000907 ] \t1\ttrue\n",
            "(0)\t 683\t [ 0.68314165 -1.3515325 ] \t0\ttrue\n",
            "(0)\t 684\t [ 1.9938066 -2.0609117] \t0\ttrue\n",
            "(1)\t 685\t [-1.1350201   0.32310513] \t1\ttrue\n",
            "(0)\t 686\t [ 2.1656055 -2.2591686] \t0\ttrue\n",
            "(1)\t 687\t [-1.0119208   0.40128192] \t1\ttrue\n",
            "(0)\t 688\t [ 0.03090272 -0.81292707] \t0\ttrue\n",
            "(0)\t 689\t [ 0.5732578 -1.3028059] \t0\ttrue\n",
            "(1)\t 690\t [-1.3239377  1.193387 ] \t1\ttrue\n",
            "(1)\t 691\t [-0.8256623  -0.05567538] \t1\ttrue\n",
            "(0)\t 692\t [ 1.434991  -1.8424298] \t0\ttrue\n",
            "(1)\t 693\t [-1.5182583  1.980972 ] \t1\ttrue\n",
            "(0)\t 694\t [ 1.2572697 -1.7380098] \t0\ttrue\n",
            "(0)\t 695\t [ 0.00733703 -0.51362884] \t0\ttrue\n",
            "(1)\t 696\t [-1.6406746  2.0217173] \t1\ttrue\n",
            "(1)\t 697\t [-1.4284643  1.4825768] \t1\ttrue\n",
            "(0)\t 698\t [ 2.1801465 -2.2199044] \t0\ttrue\n",
            "(0)\t 699\t [ 0.5961178 -1.304572 ] \t0\ttrue\n",
            "(0)\t 700\t [-1.6038957  1.5185747] \t1\tfalse\n",
            "(1)\t 701\t [ 1.4191928 -1.863713 ] \t0\tfalse\n",
            "(0)\t 702\t [ 1.996454  -1.9863487] \t0\ttrue\n",
            "(1)\t 703\t [-1.8286115  2.0027459] \t1\ttrue\n",
            "(0)\t 704\t [ 1.8149599 -1.8755124] \t0\ttrue\n",
            "(0)\t 705\t [ 0.16479582 -0.9734887 ] \t0\ttrue\n",
            "(1)\t 706\t [-1.6318092  1.9983726] \t1\ttrue\n",
            "(0)\t 707\t [ 1.94584   -2.0635092] \t0\ttrue\n",
            "(1)\t 708\t [-1.758091   1.8482842] \t1\ttrue\n",
            "(1)\t 709\t [-1.8931588  1.8550297] \t1\ttrue\n",
            "(0)\t 710\t [ 2.1302476 -2.1792169] \t0\ttrue\n",
            "(1)\t 711\t [-1.7147677  2.027405 ] \t1\ttrue\n",
            "(0)\t 712\t [ 2.070037 -2.093432] \t0\ttrue\n",
            "(0)\t 713\t [ 0.8473868 -1.2089841] \t0\ttrue\n",
            "(1)\t 714\t [-1.5250605  1.4917009] \t1\ttrue\n",
            "(0)\t 715\t [ 2.1851912 -2.1073053] \t0\ttrue\n",
            "(1)\t 716\t [-1.5509881  1.9052098] \t1\ttrue\n",
            "(1)\t 717\t [-0.73236066  0.21653353] \t1\ttrue\n",
            "(0)\t 718\t [ 1.2598164 -1.3594375] \t0\ttrue\n",
            "(0)\t 719\t [ 1.8045065 -1.8435452] \t0\ttrue\n",
            "(1)\t 720\t [-1.5638478  1.4476578] \t1\ttrue\n",
            "(0)\t 721\t [-1.2020694   0.54845345] \t1\tfalse\n",
            "(0)\t 722\t [ 1.7219281 -1.9367503] \t0\ttrue\n",
            "(1)\t 723\t [-1.7724938  1.8848102] \t1\ttrue\n",
            "(0)\t 724\t [ 0.33302653 -0.9940363 ] \t0\ttrue\n",
            "(1)\t 725\t [-1.9256692  1.6718458] \t1\ttrue\n",
            "(0)\t 726\t [ 1.1869922 -1.4178029] \t0\ttrue\n",
            "(1)\t 727\t [-1.746094   1.5494963] \t1\ttrue\n",
            "(0)\t 728\t [-1.746094   1.5494963] \t1\tfalse\n",
            "(0)\t 729\t [-1.7475288  1.7381079] \t1\tfalse\n",
            "(1)\t 730\t [-1.7940453  1.4485742] \t1\ttrue\n",
            "(0)\t 731\t [ 0.26472092 -1.0105934 ] \t0\ttrue\n",
            "(1)\t 732\t [-1.7084291  1.6628454] \t1\ttrue\n",
            "(1)\t 733\t [-1.5625472  2.0216963] \t1\ttrue\n",
            "(0)\t 734\t [-1.5912601  1.9999917] \t1\tfalse\n",
            "(0)\t 735\t [ 2.2793505 -2.05206  ] \t0\ttrue\n",
            "(0)\t 736\t [ 1.5899194 -1.914241 ] \t0\ttrue\n",
            "(1)\t 737\t [-1.8409121  1.9740374] \t1\ttrue\n",
            "(0)\t 738\t [ 1.5777851 -1.9663858] \t0\ttrue\n",
            "(0)\t 739\t [-0.12897928 -0.5981375 ] \t0\ttrue\n",
            "(1)\t 740\t [-1.4110336  1.0268083] \t1\ttrue\n",
            "(0)\t 741\t [-1.6593916  1.5363126] \t1\tfalse\n",
            "(1)\t 742\t [-1.423875   1.2308795] \t1\ttrue\n",
            "(0)\t 743\t [ 1.3108671 -1.5307146] \t0\ttrue\n",
            "(1)\t 744\t [-1.5206041  1.9996303] \t1\ttrue\n",
            "(1)\t 745\t [-1.6880225  1.6447097] \t1\ttrue\n",
            "(0)\t 746\t [-1.5029923  1.21428  ] \t1\tfalse\n",
            "(0)\t 747\t [ 2.044263 -1.927208] \t0\ttrue\n",
            "(1)\t 748\t [-1.8150799  1.8786532] \t1\ttrue\n",
            "(1)\t 749\t [-1.4434129  1.8897836] \t1\ttrue\n",
            "(0)\t 750\t [-0.91862315  0.06738218] \t1\tfalse\n",
            "(1)\t 751\t [-1.782185   1.8042126] \t1\ttrue\n",
            "(0)\t 752\t [ 1.8305602 -1.9286134] \t0\ttrue\n",
            "(0)\t 753\t [ 0.41875342 -1.0047096 ] \t0\ttrue\n",
            "(0)\t 754\t [ 1.0613189 -1.7276089] \t0\ttrue\n",
            "(1)\t 755\t [-1.51317    1.9976921] \t1\ttrue\n",
            "(0)\t 756\t [-1.5849108  1.6230628] \t1\tfalse\n",
            "(1)\t 757\t [-1.7063403  2.024978 ] \t1\ttrue\n",
            "(0)\t 758\t [ 2.2046082 -1.9954493] \t0\ttrue\n",
            "(0)\t 759\t [ 0.74677   -1.3159156] \t0\ttrue\n",
            "(0)\t 760\t [ 1.1717865 -1.7279092] \t0\ttrue\n",
            "(1)\t 761\t [-1.8090473  1.9203117] \t1\ttrue\n",
            "(0)\t 762\t [ 2.1679392 -2.1579113] \t0\ttrue\n",
            "(1)\t 763\t [-1.5803958  1.983067 ] \t1\ttrue\n",
            "(0)\t 764\t [ 0.43032798 -0.7586061 ] \t0\ttrue\n",
            "(0)\t 765\t [ 0.2524671 -0.9800001] \t0\ttrue\n",
            "(0)\t 766\t [ 2.0642703 -1.9533741] \t0\ttrue\n",
            "(1)\t 767\t [-1.7137921  2.042435 ] \t1\ttrue\n",
            "(0)\t 768\t [-0.71578914 -0.0635413 ] \t1\tfalse\n",
            "(1)\t 769\t [-1.7455944  1.8208002] \t1\ttrue\n",
            "(1)\t 770\t [-1.8893791  1.6858505] \t1\ttrue\n",
            "(0)\t 771\t [-1.8435407  1.8907927] \t1\tfalse\n",
            "(0)\t 772\t [ 2.2672668 -2.22318  ] \t0\ttrue\n",
            "(1)\t 773\t [-1.8577796  1.9275408] \t1\ttrue\n",
            "(0)\t 774\t [-1.813585   1.9781373] \t1\tfalse\n",
            "(0)\t 775\t [ 2.2716053 -2.181231 ] \t0\ttrue\n",
            "(1)\t 776\t [-1.7602301  2.0090754] \t1\ttrue\n",
            "(1)\t 777\t [-1.5988432  1.9605894] \t1\ttrue\n",
            "(1)\t 778\t [-1.5072705  2.0265877] \t1\ttrue\n",
            "(0)\t 779\t [ 2.156809  -2.0893295] \t0\ttrue\n",
            "(1)\t 780\t [-1.603433   2.0664158] \t1\ttrue\n",
            "(0)\t 781\t [ 1.5666966 -1.9406794] \t0\ttrue\n",
            "(0)\t 782\t [ 2.1882613 -2.2197044] \t0\ttrue\n",
            "(1)\t 783\t [-1.5101742  1.9948933] \t1\ttrue\n",
            "(0)\t 784\t [-1.1832513  0.6252841] \t1\tfalse\n",
            "(1)\t 785\t [-1.6786778  1.7723374] \t1\ttrue\n",
            "(0)\t 786\t [ 1.9722904 -2.0309005] \t0\ttrue\n",
            "(1)\t 787\t [-1.6811762  1.623608 ] \t1\ttrue\n",
            "(0)\t 788\t [ 2.0985055 -2.0284107] \t0\ttrue\n",
            "(0)\t 789\t [ 1.6892453 -1.8862493] \t0\ttrue\n",
            "(1)\t 790\t [-1.8022747  1.8199115] \t1\ttrue\n",
            "(0)\t 791\t [ 1.1935968 -1.5401798] \t0\ttrue\n",
            "(1)\t 792\t [-1.4864205  1.9440138] \t1\ttrue\n",
            "(1)\t 793\t [-1.0581678   0.82356966] \t1\ttrue\n",
            "(0)\t 794\t [-1.0581678   0.82356966] \t1\tfalse\n",
            "(0)\t 795\t [ 2.0007763 -2.1251616] \t0\ttrue\n",
            "(1)\t 796\t [-1.4525505  1.1753893] \t1\ttrue\n",
            "(1)\t 797\t [-1.6426617  1.5692973] \t1\ttrue\n",
            "(0)\t 798\t [ 0.6083248 -1.3596878] \t0\ttrue\n",
            "(0)\t 799\t [-0.13143241 -0.71957564] \t0\ttrue\n",
            "(1)\t 800\t [-0.13143241 -0.71957564] \t0\tfalse\n",
            "(0)\t 801\t [-0.13143241 -0.71957564] \t0\ttrue\n",
            "(1)\t 802\t [-0.13143241 -0.71957564] \t0\tfalse\n",
            "(1)\t 803\t [-1.6544125  1.6366935] \t1\ttrue\n",
            "(0)\t 804\t [-1.3830434  0.7616669] \t1\tfalse\n",
            "(0)\t 805\t [ 1.7997929 -1.9502563] \t0\ttrue\n",
            "(1)\t 806\t [ 1.4230721 -1.8272036] \t0\tfalse\n",
            "(0)\t 807\t [ 1.9780364 -2.0326629] \t0\ttrue\n",
            "(1)\t 808\t [-1.9067866  1.9150898] \t1\ttrue\n",
            "(0)\t 809\t [ 2.165432  -2.0316634] \t0\ttrue\n",
            "(1)\t 810\t [-1.7605351  1.802607 ] \t1\ttrue\n",
            "(0)\t 811\t [ 0.57109696 -1.2782027 ] \t0\ttrue\n",
            "(0)\t 812\t [ 2.0871582 -2.1693652] \t0\ttrue\n",
            "(1)\t 813\t [-1.6558183  1.9914982] \t1\ttrue\n",
            "(0)\t 814\t [-0.79134285 -0.14455369] \t1\tfalse\n",
            "(1)\t 815\t [-1.5383456  1.986371 ] \t1\ttrue\n",
            "(0)\t 816\t [ 2.0972266 -1.9378576] \t0\ttrue\n",
            "(1)\t 817\t [ 0.53657347 -1.2133138 ] \t0\tfalse\n",
            "(0)\t 818\t [ 2.0370104 -1.9277012] \t0\ttrue\n",
            "(1)\t 819\t [-1.5361851  1.4235471] \t1\ttrue\n",
            "(0)\t 820\t [-0.8273188  0.1943195] \t1\tfalse\n",
            "(0)\t 821\t [ 0.5375184 -1.2230953] \t0\ttrue\n",
            "(0)\t 822\t [ 1.4133687 -1.706966 ] \t0\ttrue\n",
            "(0)\t 823\t [-1.5135809  1.3098209] \t1\tfalse\n",
            "(0)\t 824\t [ 1.2225313 -1.7598392] \t0\ttrue\n",
            "(1)\t 825\t [-1.6688745  1.7986653] \t1\ttrue\n",
            "(0)\t 826\t [ 1.2199395 -1.7753133] \t0\ttrue\n",
            "(1)\t 827\t [-1.7652013  1.7085978] \t1\ttrue\n",
            "(0)\t 828\t [ 2.022769 -2.038523] \t0\ttrue\n",
            "(1)\t 829\t [-1.5752541  2.0255535] \t1\ttrue\n",
            "(1)\t 830\t [-1.612209  2.025856] \t1\ttrue\n",
            "(0)\t 831\t [ 1.3591856 -1.8404982] \t0\ttrue\n",
            "(1)\t 832\t [-1.812289   1.8188058] \t1\ttrue\n",
            "(0)\t 833\t [-0.68332034 -0.16481206] \t1\tfalse\n",
            "(0)\t 834\t [ 1.5604355 -1.972851 ] \t0\ttrue\n",
            "(1)\t 835\t [-1.494602   1.0216708] \t1\ttrue\n",
            "(1)\t 836\t [-1.7552787  1.8664494] \t1\ttrue\n",
            "(0)\t 837\t [ 0.5661228 -1.2234564] \t0\ttrue\n",
            "(0)\t 838\t [ 1.0728915 -1.7186548] \t0\ttrue\n",
            "(0)\t 839\t [ 1.414475  -1.6448336] \t0\ttrue\n",
            "(1)\t 840\t [-1.4471515  2.0119722] \t1\ttrue\n",
            "(1)\t 841\t [-1.4263186  1.978349 ] \t1\ttrue\n",
            "(0)\t 842\t [ 1.7336819 -1.890874 ] \t0\ttrue\n",
            "(0)\t 843\t [ 0.572544  -1.0397478] \t0\ttrue\n",
            "(1)\t 844\t [-1.5286394  2.0149648] \t1\ttrue\n",
            "(0)\t 845\t [ 0.51270276 -1.2659507 ] \t0\ttrue\n",
            "(1)\t 846\t [ 0.43185973 -1.1876961 ] \t0\tfalse\n",
            "(0)\t 847\t [ 1.0457709 -1.5735   ] \t0\ttrue\n",
            "(1)\t 848\t [ 0.2998821 -0.8898761] \t0\tfalse\n",
            "(1)\t 849\t [-0.8037011   0.03008565] \t1\ttrue\n",
            "(0)\t 850\t [-0.35193604 -0.533353  ] \t0\ttrue\n",
            "(1)\t 851\t [-0.8267564   0.19439839] \t1\ttrue\n",
            "(0)\t 852\t [-0.04404561 -0.7204728 ] \t0\ttrue\n",
            "(0)\t 853\t [ 1.0688932 -1.7264216] \t0\ttrue\n",
            "(1)\t 854\t [-1.8305758  1.7296674] \t1\ttrue\n",
            "(0)\t 855\t [ 1.6097251 -1.9362508] \t0\ttrue\n",
            "(0)\t 856\t [-0.49182773 -0.21666533] \t1\tfalse\n",
            "(1)\t 857\t [ 0.16139545 -0.86699545] \t0\tfalse\n",
            "(0)\t 858\t [ 1.6132945 -1.9611572] \t0\ttrue\n",
            "(1)\t 859\t [ 0.48166037 -1.2507232 ] \t0\tfalse\n",
            "(0)\t 860\t [ 0.48166037 -1.2507232 ] \t0\ttrue\n",
            "(0)\t 861\t [ 0.58617574 -1.3414415 ] \t0\ttrue\n",
            "(1)\t 862\t [ 0.58617574 -1.3414415 ] \t0\tfalse\n",
            "(0)\t 863\t [ 1.66433   -1.8163784] \t0\ttrue\n",
            "(0)\t 864\t [ 1.1356511 -1.2248998] \t0\ttrue\n",
            "(1)\t 865\t [-1.5622191  2.00898  ] \t1\ttrue\n",
            "(0)\t 866\t [-0.37192425 -0.38149667] \t0\ttrue\n",
            "(1)\t 867\t [ 1.6085316 -1.8896449] \t0\tfalse\n",
            "(0)\t 868\t [ 0.4039106 -1.2009943] \t0\ttrue\n",
            "(0)\t 869\t [ 1.8024008 -1.8464235] \t0\ttrue\n",
            "(0)\t 870\t [ 1.3994781 -1.7767751] \t0\ttrue\n",
            "(1)\t 871\t [-1.5034049  1.3979403] \t1\ttrue\n",
            "(0)\t 872\t [ 1.9101046 -1.9452648] \t0\ttrue\n",
            "(0)\t 873\t [ 1.955017 -1.975844] \t0\ttrue\n",
            "(1)\t 874\t [-1.5939116  2.021474 ] \t1\ttrue\n",
            "(1)\t 875\t [-1.6430577  2.0314095] \t1\ttrue\n",
            "(0)\t 876\t [-1.847821   1.7931842] \t1\tfalse\n",
            "(0)\t 877\t [-0.43290758 -0.39583862] \t1\tfalse\n",
            "(1)\t 878\t [-1.8200827  1.7948847] \t1\ttrue\n",
            "(1)\t 879\t [ 2.1567059 -2.1119971] \t0\tfalse\n",
            "(0)\t 880\t [ 2.1717708 -1.9269923] \t0\ttrue\n",
            "(0)\t 881\t [ 2.270484  -2.1577532] \t0\ttrue\n",
            "(1)\t 882\t [-1.3263736  1.0077958] \t1\ttrue\n",
            "(1)\t 883\t [ 1.2373805 -1.7980146] \t0\tfalse\n",
            "(0)\t 884\t [ 1.2373805 -1.7980146] \t0\ttrue\n",
            "(0)\t 885\t [ 0.2544011 -0.7598362] \t0\ttrue\n",
            "(1)\t 886\t [-1.4740537  2.0029826] \t1\ttrue\n",
            "(0)\t 887\t [ 0.56690556 -1.2494836 ] \t0\ttrue\n",
            "(1)\t 888\t [-1.6359781  1.5188148] \t1\ttrue\n",
            "(0)\t 889\t [ 2.115144  -2.1503913] \t0\ttrue\n",
            "(1)\t 890\t [-1.8362247  1.4341276] \t1\ttrue\n",
            "(0)\t 891\t [ 1.0138466 -1.7153261] \t0\ttrue\n",
            "(1)\t 892\t [ 1.0138466 -1.7153261] \t0\tfalse\n",
            "(1)\t 893\t [-0.38368064 -0.38224444] \t1\ttrue\n",
            "(0)\t 894\t [ 2.0886452 -2.0879743] \t0\ttrue\n",
            "(1)\t 895\t [-1.8173048  1.7373804] \t1\ttrue\n",
            "(0)\t 896\t [ 2.085858  -2.0245843] \t0\ttrue\n",
            "(0)\t 897\t [ 0.9756383 -1.5883203] \t0\ttrue\n",
            "(1)\t 898\t [ 0.87517905 -1.526083  ] \t0\tfalse\n",
            "(0)\t 899\t [ 1.9756196 -2.0238028] \t0\ttrue\n",
            "(1)\t 900\t [-1.7864144  1.886535 ] \t1\ttrue\n",
            "Number of true predictions: 765\n",
            "Number of false predictions: 135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIgsWWCTKdY",
        "colab_type": "code",
        "outputId": "f48da277-9992-4387-955e-98d722deb3fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 84.90566037735849 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print('True positives: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xnx5SIDGsaA",
        "colab_type": "code",
        "outputId": "7b0469dc-26dd-457e-ce8d-049b9c25f116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "scores = [3.0, 1.0, 0.2]\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    predictions[i][j]=softmax(predictions[i][j])\n",
        "    print(predictions[i][j])"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.02361701 0.976383  ]\n",
            "[0.9855326  0.01446736]\n",
            "[0.02129661 0.97870344]\n",
            "[0.9773602  0.02263983]\n",
            "[0.9859971  0.01400295]\n",
            "[0.06627277 0.9337272 ]\n",
            "[0.9866881  0.01331186]\n",
            "[0.9761895  0.02381056]\n",
            "[0.06843124 0.93156874]\n",
            "[0.98398376 0.01601626]\n",
            "[0.88202953 0.11797047]\n",
            "[0.554111   0.44588903]\n",
            "[0.98197514 0.01802486]\n",
            "[0.9390193  0.06098067]\n",
            "[0.02527835 0.9747217 ]\n",
            "[0.02945771 0.9705423 ]\n",
            "[0.65371144 0.34628862]\n",
            "[0.9807983  0.01920168]\n",
            "[0.02523647 0.9747635 ]\n",
            "[0.02689529 0.97310466]\n",
            "[0.83406585 0.1659342 ]\n",
            "[0.98683935 0.01316071]\n",
            "[0.14190015 0.8580998 ]\n",
            "[0.02548886 0.9745111 ]\n",
            "[0.9777018  0.02229816]\n",
            "[0.9834326  0.01656747]\n",
            "[0.02561371 0.9743862 ]\n",
            "[0.9525128  0.04748715]\n",
            "[0.9596455  0.04035445]\n",
            "[0.02495997 0.9750401 ]\n",
            "[0.02727571 0.9727243 ]\n",
            "[0.9840044  0.01599566]\n",
            "[0.02577219 0.9742278 ]\n",
            "[0.49194828 0.50805175]\n",
            "[0.9602915  0.03970851]\n",
            "[0.9762625  0.02373751]\n",
            "[0.80932355 0.19067645]\n",
            "[0.02728628 0.9727137 ]\n",
            "[0.98273957 0.01726042]\n",
            "[0.03241407 0.9675859 ]\n",
            "[0.9309198  0.06908016]\n",
            "[0.02590459 0.97409546]\n",
            "[0.9577626  0.04223743]\n",
            "[0.77093726 0.22906274]\n",
            "[0.5794559  0.42054406]\n",
            "[0.26698694 0.73301303]\n",
            "[0.02930034 0.97069967]\n",
            "[0.02999104 0.9700089 ]\n",
            "[0.22403279 0.7759672 ]\n",
            "[0.9809787  0.01902123]\n",
            "[0.01916664 0.9808334 ]\n",
            "[0.9860654  0.01393468]\n",
            "[0.86588705 0.134113  ]\n",
            "[0.02491673 0.97508323]\n",
            "[0.9721281  0.02787188]\n",
            "[0.6603534  0.33964655]\n",
            "[0.9747863  0.02521371]\n",
            "[0.6739469  0.32605302]\n",
            "[0.0249658 0.9750342]\n",
            "[0.7492434  0.25075662]\n",
            "[0.98148763 0.01851231]\n",
            "[0.03405928 0.9659407 ]\n",
            "[0.19557512 0.8044249 ]\n",
            "[0.02465932 0.9753407 ]\n",
            "[0.96690714 0.03309288]\n",
            "[0.68209404 0.317906  ]\n",
            "[0.02554001 0.97446   ]\n",
            "[0.02722567 0.9727744 ]\n",
            "[0.04109268 0.95890725]\n",
            "[0.9834351  0.01656493]\n",
            "[0.03056214 0.9694379 ]\n",
            "[0.03013528 0.96986467]\n",
            "[0.9843312  0.01566889]\n",
            "[0.9769149  0.02308505]\n",
            "[0.03057502 0.969425  ]\n",
            "[0.98308843 0.01691163]\n",
            "[0.02373435 0.9762656 ]\n",
            "[0.5491867  0.45081332]\n",
            "[0.02726663 0.9727333 ]\n",
            "[0.98029065 0.01970928]\n",
            "[0.02445705 0.97554296]\n",
            "[0.1242839 0.8757161]\n",
            "[0.9403104  0.05968959]\n",
            "[0.03040254 0.96959746]\n",
            "[0.9212196 0.0787804]\n",
            "[0.3601588  0.63984114]\n",
            "[0.02606439 0.97393566]\n",
            "[0.9825147  0.01748539]\n",
            "[0.10709146 0.8929086 ]\n",
            "[0.03531308 0.96468693]\n",
            "[0.98612344 0.01387653]\n",
            "[0.02412748 0.9758725 ]\n",
            "[0.02696835 0.97303164]\n",
            "[0.4485864 0.5514136]\n",
            "[0.9449508  0.05504924]\n",
            "[0.02383267 0.9761673 ]\n",
            "[0.94804966 0.0519503 ]\n",
            "[0.02627181 0.9737282 ]\n",
            "[0.44780245 0.5521976 ]\n",
            "[0.20359209 0.79640794]\n",
            "[0.5911349 0.4088651]\n",
            "[0.6676044  0.33239564]\n",
            "[0.6676044  0.33239564]\n",
            "[0.8977349  0.10226515]\n",
            "[0.03386067 0.9661393 ]\n",
            "[0.02629043 0.9737095 ]\n",
            "[0.97542036 0.02457966]\n",
            "[0.95578295 0.044217  ]\n",
            "[0.06279094 0.93720907]\n",
            "[0.9843106  0.01568942]\n",
            "[0.02579717 0.9742028 ]\n",
            "[0.82201725 0.17798273]\n",
            "[0.82201725 0.17798273]\n",
            "[0.9720282  0.02797181]\n",
            "[0.26925728 0.73074275]\n",
            "[0.02573246 0.97426754]\n",
            "[0.983555   0.01644499]\n",
            "[0.9851341  0.01486588]\n",
            "[0.02207596 0.97792405]\n",
            "[0.97746867 0.02253125]\n",
            "[0.02877748 0.9712225 ]\n",
            "[0.04424312 0.95575684]\n",
            "[0.98738074 0.01261932]\n",
            "[0.9681514  0.03184854]\n",
            "[0.0300945 0.9699055]\n",
            "[0.02941753 0.9705824 ]\n",
            "[0.91726935 0.08273063]\n",
            "[0.94907916 0.05092081]\n",
            "[0.985357   0.01464302]\n",
            "[0.9866368  0.01336315]\n",
            "[0.03541628 0.9645837 ]\n",
            "[0.91655856 0.08344147]\n",
            "[0.0271456 0.9728544]\n",
            "[0.9805635  0.01943646]\n",
            "[0.9582426  0.04175738]\n",
            "[0.02396228 0.97603774]\n",
            "[0.02761964 0.9723804 ]\n",
            "[0.9756442 0.0243558]\n",
            "[0.03074208 0.9692579 ]\n",
            "[0.87606364 0.12393641]\n",
            "[0.94880843 0.05119157]\n",
            "[0.78495145 0.21504861]\n",
            "[0.53853595 0.46146405]\n",
            "[0.97698385 0.02301614]\n",
            "[0.97389096 0.02610902]\n",
            "[0.03100239 0.96899754]\n",
            "[0.861006 0.138994]\n",
            "[0.02458119 0.97541887]\n",
            "[0.02146057 0.97853947]\n",
            "[0.9844467  0.01555337]\n",
            "[0.98433924 0.01566071]\n",
            "[0.02677982 0.97322017]\n",
            "[0.60845196 0.39154807]\n",
            "[0.9844552  0.01554481]\n",
            "[0.02745575 0.9725442 ]\n",
            "[0.98220634 0.01779369]\n",
            "[0.67092395 0.329076  ]\n",
            "[0.67092395 0.329076  ]\n",
            "[0.02858658 0.9714135 ]\n",
            "[0.08469769 0.9153023 ]\n",
            "[0.943117   0.05688301]\n",
            "[0.08265126 0.9173488 ]\n",
            "[0.03092702 0.969073  ]\n",
            "[0.03554721 0.9644528 ]\n",
            "[0.9823992  0.01760071]\n",
            "[0.03059086 0.9694091 ]\n",
            "[0.9852555  0.01474451]\n",
            "[0.02546095 0.97453904]\n",
            "[0.9684446  0.03155538]\n",
            "[0.73276174 0.2672383 ]\n",
            "[0.73276174 0.2672383 ]\n",
            "[0.9679997 0.0320003]\n",
            "[0.98155606 0.01844395]\n",
            "[0.02386771 0.97613233]\n",
            "[0.97358096 0.02641907]\n",
            "[0.9831777  0.01682233]\n",
            "[0.03260067 0.96739936]\n",
            "[0.01987146 0.9801286 ]\n",
            "[0.9817461  0.01825392]\n",
            "[0.38862458 0.61137545]\n",
            "[0.96899563 0.03100432]\n",
            "[0.9854232  0.01457683]\n",
            "[0.03022556 0.9697744 ]\n",
            "[0.8353304  0.16466954]\n",
            "[0.06921174 0.9307882 ]\n",
            "[0.80098677 0.19901326]\n",
            "[0.02194222 0.9780578 ]\n",
            "[0.03135135 0.9686487 ]\n",
            "[0.9809348  0.01906518]\n",
            "[0.9859081  0.01409191]\n",
            "[0.02106033 0.9789396 ]\n",
            "[0.02576228 0.97423774]\n",
            "[0.921546   0.07845399]\n",
            "[0.0372006 0.9627994]\n",
            "[0.5790337 0.4209663]\n",
            "[0.02927871 0.97072124]\n",
            "[0.9287913 0.0712087]\n",
            "[0.02865975 0.97134024]\n",
            "[0.9616321  0.03836789]\n",
            "[0.5785912 0.4214087]\n",
            "[0.44800884 0.5519912 ]\n",
            "[0.0283387 0.9716613]\n",
            "[0.98349077 0.01650919]\n",
            "[0.9809734  0.01902659]\n",
            "[0.9834013  0.01659866]\n",
            "[0.02744433 0.9725557 ]\n",
            "[0.02765101 0.9723489 ]\n",
            "[0.83452433 0.16547564]\n",
            "[0.02747642 0.97252357]\n",
            "[0.9821607  0.01783933]\n",
            "[0.86441267 0.13558738]\n",
            "[0.02714943 0.9728505 ]\n",
            "[0.02524976 0.9747502 ]\n",
            "[0.98381215 0.01618784]\n",
            "[0.27006507 0.72993493]\n",
            "[0.03332335 0.9666766 ]\n",
            "[0.02509403 0.974906  ]\n",
            "[0.02748712 0.97251284]\n",
            "[0.02839612 0.9716039 ]\n",
            "[0.02950122 0.97049874]\n",
            "[0.02472624 0.9752738 ]\n",
            "[0.9816005  0.01839949]\n",
            "[0.0299223 0.9700777]\n",
            "[0.98508066 0.01491938]\n",
            "[0.98508066 0.01491938]\n",
            "[0.07724589 0.9227541 ]\n",
            "[0.02308618 0.97691387]\n",
            "[0.02072671 0.9792733 ]\n",
            "[0.75134945 0.24865048]\n",
            "[0.75134945 0.24865048]\n",
            "[0.9768282  0.02317176]\n",
            "[0.04705589 0.9529441 ]\n",
            "[0.06899335 0.9310066 ]\n",
            "[0.28305107 0.7169489 ]\n",
            "[0.0231063 0.9768937]\n",
            "[0.9878127  0.01218731]\n",
            "[0.5389197  0.46108034]\n",
            "[0.04200636 0.9579936 ]\n",
            "[0.0252803  0.97471964]\n",
            "[0.9854938  0.01450622]\n",
            "[0.9498371  0.05016287]\n",
            "[0.02668168 0.9733183 ]\n",
            "[0.7968168  0.20318314]\n",
            "[0.9861994  0.01380067]\n",
            "[0.02607092 0.9739291 ]\n",
            "[0.21808104 0.78191894]\n",
            "[0.02649776 0.97350216]\n",
            "[0.9266359  0.07336409]\n",
            "[0.35845423 0.64154583]\n",
            "[0.945018   0.05498201]\n",
            "[0.9876572  0.01234277]\n",
            "[0.02555592 0.9744441 ]\n",
            "[0.7765546  0.22344546]\n",
            "[0.80685747 0.1931425 ]\n",
            "[0.9452426  0.05475741]\n",
            "[0.0329665  0.96703357]\n",
            "[0.9757671  0.02423292]\n",
            "[0.8045926  0.19540735]\n",
            "[0.07020751 0.92979246]\n",
            "[0.9536028  0.04639729]\n",
            "[0.02694248 0.9730575 ]\n",
            "[0.9554628  0.04453715]\n",
            "[0.38910955 0.61089045]\n",
            "[0.38910955 0.61089045]\n",
            "[0.8007274  0.19927262]\n",
            "[0.9811391  0.01886084]\n",
            "[0.0250809  0.97491914]\n",
            "[0.9832212  0.01677879]\n",
            "[0.9728674  0.02713254]\n",
            "[0.08000327 0.9199968 ]\n",
            "[0.96388465 0.03611537]\n",
            "[0.03833093 0.961669  ]\n",
            "[0.02862208 0.9713779 ]\n",
            "[0.11022315 0.8897769 ]\n",
            "[0.9640613  0.03593867]\n",
            "[0.02547262 0.9745274 ]\n",
            "[0.26891664 0.73108333]\n",
            "[0.03539947 0.9646005 ]\n",
            "[0.05028322 0.94971675]\n",
            "[0.98198205 0.01801793]\n",
            "[0.02645211 0.9735478 ]\n",
            "[0.9827864  0.01721355]\n",
            "[0.9693518  0.03064816]\n",
            "[0.12569802 0.874302  ]\n",
            "[0.9582315  0.04176843]\n",
            "[0.05379511 0.9462049 ]\n",
            "[0.02534804 0.97465193]\n",
            "[0.9378607  0.06213927]\n",
            "[0.02226943 0.97773063]\n",
            "[0.02249583 0.97750413]\n",
            "[0.5635839 0.4364161]\n",
            "[0.78175384 0.21824619]\n",
            "[0.78175384 0.21824619]\n",
            "[0.9820192  0.01798087]\n",
            "[0.02331754 0.9766824 ]\n",
            "[0.9832207  0.01677928]\n",
            "[0.9768669  0.02313314]\n",
            "[0.02511741 0.97488254]\n",
            "[0.97644436 0.02355561]\n",
            "[0.02952193 0.9704781 ]\n",
            "[0.84172344 0.15827657]\n",
            "[0.02573661 0.9742634 ]\n",
            "[0.02471392 0.97528607]\n",
            "[0.98262584 0.01737414]\n",
            "[0.02649053 0.97350943]\n",
            "[0.16178407 0.8382159 ]\n",
            "[0.8416794 0.1583206]\n",
            "[0.02788721 0.9721128 ]\n",
            "[0.80639935 0.19360061]\n",
            "[0.17569661 0.8243033 ]\n",
            "[0.03487784 0.9651221 ]\n",
            "[0.0245963  0.97540367]\n",
            "[0.57879287 0.42120713]\n",
            "[0.14650057 0.8534994 ]\n",
            "[0.9826933  0.01730668]\n",
            "[0.02448811 0.97551185]\n",
            "[0.8247031  0.17529687]\n",
            "[0.98293185 0.01706814]\n",
            "[0.0284378 0.9715622]\n",
            "[0.03377039 0.96622956]\n",
            "[0.02468955 0.97531044]\n",
            "[0.02698008 0.9730199 ]\n",
            "[0.9827444  0.01725565]\n",
            "[0.97064006 0.02936   ]\n",
            "[0.02349404 0.97650594]\n",
            "[0.9853436  0.01465646]\n",
            "[0.6044573 0.3955427]\n",
            "[0.6044573 0.3955427]\n",
            "[0.02040329 0.97959673]\n",
            "[0.8390076  0.16099237]\n",
            "[0.98676527 0.0132347 ]\n",
            "[0.0803021  0.91969794]\n",
            "[0.0302815 0.9697185]\n",
            "[0.02826762 0.9717324 ]\n",
            "[0.7489809  0.25101918]\n",
            "[0.02656953 0.97343045]\n",
            "[0.97762567 0.02237432]\n",
            "[0.9853033  0.01469669]\n",
            "[0.89509726 0.10490277]\n",
            "[0.02454411 0.97545594]\n",
            "[0.03241877 0.9675812 ]\n",
            "[0.9863142  0.01368583]\n",
            "[0.28202686 0.7179731 ]\n",
            "[0.03185278 0.96814716]\n",
            "[0.0242501 0.9757499]\n",
            "[0.94822115 0.05177891]\n",
            "[0.11046295 0.88953704]\n",
            "[0.98271    0.01728997]\n",
            "[0.7475265  0.25247353]\n",
            "[0.7475265  0.25247353]\n",
            "[0.94868034 0.05131969]\n",
            "[0.94868034 0.05131969]\n",
            "[0.87688863 0.12311131]\n",
            "[0.02551632 0.97448367]\n",
            "[0.04660044 0.95339954]\n",
            "[0.98642504 0.01357498]\n",
            "[0.92525655 0.07474346]\n",
            "[0.02699609 0.97300386]\n",
            "[0.0516232 0.9483768]\n",
            "[0.98195904 0.01804095]\n",
            "[0.0255685 0.9744315]\n",
            "[0.506106   0.49389398]\n",
            "[0.0319363 0.9680637]\n",
            "[0.9834073  0.01659271]\n",
            "[0.02742857 0.9725714 ]\n",
            "[0.9532676  0.04673244]\n",
            "[0.08399745 0.9160025 ]\n",
            "[0.04715486 0.9528451 ]\n",
            "[0.0259679 0.9740321]\n",
            "[0.98642063 0.01357929]\n",
            "[0.96701807 0.03298189]\n",
            "[0.02324326 0.9767568 ]\n",
            "[0.0262217  0.97377825]\n",
            "[0.9820112  0.01798882]\n",
            "[0.9523582  0.04764187]\n",
            "[0.02319913 0.9768009 ]\n",
            "[0.03217796 0.967822  ]\n",
            "[0.9758572  0.02414278]\n",
            "[0.98319614 0.01680385]\n",
            "[0.9548101  0.04518992]\n",
            "[0.98379874 0.0162013 ]\n",
            "[0.8229895  0.17701045]\n",
            "[0.8229895  0.17701045]\n",
            "[0.9743543  0.02564564]\n",
            "[0.9874792  0.01252084]\n",
            "[0.9540151  0.04598486]\n",
            "[0.02436443 0.9756355 ]\n",
            "[0.9819443  0.01805572]\n",
            "[0.5942663  0.40573367]\n",
            "[0.5942663  0.40573367]\n",
            "[0.8946128  0.10538717]\n",
            "[0.8076019  0.19239806]\n",
            "[0.0265424 0.9734576]\n",
            "[0.6372397 0.3627603]\n",
            "[0.9154578  0.08454219]\n",
            "[0.9844796  0.01552039]\n",
            "[0.02544893 0.974551  ]\n",
            "[0.0330888  0.96691114]\n",
            "[0.05885901 0.941141  ]\n",
            "[0.9867014  0.01329857]\n",
            "[0.9847793  0.01522066]\n",
            "[0.02572586 0.9742741 ]\n",
            "[0.2211689  0.77883106]\n",
            "[0.6464292 0.3535708]\n",
            "[0.9657444  0.03425571]\n",
            "[0.02855761 0.9714424 ]\n",
            "[0.02538161 0.97461843]\n",
            "[0.97499067 0.02500932]\n",
            "[0.819076   0.18092397]\n",
            "[0.98816514 0.01183491]\n",
            "[0.02369363 0.9763064 ]\n",
            "[0.9702303  0.02976971]\n",
            "[0.9818652  0.01813474]\n",
            "[0.02129455 0.97870547]\n",
            "[0.02791227 0.97208774]\n",
            "[0.98446923 0.01553075]\n",
            "[0.02541271 0.97458726]\n",
            "[0.8906036  0.10939635]\n",
            "[0.8802619  0.11973815]\n",
            "[0.06008581 0.93991417]\n",
            "[0.02946794 0.970532  ]\n",
            "[0.88724923 0.11275081]\n",
            "[0.1445036  0.85549647]\n",
            "[0.71756357 0.28243646]\n",
            "[0.9170661  0.08293389]\n",
            "[0.14406839 0.8559316 ]\n",
            "[0.43764955 0.5623504 ]\n",
            "[0.02594797 0.974052  ]\n",
            "[0.9309508  0.06904913]\n",
            "[0.5615769  0.43842304]\n",
            "[0.7702039  0.22979605]\n",
            "[0.8109706  0.18902938]\n",
            "[0.07691632 0.9230837 ]\n",
            "[0.232097   0.76790303]\n",
            "[0.8723629  0.12763707]\n",
            "[0.9802351 0.0197649]\n",
            "[0.4792359 0.5207641]\n",
            "[0.23990439 0.7600956 ]\n",
            "[0.02689287 0.97310716]\n",
            "[0.02057898 0.979421  ]\n",
            "[0.02688958 0.97311044]\n",
            "[0.9837013  0.01629868]\n",
            "[0.93613213 0.06386786]\n",
            "[0.02500934 0.97499067]\n",
            "[0.8919149  0.10808504]\n",
            "[0.9847873  0.01521272]\n",
            "[0.76226395 0.23773602]\n",
            "[0.5965325  0.40346748]\n",
            "[0.959588   0.04041195]\n",
            "[0.02792259 0.97207737]\n",
            "[0.8959594  0.10404066]\n",
            "[0.8959594  0.10404066]\n",
            "[0.01924926 0.98075074]\n",
            "[0.8099915  0.19000854]\n",
            "[0.8016242  0.19837585]\n",
            "[0.9230438  0.07695621]\n",
            "[0.0279771 0.9720229]\n",
            "[0.02684408 0.9731559 ]\n",
            "[0.9826797  0.01732031]\n",
            "[0.43714705 0.562853  ]\n",
            "[0.02848767 0.9715124 ]\n",
            "[0.98272216 0.01727777]\n",
            "[0.9743946  0.02560537]\n",
            "[0.02576253 0.97423744]\n",
            "[0.98741406 0.01258596]\n",
            "[0.0269662 0.9730338]\n",
            "[0.9468737  0.05312632]\n",
            "[0.9247499  0.07525007]\n",
            "[0.9124371  0.08756295]\n",
            "[0.46204975 0.5379503 ]\n",
            "[0.02092765 0.9790724 ]\n",
            "[0.98342556 0.01657448]\n",
            "[0.02341637 0.9765836 ]\n",
            "[0.9800047  0.01999526]\n",
            "[0.08950925 0.91049075]\n",
            "[0.98483986 0.01516011]\n",
            "[0.976597 0.023403]\n",
            "[0.03755302 0.962447  ]\n",
            "[0.98512936 0.01487065]\n",
            "[0.02554207 0.97445786]\n",
            "[0.05423553 0.9457644 ]\n",
            "[0.03948617 0.96051383]\n",
            "[0.02848452 0.97151554]\n",
            "[0.9358223  0.06417774]\n",
            "[0.02560645 0.9743936 ]\n",
            "[0.98547965 0.0145203 ]\n",
            "[0.98516166 0.01483842]\n",
            "[0.0268964 0.9731035]\n",
            "[0.27047396 0.72952604]\n",
            "[0.24702129 0.75297874]\n",
            "[0.03150178 0.9684983 ]\n",
            "[0.9446778  0.05532214]\n",
            "[0.87481856 0.12518142]\n",
            "[0.02788567 0.9721143 ]\n",
            "[0.9653872  0.03461272]\n",
            "[0.3038187 0.6961813]\n",
            "[0.06941541 0.9305846 ]\n",
            "[0.025754 0.974246]\n",
            "[0.93866616 0.06133381]\n",
            "[0.02572192 0.97427803]\n",
            "[0.37122867 0.6287713 ]\n",
            "[0.02580369 0.9741964 ]\n",
            "[0.9769925  0.02300745]\n",
            "[0.42693582 0.5730642 ]\n",
            "[0.42693582 0.5730642 ]\n",
            "[0.9822848  0.01771515]\n",
            "[0.94465256 0.05534744]\n",
            "[0.0213059 0.9786941]\n",
            "[0.9413307  0.05866931]\n",
            "[0.97988784 0.02011223]\n",
            "[0.97901595 0.02098407]\n",
            "[0.9345949 0.0654051]\n",
            "[0.97802573 0.02197425]\n",
            "[0.2590743  0.74092567]\n",
            "[0.8822401  0.11775991]\n",
            "[0.98457795 0.01542204]\n",
            "[0.02365414 0.97634584]\n",
            "[0.02458124 0.97541875]\n",
            "[0.9157018  0.08429815]\n",
            "[0.03430548 0.96569455]\n",
            "[0.0337313 0.9662687]\n",
            "[0.49519572 0.5048043 ]\n",
            "[0.18432263 0.81567734]\n",
            "[0.95584905 0.0441509 ]\n",
            "[0.01911409 0.9808859 ]\n",
            "[0.9691135  0.03088641]\n",
            "[0.18379238 0.8162076 ]\n",
            "[0.02713682 0.97286314]\n",
            "[0.6790862  0.32091376]\n",
            "[0.02771587 0.97228414]\n",
            "[0.04386428 0.95613575]\n",
            "[0.0231171  0.97688293]\n",
            "[0.9802331  0.01976699]\n",
            "[0.02803103 0.97196895]\n",
            "[0.968954   0.03104595]\n",
            "[0.983451   0.01654896]\n",
            "[0.02472694 0.975273  ]\n",
            "[0.3217099 0.6782901]\n",
            "[0.83098245 0.16901761]\n",
            "[0.9722527  0.02774734]\n",
            "[0.02514709 0.97485286]\n",
            "[0.9873247  0.01267529]\n",
            "[0.02647207 0.97352797]\n",
            "[0.9841096 0.0158904]\n",
            "[0.81259465 0.18740535]\n",
            "[0.27092662 0.72907335]\n",
            "[0.02481664 0.9751833 ]\n",
            "[0.9702955  0.02970455]\n",
            "[0.9702955  0.02970455]\n",
            "[0.97802097 0.02197901]\n",
            "[0.0275184 0.9724816]\n",
            "[0.94288456 0.05711542]\n",
            "[0.12148152 0.8785185 ]\n",
            "[0.0226434 0.9773566]\n",
            "[0.9850522  0.01494785]\n",
            "[0.86710966 0.13289036]\n",
            "[0.02477967 0.9752203 ]\n",
            "[0.9846942  0.01530582]\n",
            "[0.97215164 0.02784835]\n",
            "[0.02573136 0.9742687 ]\n",
            "[0.04070638 0.9592936 ]\n",
            "[0.20029998 0.7997    ]\n",
            "[0.48372906 0.51627094]\n",
            "[0.48372906 0.51627094]\n",
            "[0.97079337 0.02920663]\n",
            "[0.02470415 0.9752958 ]\n",
            "[0.02999709 0.97000283]\n",
            "[0.9873639 0.0126361]\n",
            "[0.9831051  0.01689483]\n",
            "[0.02504981 0.9749502 ]\n",
            "[0.83161294 0.16838701]\n",
            "[0.07005505 0.92994493]\n",
            "[0.9857717  0.01422826]\n",
            "[0.02617545 0.9738245 ]\n",
            "[0.02877932 0.9712207 ]\n",
            "[0.8751684  0.12483156]\n",
            "[0.04968262 0.9503173 ]\n",
            "[0.04968262 0.9503173 ]\n",
            "[0.04024016 0.95975983]\n",
            "[0.9847371  0.01526286]\n",
            "[0.69089025 0.30910972]\n",
            "[0.69089025 0.30910972]\n",
            "[0.9293605  0.07063947]\n",
            "[0.9293605  0.07063947]\n",
            "[0.03277038 0.96722955]\n",
            "[0.9446072  0.05539278]\n",
            "[0.02629859 0.9737014 ]\n",
            "[0.13258623 0.8674138 ]\n",
            "[0.02610627 0.9738937 ]\n",
            "[0.02610627 0.9738937 ]\n",
            "[0.07026864 0.92973137]\n",
            "[0.03418421 0.96581584]\n",
            "[0.97933817 0.02066183]\n",
            "[0.02408608 0.97591394]\n",
            "[0.04340982 0.9565902 ]\n",
            "[0.39687353 0.6031265 ]\n",
            "[0.17931654 0.8206835 ]\n",
            "[0.96758527 0.03241477]\n",
            "[0.03642222 0.96357775]\n",
            "[0.94923717 0.0507628 ]\n",
            "[0.57686085 0.42313913]\n",
            "[0.0206677 0.9793323]\n",
            "[0.91785234 0.08214761]\n",
            "[0.08106259 0.9189374 ]\n",
            "[0.02747133 0.97252864]\n",
            "[0.88051033 0.11948965]\n",
            "[0.0455997 0.9544003]\n",
            "[0.7596379 0.2403621]\n",
            "[0.02721825 0.9727817 ]\n",
            "[0.9812587  0.01874134]\n",
            "[0.9835961  0.01640395]\n",
            "[0.18276174 0.8172382 ]\n",
            "[0.93025315 0.06974687]\n",
            "[0.02520845 0.9747916 ]\n",
            "[0.9870172  0.01298282]\n",
            "[0.086348 0.913652]\n",
            "[0.7551887  0.24481131]\n",
            "[0.9780157  0.02198424]\n",
            "[0.16764167 0.83235836]\n",
            "[0.02769629 0.97230375]\n",
            "[0.94119084 0.05880911]\n",
            "[0.02632879 0.97367126]\n",
            "[0.02689206 0.97310793]\n",
            "[0.6759681  0.32403192]\n",
            "[0.3303497 0.6696503]\n",
            "[0.98819274 0.01180729]\n",
            "[0.02552441 0.97447556]\n",
            "[0.02408885 0.97591114]\n",
            "[0.98284376 0.01715625]\n",
            "[0.02623252 0.9737675 ]\n",
            "[0.9848899  0.01511004]\n",
            "[0.9800969  0.01990311]\n",
            "[0.05574659 0.94425344]\n",
            "[0.81243813 0.18756185]\n",
            "[0.50988144 0.49011853]\n",
            "[0.79145896 0.208541  ]\n",
            "[0.02982388 0.9701761 ]\n",
            "[0.01887261 0.98112744]\n",
            "[0.9860422  0.01395785]\n",
            "[0.85885644 0.14114356]\n",
            "[0.03536927 0.9646307 ]\n",
            "[0.887409   0.11259097]\n",
            "[0.887409   0.11259097]\n",
            "[0.03048632 0.9695137 ]\n",
            "[0.9706325  0.02936746]\n",
            "[0.62900543 0.37099454]\n",
            "[0.2801644 0.7198356]\n",
            "[0.45073193 0.549268  ]\n",
            "[0.02739327 0.9726068 ]\n",
            "[0.02425791 0.9757421 ]\n",
            "[0.9869186  0.01308139]\n",
            "[0.96651244 0.03348753]\n",
            "[0.03037975 0.9696203 ]\n",
            "[0.02557461 0.9744254 ]\n",
            "[0.9758657  0.02413433]\n",
            "[0.02316631 0.9768337 ]\n",
            "[0.2829656  0.71703434]\n",
            "[0.98015416 0.01984591]\n",
            "[0.98192906 0.01807098]\n",
            "[0.95542806 0.0445719 ]\n",
            "[0.9653765  0.03462349]\n",
            "[0.97498983 0.02501016]\n",
            "[0.8275866  0.17241347]\n",
            "[0.02567461 0.97432536]\n",
            "[0.17118146 0.82881856]\n",
            "[0.70693094 0.29306903]\n",
            "[0.02096867 0.9790314 ]\n",
            "[0.02969092 0.9703091 ]\n",
            "[0.95967406 0.04032597]\n",
            "[0.97533196 0.02466799]\n",
            "[0.98072207 0.01927795]\n",
            "[0.02557373 0.9744263 ]\n",
            "[0.13236886 0.8676311 ]\n",
            "[0.02316549 0.9768345 ]\n",
            "[0.9874314  0.01256857]\n",
            "[0.02702484 0.9729751 ]\n",
            "[0.51085013 0.48914987]\n",
            "[0.9736736  0.02632639]\n",
            "[0.9759863  0.02401366]\n",
            "[0.02911139 0.9708887 ]\n",
            "[0.9843763  0.01562365]\n",
            "[0.02791646 0.97208357]\n",
            "[0.8843898  0.11561015]\n",
            "[0.98295516 0.0170448 ]\n",
            "[0.18875423 0.81124574]\n",
            "[0.9881648  0.01183517]\n",
            "[0.19572937 0.80427057]\n",
            "[0.69927114 0.3007288 ]\n",
            "[0.86715835 0.13284166]\n",
            "[0.07465255 0.92534745]\n",
            "[0.31648195 0.68351805]\n",
            "[0.963646   0.03635396]\n",
            "[0.02933414 0.9706659 ]\n",
            "[0.95236045 0.04763959]\n",
            "[0.6273736 0.3726264]\n",
            "[0.02502853 0.9749715 ]\n",
            "[0.05161045 0.94838953]\n",
            "[0.9878721  0.01212782]\n",
            "[0.8699696  0.13003042]\n",
            "[0.04218983 0.9578102 ]\n",
            "[0.9638377 0.0361623]\n",
            "[0.9817075  0.01829249]\n",
            "[0.02122011 0.9787799 ]\n",
            "[0.9756477  0.02435238]\n",
            "[0.7573645  0.24263549]\n",
            "[0.02582666 0.97417337]\n",
            "[0.98217815 0.01782182]\n",
            "[0.02643244 0.9735676 ]\n",
            "[0.02301807 0.97698194]\n",
            "[0.98673755 0.01326249]\n",
            "[0.02315375 0.9768463 ]\n",
            "[0.9846847  0.01531531]\n",
            "[0.8865898  0.11341022]\n",
            "[0.04667437 0.9533256 ]\n",
            "[0.98651356 0.01348638]\n",
            "[0.03058457 0.9694155 ]\n",
            "[0.27910727 0.7208927 ]\n",
            "[0.9320905  0.06790951]\n",
            "[0.9746191  0.02538085]\n",
            "[0.04690879 0.95309126]\n",
            "[0.14798126 0.8520187 ]\n",
            "[0.97488075 0.0251193 ]\n",
            "[0.02515298 0.97484696]\n",
            "[0.7903544  0.20964561]\n",
            "[0.0266614 0.9733386]\n",
            "[0.9311695  0.06883046]\n",
            "[0.03572277 0.9642772 ]\n",
            "[0.03572277 0.9642772 ]\n",
            "[0.02972368 0.9702763 ]\n",
            "[0.037593 0.962407]\n",
            "[0.78165114 0.21834889]\n",
            "[0.03320537 0.9667946 ]\n",
            "[0.02700798 0.97299206]\n",
            "[0.02682442 0.9731756 ]\n",
            "[0.9870216  0.01297834]\n",
            "[0.97080594 0.02919409]\n",
            "[0.02156359 0.9784364 ]\n",
            "[0.9719188  0.02808123]\n",
            "[0.6151845  0.38481548]\n",
            "[0.0803322 0.9196678]\n",
            "[0.0393277 0.9606723]\n",
            "[0.06569657 0.93430346]\n",
            "[0.9448819 0.0551181]\n",
            "[0.02874194 0.97125804]\n",
            "[0.03446519 0.9655348 ]\n",
            "[0.06196182 0.9380382 ]\n",
            "[0.9815029 0.0184971]\n",
            "[0.02427501 0.97572494]\n",
            "[0.03444974 0.96555024]\n",
            "[0.2717018 0.7282981]\n",
            "[0.02695143 0.97304857]\n",
            "[0.9772276  0.02277233]\n",
            "[0.8058807  0.19411927]\n",
            "[0.9420746  0.05792544]\n",
            "[0.02900475 0.9709953 ]\n",
            "[0.03886677 0.9611332 ]\n",
            "[0.02340052 0.9765995 ]\n",
            "[0.98522675 0.01477319]\n",
            "[0.8872232  0.11277685]\n",
            "[0.94783133 0.0521686 ]\n",
            "[0.02344533 0.97655463]\n",
            "[0.9869502  0.01304975]\n",
            "[0.02755947 0.9724405 ]\n",
            "[0.7665504  0.23344964]\n",
            "[0.7742501  0.22574991]\n",
            "[0.98232275 0.0176772 ]\n",
            "[0.02283799 0.97716206]\n",
            "[0.3424832  0.65751684]\n",
            "[0.027481  0.9725189]\n",
            "[0.02724587 0.9727542 ]\n",
            "[0.02333171 0.9766683 ]\n",
            "[0.9889087  0.01109123]\n",
            "[0.02219767 0.9778023 ]\n",
            "[0.02205914 0.9779409 ]\n",
            "[0.9884886  0.01151144]\n",
            "[0.02254794 0.97745204]\n",
            "[0.02766768 0.97233224]\n",
            "[0.02836406 0.97163594]\n",
            "[0.98588276 0.01411727]\n",
            "[0.02484721 0.97515285]\n",
            "[0.97089696 0.02910308]\n",
            "[0.9879666  0.01203337]\n",
            "[0.02916839 0.9708316 ]\n",
            "[0.14081523 0.85918474]\n",
            "[0.0307386  0.96926147]\n",
            "[0.9820701  0.01792994]\n",
            "[0.03540742 0.9645925 ]\n",
            "[0.9841235  0.01587643]\n",
            "[0.97276115 0.02723884]\n",
            "[0.0260286  0.97397137]\n",
            "[0.93899053 0.06100946]\n",
            "[0.03135774 0.9686423 ]\n",
            "[0.13218944 0.8678106 ]\n",
            "[0.13218944 0.8678106 ]\n",
            "[0.98410827 0.01589172]\n",
            "[0.06736177 0.9326382 ]\n",
            "[0.03871816 0.9612819 ]\n",
            "[0.8773975  0.12260252]\n",
            "[0.64293903 0.35706103]\n",
            "[0.64293903 0.35706103]\n",
            "[0.64293903 0.35706103]\n",
            "[0.64293903 0.35706103]\n",
            "[0.03587757 0.9641225 ]\n",
            "[0.10482657 0.8951735 ]\n",
            "[0.9770238  0.02297627]\n",
            "[0.962683   0.03731699]\n",
            "[0.98220176 0.0177982 ]\n",
            "[0.02141792 0.9785821 ]\n",
            "[0.9851836  0.01481637]\n",
            "[0.02756806 0.9724319 ]\n",
            "[0.86404485 0.13595514]\n",
            "[0.9860265  0.01397347]\n",
            "[0.02539905 0.974601  ]\n",
            "[0.34371346 0.65628654]\n",
            "[0.02861709 0.97138286]\n",
            "[0.9826231  0.01737689]\n",
            "[0.85193855 0.14806142]\n",
            "[0.9813798  0.01862022]\n",
            "[0.04927855 0.95072144]\n",
            "[0.2647084 0.7352916]\n",
            "[0.85328645 0.1467135 ]\n",
            "[0.9577238  0.04227622]\n",
            "[0.0560726  0.94392735]\n",
            "[0.9517713  0.04822871]\n",
            "[0.03025007 0.96974987]\n",
            "[0.95235914 0.0476408 ]\n",
            "[0.03006699 0.969933  ]\n",
            "[0.983065   0.01693502]\n",
            "[0.02657609 0.9734239 ]\n",
            "[0.02562907 0.97437096]\n",
            "[0.9608224  0.03917763]\n",
            "[0.0258037 0.9741964]\n",
            "[0.3732011 0.6267989]\n",
            "[0.97162014 0.02837982]\n",
            "[0.07472524 0.9252748 ]\n",
            "[0.02604022 0.97395974]\n",
            "[0.85687566 0.14312433]\n",
            "[0.94221735 0.05778271]\n",
            "[0.9551827  0.04481729]\n",
            "[0.03049794 0.9695021 ]\n",
            "[0.03214991 0.96785   ]\n",
            "[0.97403145 0.02596858]\n",
            "[0.8337294  0.16627067]\n",
            "[0.02809669 0.97190326]\n",
            "[0.85553044 0.1444695 ]\n",
            "[0.8347339  0.16526614]\n",
            "[0.9320916  0.06790844]\n",
            "[0.7666978  0.23330216]\n",
            "[0.30284497 0.697155  ]\n",
            "[0.5452302  0.45476973]\n",
            "[0.26480255 0.7351975 ]\n",
            "[0.66294086 0.3370592 ]\n",
            "[0.94242215 0.05757789]\n",
            "[0.02764589 0.9723541 ]\n",
            "[0.97196805 0.02803201]\n",
            "[0.43164018 0.56835985]\n",
            "[0.73660386 0.26339617]\n",
            "[0.97273356 0.02726649]\n",
            "[0.849717   0.15028293]\n",
            "[0.849717   0.15028293]\n",
            "[0.8729854  0.12701455]\n",
            "[0.8729854  0.12701455]\n",
            "[0.9701339  0.02986615]\n",
            "[0.91376925 0.08623078]\n",
            "[0.02735289 0.97264713]\n",
            "[0.50239307 0.49760693]\n",
            "[0.9706359  0.02936416]\n",
            "[0.8327028 0.1672972]\n",
            "[0.9746382  0.02536175]\n",
            "[0.95993084 0.0400692 ]\n",
            "[0.0520871 0.9479129]\n",
            "[0.97927296 0.02072708]\n",
            "[0.98075104 0.01924897]\n",
            "[0.02620156 0.9737985 ]\n",
            "[0.02473555 0.9752644 ]\n",
            "[0.02555575 0.97444427]\n",
            "[0.49073383 0.5092662 ]\n",
            "[0.02621223 0.9737878 ]\n",
            "[0.98619336 0.01380664]\n",
            "[0.9836776  0.01632235]\n",
            "[0.9882053  0.01179474]\n",
            "[0.08833233 0.91166764]\n",
            "[0.9541478  0.04585221]\n",
            "[0.9541478  0.04585221]\n",
            "[0.7338485  0.26615143]\n",
            "[0.02997272 0.9700273 ]\n",
            "[0.8601323  0.13986772]\n",
            "[0.04090285 0.95909715]\n",
            "[0.9861501  0.01384983]\n",
            "[0.0366024  0.96339756]\n",
            "[0.93872625 0.06127373]\n",
            "[0.93872625 0.06127373]\n",
            "[0.49964094 0.50035906]\n",
            "[0.9848817  0.01511824]\n",
            "[0.02779569 0.9722044 ]\n",
            "[0.98386407 0.01613588]\n",
            "[0.9285057  0.07149431]\n",
            "[0.91692346 0.08307651]\n",
            "[0.9820035  0.01799642]\n",
            "[0.02477219 0.9752278 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/task1hindi-results.txt\",predictions, fmt=\"%s\")\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/buffer/true labels/task1hindi-lebels.txt\",true_labels, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "outputId": "3f2dd067-39de-4a26-f12c-4fe94cd280ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI2K1bqaR5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "outputId": "aa568c9b-f984-4048-c2dd-509e2d4952b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QowNXrYZSRMR",
        "colab_type": "code",
        "outputId": "82c5396e-1eaa-490a-a843-e4b9734c82b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "print(flat_predictions)\n",
        "print(\"************\")\n",
        "print(flat_true_labels)"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0\n",
            " 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 1 0 0 0 0 1]\n",
            "************\n",
            "[1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "outputId": "14b23e7b-799a-4608-8b1d-c5d487f9b765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-task1hindi'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task1hindi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task1hindi/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1hindi/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1hindi/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    }
  ]
}
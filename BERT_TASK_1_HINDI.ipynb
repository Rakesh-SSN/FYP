{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT TASK 1 HINDI",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e15f320351bb4f298ee696605e402d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_47d796d72df747e0a3153a0ce3667115",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2d78b34f25534f80bf3e2a1ae44cde25",
              "IPY_MODEL_7382683c4adc4944984404f17da2ab62"
            ]
          }
        },
        "47d796d72df747e0a3153a0ce3667115": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d78b34f25534f80bf3e2a1ae44cde25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6f35f78cc46d4677b696f2c94af49cba",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2b71223569e4b91a0b809d6225c9ccc"
          }
        },
        "7382683c4adc4944984404f17da2ab62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_88017b19ec8642cf9eb57abf69732814",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 996k/996k [00:00&lt;00:00, 1.09MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2d9c10657761469a8d92c9bcfda03ec1"
          }
        },
        "6f35f78cc46d4677b696f2c94af49cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2b71223569e4b91a0b809d6225c9ccc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88017b19ec8642cf9eb57abf69732814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2d9c10657761469a8d92c9bcfda03ec1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37ba2eb2de9f4a0aaeafd383dbc415c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2ddc38d12d30455a8374b5bcb3814ee8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_44e0660a72a5424bb62c9a78726a55a9",
              "IPY_MODEL_f1f13410d7e846dbafc5b87d85049c57"
            ]
          }
        },
        "2ddc38d12d30455a8374b5bcb3814ee8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "44e0660a72a5424bb62c9a78726a55a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_06e8d8a3d25246adb98bf66047d9eb38",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 569,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 569,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f26750316b274ef8837fa05e6cbf7830"
          }
        },
        "f1f13410d7e846dbafc5b87d85049c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d6d59d81501945dca36687f4cb1732c4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 569/569 [00:00&lt;00:00, 15.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c074170834be435f8f6832724b4880d0"
          }
        },
        "06e8d8a3d25246adb98bf66047d9eb38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f26750316b274ef8837fa05e6cbf7830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6d59d81501945dca36687f4cb1732c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c074170834be435f8f6832724b4880d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7a91c4a9d964fd7bfbbe4d141b48b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_749c0b678fdb4719a4baef4dd9159861",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1db4754056084fdbb2b39208ac150bba",
              "IPY_MODEL_5f197336f1cb4b6791ca638f6caa998b"
            ]
          }
        },
        "749c0b678fdb4719a4baef4dd9159861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1db4754056084fdbb2b39208ac150bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d42bd679fed64ef2a01a08dc6722bcd9",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b067773a6dc3436cb726eb8d7876c241"
          }
        },
        "5f197336f1cb4b6791ca638f6caa998b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6d9611871f154e66857615bd118084d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 714M/714M [00:55&lt;00:00, 12.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c607a35f58384749b44f4cc90e91049e"
          }
        },
        "d42bd679fed64ef2a01a08dc6722bcd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b067773a6dc3436cb726eb8d7876c241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d9611871f154e66857615bd118084d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c607a35f58384749b44f4cc90e91049e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT_TASK_1_HINDI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "38df0bd7-ec02-4701-8450-6bd419aa6faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "eb61370b-0308-4d4d-9db4-3242ddb3b4d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "ea9e9f10-7008-42fc-aae8-25e4b4d8ed82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/hindi/task1hindi.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>HIN0566</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>इसके साथ ही दो हज़ार पांच सौ सतायीस लोगों के जख...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2301</th>\n",
              "      <td>HIN2302</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>कुलपति अप्पा राव शुक्रवार को दो प्राध्यापकों ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>HIN0228</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>पुलिस ने करीब पच्चास  छात्रों को हिरासत में लि...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2438</th>\n",
              "      <td>HIN2439</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>सौरभ सेनगुप्ता द्वारा निर्देशित 'इट्स मैन्स वर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2445</th>\n",
              "      <td>HIN2446</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>भारत के इस समय रैंकिंग में एक सौ सतायिस  अंक ह...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>HIN0501</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>पीडीपी की अध्यक्ष महबूबा मुफ्ती सोमवार को जम्म...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1971</th>\n",
              "      <td>HIN1972</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>उन्होंने ट्वीट किया, 'विजय माल्या से मिले जवाब...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1875</th>\n",
              "      <td>HIN1876</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>वकील निलेश ने कहा,' राहुल एक ड्रग माफिया है और...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1322</th>\n",
              "      <td>HIN1323</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>उनका पार्थिव शरीर पंच तत्व में विलीन हो गया। &lt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1427</th>\n",
              "      <td>HIN1428</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>हम ऐसी दुनिया में हैं जिसमें लोग डिजिटली कहीं ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "565          HIN0566  ...  इसके साथ ही दो हज़ार पांच सौ सतायीस लोगों के जख...\n",
              "2301         HIN2302  ...   कुलपति अप्पा राव शुक्रवार को दो प्राध्यापकों ...\n",
              "227          HIN0228  ...  पुलिस ने करीब पच्चास  छात्रों को हिरासत में लि...\n",
              "2438         HIN2439  ...  सौरभ सेनगुप्ता द्वारा निर्देशित 'इट्स मैन्स वर...\n",
              "2445         HIN2446  ...  भारत के इस समय रैंकिंग में एक सौ सतायिस  अंक ह...\n",
              "500          HIN0501  ...  पीडीपी की अध्यक्ष महबूबा मुफ्ती सोमवार को जम्म...\n",
              "1971         HIN1972  ...  उन्होंने ट्वीट किया, 'विजय माल्या से मिले जवाब...\n",
              "1875         HIN1876  ...  वकील निलेश ने कहा,' राहुल एक ड्रग माफिया है और...\n",
              "1322         HIN1323  ...  उनका पार्थिव शरीर पंच तत्व में विलीन हो गया। <...\n",
              "1427         HIN1428  ...  हम ऐसी दुनिया में हैं जिसमें लोग डिजिटली कहीं ...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "389c3e69-fcc9-478a-f8f6-659b552f087d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1149</th>\n",
              "      <td>डॉक्युमेंट के फॉर्मेट में तैयार की गई इस रिपोर...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980</th>\n",
              "      <td>प्रवर्तन निदेशालय की सलाह पर विदेश मंत्रालय ने...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1482</th>\n",
              "      <td>पाकिस्तान खैबर पख्तूनख्वाह प्रांत की राजधानी प...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1222</th>\n",
              "      <td>इस महिला का शव बुरी हालत में मिला है।&lt;eol&gt;अगस्...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1143</th>\n",
              "      <td>सेमूर ने कहा कि दो हज़ार ग्यारह्  में लादेन पर ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1149  डॉक्युमेंट के फॉर्मेट में तैयार की गई इस रिपोर...      0\n",
              "1980  प्रवर्तन निदेशालय की सलाह पर विदेश मंत्रालय ने...      0\n",
              "1482  पाकिस्तान खैबर पख्तूनख्वाह प्रांत की राजधानी प...      0\n",
              "1222  इस महिला का शव बुरी हालत में मिला है।<eol>अगस्...      0\n",
              "1143  सेमूर ने कहा कि दो हज़ार ग्यारह्  में लादेन पर ...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "b5f1aff8-f9e0-49e3-c317-4a9ab6f1d260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "e15f320351bb4f298ee696605e402d23",
            "47d796d72df747e0a3153a0ce3667115",
            "2d78b34f25534f80bf3e2a1ae44cde25",
            "7382683c4adc4944984404f17da2ab62",
            "6f35f78cc46d4677b696f2c94af49cba",
            "e2b71223569e4b91a0b809d6225c9ccc",
            "88017b19ec8642cf9eb57abf69732814",
            "2d9c10657761469a8d92c9bcfda03ec1"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e15f320351bb4f298ee696605e402d23",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=995526, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "e2f519ac-9727-4404-a649-6041b8d21ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  भारतीय मुस्लिमों की वजह से नहीं पनप सकता आईएस|<eol>भारत में कभी वर्चस्व कायम नहीं कर सकता आईएस|\n",
            "Tokenized:  ['भारतीय', 'म', '##स', '##ल', '##िम', '##ो', 'की', 'व', '##ज', '##ह', 'स', 'न', '##ही', 'प', '##न', '##प', 'सकता', 'आई', '##ए', '##स', '|', '<', 'eo', '##l', '>', 'भारत', 'म', 'कभी', 'व', '##र', '##च', '##स', '##व', 'का', '##यम', 'न', '##ही', 'कर', 'सकता', 'आई', '##ए', '##स', '|']\n",
            "Token IDs:  [18725, 889, 13432, 11714, 50419, 13718, 10826, 895, 17413, 17110, 898, 884, 24667, 885, 11453, 18187, 26886, 44881, 22599, 13432, 196, 133, 13934, 10161, 135, 14311, 889, 50058, 895, 11549, 16940, 13432, 15070, 11081, 87136, 884, 24667, 16192, 26886, 44881, 22599, 13432, 196]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "3e9e53b6-27e8-4ccb-a299-1ecc31585553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  भारतीय मुस्लिमों की वजह से नहीं पनप सकता आईएस|<eol>भारत में कभी वर्चस्व कायम नहीं कर सकता आईएस|\n",
            "Token IDs: [101, 18725, 889, 13432, 11714, 50419, 13718, 10826, 895, 17413, 17110, 898, 884, 24667, 885, 11453, 18187, 26886, 44881, 22599, 13432, 196, 133, 13934, 10161, 135, 14311, 889, 50058, 895, 11549, 16940, 13432, 15070, 11081, 87136, 884, 24667, 16192, 26886, 44881, 22599, 13432, 196, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "e2d2fa86-5de3-4c75-bb26-5838c834ed63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "8de1df59-832b-476e-a947-cf2556b8d605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "41d3a861-0d96-4105-fc6d-7840aed55a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "print(train_inputs)\n",
        "print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "#print(train_masks)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101    881  11549 ...      0      0      0]\n",
            " [   101    882  27391 ...      0      0      0]\n",
            " [   101    899  15552 ...  10826    889  78530]\n",
            " ...\n",
            " [   101    882  27391 ...  14265    886  13432]\n",
            " [   101    882  27391 ...    884 100915  16380]\n",
            " [   101    889  18351 ...      0      0      0]]\n",
            "[1 0 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "ba0d6836-6981-47ec-b3c3-3638287bbb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "37ba2eb2de9f4a0aaeafd383dbc415c5",
            "2ddc38d12d30455a8374b5bcb3814ee8",
            "44e0660a72a5424bb62c9a78726a55a9",
            "f1f13410d7e846dbafc5b87d85049c57",
            "06e8d8a3d25246adb98bf66047d9eb38",
            "f26750316b274ef8837fa05e6cbf7830",
            "d6d59d81501945dca36687f4cb1732c4",
            "c074170834be435f8f6832724b4880d0",
            "c7a91c4a9d964fd7bfbbe4d141b48b25",
            "749c0b678fdb4719a4baef4dd9159861",
            "1db4754056084fdbb2b39208ac150bba",
            "5f197336f1cb4b6791ca638f6caa998b",
            "d42bd679fed64ef2a01a08dc6722bcd9",
            "b067773a6dc3436cb726eb8d7876c241",
            "6d9611871f154e66857615bd118084d0",
            "c607a35f58384749b44f4cc90e91049e"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37ba2eb2de9f4a0aaeafd383dbc415c5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=569, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7a91c4a9d964fd7bfbbe4d141b48b25",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=714314041, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "outputId": "05b43f69-e688-4aa0-934b-785dcd6283f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "outputId": "7b4cebdf-31a8-44f2-8943-457f0096e18d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:10.\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epcoh took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:10.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epcoh took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:10.\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:10.\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epcoh took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "outputId": "e86a5e00-c72b-4693-8f1e-a3690afce8fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task1hindi-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "outputId": "37bb5294-3de5-4f1e-e73b-184b9c6def57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "#print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 900 test sentences...\n",
            "    DONE.\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "outputId": "60f3327d-54b9-4536-b811-f51b82e2f32e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] ):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] ):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    # elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "    #   #print(\" Semi Paraphrase     \",end=\"\")\n",
        "    #   #count_NP+=1      \n",
        "    #   print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "    #   x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(1)\t 1\t [-1.8400308  2.248785 ] \t1\ttrue\n",
            "(0)\t 2\t [ 1.2812768 -1.6922523] \t0\ttrue\n",
            "(1)\t 3\t [-1.8084437  2.1669834] \t1\ttrue\n",
            "(0)\t 4\t [ 1.2765177 -1.6855094] \t0\ttrue\n",
            "(0)\t 5\t [ 1.2858537 -1.6939317] \t0\ttrue\n",
            "(1)\t 6\t [-1.4833971  2.1436317] \t1\ttrue\n",
            "(0)\t 7\t [ 1.2861217 -1.6925069] \t0\ttrue\n",
            "(0)\t 8\t [ 1.2738874 -1.6813436] \t0\ttrue\n",
            "(1)\t 9\t [ 0.03402432 -0.62467885] \t0\tfalse\n",
            "(0)\t 10\t [ 1.2826763 -1.6874285] \t0\ttrue\n",
            "(0)\t 11\t [ 1.1609918 -1.5856249] \t0\ttrue\n",
            "(1)\t 12\t [ 1.0881252 -1.5334195] \t0\tfalse\n",
            "(0)\t 13\t [ 1.282858  -1.6925603] \t0\ttrue\n",
            "(0)\t 14\t [ 1.2831455 -1.6938332] \t0\ttrue\n",
            "(1)\t 15\t [-1.6783828  2.2205546] \t1\ttrue\n",
            "(0)\t 16\t [-1.1486683  1.1017929] \t1\tfalse\n",
            "(0)\t 17\t [-0.13999651 -0.06222849] \t1\tfalse\n",
            "(0)\t 18\t [ 1.2823521 -1.6872134] \t0\ttrue\n",
            "(1)\t 19\t [-1.726871  2.0678  ] \t1\ttrue\n",
            "(1)\t 20\t [-1.8335205  2.253854 ] \t1\ttrue\n",
            "(0)\t 21\t [ 0.7655314 -1.2433482] \t0\ttrue\n",
            "(0)\t 22\t [ 1.2849723 -1.6904335] \t0\ttrue\n",
            "(0)\t 23\t [-0.52108824  0.4342127 ] \t1\tfalse\n",
            "(1)\t 24\t [-1.8863289  2.2846353] \t1\ttrue\n",
            "(0)\t 25\t [ 1.283598  -1.6899492] \t0\ttrue\n",
            "(0)\t 26\t [ 1.2840358 -1.691486 ] \t0\ttrue\n",
            "(1)\t 27\t [-1.6487473  1.8841503] \t1\ttrue\n",
            "(0)\t 28\t [ 1.2709392 -1.686241 ] \t0\ttrue\n",
            "(0)\t 29\t [ 1.2829839 -1.6935581] \t0\ttrue\n",
            "(1)\t 30\t [-1.7058295  2.1956794] \t1\ttrue\n",
            "(1)\t 31\t [-1.6848446  2.181918 ] \t1\ttrue\n",
            "(0)\t 32\t [ 1.2851187 -1.6930693] \t0\ttrue\n",
            "(1)\t 33\t [-1.7000873  2.1784003] \t1\ttrue\n",
            "(0)\t 34\t [-1.0792716  1.5330912] \t1\tfalse\n",
            "(0)\t 35\t [ 1.2826002 -1.6911949] \t0\ttrue\n",
            "(0)\t 36\t [ 1.2835165 -1.6977278] \t0\ttrue\n",
            "(0)\t 37\t [-0.8681917  0.7559213] \t1\tfalse\n",
            "(1)\t 38\t [-1.799853   2.1991334] \t1\ttrue\n",
            "(0)\t 39\t [ 1.2841058 -1.6934754] \t0\ttrue\n",
            "(1)\t 40\t [-1.8443462  2.0979307] \t1\ttrue\n",
            "(0)\t 41\t [ 1.2527109 -1.6762189] \t0\ttrue\n",
            "(1)\t 42\t [-1.7263545  2.1243885] \t1\ttrue\n",
            "(0)\t 43\t [ 1.0663321 -1.5290953] \t0\ttrue\n",
            "(0)\t 44\t [ 1.2578213 -1.6872374] \t0\ttrue\n",
            "(0)\t 45\t [ 1.0015258 -1.4595052] \t0\ttrue\n",
            "(0)\t 46\t [ 1.1791217 -1.6130301] \t0\ttrue\n",
            "(1)\t 47\t [-1.6147822  1.9282587] \t1\ttrue\n",
            "(1)\t 48\t [-1.7383667  2.086767 ] \t1\ttrue\n",
            "(0)\t 49\t [-0.9830452  1.1184026] \t1\tfalse\n",
            "(0)\t 50\t [ 1.2838763 -1.6930393] \t0\ttrue\n",
            "(1)\t 51\t [-1.8444186  2.3623054] \t1\ttrue\n",
            "(0)\t 52\t [ 1.2888855 -1.6974434] \t0\ttrue\n",
            "(1)\t 53\t [-1.2466336  0.9396429] \t1\ttrue\n",
            "(1)\t 54\t [-1.8694648  2.3100097] \t1\ttrue\n",
            "(0)\t 55\t [ 1.2778236 -1.6898576] \t0\ttrue\n",
            "(0)\t 56\t [ 0.83290935 -1.3094748 ] \t0\ttrue\n",
            "(0)\t 57\t [ 1.2834228 -1.6903795] \t0\ttrue\n",
            "(0)\t 58\t [ 1.2742636 -1.6890832] \t0\ttrue\n",
            "(1)\t 59\t [-1.8534855  2.173094 ] \t1\ttrue\n",
            "(0)\t 60\t [ 1.243439  -1.6786638] \t0\ttrue\n",
            "(0)\t 61\t [ 1.282349  -1.6966889] \t0\ttrue\n",
            "(1)\t 62\t [-1.8435124  2.0675225] \t1\ttrue\n",
            "(0)\t 63\t [-1.637662   2.1021972] \t1\tfalse\n",
            "(1)\t 64\t [-1.6632782  1.9686428] \t1\ttrue\n",
            "(0)\t 65\t [ 1.2800552 -1.6912128] \t0\ttrue\n",
            "(0)\t 66\t [ 1.2105601 -1.6473355] \t0\ttrue\n",
            "(1)\t 67\t [-1.6728421  1.9355743] \t1\ttrue\n",
            "(1)\t 68\t [-1.6474773  1.8970201] \t1\ttrue\n",
            "(0)\t 69\t [-1.3534495  1.6385119] \t1\tfalse\n",
            "(0)\t 70\t [ 1.2798411 -1.6899295] \t0\ttrue\n",
            "(1)\t 71\t [-1.6278373  1.9404105] \t1\ttrue\n",
            "(1)\t 72\t [-1.6944929  2.0817568] \t1\ttrue\n",
            "(0)\t 73\t [ 1.2773159 -1.6893547] \t0\ttrue\n",
            "(0)\t 74\t [ 1.2795631 -1.6907952] \t0\ttrue\n",
            "(1)\t 75\t [-1.6961423  2.1432807] \t1\ttrue\n",
            "(0)\t 76\t [ 1.2769797 -1.6953188] \t0\ttrue\n",
            "(1)\t 77\t [-1.8366903  2.1500976] \t1\ttrue\n",
            "(0)\t 78\t [ 0.09305989 -0.62825876] \t0\ttrue\n",
            "(1)\t 79\t [-1.7423275  2.0984454] \t1\ttrue\n",
            "(0)\t 80\t [ 1.2746617 -1.6905365] \t0\ttrue\n",
            "(1)\t 81\t [-1.5480863  1.7980402] \t1\ttrue\n",
            "(0)\t 82\t [ 1.2450279 -1.6465942] \t0\ttrue\n",
            "(0)\t 83\t [ 1.2711989 -1.6877794] \t0\ttrue\n",
            "(1)\t 84\t [-1.8972164  2.193672 ] \t1\ttrue\n",
            "(0)\t 85\t [ 0.6510454 -1.1670396] \t0\ttrue\n",
            "(1)\t 86\t [-0.6515494  0.6775453] \t1\ttrue\n",
            "(1)\t 87\t [-1.7901491  1.9936067] \t1\ttrue\n",
            "(0)\t 88\t [ 1.28688   -1.6975943] \t0\ttrue\n",
            "(1)\t 89\t [-1.046664   1.1318115] \t1\ttrue\n",
            "(0)\t 90\t [-1.7832361  2.1395953] \t1\tfalse\n",
            "(0)\t 91\t [ 1.2865365 -1.6968454] \t0\ttrue\n",
            "(1)\t 92\t [-1.7612102  2.225992 ] \t1\ttrue\n",
            "(1)\t 93\t [-1.7857293  2.1283748] \t1\ttrue\n",
            "(0)\t 94\t [-0.40279433  0.11926281] \t1\tfalse\n",
            "(0)\t 95\t [ 1.1828171 -1.6531155] \t0\ttrue\n",
            "(1)\t 96\t [-1.8450316  2.2743528] \t1\ttrue\n",
            "(0)\t 97\t [ 1.2208558 -1.6524111] \t0\ttrue\n",
            "(1)\t 98\t [-1.8816375  2.2192283] \t1\ttrue\n",
            "(0)\t 99\t [ 1.2600536 -1.662793 ] \t0\ttrue\n",
            "(1)\t 100\t [-0.56054187  0.2744798 ] \t1\ttrue\n",
            "(0)\t 101\t [ 0.13674276 -0.58395433] \t0\ttrue\n",
            "(1)\t 102\t [-0.26052538  0.17869702] \t1\ttrue\n",
            "(0)\t 103\t [-0.26052538  0.17869702] \t1\tfalse\n",
            "(0)\t 104\t [ 1.2803879 -1.6921624] \t0\ttrue\n",
            "(1)\t 105\t [-1.3864309  1.8259792] \t1\ttrue\n",
            "(1)\t 106\t [-1.7434205  2.2441382] \t1\ttrue\n",
            "(0)\t 107\t [ 0.5700661 -1.0702325] \t0\ttrue\n",
            "(0)\t 108\t [ 0.92710847 -1.4335786 ] \t0\ttrue\n",
            "(1)\t 109\t [-1.4654568  1.9682237] \t1\ttrue\n",
            "(0)\t 110\t [ 1.2853829 -1.6942285] \t0\ttrue\n",
            "(1)\t 111\t [-1.7311107  2.1569657] \t1\ttrue\n",
            "(1)\t 112\t [ 0.16346067 -0.35646108] \t0\tfalse\n",
            "(0)\t 113\t [ 0.16346067 -0.35646108] \t0\ttrue\n",
            "(0)\t 114\t [ 1.2761644 -1.691139 ] \t0\ttrue\n",
            "(1)\t 115\t [-1.0857855  1.4033548] \t1\ttrue\n",
            "(1)\t 116\t [-1.6570139  2.1104345] \t1\ttrue\n",
            "(0)\t 117\t [ 1.2840286 -1.6941051] \t0\ttrue\n",
            "(0)\t 118\t [ 1.2854172 -1.6932188] \t0\ttrue\n",
            "(1)\t 119\t [-1.7831115  2.3233695] \t1\ttrue\n",
            "(0)\t 120\t [ 1.2790428 -1.6907583] \t0\ttrue\n",
            "(1)\t 121\t [-1.5283359  1.8311983] \t1\ttrue\n",
            "(1)\t 122\t [-1.5675486  1.2802483] \t1\ttrue\n",
            "(0)\t 123\t [ 1.288318  -1.6958265] \t0\ttrue\n",
            "(0)\t 124\t [ 1.2843103 -1.6929222] \t0\ttrue\n",
            "(1)\t 125\t [-1.7420164  2.089257 ] \t1\ttrue\n",
            "(1)\t 126\t [-1.8813833  2.1463664] \t1\ttrue\n",
            "(0)\t 127\t [ 1.2832713 -1.6948864] \t0\ttrue\n",
            "(0)\t 128\t [ 1.2829    -1.6932845] \t0\ttrue\n",
            "(0)\t 129\t [ 1.283939  -1.6910099] \t0\ttrue\n",
            "(0)\t 130\t [ 1.2841265 -1.6914488] \t0\ttrue\n",
            "(1)\t 131\t [-1.7643341  1.9951459] \t1\ttrue\n",
            "(0)\t 132\t [ 1.245386  -1.6712523] \t0\ttrue\n",
            "(1)\t 133\t [-1.7179211  1.7852103] \t1\ttrue\n",
            "(0)\t 134\t [ 1.2852528 -1.6998333] \t0\ttrue\n",
            "(0)\t 135\t [ 1.2622575 -1.6831287] \t0\ttrue\n",
            "(1)\t 136\t [-1.8051099  2.1923187] \t1\ttrue\n",
            "(1)\t 137\t [-1.8023194  2.0950718] \t1\ttrue\n",
            "(0)\t 138\t [ 1.2803538 -1.6943187] \t0\ttrue\n",
            "(1)\t 139\t [-1.7493402  2.0498636] \t1\ttrue\n",
            "(0)\t 140\t [ 0.20386015 -0.42994478] \t0\ttrue\n",
            "(0)\t 141\t [ 1.277269  -1.6925377] \t0\ttrue\n",
            "(1)\t 142\t [-0.99511     0.80694044] \t1\ttrue\n",
            "(0)\t 143\t [ 0.16481128 -0.49589384] \t0\ttrue\n",
            "(1)\t 144\t [ 1.2832407 -1.6977328] \t0\tfalse\n",
            "(0)\t 145\t [ 1.2807232 -1.6954072] \t0\ttrue\n",
            "(1)\t 146\t [-1.6758349  1.8326792] \t1\ttrue\n",
            "(0)\t 147\t [ 1.2827699 -1.6890451] \t0\ttrue\n",
            "(1)\t 148\t [-1.7356229  2.3091676] \t1\ttrue\n",
            "(1)\t 149\t [-1.6353458  2.1337128] \t1\ttrue\n",
            "(0)\t 150\t [ 1.2848941 -1.6949271] \t0\ttrue\n",
            "(0)\t 151\t [ 1.284556  -1.6913806] \t0\ttrue\n",
            "(1)\t 152\t [-1.7515283  2.215007 ] \t1\ttrue\n",
            "(0)\t 153\t [ 0.7416612 -1.143493 ] \t0\ttrue\n",
            "(0)\t 154\t [ 1.2830125 -1.6910101] \t0\ttrue\n",
            "(1)\t 155\t [-1.54238    2.0725117] \t1\ttrue\n",
            "(0)\t 156\t [ 1.2842125 -1.6904539] \t0\ttrue\n",
            "(0)\t 157\t [ 0.1904463 -0.3200108] \t0\ttrue\n",
            "(1)\t 158\t [ 0.1904463 -0.3200108] \t0\tfalse\n",
            "(1)\t 159\t [-1.774511   2.3071587] \t1\ttrue\n",
            "(0)\t 160\t [-1.5204855  1.9854139] \t1\tfalse\n",
            "(0)\t 161\t [ 1.2802716 -1.6845664] \t0\ttrue\n",
            "(0)\t 162\t [-1.4436053  1.7153659] \t1\tfalse\n",
            "(1)\t 163\t [-1.7571033  2.2195258] \t1\ttrue\n",
            "(1)\t 164\t [-1.8222289  2.1557608] \t1\ttrue\n",
            "(0)\t 165\t [ 1.2848507 -1.6923945] \t0\ttrue\n",
            "(1)\t 166\t [-1.7660173  2.2566972] \t1\ttrue\n",
            "(0)\t 167\t [ 1.2767152 -1.6907462] \t0\ttrue\n",
            "(1)\t 168\t [-1.7778339  2.1480753] \t1\ttrue\n",
            "(0)\t 169\t [ 1.2831887 -1.6919458] \t0\ttrue\n",
            "(0)\t 170\t [ 0.27220783 -0.6649466 ] \t0\ttrue\n",
            "(1)\t 171\t [ 0.27220783 -0.6649466 ] \t0\tfalse\n",
            "(0)\t 172\t [-0.3699095  0.4228145] \t1\tfalse\n",
            "(0)\t 173\t [ 1.2841178 -1.6924577] \t0\ttrue\n",
            "(1)\t 174\t [-1.7329551  2.2481802] \t1\ttrue\n",
            "(0)\t 175\t [ 1.2771254 -1.6903901] \t0\ttrue\n",
            "(0)\t 176\t [ 1.2845408 -1.6917001] \t0\ttrue\n",
            "(1)\t 177\t [-1.7639877  2.2460947] \t1\ttrue\n",
            "(1)\t 178\t [-1.5919656  2.0564191] \t1\ttrue\n",
            "(0)\t 179\t [ 1.2831389 -1.6926621] \t0\ttrue\n",
            "(1)\t 180\t [ 0.8352399 -1.3746915] \t0\tfalse\n",
            "(0)\t 181\t [ 1.2812853 -1.6882911] \t0\ttrue\n",
            "(0)\t 182\t [ 1.2839004 -1.6926472] \t0\ttrue\n",
            "(1)\t 183\t [-1.740755   2.2018127] \t1\ttrue\n",
            "(1)\t 184\t [ 1.1388303 -1.606826 ] \t0\tfalse\n",
            "(0)\t 185\t [-1.2565707  1.6453573] \t1\tfalse\n",
            "(0)\t 186\t [ 0.40210792 -0.89655954] \t0\ttrue\n",
            "(1)\t 187\t [-1.7577723  2.2156272] \t1\ttrue\n",
            "(1)\t 188\t [-1.7843297  2.0476804] \t1\ttrue\n",
            "(0)\t 189\t [ 1.2839438 -1.6928757] \t0\ttrue\n",
            "(0)\t 190\t [ 1.2855397 -1.6923982] \t0\ttrue\n",
            "(1)\t 191\t [-1.7895266  2.2016184] \t1\ttrue\n",
            "(1)\t 192\t [-1.8677435  2.0265925] \t1\ttrue\n",
            "(0)\t 193\t [ 1.256572  -1.6909146] \t0\ttrue\n",
            "(1)\t 194\t [-1.6557243  2.0229342] \t1\ttrue\n",
            "(0)\t 195\t [-0.45809904  0.33968008] \t1\tfalse\n",
            "(1)\t 196\t [-1.7810993  2.132992 ] \t1\ttrue\n",
            "(0)\t 197\t [ 1.2796111 -1.690692 ] \t0\ttrue\n",
            "(1)\t 198\t [-1.7975297  2.275428 ] \t1\ttrue\n",
            "(0)\t 199\t [ 1.219171  -1.6391743] \t0\ttrue\n",
            "(0)\t 200\t [-0.07737329 -0.10338888] \t0\ttrue\n",
            "(0)\t 201\t [ 0.9465071 -1.4306612] \t0\ttrue\n",
            "(1)\t 202\t [-1.855395   2.0473657] \t1\ttrue\n",
            "(1)\t 203\t [ 1.2841223 -1.6900905] \t0\tfalse\n",
            "(0)\t 204\t [ 1.284159  -1.6886661] \t0\ttrue\n",
            "(0)\t 205\t [ 1.2832671 -1.6921645] \t0\ttrue\n",
            "(1)\t 206\t [-1.6196467  2.133896 ] \t1\ttrue\n",
            "(1)\t 207\t [-1.8160557  2.0686753] \t1\ttrue\n",
            "(0)\t 208\t [ 1.1301476 -1.6181693] \t0\ttrue\n",
            "(1)\t 209\t [-1.827022  2.200532] \t1\ttrue\n",
            "(0)\t 210\t [ 1.2832075 -1.6935787] \t0\ttrue\n",
            "(0)\t 211\t [ 1.1213539 -1.549589 ] \t0\ttrue\n",
            "(1)\t 212\t [-1.7578528  2.2351325] \t1\ttrue\n",
            "(1)\t 213\t [-1.9230516  2.235634 ] \t1\ttrue\n",
            "(0)\t 214\t [ 1.2812434 -1.6950693] \t0\ttrue\n",
            "(0)\t 215\t [-0.8376153  0.8293202] \t1\tfalse\n",
            "(1)\t 216\t [-1.5676821  2.015384 ] \t1\ttrue\n",
            "(0)\t 217\t [-1.904668  2.15333 ] \t1\tfalse\n",
            "(1)\t 218\t [-1.7999414  2.2476156] \t1\ttrue\n",
            "(0)\t 219\t [-1.7440448  2.2424793] \t1\tfalse\n",
            "(1)\t 220\t [-1.7650031  2.208078 ] \t1\ttrue\n",
            "(1)\t 221\t [-1.7992111  2.034545 ] \t1\ttrue\n",
            "(0)\t 222\t [ 1.2840737 -1.6947999] \t0\ttrue\n",
            "(1)\t 223\t [-1.6962279  2.16643  ] \t1\ttrue\n",
            "(0)\t 224\t [ 1.2854139 -1.6962472] \t0\ttrue\n",
            "(0)\t 225\t [ 1.2854139 -1.6962472] \t0\ttrue\n",
            "(1)\t 226\t [-1.0710849  1.1868196] \t1\ttrue\n",
            "(0)\t 227\t [-1.5094327  1.8574411] \t1\tfalse\n",
            "(1)\t 228\t [-1.6475239  2.0338123] \t1\ttrue\n",
            "(1)\t 229\t [-0.46635032  0.3308694 ] \t1\ttrue\n",
            "(0)\t 230\t [-0.46635032  0.3308694 ] \t1\tfalse\n",
            "(0)\t 231\t [ 1.2819254 -1.6940616] \t0\ttrue\n",
            "(1)\t 232\t [-1.5528105  1.6416193] \t1\ttrue\n",
            "(1)\t 233\t [-1.4103523  1.7220885] \t1\ttrue\n",
            "(0)\t 234\t [-0.4390139  0.335981 ] \t1\tfalse\n",
            "(1)\t 235\t [-1.506895   1.9052931] \t1\ttrue\n",
            "(0)\t 236\t [ 1.2856482 -1.6954262] \t0\ttrue\n",
            "(0)\t 237\t [ 0.5911354 -1.1455654] \t0\ttrue\n",
            "(1)\t 238\t [-1.5680518  2.0155683] \t1\ttrue\n",
            "(1)\t 239\t [-1.6981032  2.0326924] \t1\ttrue\n",
            "(0)\t 240\t [ 1.2845007 -1.690753 ] \t0\ttrue\n",
            "(0)\t 241\t [ 1.2828511 -1.6885468] \t0\ttrue\n",
            "(1)\t 242\t [-1.7596773  1.95496  ] \t1\ttrue\n",
            "(1)\t 243\t [ 0.9468479 -1.4779717] \t0\tfalse\n",
            "(0)\t 244\t [ 1.2860498 -1.6926469] \t0\ttrue\n",
            "(1)\t 245\t [-1.8162988  2.3314888] \t1\ttrue\n",
            "(0)\t 246\t [-1.3998466  1.8272513] \t1\tfalse\n",
            "(1)\t 247\t [-1.7705678  2.109459 ] \t1\ttrue\n",
            "(0)\t 248\t [ 1.1573796 -1.6263767] \t0\ttrue\n",
            "(0)\t 249\t [-0.57186186  0.53868663] \t1\tfalse\n",
            "(1)\t 250\t [ 0.9553448 -1.4234353] \t0\tfalse\n",
            "(0)\t 251\t [ 1.285457  -1.6941162] \t0\ttrue\n",
            "(1)\t 252\t [-1.7252897  2.178541 ] \t1\ttrue\n",
            "(0)\t 253\t [ 1.0861831 -1.5972584] \t0\ttrue\n",
            "(1)\t 254\t [ 0.69424176 -1.1885599 ] \t0\tfalse\n",
            "(0)\t 255\t [ 1.2791845 -1.6932002] \t0\ttrue\n",
            "(1)\t 256\t [-1.6070355  1.454503 ] \t1\ttrue\n",
            "(0)\t 257\t [ 1.2842423 -1.6906432] \t0\ttrue\n",
            "(1)\t 258\t [-0.6936468   0.75009096] \t1\ttrue\n",
            "(1)\t 259\t [-0.9540945  1.476428 ] \t1\ttrue\n",
            "(0)\t 260\t [ 1.1332659 -1.5929542] \t0\ttrue\n",
            "(1)\t 261\t [-1.7570609  2.1465843] \t1\ttrue\n",
            "(0)\t 262\t [ 1.2561041 -1.6623758] \t0\ttrue\n",
            "(1)\t 263\t [-0.46502265  0.47928447] \t1\ttrue\n",
            "(0)\t 264\t [-0.46502265  0.47928447] \t1\tfalse\n",
            "(1)\t 265\t [ 0.6124004 -1.127823 ] \t0\tfalse\n",
            "(0)\t 266\t [ 1.2835301 -1.6911463] \t0\ttrue\n",
            "(1)\t 267\t [-1.8243006  2.2447064] \t1\ttrue\n",
            "(0)\t 268\t [ 1.2835137 -1.6897289] \t0\ttrue\n",
            "(0)\t 269\t [ 1.2831256 -1.6874563] \t0\ttrue\n",
            "(1)\t 270\t [-1.3712457  1.9472467] \t1\ttrue\n",
            "(0)\t 271\t [ 1.2840213 -1.6982508] \t0\ttrue\n",
            "(1)\t 272\t [-1.5710549  1.9956121] \t1\ttrue\n",
            "(1)\t 273\t [-1.8115268  2.2012146] \t1\ttrue\n",
            "(0)\t 274\t [-0.40010777  0.04254279] \t1\tfalse\n",
            "(0)\t 275\t [ 1.1612817 -1.6105864] \t0\ttrue\n",
            "(1)\t 276\t [-1.7249435  1.9319899] \t1\ttrue\n",
            "(0)\t 277\t [-0.6086983  0.2367461] \t1\tfalse\n",
            "(1)\t 278\t [-1.7648758  2.2074134] \t1\ttrue\n",
            "(1)\t 279\t [-1.2185637  1.6137906] \t1\ttrue\n",
            "(0)\t 280\t [ 1.2834231 -1.6940606] \t0\ttrue\n",
            "(1)\t 281\t [-1.7642432  1.9551111] \t1\ttrue\n",
            "(0)\t 282\t [ 1.283902  -1.6916205] \t0\ttrue\n",
            "(0)\t 283\t [ 1.2819873 -1.6893741] \t0\ttrue\n",
            "(1)\t 284\t [-1.20034    1.5641316] \t1\ttrue\n",
            "(0)\t 285\t [ 1.2820652 -1.6911416] \t0\ttrue\n",
            "(0)\t 286\t [-1.8190111  2.1044652] \t1\tfalse\n",
            "(1)\t 287\t [-1.877864   2.2460678] \t1\ttrue\n",
            "(0)\t 288\t [ 0.14137168 -0.6353122 ] \t0\ttrue\n",
            "(1)\t 289\t [-1.931455  2.250832] \t1\ttrue\n",
            "(1)\t 290\t [-1.8035418  2.264405 ] \t1\ttrue\n",
            "(0)\t 291\t [-0.4089376  -0.01206874] \t1\tfalse\n",
            "(0)\t 292\t [-0.16205937  0.06032225] \t1\tfalse\n",
            "(1)\t 293\t [-0.16205937  0.06032225] \t1\ttrue\n",
            "(0)\t 294\t [ 1.2785866 -1.6843406] \t0\ttrue\n",
            "(1)\t 295\t [-1.7201991  2.214559 ] \t1\ttrue\n",
            "(0)\t 296\t [ 1.2827816 -1.6922015] \t0\ttrue\n",
            "(0)\t 297\t [ 1.2830459 -1.6917058] \t0\ttrue\n",
            "(1)\t 298\t [-1.8045082  2.325945 ] \t1\ttrue\n",
            "(0)\t 299\t [ 1.2802302 -1.6911937] \t0\ttrue\n",
            "(1)\t 300\t [-1.8087854  2.2474988] \t1\ttrue\n",
            "(0)\t 301\t [ 1.2785019 -1.6894368] \t0\ttrue\n",
            "(1)\t 302\t [-1.7771753  1.9522493] \t1\ttrue\n",
            "(1)\t 303\t [-1.8405277  2.2295065] \t1\ttrue\n",
            "(0)\t 304\t [ 1.2833331 -1.6924613] \t0\ttrue\n",
            "(1)\t 305\t [-1.8211654  2.235708 ] \t1\ttrue\n",
            "(0)\t 306\t [-0.5184493   0.17508522] \t1\tfalse\n",
            "(0)\t 307\t [ 1.2720975 -1.6780082] \t0\ttrue\n",
            "(1)\t 308\t [-1.7136769  2.1817496] \t1\ttrue\n",
            "(0)\t 309\t [ 1.278222  -1.6934885] \t0\ttrue\n",
            "(0)\t 310\t [-0.89060545  1.0663632 ] \t1\tfalse\n",
            "(1)\t 311\t [-1.7196382  2.1585383] \t1\ttrue\n",
            "(1)\t 312\t [-1.318594   1.6228875] \t1\ttrue\n",
            "(0)\t 313\t [ 1.182824  -1.6174866] \t0\ttrue\n",
            "(1)\t 314\t [ 1.0740541 -1.5281528] \t0\tfalse\n",
            "(0)\t 315\t [ 1.2846955 -1.693821 ] \t0\ttrue\n",
            "(1)\t 316\t [-1.7432843  2.2671278] \t1\ttrue\n",
            "(0)\t 317\t [ 1.2820295 -1.6892762] \t0\ttrue\n",
            "(0)\t 318\t [ 1.2848481 -1.6921068] \t0\ttrue\n",
            "(1)\t 319\t [-1.8041059  2.2665594] \t1\ttrue\n",
            "(0)\t 320\t [-1.004512   0.5701134] \t1\tfalse\n",
            "(1)\t 321\t [-1.7095762  1.8595287] \t1\ttrue\n",
            "(1)\t 322\t [-1.8416404  2.2115238] \t1\ttrue\n",
            "(0)\t 323\t [ 1.284656  -1.6964239] \t0\ttrue\n",
            "(0)\t 324\t [ 1.2851362 -1.6922214] \t0\ttrue\n",
            "(1)\t 325\t [-1.7519927  2.1007469] \t1\ttrue\n",
            "(0)\t 326\t [ 1.2852858 -1.6935922] \t0\ttrue\n",
            "(0)\t 327\t [-0.18511422  0.11121536] \t1\tfalse\n",
            "(1)\t 328\t [-0.18511422  0.11121536] \t1\ttrue\n",
            "(1)\t 329\t [-1.6806964  1.9021946] \t1\ttrue\n",
            "(0)\t 330\t [ 1.2581264 -1.6916225] \t0\ttrue\n",
            "(0)\t 331\t [ 1.2859378 -1.6916279] \t0\ttrue\n",
            "(1)\t 332\t [-1.5613867  1.816302 ] \t1\ttrue\n",
            "(0)\t 333\t [-1.5320169  1.8795813] \t1\tfalse\n",
            "(1)\t 334\t [-1.7376095  2.0916703] \t1\ttrue\n",
            "(0)\t 335\t [ 0.89051694 -1.3647273 ] \t0\ttrue\n",
            "(1)\t 336\t [-1.7281559  2.064736 ] \t1\ttrue\n",
            "(0)\t 337\t [ 1.2837751 -1.6888858] \t0\ttrue\n",
            "(0)\t 338\t [ 1.2877206 -1.6929641] \t0\ttrue\n",
            "(0)\t 339\t [ 1.2837874 -1.6947881] \t0\ttrue\n",
            "(1)\t 340\t [-1.8301533  2.042329 ] \t1\ttrue\n",
            "(1)\t 341\t [-1.6543652  2.1058385] \t1\ttrue\n",
            "(0)\t 342\t [ 1.2859956 -1.6934742] \t0\ttrue\n",
            "(0)\t 343\t [-0.94877344  1.1281255 ] \t1\tfalse\n",
            "(1)\t 344\t [-1.7934519  1.865656 ] \t1\ttrue\n",
            "(1)\t 345\t [-1.849766   2.3926938] \t1\ttrue\n",
            "(0)\t 346\t [ 1.2696464 -1.6844227] \t0\ttrue\n",
            "(1)\t 347\t [ 0.02134234 -0.4206766 ] \t0\tfalse\n",
            "(0)\t 348\t [ 1.2842145 -1.6908989] \t0\ttrue\n",
            "(0)\t 349\t [ 0.07035144 -0.26496562] \t0\ttrue\n",
            "(1)\t 350\t [ 0.07035144 -0.26496562] \t0\tfalse\n",
            "(1)\t 351\t [ 1.0291066 -1.5105755] \t0\tfalse\n",
            "(0)\t 352\t [ 1.0291066 -1.5105755] \t0\ttrue\n",
            "(0)\t 353\t [ 1.2808138 -1.6857234] \t0\ttrue\n",
            "(1)\t 354\t [-1.6188825  1.7675699] \t1\ttrue\n",
            "(1)\t 355\t [-1.6395906  2.188442 ] \t1\ttrue\n",
            "(0)\t 356\t [ 1.2844769 -1.6922936] \t0\ttrue\n",
            "(0)\t 357\t [ 0.9397215 -1.4369524] \t0\ttrue\n",
            "(1)\t 358\t [-1.8143352  2.167404 ] \t1\ttrue\n",
            "(1)\t 359\t [-1.5875498  1.905615 ] \t1\ttrue\n",
            "(0)\t 360\t [ 1.2867761 -1.6948689] \t0\ttrue\n",
            "(1)\t 361\t [-1.7658656  1.9180572] \t1\ttrue\n",
            "(0)\t 362\t [ 1.2803645 -1.696016 ] \t0\ttrue\n",
            "(1)\t 363\t [-1.8319932  2.1771789] \t1\ttrue\n",
            "(0)\t 364\t [ 1.2806984 -1.6934488] \t0\ttrue\n",
            "(1)\t 365\t [-1.7625124  2.1696584] \t1\ttrue\n",
            "(0)\t 366\t [ 1.2762808 -1.7004632] \t0\ttrue\n",
            "(0)\t 367\t [-1.1810601  1.3751854] \t1\tfalse\n",
            "(1)\t 368\t [-1.6133742  2.046776 ] \t1\ttrue\n",
            "(1)\t 369\t [-1.7587844  2.1698313] \t1\ttrue\n",
            "(0)\t 370\t [ 1.2846056 -1.6927809] \t0\ttrue\n",
            "(0)\t 371\t [ 1.2830364 -1.695176 ] \t0\ttrue\n",
            "(1)\t 372\t [-1.6548322  2.1401088] \t1\ttrue\n",
            "(1)\t 373\t [-1.7408389  2.1583614] \t1\ttrue\n",
            "(0)\t 374\t [ 1.283793  -1.6915072] \t0\ttrue\n",
            "(0)\t 375\t [ 1.2682773 -1.6847341] \t0\ttrue\n",
            "(1)\t 376\t [-1.7744035  2.2199137] \t1\ttrue\n",
            "(1)\t 377\t [-1.1195362  1.6157566] \t1\ttrue\n",
            "(0)\t 378\t [ 1.286885  -1.6965121] \t0\ttrue\n",
            "(1)\t 379\t [ 1.2856158 -1.6971456] \t0\tfalse\n",
            "(0)\t 380\t [ 1.2825676 -1.6942018] \t0\ttrue\n",
            "(0)\t 381\t [ 1.2838907 -1.6903535] \t0\ttrue\n",
            "(1)\t 382\t [ 0.43498445 -0.87381446] \t0\tfalse\n",
            "(0)\t 383\t [ 0.43498445 -0.87381446] \t0\ttrue\n",
            "(0)\t 384\t [ 1.2375878 -1.6693779] \t0\ttrue\n",
            "(0)\t 385\t [ 1.2865937 -1.6937344] \t0\ttrue\n",
            "(0)\t 386\t [ 1.2847272 -1.6934605] \t0\ttrue\n",
            "(1)\t 387\t [-1.5076765  1.899436 ] \t1\ttrue\n",
            "(0)\t 388\t [ 1.2840091 -1.6910189] \t0\ttrue\n",
            "(1)\t 389\t [-0.04887361 -0.14570141] \t0\tfalse\n",
            "(0)\t 390\t [-0.04887361 -0.14570141] \t0\ttrue\n",
            "(0)\t 391\t [ 1.0633582 -1.5253061] \t0\ttrue\n",
            "(0)\t 392\t [-0.09481496 -0.07517783] \t1\tfalse\n",
            "(1)\t 393\t [-1.6372534  1.9591607] \t1\ttrue\n",
            "(0)\t 394\t [-0.23711106  0.09268757] \t1\tfalse\n",
            "(1)\t 395\t [ 1.2729555 -1.7047712] \t0\tfalse\n",
            "(0)\t 396\t [ 1.2838117 -1.6955373] \t0\ttrue\n",
            "(1)\t 397\t [-1.8142017  1.901327 ] \t1\ttrue\n",
            "(0)\t 398\t [-1.459471  1.856076] \t1\tfalse\n",
            "(1)\t 399\t [-1.0627823  1.2279322] \t1\ttrue\n",
            "(0)\t 400\t [ 1.2861406 -1.6968051] \t0\ttrue\n",
            "(0)\t 401\t [ 1.285633  -1.6916646] \t0\ttrue\n",
            "(1)\t 402\t [-1.7818041  2.0338602] \t1\ttrue\n",
            "(1)\t 403\t [-0.9603857  1.1993771] \t1\ttrue\n",
            "(0)\t 404\t [ 0.07154296 -0.4307334 ] \t0\ttrue\n",
            "(0)\t 405\t [ 1.2709664 -1.6905955] \t0\ttrue\n",
            "(1)\t 406\t [-1.8689755  2.368035 ] \t1\ttrue\n",
            "(1)\t 407\t [-1.690297   1.8325297] \t1\ttrue\n",
            "(0)\t 408\t [ 1.2813663 -1.6908236] \t0\ttrue\n",
            "(0)\t 409\t [ 1.2105385 -1.6465741] \t0\ttrue\n",
            "(0)\t 410\t [ 1.2850606 -1.6928777] \t0\ttrue\n",
            "(1)\t 411\t [-1.725529   2.1759546] \t1\ttrue\n",
            "(0)\t 412\t [ 1.2839428 -1.692953 ] \t0\ttrue\n",
            "(0)\t 413\t [ 1.2813216 -1.6910692] \t0\ttrue\n",
            "(1)\t 414\t [-1.7178569  2.2886395] \t1\ttrue\n",
            "(1)\t 415\t [-1.7179674  2.1974819] \t1\ttrue\n",
            "(0)\t 416\t [ 1.2820852 -1.690893 ] \t0\ttrue\n",
            "(1)\t 417\t [-1.7131845  2.1658034] \t1\ttrue\n",
            "(0)\t 418\t [ 1.2831829 -1.6944292] \t0\ttrue\n",
            "(0)\t 419\t [ 1.0467142 -1.5437901] \t0\ttrue\n",
            "(1)\t 420\t [-1.2320007  1.4694141] \t1\ttrue\n",
            "(1)\t 421\t [-1.8076527  2.1649547] \t1\ttrue\n",
            "(0)\t 422\t [ 0.17649892 -0.70888656] \t0\ttrue\n",
            "(1)\t 423\t [-1.0023679  1.2194811] \t1\ttrue\n",
            "(0)\t 424\t [ 1.0324012 -1.533306 ] \t0\ttrue\n",
            "(0)\t 425\t [ 1.1644754 -1.6532596] \t0\ttrue\n",
            "(1)\t 426\t [-1.2633444  1.256327 ] \t1\ttrue\n",
            "(0)\t 427\t [ 0.479383  -1.0590225] \t0\ttrue\n",
            "(1)\t 428\t [-1.8400253  2.0482724] \t1\ttrue\n",
            "(0)\t 429\t [ 1.267878  -1.6990843] \t0\ttrue\n",
            "(1)\t 430\t [-0.432447    0.34999964] \t1\ttrue\n",
            "(0)\t 431\t [ 0.54732585 -1.0897887 ] \t0\ttrue\n",
            "(0)\t 432\t [ 0.9187176 -1.4182788] \t0\ttrue\n",
            "(1)\t 433\t [-1.5572582  1.9638302] \t1\ttrue\n",
            "(1)\t 434\t [-0.57771593  0.46973887] \t1\ttrue\n",
            "(0)\t 435\t [ 0.7610793 -1.2465843] \t0\ttrue\n",
            "(0)\t 436\t [ 1.2840335 -1.6921344] \t0\ttrue\n",
            "(0)\t 437\t [ 1.2778934 -1.6930627] \t0\ttrue\n",
            "(1)\t 438\t [-0.35666686  0.1659053 ] \t1\ttrue\n",
            "(0)\t 439\t [-1.4629613  1.6272422] \t1\tfalse\n",
            "(1)\t 440\t [-1.8941275  2.3123846] \t1\ttrue\n",
            "(1)\t 441\t [-1.6563331  2.0863588] \t1\ttrue\n",
            "(0)\t 442\t [ 1.2855793 -1.6938783] \t0\ttrue\n",
            "(0)\t 443\t [ 1.2807671 -1.696229 ] \t0\ttrue\n",
            "(1)\t 444\t [-1.6571177  1.6688104] \t1\ttrue\n",
            "(0)\t 445\t [ 0.20304018 -0.4494082 ] \t0\ttrue\n",
            "(0)\t 446\t [ 1.2844197 -1.6902093] \t0\ttrue\n",
            "(0)\t 447\t [ 1.1582415 -1.6067678] \t0\ttrue\n",
            "(0)\t 448\t [ 0.96409965 -1.4709274 ] \t0\ttrue\n",
            "(0)\t 449\t [ 1.2853048 -1.6925187] \t0\ttrue\n",
            "(1)\t 450\t [-1.8243345  2.160966 ] \t1\ttrue\n",
            "(1)\t 451\t [ 0.2776505 -0.4507377] \t0\tfalse\n",
            "(0)\t 452\t [ 0.2776505 -0.4507377] \t0\ttrue\n",
            "(1)\t 453\t [-1.9760648  2.2005794] \t1\ttrue\n",
            "(0)\t 454\t [ 1.1996658 -1.6194698] \t0\ttrue\n",
            "(0)\t 455\t [ 0.9083797 -1.4426756] \t0\ttrue\n",
            "(0)\t 456\t [ 1.2806851 -1.6916842] \t0\ttrue\n",
            "(1)\t 457\t [-1.716187  2.114157] \t1\ttrue\n",
            "(1)\t 458\t [-1.5628952  1.9167576] \t1\ttrue\n",
            "(0)\t 459\t [ 1.284773  -1.6972662] \t0\ttrue\n",
            "(0)\t 460\t [ 1.2447699 -1.676592 ] \t0\ttrue\n",
            "(1)\t 461\t [-1.5791105  1.807451 ] \t1\ttrue\n",
            "(0)\t 462\t [ 1.2851316 -1.6932789] \t0\ttrue\n",
            "(0)\t 463\t [ 1.2820295 -1.6967864] \t0\ttrue\n",
            "(1)\t 464\t [-1.8473929  2.130817 ] \t1\ttrue\n",
            "(0)\t 465\t [ 1.2839037 -1.6944448] \t0\ttrue\n",
            "(1)\t 466\t [-1.7609042  2.1444244] \t1\ttrue\n",
            "(0)\t 467\t [ 1.0551144 -1.4638507] \t0\ttrue\n",
            "(0)\t 468\t [ 1.2247618 -1.6450081] \t0\ttrue\n",
            "(0)\t 469\t [ 1.2783273 -1.6910725] \t0\ttrue\n",
            "(0)\t 470\t [-0.58511573  0.6422998 ] \t1\tfalse\n",
            "(1)\t 471\t [-1.7695451  2.1671765] \t1\ttrue\n",
            "(0)\t 472\t [ 1.2856369 -1.6945109] \t0\ttrue\n",
            "(1)\t 473\t [-1.5209179  1.9819208] \t1\ttrue\n",
            "(0)\t 474\t [ 1.2853843 -1.6950914] \t0\ttrue\n",
            "(1)\t 475\t [-1.6395892  1.5392765] \t1\ttrue\n",
            "(0)\t 476\t [ 1.285534  -1.6944216] \t0\ttrue\n",
            "(0)\t 477\t [ 1.2836304 -1.6922457] \t0\ttrue\n",
            "(1)\t 478\t [-1.2206502  1.0736495] \t1\ttrue\n",
            "(0)\t 479\t [ 1.2816634 -1.6907399] \t0\ttrue\n",
            "(1)\t 480\t [-1.8631155  2.2307813] \t1\ttrue\n",
            "(1)\t 481\t [-1.5018839  1.831019 ] \t1\ttrue\n",
            "(0)\t 482\t [-1.5321995  1.8720757] \t1\tfalse\n",
            "(1)\t 483\t [-1.5042789  1.9568863] \t1\ttrue\n",
            "(0)\t 484\t [ 1.2794021 -1.6920034] \t0\ttrue\n",
            "(1)\t 485\t [-1.6082324  1.9566253] \t1\ttrue\n",
            "(0)\t 486\t [ 1.2863594 -1.6932479] \t0\ttrue\n",
            "(0)\t 487\t [ 1.2863966 -1.6958734] \t0\ttrue\n",
            "(1)\t 488\t [-1.8262224  2.2534542] \t1\ttrue\n",
            "(0)\t 489\t [ 0.07540883 -0.5708377 ] \t0\ttrue\n",
            "(1)\t 490\t [ 0.6466645 -1.2061176] \t0\tfalse\n",
            "(1)\t 491\t [-1.7332286  2.1388967] \t1\ttrue\n",
            "(0)\t 492\t [-0.52368855  0.51059616] \t1\tfalse\n",
            "(0)\t 493\t [ 1.281833  -1.6906046] \t0\ttrue\n",
            "(1)\t 494\t [-1.6267325  2.0247545] \t1\ttrue\n",
            "(0)\t 495\t [ 1.2777278 -1.6960129] \t0\ttrue\n",
            "(0)\t 496\t [-1.2991058  1.4820118] \t1\tfalse\n",
            "(1)\t 497\t [-1.6217953  2.1172361] \t1\ttrue\n",
            "(1)\t 498\t [-1.7023189  2.038008 ] \t1\ttrue\n",
            "(0)\t 499\t [ 1.2788138 -1.6876689] \t0\ttrue\n",
            "(1)\t 500\t [-1.703751   1.8734509] \t1\ttrue\n",
            "(0)\t 501\t [ 1.2753019 -1.6907803] \t0\ttrue\n",
            "(1)\t 502\t [-1.8071569  2.1078103] \t1\ttrue\n",
            "(0)\t 503\t [ 1.282214  -1.6951791] \t0\ttrue\n",
            "(0)\t 504\t [-0.04879064 -0.11756647] \t0\ttrue\n",
            "(1)\t 505\t [-0.04879064 -0.11756647] \t0\tfalse\n",
            "(0)\t 506\t [ 1.2874987 -1.6963776] \t0\ttrue\n",
            "(0)\t 507\t [ 1.2728497 -1.6795583] \t0\ttrue\n",
            "(1)\t 508\t [-1.7976097  2.250758 ] \t1\ttrue\n",
            "(0)\t 509\t [ 0.99927866 -1.4885029 ] \t0\ttrue\n",
            "(0)\t 510\t [ 1.2846481 -1.695743 ] \t0\ttrue\n",
            "(0)\t 511\t [ 1.284953  -1.6911836] \t0\ttrue\n",
            "(0)\t 512\t [ 1.2705007 -1.6875379] \t0\ttrue\n",
            "(1)\t 513\t [ 1.2598449 -1.6744808] \t0\tfalse\n",
            "(0)\t 514\t [ 0.00066244 -0.19150546] \t0\ttrue\n",
            "(0)\t 515\t [ 1.1158916 -1.5643169] \t0\ttrue\n",
            "(0)\t 516\t [ 1.2859751 -1.6944739] \t0\ttrue\n",
            "(1)\t 517\t [-1.8236082  2.2577178] \t1\ttrue\n",
            "(1)\t 518\t [-1.8805043  2.2725542] \t1\ttrue\n",
            "(0)\t 519\t [ 1.2650577 -1.6825877] \t0\ttrue\n",
            "(0)\t 520\t [-1.698908   2.0712786] \t1\tfalse\n",
            "(1)\t 521\t [-1.8922132  2.0906684] \t1\ttrue\n",
            "(0)\t 522\t [ 1.2092872 -1.6478306] \t0\ttrue\n",
            "(1)\t 523\t [-1.2160394  1.3650628] \t1\ttrue\n",
            "(0)\t 524\t [ 1.2801714 -1.6910174] \t0\ttrue\n",
            "(1)\t 525\t [-1.8186783  2.0908558] \t1\ttrue\n",
            "(0)\t 526\t [ 1.284115  -1.6968827] \t0\ttrue\n",
            "(0)\t 527\t [-0.9124148  0.5135181] \t1\tfalse\n",
            "(1)\t 528\t [-1.682753   1.6571792] \t1\ttrue\n",
            "(0)\t 529\t [ 0.26145634 -0.68529433] \t0\ttrue\n",
            "(1)\t 530\t [-1.9003073  2.2811892] \t1\ttrue\n",
            "(0)\t 531\t [-1.2467523  1.6926962] \t1\tfalse\n",
            "(1)\t 532\t [-1.3331459  1.8296148] \t1\ttrue\n",
            "(0)\t 533\t [ 1.2804471 -1.695681 ] \t0\ttrue\n",
            "(1)\t 534\t [-1.7687961  2.1249201] \t1\ttrue\n",
            "(0)\t 535\t [ 1.2817465 -1.698351 ] \t0\ttrue\n",
            "(0)\t 536\t [ 1.2846911 -1.6952329] \t0\ttrue\n",
            "(1)\t 537\t [-1.5987633  1.8492576] \t1\ttrue\n",
            "(1)\t 538\t [ 0.49651524 -0.98505044] \t0\tfalse\n",
            "(0)\t 539\t [ 1.1218148 -1.5995411] \t0\ttrue\n",
            "(0)\t 540\t [ 1.2740042 -1.6873324] \t0\ttrue\n",
            "(1)\t 541\t [-1.633562   1.8713828] \t1\ttrue\n",
            "(0)\t 542\t [ 1.2847602 -1.6967593] \t0\ttrue\n",
            "(1)\t 543\t [-1.5861613  1.8487905] \t1\ttrue\n",
            "(0)\t 544\t [ 1.2859336 -1.6940045] \t0\ttrue\n",
            "(0)\t 545\t [ 0.12231407 -0.35650215] \t0\ttrue\n",
            "(0)\t 546\t [-0.9664865  0.9219078] \t1\tfalse\n",
            "(1)\t 547\t [-1.922119  2.19569 ] \t1\ttrue\n",
            "(1)\t 548\t [ 0.7341579 -1.2715708] \t0\tfalse\n",
            "(0)\t 549\t [ 0.7341579 -1.2715708] \t0\ttrue\n",
            "(0)\t 550\t [ 1.2804022 -1.6932553] \t0\ttrue\n",
            "(1)\t 551\t [-1.7920429  2.1585681] \t1\ttrue\n",
            "(0)\t 552\t [ 1.2784737 -1.6938684] \t0\ttrue\n",
            "(1)\t 553\t [-0.4285457   0.05141085] \t1\ttrue\n",
            "(1)\t 554\t [-1.7776041  2.1190405] \t1\ttrue\n",
            "(0)\t 555\t [ 1.2873544 -1.695333 ] \t0\ttrue\n",
            "(0)\t 556\t [ 0.7870785 -1.2760499] \t0\ttrue\n",
            "(1)\t 557\t [-1.7908739  2.0968869] \t1\ttrue\n",
            "(0)\t 558\t [ 1.2828482 -1.6945099] \t0\ttrue\n",
            "(0)\t 559\t [ 1.2838532 -1.6912676] \t0\ttrue\n",
            "(1)\t 560\t [-1.8171698  2.3170898] \t1\ttrue\n",
            "(1)\t 561\t [-1.6284314  1.8519584] \t1\ttrue\n",
            "(0)\t 562\t [-0.8010415  0.8523845] \t1\tfalse\n",
            "(0)\t 563\t [ 0.39260384 -0.7771863 ] \t0\ttrue\n",
            "(1)\t 564\t [ 0.39260384 -0.7771863 ] \t0\tfalse\n",
            "(0)\t 565\t [ 1.2669104 -1.696529 ] \t0\ttrue\n",
            "(1)\t 566\t [-1.8160659  1.9353676] \t1\ttrue\n",
            "(1)\t 567\t [-1.8306944  2.2142208] \t1\ttrue\n",
            "(0)\t 568\t [ 1.285453  -1.6963063] \t0\ttrue\n",
            "(0)\t 569\t [ 1.2846247 -1.6927137] \t0\ttrue\n",
            "(1)\t 570\t [-1.7433262  1.8722245] \t1\ttrue\n",
            "(0)\t 571\t [ 1.260903  -1.6774944] \t0\ttrue\n",
            "(1)\t 572\t [-1.6269753  2.0580342] \t1\ttrue\n",
            "(0)\t 573\t [ 1.2837342 -1.6912314] \t0\ttrue\n",
            "(1)\t 574\t [-1.7996039  2.2133539] \t1\ttrue\n",
            "(1)\t 575\t [-1.7002314  1.9623252] \t1\ttrue\n",
            "(0)\t 576\t [ 1.2810625 -1.6912911] \t0\ttrue\n",
            "(1)\t 577\t [-1.6927775  2.1038525] \t1\ttrue\n",
            "(0)\t 578\t [-1.6927775  2.1038525] \t1\tfalse\n",
            "(1)\t 579\t [-1.4421053  1.8245153] \t1\ttrue\n",
            "(0)\t 580\t [ 1.2843772 -1.6929477] \t0\ttrue\n",
            "(0)\t 581\t [ 0.17277552 -0.3868481 ] \t0\ttrue\n",
            "(1)\t 582\t [ 0.17277552 -0.3868481 ] \t0\tfalse\n",
            "(0)\t 583\t [ 0.1802344  -0.38346943] \t0\ttrue\n",
            "(1)\t 584\t [ 0.1802344  -0.38346943] \t0\tfalse\n",
            "(1)\t 585\t [-1.6353033  2.1495388] \t1\ttrue\n",
            "(0)\t 586\t [ 1.2165747 -1.6330625] \t0\ttrue\n",
            "(1)\t 587\t [-1.7927283  2.1235182] \t1\ttrue\n",
            "(0)\t 588\t [-0.5426234  -0.04369069] \t1\tfalse\n",
            "(1)\t 589\t [-1.8161861  2.2414265] \t1\ttrue\n",
            "(0)\t 590\t [-1.8161861  2.2414265] \t1\tfalse\n",
            "(0)\t 591\t [-1.5820287  1.9307276] \t1\tfalse\n",
            "(1)\t 592\t [-1.5960369  2.048045 ] \t1\ttrue\n",
            "(0)\t 593\t [ 1.2836857 -1.694223 ] \t0\ttrue\n",
            "(1)\t 594\t [-1.4957143  1.8758216] \t1\ttrue\n",
            "(1)\t 595\t [-1.5567956  2.0108225] \t1\ttrue\n",
            "(0)\t 596\t [-0.23391849 -0.11254518] \t1\tfalse\n",
            "(1)\t 597\t [-1.4371301  1.6534234] \t1\ttrue\n",
            "(0)\t 598\t [ 1.2631446 -1.6992332] \t0\ttrue\n",
            "(1)\t 599\t [-1.5782338  1.9695189] \t1\ttrue\n",
            "(0)\t 600\t [ 1.2818774 -1.6940016] \t0\ttrue\n",
            "(0)\t 601\t [ 0.52807367 -1.0550414 ] \t0\ttrue\n",
            "(1)\t 602\t [-1.8007616  2.2505157] \t1\ttrue\n",
            "(0)\t 603\t [ 0.93753415 -1.423305  ] \t0\ttrue\n",
            "(1)\t 604\t [-0.9230668  1.2331233] \t1\ttrue\n",
            "(1)\t 605\t [-1.5624884  1.9470261] \t1\ttrue\n",
            "(0)\t 606\t [ 1.2827975 -1.6902782] \t0\ttrue\n",
            "(1)\t 607\t [-1.3344706  1.8054742] \t1\ttrue\n",
            "(0)\t 608\t [ 1.1801051 -1.6397686] \t0\ttrue\n",
            "(1)\t 609\t [-1.6137013  1.8197023] \t1\ttrue\n",
            "(0)\t 610\t [ 1.2836158 -1.6919388] \t0\ttrue\n",
            "(0)\t 611\t [ 1.2820195 -1.6922965] \t0\ttrue\n",
            "(0)\t 612\t [ 1.2335607 -1.6432593] \t0\ttrue\n",
            "(0)\t 613\t [ 1.2798209 -1.6965436] \t0\ttrue\n",
            "(1)\t 614\t [-1.8008397  2.2412071] \t1\ttrue\n",
            "(0)\t 615\t [ 1.2846683 -1.6906972] \t0\ttrue\n",
            "(1)\t 616\t [-0.93811107  1.0707974 ] \t1\ttrue\n",
            "(0)\t 617\t [ 1.2515525 -1.6860182] \t0\ttrue\n",
            "(0)\t 618\t [ 1.2835969 -1.6892769] \t0\ttrue\n",
            "(0)\t 619\t [-0.7843236   0.66398597] \t1\tfalse\n",
            "(1)\t 620\t [-1.4705753  1.9074329] \t1\ttrue\n",
            "(0)\t 621\t [ 1.2809387 -1.6915125] \t0\ttrue\n",
            "(1)\t 622\t [-1.7849507  2.2099564] \t1\ttrue\n",
            "(1)\t 623\t [-1.7985511  2.2945764] \t1\ttrue\n",
            "(0)\t 624\t [ 1.0405737 -1.4983587] \t0\ttrue\n",
            "(0)\t 625\t [-0.22370924 -0.21102296] \t1\tfalse\n",
            "(0)\t 626\t [ 1.2862524 -1.6936479] \t0\ttrue\n",
            "(1)\t 627\t [-1.8522499  2.1582036] \t1\ttrue\n",
            "(1)\t 628\t [-1.6023777  2.1257644] \t1\ttrue\n",
            "(0)\t 629\t [ 1.2839333 -1.6934873] \t0\ttrue\n",
            "(1)\t 630\t [-1.8271613  2.3494475] \t1\ttrue\n",
            "(0)\t 631\t [ 1.2832011 -1.6893653] \t0\ttrue\n",
            "(0)\t 632\t [ 1.282288  -1.6932026] \t0\ttrue\n",
            "(1)\t 633\t [-1.0798086  1.5874337] \t1\ttrue\n",
            "(0)\t 634\t [-0.25995532  0.11964873] \t1\tfalse\n",
            "(1)\t 635\t [-0.70302004  0.69417775] \t1\ttrue\n",
            "(0)\t 636\t [ 1.2812042 -1.6873016] \t0\ttrue\n",
            "(1)\t 637\t [-1.3593854  1.6962615] \t1\ttrue\n",
            "(1)\t 638\t [-1.8529379  2.1609   ] \t1\ttrue\n",
            "(0)\t 639\t [ 1.284433  -1.6929219] \t0\ttrue\n",
            "(0)\t 640\t [ 1.2472547 -1.6779257] \t0\ttrue\n",
            "(1)\t 641\t [-1.536916  1.779981] \t1\ttrue\n",
            "(0)\t 642\t [-0.08295275 -0.01901758] \t1\tfalse\n",
            "(1)\t 643\t [-0.08295275 -0.01901758] \t1\ttrue\n",
            "(1)\t 644\t [-1.7571093  2.1181118] \t1\ttrue\n",
            "(0)\t 645\t [ 1.2701885 -1.6935464] \t0\ttrue\n",
            "(0)\t 646\t [-0.7137895  0.9796055] \t1\tfalse\n",
            "(1)\t 647\t [-0.67312515  0.84058475] \t1\ttrue\n",
            "(0)\t 648\t [-1.0936052  1.2570679] \t1\tfalse\n",
            "(1)\t 649\t [-1.8813887  2.3026407] \t1\ttrue\n",
            "(1)\t 650\t [-1.6608509  2.308653 ] \t1\ttrue\n",
            "(0)\t 651\t [ 1.2867316 -1.6922535] \t0\ttrue\n",
            "(0)\t 652\t [ 1.2814418 -1.6894224] \t0\ttrue\n",
            "(1)\t 653\t [-1.7667443  2.11648  ] \t1\ttrue\n",
            "(1)\t 654\t [-1.7632971  2.1508014] \t1\ttrue\n",
            "(0)\t 655\t [ 1.2825626 -1.6939979] \t0\ttrue\n",
            "(1)\t 656\t [-1.7901906  2.2484896] \t1\ttrue\n",
            "(0)\t 657\t [ 1.0776647 -1.5451573] \t0\ttrue\n",
            "(1)\t 658\t [ 1.2826679 -1.6952862] \t0\tfalse\n",
            "(0)\t 659\t [ 1.2840521 -1.6938297] \t0\ttrue\n",
            "(1)\t 660\t [ 1.2848314 -1.6954862] \t0\tfalse\n",
            "(0)\t 661\t [ 1.2449108 -1.6792036] \t0\ttrue\n",
            "(0)\t 662\t [ 1.2829541 -1.6929344] \t0\ttrue\n",
            "(1)\t 663\t [-0.5480117   0.23663235] \t1\ttrue\n",
            "(1)\t 664\t [-1.6467003  2.1303828] \t1\ttrue\n",
            "(0)\t 665\t [-1.1380172  1.1056696] \t1\tfalse\n",
            "(0)\t 666\t [ 1.259969  -1.6697624] \t0\ttrue\n",
            "(1)\t 667\t [-1.7146502  2.2979486] \t1\ttrue\n",
            "(1)\t 668\t [-1.7191231  2.135361 ] \t1\ttrue\n",
            "(0)\t 669\t [ 1.2523669 -1.6805099] \t0\ttrue\n",
            "(0)\t 670\t [ 1.283432  -1.6871041] \t0\ttrue\n",
            "(0)\t 671\t [ 1.2848574 -1.6907803] \t0\ttrue\n",
            "(1)\t 672\t [-1.773473   2.2846265] \t1\ttrue\n",
            "(0)\t 673\t [-0.6277683   0.44279277] \t1\tfalse\n",
            "(1)\t 674\t [-1.7798871  2.1782203] \t1\ttrue\n",
            "(0)\t 675\t [ 1.2863444 -1.6926452] \t0\ttrue\n",
            "(1)\t 676\t [-1.8156933  2.3216546] \t1\ttrue\n",
            "(0)\t 677\t [-0.34041533  0.2100954 ] \t1\tfalse\n",
            "(0)\t 678\t [ 1.2826132 -1.6930248] \t0\ttrue\n",
            "(0)\t 679\t [ 1.282384  -1.6911577] \t0\ttrue\n",
            "(1)\t 680\t [-1.7647091  2.2293425] \t1\ttrue\n",
            "(0)\t 681\t [ 1.2851549 -1.691228 ] \t0\ttrue\n",
            "(1)\t 682\t [-1.7167537  2.1535451] \t1\ttrue\n",
            "(0)\t 683\t [ 0.84363353 -1.3040936 ] \t0\ttrue\n",
            "(0)\t 684\t [ 1.2833661 -1.6946474] \t0\ttrue\n",
            "(1)\t 685\t [-1.1773437   0.96455806] \t1\ttrue\n",
            "(0)\t 686\t [ 1.2839383 -1.6903812] \t0\ttrue\n",
            "(1)\t 687\t [-1.3204993  1.4507471] \t1\ttrue\n",
            "(0)\t 688\t [ 1.2463696 -1.6769357] \t0\ttrue\n",
            "(0)\t 689\t [ 1.0691524 -1.5685128] \t0\ttrue\n",
            "(1)\t 690\t [-1.4274689  1.7738723] \t1\ttrue\n",
            "(1)\t 691\t [-0.7585356  0.6227622] \t1\ttrue\n",
            "(0)\t 692\t [ 1.277364 -1.699454] \t0\ttrue\n",
            "(1)\t 693\t [-1.6609317  2.0821245] \t1\ttrue\n",
            "(0)\t 694\t [ 1.2780037 -1.689894 ] \t0\ttrue\n",
            "(0)\t 695\t [ 1.2540709 -1.6734905] \t0\ttrue\n",
            "(1)\t 696\t [-1.8299853  2.1594236] \t1\ttrue\n",
            "(1)\t 697\t [-0.9737389  1.4600259] \t1\ttrue\n",
            "(0)\t 698\t [ 1.2825985 -1.6917175] \t0\ttrue\n",
            "(0)\t 699\t [-0.15141155 -0.08932588] \t1\tfalse\n",
            "(0)\t 700\t [-1.7679011  2.0457804] \t1\tfalse\n",
            "(1)\t 701\t [ 1.2836984 -1.694493 ] \t0\tfalse\n",
            "(0)\t 702\t [ 1.284856  -1.6905333] \t0\ttrue\n",
            "(1)\t 703\t [-1.8384069  2.0315335] \t1\ttrue\n",
            "(0)\t 704\t [ 1.2790114 -1.6912674] \t0\ttrue\n",
            "(0)\t 705\t [ 1.2824948 -1.6969658] \t0\ttrue\n",
            "(1)\t 706\t [-1.5481366  1.9837664] \t1\ttrue\n",
            "(0)\t 707\t [ 1.2862073 -1.6980141] \t0\ttrue\n",
            "(1)\t 708\t [-1.8869843  2.1921132] \t1\ttrue\n",
            "(1)\t 709\t [-1.5024806  1.9586812] \t1\ttrue\n",
            "(0)\t 710\t [ 1.2837583 -1.6925266] \t0\ttrue\n",
            "(1)\t 711\t [-1.8615284  2.219566 ] \t1\ttrue\n",
            "(0)\t 712\t [ 1.2835425 -1.6939257] \t0\ttrue\n",
            "(0)\t 713\t [ 0.03332653 -0.43328726] \t0\ttrue\n",
            "(1)\t 714\t [-1.2663807  1.4856454] \t1\ttrue\n",
            "(0)\t 715\t [ 1.2833904 -1.6883861] \t0\ttrue\n",
            "(1)\t 716\t [-1.6716549  1.8829001] \t1\ttrue\n",
            "(1)\t 717\t [-1.0664192  1.5410529] \t1\ttrue\n",
            "(0)\t 718\t [ 0.833898  -1.3472292] \t0\ttrue\n",
            "(0)\t 719\t [ 1.279868  -1.6903802] \t0\ttrue\n",
            "(1)\t 720\t [-1.7012502  1.9122422] \t1\ttrue\n",
            "(0)\t 721\t [-1.2011656  1.4008716] \t1\tfalse\n",
            "(0)\t 722\t [ 1.2763888 -1.6901582] \t0\ttrue\n",
            "(1)\t 723\t [-1.9133722  2.2169235] \t1\ttrue\n",
            "(0)\t 724\t [ 1.2731684 -1.6905204] \t0\ttrue\n",
            "(1)\t 725\t [-1.2211084  1.6326667] \t1\ttrue\n",
            "(0)\t 726\t [ 1.270453  -1.6811229] \t0\ttrue\n",
            "(1)\t 727\t [-1.539454  2.051504] \t1\ttrue\n",
            "(0)\t 728\t [-1.539454  2.051504] \t1\tfalse\n",
            "(0)\t 729\t [-1.8057793  2.1048872] \t1\tfalse\n",
            "(1)\t 730\t [-1.2412184  1.4877241] \t1\ttrue\n",
            "(0)\t 731\t [ 1.2061961 -1.64494  ] \t0\ttrue\n",
            "(1)\t 732\t [-1.7414194  2.0956213] \t1\ttrue\n",
            "(1)\t 733\t [-1.8285702  2.2781074] \t1\ttrue\n",
            "(0)\t 734\t [-1.8027356  2.1938236] \t1\tfalse\n",
            "(0)\t 735\t [ 1.28556   -1.6945575] \t0\ttrue\n",
            "(0)\t 736\t [ 1.2765065 -1.6872025] \t0\ttrue\n",
            "(1)\t 737\t [-1.7969613  2.2595088] \t1\ttrue\n",
            "(0)\t 738\t [ 1.2845535 -1.692287 ] \t0\ttrue\n",
            "(0)\t 739\t [ 0.8540571 -1.3415416] \t0\ttrue\n",
            "(1)\t 740\t [-1.0744274  1.3029801] \t1\ttrue\n",
            "(0)\t 741\t [-1.479495   1.9040151] \t1\tfalse\n",
            "(1)\t 742\t [-0.0455951  -0.14565429] \t0\tfalse\n",
            "(0)\t 743\t [ 1.2754424 -1.6879085] \t0\ttrue\n",
            "(1)\t 744\t [-1.8379427  2.2585452] \t1\ttrue\n",
            "(1)\t 745\t [-1.6068119  2.0858576] \t1\ttrue\n",
            "(0)\t 746\t [-0.78083545  0.78383476] \t1\tfalse\n",
            "(0)\t 747\t [ 1.2861859 -1.695082 ] \t0\ttrue\n",
            "(1)\t 748\t [-1.7852472  2.2370658] \t1\ttrue\n",
            "(1)\t 749\t [-1.7277638  1.993236 ] \t1\ttrue\n",
            "(0)\t 750\t [-1.6675972  1.7229671] \t1\tfalse\n",
            "(1)\t 751\t [-1.6001061  1.7780368] \t1\ttrue\n",
            "(0)\t 752\t [ 1.2841734 -1.6924778] \t0\ttrue\n",
            "(0)\t 753\t [ 0.8001243 -1.292006 ] \t0\ttrue\n",
            "(0)\t 754\t [ 1.2828773 -1.6946012] \t0\ttrue\n",
            "(1)\t 755\t [-1.7087835  2.1824467] \t1\ttrue\n",
            "(0)\t 756\t [-1.267014   1.9745286] \t1\tfalse\n",
            "(1)\t 757\t [-1.6610699  2.2371624] \t1\ttrue\n",
            "(0)\t 758\t [ 1.284938  -1.6917564] \t0\ttrue\n",
            "(0)\t 759\t [ 1.0231857 -1.4285053] \t0\ttrue\n",
            "(0)\t 760\t [ 1.2808306 -1.691628 ] \t0\ttrue\n",
            "(1)\t 761\t [-1.7847841  2.0883868] \t1\ttrue\n",
            "(0)\t 762\t [ 1.2865711 -1.6935985] \t0\ttrue\n",
            "(1)\t 763\t [-1.755066   2.0688393] \t1\ttrue\n",
            "(0)\t 764\t [ 1.2733079 -1.683332 ] \t0\ttrue\n",
            "(0)\t 765\t [ 0.03373086 -0.26206818] \t0\ttrue\n",
            "(0)\t 766\t [ 1.277505  -1.6881348] \t0\ttrue\n",
            "(1)\t 767\t [-1.8429238  2.2444365] \t1\ttrue\n",
            "(0)\t 768\t [-0.17346837 -0.03014379] \t1\tfalse\n",
            "(1)\t 769\t [-1.7583345  2.1154451] \t1\ttrue\n",
            "(1)\t 770\t [-1.6512526  1.7513646] \t1\ttrue\n",
            "(0)\t 771\t [-1.7783426  2.1757019] \t1\tfalse\n",
            "(0)\t 772\t [ 1.286286  -1.6941085] \t0\ttrue\n",
            "(1)\t 773\t [-1.662506   2.1048446] \t1\ttrue\n",
            "(0)\t 774\t [-1.7738633  2.2544687] \t1\tfalse\n",
            "(0)\t 775\t [ 1.2867817 -1.6928339] \t0\ttrue\n",
            "(1)\t 776\t [-1.7013506  2.265458 ] \t1\ttrue\n",
            "(1)\t 777\t [-1.6528336  2.1228278] \t1\ttrue\n",
            "(1)\t 778\t [-1.7031733  2.0737755] \t1\ttrue\n",
            "(0)\t 779\t [ 1.2729241 -1.7075552] \t0\ttrue\n",
            "(1)\t 780\t [-1.6672816  2.1833274] \t1\ttrue\n",
            "(0)\t 781\t [ 1.2829175 -1.6920061] \t0\ttrue\n",
            "(0)\t 782\t [ 1.2857817 -1.692177 ] \t0\ttrue\n",
            "(1)\t 783\t [-1.6544662  2.067868 ] \t1\ttrue\n",
            "(0)\t 784\t [-0.7909      0.63180214] \t1\tfalse\n",
            "(1)\t 785\t [-1.5794209  1.9074354] \t1\ttrue\n",
            "(0)\t 786\t [ 1.28186   -1.6958117] \t0\ttrue\n",
            "(1)\t 787\t [-1.8160968  2.0464065] \t1\ttrue\n",
            "(0)\t 788\t [ 1.2809228 -1.6897405] \t0\ttrue\n",
            "(0)\t 789\t [ 1.285999 -1.693426] \t0\ttrue\n",
            "(1)\t 790\t [-1.8884315  2.2247326] \t1\ttrue\n",
            "(0)\t 791\t [ 1.2507365 -1.676948 ] \t0\ttrue\n",
            "(1)\t 792\t [-1.5723296  1.9542458] \t1\ttrue\n",
            "(1)\t 793\t [ 1.1502056 -1.5678164] \t0\tfalse\n",
            "(0)\t 794\t [ 1.1502056 -1.5678164] \t0\ttrue\n",
            "(0)\t 795\t [ 1.2866241 -1.6935005] \t0\ttrue\n",
            "(1)\t 796\t [-0.21555795 -0.19049972] \t1\ttrue\n",
            "(1)\t 797\t [-1.7611923  2.0272672] \t1\ttrue\n",
            "(0)\t 798\t [ 0.6923457 -1.1885301] \t0\ttrue\n",
            "(0)\t 799\t [ 0.16208099 -0.5749716 ] \t0\ttrue\n",
            "(1)\t 800\t [ 0.16208099 -0.5749716 ] \t0\tfalse\n",
            "(0)\t 801\t [ 0.16208099 -0.5749716 ] \t0\ttrue\n",
            "(1)\t 802\t [ 0.16208099 -0.5749716 ] \t0\tfalse\n",
            "(1)\t 803\t [-1.236411   1.6195655] \t1\ttrue\n",
            "(0)\t 804\t [-0.34414244  0.19100969] \t1\tfalse\n",
            "(0)\t 805\t [ 1.2767577 -1.6931957] \t0\ttrue\n",
            "(1)\t 806\t [ 1.2755474 -1.6940722] \t0\tfalse\n",
            "(0)\t 807\t [ 1.2821196 -1.6952232] \t0\ttrue\n",
            "(1)\t 808\t [-1.7742358  2.2190073] \t1\ttrue\n",
            "(0)\t 809\t [ 1.2859192 -1.692461 ] \t0\ttrue\n",
            "(1)\t 810\t [-1.8427088  1.9887376] \t1\ttrue\n",
            "(0)\t 811\t [ 1.1860069 -1.6492822] \t0\ttrue\n",
            "(0)\t 812\t [ 1.2835052 -1.6891671] \t0\ttrue\n",
            "(1)\t 813\t [-1.7810022  2.275807 ] \t1\ttrue\n",
            "(0)\t 814\t [ 0.98967886 -1.4916496 ] \t0\ttrue\n",
            "(1)\t 815\t [-1.684423   2.1112769] \t1\ttrue\n",
            "(0)\t 816\t [ 1.2846161 -1.6935263] \t0\ttrue\n",
            "(1)\t 817\t [ 1.2401804 -1.675586 ] \t0\tfalse\n",
            "(0)\t 818\t [ 1.2845687 -1.693096 ] \t0\ttrue\n",
            "(1)\t 819\t [-1.6294193  2.0913022] \t1\ttrue\n",
            "(0)\t 820\t [-0.598828    0.37637508] \t1\tfalse\n",
            "(0)\t 821\t [ 1.1986324 -1.6615468] \t0\ttrue\n",
            "(0)\t 822\t [ 1.2828214 -1.690131 ] \t0\ttrue\n",
            "(0)\t 823\t [-1.5344335  2.0152905] \t1\tfalse\n",
            "(0)\t 824\t [ 1.2639073 -1.7092001] \t0\ttrue\n",
            "(1)\t 825\t [-1.6360236  1.918415 ] \t1\ttrue\n",
            "(0)\t 826\t [ 1.2738196 -1.6957933] \t0\ttrue\n",
            "(1)\t 827\t [-1.7579024  2.1601226] \t1\ttrue\n",
            "(0)\t 828\t [ 1.2854563 -1.6935161] \t0\ttrue\n",
            "(1)\t 829\t [-1.722478   2.2005742] \t1\ttrue\n",
            "(1)\t 830\t [-1.7240337  2.0977364] \t1\ttrue\n",
            "(0)\t 831\t [ 1.2828208 -1.6946703] \t0\ttrue\n",
            "(1)\t 832\t [-1.6864592  1.9495999] \t1\ttrue\n",
            "(0)\t 833\t [ 0.21353287 -0.6982112 ] \t0\ttrue\n",
            "(0)\t 834\t [ 1.2877048 -1.7012714] \t0\ttrue\n",
            "(1)\t 835\t [-1.5822002  2.005176 ] \t1\ttrue\n",
            "(1)\t 836\t [-1.882347  2.156652] \t1\ttrue\n",
            "(0)\t 837\t [ 1.2767318 -1.6879131] \t0\ttrue\n",
            "(0)\t 838\t [ 1.0477848 -1.5137528] \t0\ttrue\n",
            "(0)\t 839\t [ 1.2838591 -1.6930019] \t0\ttrue\n",
            "(1)\t 840\t [-1.6492379  2.2242227] \t1\ttrue\n",
            "(1)\t 841\t [-1.6419431  2.2205875] \t1\ttrue\n",
            "(0)\t 842\t [ 1.2822291 -1.6924629] \t0\ttrue\n",
            "(0)\t 843\t [ 1.2540405 -1.673103 ] \t0\ttrue\n",
            "(1)\t 844\t [-1.6075999  2.0526588] \t1\ttrue\n",
            "(0)\t 845\t [ 0.89818823 -1.3674223 ] \t0\ttrue\n",
            "(1)\t 846\t [ 0.12558082 -0.3068751 ] \t0\tfalse\n",
            "(0)\t 847\t [ 1.2182165 -1.6533631] \t0\ttrue\n",
            "(1)\t 848\t [-0.5631121  0.5891361] \t1\ttrue\n",
            "(1)\t 849\t [-0.75191736  1.0185682 ] \t1\ttrue\n",
            "(0)\t 850\t [-0.7042916   0.58089423] \t1\tfalse\n",
            "(1)\t 851\t [-0.40488526  0.11788034] \t1\ttrue\n",
            "(0)\t 852\t [ 1.0490808 -1.4739233] \t0\ttrue\n",
            "(0)\t 853\t [ 1.2755044 -1.6987885] \t0\ttrue\n",
            "(1)\t 854\t [-1.8382852  2.1065648] \t1\ttrue\n",
            "(0)\t 855\t [ 1.2830545 -1.6956512] \t0\ttrue\n",
            "(0)\t 856\t [-0.46046335  0.2745041 ] \t1\tfalse\n",
            "(1)\t 857\t [-0.90316546  0.95797163] \t1\ttrue\n",
            "(0)\t 858\t [ 1.2783839 -1.6857704] \t0\ttrue\n",
            "(1)\t 859\t [ 0.2220815  -0.34880233] \t0\tfalse\n",
            "(0)\t 860\t [ 0.2220815  -0.34880233] \t0\ttrue\n",
            "(0)\t 861\t [ 0.19751902 -0.3780013 ] \t0\ttrue\n",
            "(1)\t 862\t [ 0.19751902 -0.3780013 ] \t0\tfalse\n",
            "(0)\t 863\t [ 1.2834424 -1.6927638] \t0\ttrue\n",
            "(0)\t 864\t [ 1.2704264 -1.6832919] \t0\ttrue\n",
            "(1)\t 865\t [-1.8056532  2.117356 ] \t1\ttrue\n",
            "(0)\t 866\t [-0.22127807  0.10859259] \t1\tfalse\n",
            "(1)\t 867\t [ 1.2769479 -1.6972102] \t0\tfalse\n",
            "(0)\t 868\t [-0.8000135  0.8697548] \t1\tfalse\n",
            "(0)\t 869\t [ 1.2823509 -1.6945933] \t0\ttrue\n",
            "(0)\t 870\t [ 1.2837007 -1.6959946] \t0\ttrue\n",
            "(1)\t 871\t [-1.5132214  1.8843518] \t1\ttrue\n",
            "(0)\t 872\t [ 1.2803304 -1.6921171] \t0\ttrue\n",
            "(0)\t 873\t [ 1.2823944 -1.6932641] \t0\ttrue\n",
            "(1)\t 874\t [-1.7909677  2.2873013] \t1\ttrue\n",
            "(1)\t 875\t [-1.7866317  2.3053539] \t1\ttrue\n",
            "(0)\t 876\t [-1.7375871  2.0507526] \t1\tfalse\n",
            "(0)\t 877\t [-0.145661   -0.13229883] \t1\tfalse\n",
            "(1)\t 878\t [-1.8123931  2.1395805] \t1\ttrue\n",
            "(1)\t 879\t [ 1.2845064 -1.6892506] \t0\tfalse\n",
            "(0)\t 880\t [ 1.2806269 -1.6881485] \t0\ttrue\n",
            "(0)\t 881\t [ 1.2862921 -1.692977 ] \t0\ttrue\n",
            "(1)\t 882\t [-1.2300609  1.8385495] \t1\ttrue\n",
            "(1)\t 883\t [ 0.532246  -0.9500353] \t0\tfalse\n",
            "(0)\t 884\t [ 0.532246  -0.9500353] \t0\ttrue\n",
            "(0)\t 885\t [ 1.0904778 -1.5866332] \t0\ttrue\n",
            "(1)\t 886\t [-1.8753941  2.214256 ] \t1\ttrue\n",
            "(0)\t 887\t [ 0.08522395 -0.5109041 ] \t0\ttrue\n",
            "(1)\t 888\t [-1.748186   2.1981413] \t1\ttrue\n",
            "(0)\t 889\t [ 1.282634  -1.6922915] \t0\ttrue\n",
            "(1)\t 890\t [-1.2393554  1.803114 ] \t1\ttrue\n",
            "(0)\t 891\t [ 0.23064908 -0.41517994] \t0\ttrue\n",
            "(1)\t 892\t [ 0.23064908 -0.41517994] \t0\tfalse\n",
            "(1)\t 893\t [ 1.1962646 -1.6298388] \t0\tfalse\n",
            "(0)\t 894\t [ 1.2863349 -1.6948261] \t0\ttrue\n",
            "(1)\t 895\t [-1.5516587  1.9608239] \t1\ttrue\n",
            "(0)\t 896\t [ 1.2866706 -1.6948469] \t0\ttrue\n",
            "(0)\t 897\t [ 1.2809876 -1.6908767] \t0\ttrue\n",
            "(1)\t 898\t [ 0.48513278 -0.9937899 ] \t0\tfalse\n",
            "(0)\t 899\t [ 1.281401  -1.6906866] \t0\ttrue\n",
            "(1)\t 900\t [-1.6750282  1.9418687] \t1\ttrue\n",
            "Number of true predictions: 765\n",
            "Number of false predictions: 135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIgsWWCTKdY",
        "colab_type": "code",
        "outputId": "74d8c0e9-9dc9-4993-9b9f-5430923bf4cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 84.90566037735849 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print('True positives: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xnx5SIDGsaA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d142161-3a1e-4e72-8ec7-d6e48256ee09"
      },
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "scores = [3.0, 1.0, 0.2]\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    predictions[i][j]=softmax(predictions[i][j])\n",
        "    print(predictions[i][j])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01648283 0.98351717]\n",
            "[0.95136386 0.04863617]\n",
            "[0.01842541 0.9815746 ]\n",
            "[0.9508289  0.04917115]\n",
            "[0.95165247 0.0483475 ]\n",
            "[0.02590611 0.97409385]\n",
            "[0.95159924 0.04840075]\n",
            "[0.9505101  0.04948986]\n",
            "[0.658969 0.341031]\n",
            "[0.9512052  0.04879487]\n",
            "[0.939722   0.06027801]\n",
            "[0.93223536 0.06776463]\n",
            "[0.9514512  0.04854883]\n",
            "[0.95152324 0.0484768 ]\n",
            "[0.01986098 0.980139  ]\n",
            "[0.0953097 0.9046903]\n",
            "[0.48056778 0.51943225]\n",
            "[0.95118016 0.0488199 ]\n",
            "[0.02199562 0.9780044 ]\n",
            "[0.01650621 0.9834938 ]\n",
            "[0.88172626 0.11827376]\n",
            "[0.9514506  0.04854941]\n",
            "[0.27782002 0.72218   ]\n",
            "[0.01520268 0.9847973 ]\n",
            "[0.95136464 0.04863534]\n",
            "[0.9514559  0.04854405]\n",
            "[0.02839055 0.9716095 ]\n",
            "[0.95060176 0.04939825]\n",
            "[0.95150304 0.04849695]\n",
            "[0.01981099 0.9801891 ]\n",
            "[0.02049708 0.9795029 ]\n",
            "[0.9515789  0.04842105]\n",
            "[0.020263 0.979737]\n",
            "[0.06834698 0.93165296]\n",
            "[0.95137614 0.04862387]\n",
            "[0.95171964 0.04828042]\n",
            "[0.16463841 0.83536166]\n",
            "[0.01800412 0.9819958 ]\n",
            "[0.95155096 0.04844902]\n",
            "[0.01903464 0.98096544]\n",
            "[0.9492582  0.05074185]\n",
            "[0.02082119 0.9791788 ]\n",
            "[0.9305668  0.06943329]\n",
            "[0.95002943 0.04997056]\n",
            "[0.92136437 0.0786356 ]\n",
            "[0.94225025 0.05774975]\n",
            "[0.02811209 0.97188795]\n",
            "[0.02134976 0.9786502 ]\n",
            "[0.10895618 0.8910438 ]\n",
            "[0.9515203  0.04847971]\n",
            "[0.01467647 0.98532355]\n",
            "[0.9519527  0.04804733]\n",
            "[0.10098965 0.89901036]\n",
            "[0.01507579 0.9849242 ]\n",
            "[0.9510926  0.04890748]\n",
            "[0.894955   0.10504505]\n",
            "[0.9513765  0.04862353]\n",
            "[0.95089054 0.04910947]\n",
            "[0.01752271 0.98247725]\n",
            "[0.9489283  0.05107169]\n",
            "[0.95161813 0.04838192]\n",
            "[0.01962684 0.98037314]\n",
            "[0.02320613 0.9767939 ]\n",
            "[0.02578294 0.97421706]\n",
            "[0.9512591 0.0487409]\n",
            "[0.9457254  0.05427462]\n",
            "[0.02637996 0.97362006]\n",
            "[0.02807232 0.9719277 ]\n",
            "[0.04779035 0.95220965]\n",
            "[0.95118964 0.04881038]\n",
            "[0.02743152 0.97256845]\n",
            "[0.0223954  0.97760457]\n",
            "[0.95104545 0.0489545 ]\n",
            "[0.95121694 0.04878309]\n",
            "[0.02105323 0.9789468 ]\n",
            "[0.9513069  0.04869314]\n",
            "[0.01822107 0.981779  ]\n",
            "[0.67289734 0.32710266]\n",
            "[0.02102543 0.9789746 ]\n",
            "[0.9509769 0.0490231]\n",
            "[0.03402223 0.9659778 ]\n",
            "[0.9474308  0.05256927]\n",
            "[0.9506861  0.04931389]\n",
            "[0.01644926 0.9835507 ]\n",
            "[0.8603362  0.13966382]\n",
            "[0.20930915 0.7906908 ]\n",
            "[0.02223165 0.97776836]\n",
            "[0.95186776 0.04813223]\n",
            "[0.10170013 0.8982999 ]\n",
            "[0.01940114 0.9805988 ]\n",
            "[0.9518177 0.0481823]\n",
            "[0.01821365 0.9817863 ]\n",
            "[0.01956788 0.98043215]\n",
            "[0.37237135 0.6276287 ]\n",
            "[0.944587   0.05541304]\n",
            "[0.01599454 0.98400545]\n",
            "[0.94650894 0.05349101]\n",
            "[0.01628862 0.9837114 ]\n",
            "[0.9489644  0.05103565]\n",
            "[0.30258432 0.6974157 ]\n",
            "[0.67276055 0.3272395 ]\n",
            "[0.3919263 0.6080737]\n",
            "[0.3919263 0.6080737]\n",
            "[0.95131856 0.04868148]\n",
            "[0.03870137 0.96129864]\n",
            "[0.01820728 0.98179275]\n",
            "[0.83757555 0.16242443]\n",
            "[0.91378    0.08622005]\n",
            "[0.03125929 0.96874076]\n",
            "[0.9516445  0.04835551]\n",
            "[0.02007351 0.9799265 ]\n",
            "[0.6271295  0.37287053]\n",
            "[0.6271295  0.37287053]\n",
            "[0.9510749  0.04892505]\n",
            "[0.076623   0.92337704]\n",
            "[0.02258891 0.9774111 ]\n",
            "[0.9515765  0.04842355]\n",
            "[0.95159954 0.04840041]\n",
            "[0.01619889 0.98380107]\n",
            "[0.95119107 0.04880895]\n",
            "[0.03358434 0.9664157 ]\n",
            "[0.05479531 0.9452046 ]\n",
            "[0.9518527  0.04814732]\n",
            "[0.9515349 0.0484651]\n",
            "[0.02122185 0.9787781 ]\n",
            "[0.01750258 0.9824974 ]\n",
            "[0.95157754 0.04842244]\n",
            "[0.9514865  0.04851346]\n",
            "[0.9514295  0.04857052]\n",
            "[0.9514584  0.04854157]\n",
            "[0.02276551 0.9772345 ]\n",
            "[0.9486628  0.05133717]\n",
            "[0.02922327 0.97077674]\n",
            "[0.9518958 0.0481042]\n",
            "[0.95004493 0.04995503]\n",
            "[0.01803169 0.98196834]\n",
            "[0.0198911 0.9801089]\n",
            "[0.95141673 0.0485833 ]\n",
            "[0.02189832 0.9781017 ]\n",
            "[0.6533517 0.3466483]\n",
            "[0.95119125 0.04880869]\n",
            "[0.14160167 0.8583984 ]\n",
            "[0.6594187  0.34058124]\n",
            "[0.95170707 0.04829286]\n",
            "[0.9514841  0.04851594]\n",
            "[0.02907095 0.9709291 ]\n",
            "[0.95128447 0.04871554]\n",
            "[0.01721194 0.982788  ]\n",
            "[0.02255338 0.9774466 ]\n",
            "[0.9516542  0.04834586]\n",
            "[0.9514751  0.04852489]\n",
            "[0.01858692 0.9814131 ]\n",
            "[0.86820203 0.13179797]\n",
            "[0.95138663 0.04861334]\n",
            "[0.02621416 0.9737858 ]\n",
            "[0.95141643 0.04858357]\n",
            "[0.62491363 0.37508637]\n",
            "[0.62491363 0.37508637]\n",
            "[0.01659908 0.983401  ]\n",
            "[0.02914484 0.9708552 ]\n",
            "[0.95096004 0.04903989]\n",
            "[0.04073923 0.95926076]\n",
            "[0.01840368 0.9815963 ]\n",
            "[0.01837913 0.98162085]\n",
            "[0.9515355  0.04846451]\n",
            "[0.01758937 0.98241067]\n",
            "[0.95108235 0.0489177 ]\n",
            "[0.01934267 0.9806573 ]\n",
            "[0.95143807 0.04856195]\n",
            "[0.71852446 0.28147548]\n",
            "[0.71852446 0.28147548]\n",
            "[0.31158406 0.6884159 ]\n",
            "[0.9515046  0.04849541]\n",
            "[0.01832246 0.9816775 ]\n",
            "[0.9510848  0.04891518]\n",
            "[0.95148915 0.04851085]\n",
            "[0.01780899 0.9821911 ]\n",
            "[0.02537262 0.9746274 ]\n",
            "[0.9514688  0.04853116]\n",
            "[0.9011378  0.09886219]\n",
            "[0.9511806  0.04881939]\n",
            "[0.9515033  0.04849668]\n",
            "[0.0190292 0.9809708]\n",
            "[0.9396676  0.06033244]\n",
            "[0.05205834 0.94794166]\n",
            "[0.7856106  0.21438938]\n",
            "[0.01846212 0.9815378 ]\n",
            "[0.02120656 0.97879344]\n",
            "[0.9515159  0.04848415]\n",
            "[0.9515674  0.04843257]\n",
            "[0.01814328 0.98185676]\n",
            "[0.01995075 0.98004925]\n",
            "[0.9501446  0.04985544]\n",
            "[0.02463464 0.97536534]\n",
            "[0.3105008 0.6894992]\n",
            "[0.01956812 0.9804318 ]\n",
            "[0.9512143  0.04878566]\n",
            "[0.01674188 0.9832582 ]\n",
            "[0.94574845 0.05425153]\n",
            "[0.5065035  0.49349645]\n",
            "[0.91506964 0.08493039]\n",
            "[0.0197867  0.98021334]\n",
            "[0.95139545 0.04860454]\n",
            "[0.95133126 0.04866876]\n",
            "[0.9514518  0.04854821]\n",
            "[0.02289797 0.9771021 ]\n",
            "[0.02013943 0.9798606 ]\n",
            "[0.93981826 0.06018179]\n",
            "[0.01750594 0.98249406]\n",
            "[0.9515143  0.04848569]\n",
            "[0.93529016 0.06470989]\n",
            "[0.01811053 0.9818895 ]\n",
            "[0.01538761 0.9846124 ]\n",
            "[0.9514925  0.04850753]\n",
            "[0.15883319 0.8411668 ]\n",
            "[0.02703894 0.9729611 ]\n",
            "[0.01698994 0.9830101 ]\n",
            "[0.0171652 0.9828348]\n",
            "[0.01822578 0.9817742 ]\n",
            "[0.01846789 0.9815321 ]\n",
            "[0.02117035 0.9788296 ]\n",
            "[0.95161057 0.04838946]\n",
            "[0.02057965 0.97942036]\n",
            "[0.9517387  0.04826127]\n",
            "[0.9517387  0.04826127]\n",
            "[0.0946698 0.9053301]\n",
            "[0.03334694 0.9666531 ]\n",
            "[0.02457038 0.97542965]\n",
            "[0.31062058 0.68937945]\n",
            "[0.31062058 0.68937945]\n",
            "[0.95147747 0.04852257]\n",
            "[0.03937588 0.96062416]\n",
            "[0.04178876 0.9582113 ]\n",
            "[0.3153996 0.6846004]\n",
            "[0.03191672 0.96808326]\n",
            "[0.9517117  0.04828823]\n",
            "[0.85026747 0.14973247]\n",
            "[0.02702437 0.9729757 ]\n",
            "[0.02341247 0.9765876 ]\n",
            "[0.9514436  0.04855644]\n",
            "[0.95126516 0.04873488]\n",
            "[0.02378478 0.97621524]\n",
            "[0.91870046 0.08129957]\n",
            "[0.95160234 0.04839761]\n",
            "[0.0155536  0.98444647]\n",
            "[0.03815861 0.96184134]\n",
            "[0.02023247 0.97976756]\n",
            "[0.94179165 0.05820829]\n",
            "[0.24776864 0.75223136]\n",
            "[0.9151948 0.0848052]\n",
            "[0.95164275 0.04835726]\n",
            "[0.01976595 0.9802341 ]\n",
            "[0.9360425  0.06395753]\n",
            "[0.8679326 0.1320674]\n",
            "[0.9513109  0.04868915]\n",
            "[0.04472192 0.95527804]\n",
            "[0.95142657 0.04857345]\n",
            "[0.19096722 0.8090328 ]\n",
            "[0.08087462 0.9191254 ]\n",
            "[0.9385562  0.06144378]\n",
            "[0.01976955 0.9802304 ]\n",
            "[0.9487524  0.05124756]\n",
            "[0.28003117 0.71996886]\n",
            "[0.28003117 0.71996886]\n",
            "[0.85071546 0.14928456]\n",
            "[0.95141685 0.04858311]\n",
            "[0.01680705 0.9831929 ]\n",
            "[0.95135057 0.04864943]\n",
            "[0.9512273  0.04877272]\n",
            "[0.03494221 0.96505773]\n",
            "[0.9517668  0.04823321]\n",
            "[0.02747372 0.97252625]\n",
            "[0.01776254 0.9822374 ]\n",
            "[0.39110956 0.6088904 ]\n",
            "[0.94113654 0.05886343]\n",
            "[0.02516207 0.9748379 ]\n",
            "[0.30038938 0.69961065]\n",
            "[0.01848225 0.98151773]\n",
            "[0.05560064 0.9443993 ]\n",
            "[0.95154643 0.04845351]\n",
            "[0.0236755  0.97632444]\n",
            "[0.951456   0.04854402]\n",
            "[0.9512634  0.04873657]\n",
            "[0.05927454 0.94072545]\n",
            "[0.95134884 0.04865108]\n",
            "[0.01938888 0.9806111 ]\n",
            "[0.01592312 0.9840769 ]\n",
            "[0.684965   0.31503505]\n",
            "[0.01503408 0.9849659 ]\n",
            "[0.01682458 0.9831754 ]\n",
            "[0.4020649  0.59793514]\n",
            "[0.44463256 0.5553674 ]\n",
            "[0.44463256 0.5553674 ]\n",
            "[0.95087093 0.04912907]\n",
            "[0.01917554 0.9808245 ]\n",
            "[0.9514311  0.04856893]\n",
            "[0.9514204  0.04857963]\n",
            "[0.01582126 0.9841787 ]\n",
            "[0.95126635 0.04873367]\n",
            "[0.01701859 0.9829814 ]\n",
            "[0.9511045 0.0488955]\n",
            "[0.02344384 0.9765562 ]\n",
            "[0.01679008 0.98320985]\n",
            "[0.9514685  0.04853146]\n",
            "[0.01700873 0.98299134]\n",
            "[0.33324727 0.6667527 ]\n",
            "[0.95026845 0.04973152]\n",
            "[0.01992944 0.98007053]\n",
            "[0.9512796  0.04872039]\n",
            "[0.12379549 0.87620455]\n",
            "[0.02026917 0.9797308 ]\n",
            "[0.05014066 0.9498594 ]\n",
            "[0.94269264 0.0573074 ]\n",
            "[0.9310035  0.06899653]\n",
            "[0.95159405 0.04840591]\n",
            "[0.01780322 0.9821968 ]\n",
            "[0.9512608  0.04873914]\n",
            "[0.95152205 0.0484779 ]\n",
            "[0.01677967 0.98322034]\n",
            "[0.17155801 0.828442  ]\n",
            "[0.02740866 0.97259134]\n",
            "[0.01707086 0.9829291 ]\n",
            "[0.9517121  0.04828797]\n",
            "[0.95154065 0.04845932]\n",
            "[0.02078052 0.9792195 ]\n",
            "[0.9516107  0.04838926]\n",
            "[0.426455   0.57354504]\n",
            "[0.426455   0.57354504]\n",
            "[0.02704355 0.9729565 ]\n",
            "[0.9502516  0.04974837]\n",
            "[0.95155025 0.04844973]\n",
            "[0.03300007 0.9669999 ]\n",
            "[0.03193495 0.9680651 ]\n",
            "[0.0212633  0.97873664]\n",
            "[0.90510195 0.09489807]\n",
            "[0.02203392 0.9779661 ]\n",
            "[0.9513236  0.04867635]\n",
            "[0.95169383 0.04830613]\n",
            "[0.95159674 0.0484032 ]\n",
            "[0.02038256 0.9796174 ]\n",
            "[0.02274941 0.97725064]\n",
            "[0.951638   0.04836203]\n",
            "[0.11136247 0.88863754]\n",
            "[0.02510879 0.9748912 ]\n",
            "[0.01416856 0.9858314 ]\n",
            "[0.9504555  0.04954455]\n",
            "[0.60874003 0.39126   ]\n",
            "[0.9514371  0.04856292]\n",
            "[0.5830526  0.41694748]\n",
            "[0.5830526  0.41694748]\n",
            "[0.92687726 0.07312271]\n",
            "[0.92687726 0.07312271]\n",
            "[0.9510393  0.04896071]\n",
            "[0.03272155 0.9672784 ]\n",
            "[0.02128928 0.9787107 ]\n",
            "[0.95151365 0.04848641]\n",
            "[0.9150312  0.08496881]\n",
            "[0.0183116  0.98168844]\n",
            "[0.02950734 0.9704927 ]\n",
            "[0.951738   0.04826201]\n",
            "[0.02450847 0.9754916 ]\n",
            "[0.9514956  0.04850441]\n",
            "[0.01782492 0.98217505]\n",
            "[0.9513925  0.04860757]\n",
            "[0.01922426 0.9807757 ]\n",
            "[0.95151234 0.04848763]\n",
            "[0.07200804 0.9279919 ]\n",
            "[0.02508328 0.97491664]\n",
            "[0.01929141 0.98070866]\n",
            "[0.951542 0.048458]\n",
            "[0.95158005 0.04841993]\n",
            "[0.02198981 0.97801024]\n",
            "[0.01985586 0.98014414]\n",
            "[0.95144576 0.04855428]\n",
            "[0.95040566 0.04959437]\n",
            "[0.01808686 0.98191315]\n",
            "[0.06092264 0.9390774 ]\n",
            "[0.9518184 0.0481816]\n",
            "[0.95178926 0.04821076]\n",
            "[0.9515135  0.04848645]\n",
            "[0.9513969 0.0486031]\n",
            "[0.7873121  0.21268791]\n",
            "[0.7873121  0.21268791]\n",
            "[0.94818974 0.0518103 ]\n",
            "[0.9516775  0.04832254]\n",
            "[0.9515789  0.04842107]\n",
            "[0.03207392 0.9679261 ]\n",
            "[0.9514331  0.04856686]\n",
            "[0.5241881  0.47581196]\n",
            "[0.5241881  0.47581196]\n",
            "[0.93012846 0.06987154]\n",
            "[0.49509087 0.50490916]\n",
            "[0.02668999 0.97331005]\n",
            "[0.41828963 0.5817104 ]\n",
            "[0.9515577  0.04844231]\n",
            "[0.9516324  0.04836758]\n",
            "[0.02376409 0.9762359 ]\n",
            "[0.03504167 0.96495837]\n",
            "[0.09189491 0.9081051 ]\n",
            "[0.95179766 0.0482023 ]\n",
            "[0.9515379 0.0484621]\n",
            "[0.02154852 0.9784515 ]\n",
            "[0.10342243 0.89657754]\n",
            "[0.6229941  0.37700585]\n",
            "[0.9508071 0.0491929]\n",
            "[0.01424488 0.9857552 ]\n",
            "[0.02866967 0.97133034]\n",
            "[0.9513018  0.04869817]\n",
            "[0.9456852  0.05431482]\n",
            "[0.9515674  0.04843256]\n",
            "[0.01981148 0.9801885 ]\n",
            "[0.95151937 0.04848063]\n",
            "[0.9513111  0.04868886]\n",
            "[0.01787183 0.98212814]\n",
            "[0.01954209 0.98045796]\n",
            "[0.9513383  0.04866167]\n",
            "[0.02025307 0.97974694]\n",
            "[0.9515524 0.0484476]\n",
            "[0.93024796 0.06975207]\n",
            "[0.06288992 0.93711007]\n",
            "[0.01847648 0.98152345]\n",
            "[0.707937   0.29206303]\n",
            "[0.09780554 0.9021945 ]\n",
            "[0.92862165 0.07137832]\n",
            "[0.94362664 0.05637329]\n",
            "[0.07449059 0.9255094 ]\n",
            "[0.82323277 0.17676719]\n",
            "[0.02006916 0.9799309 ]\n",
            "[0.95105904 0.04894092]\n",
            "[0.3137928 0.6862072]\n",
            "[0.8371419  0.16285807]\n",
            "[0.9118951  0.08810493]\n",
            "[0.02871812 0.9712819 ]\n",
            "[0.25971413 0.7402858 ]\n",
            "[0.8815993  0.11840063]\n",
            "[0.9514858  0.04851422]\n",
            "[0.95124465 0.04875536]\n",
            "[0.372251 0.627749]\n",
            "[0.04351316 0.9564869 ]\n",
            "[0.01467954 0.9853204 ]\n",
            "[0.023142 0.976858]\n",
            "[0.95163745 0.04836259]\n",
            "[0.951524   0.04847601]\n",
            "[0.03469234 0.9653077 ]\n",
            "[0.657562   0.34243804]\n",
            "[0.9514147 0.0485853]\n",
            "[0.9407554  0.05924455]\n",
            "[0.91945964 0.08054041]\n",
            "[0.9515621  0.04843785]\n",
            "[0.01824769 0.9817523 ]\n",
            "[0.6744515  0.32554853]\n",
            "[0.6744515  0.32554853]\n",
            "[0.01511787 0.98488206]\n",
            "[0.9437012  0.05629884]\n",
            "[0.91301805 0.08698194]\n",
            "[0.95131016 0.04868986]\n",
            "[0.02124117 0.9787588 ]\n",
            "[0.02989675 0.97010326]\n",
            "[0.9517561  0.04824391]\n",
            "[0.94889235 0.05110761]\n",
            "[0.0327181 0.9672819]\n",
            "[0.95158917 0.0484108 ]\n",
            "[0.9516079  0.04839212]\n",
            "[0.01837515 0.9816249 ]\n",
            "[0.95158637 0.04841366]\n",
            "[0.01973694 0.98026305]\n",
            "[0.9254607 0.0745393]\n",
            "[0.94633174 0.05366833]\n",
            "[0.95117235 0.04882758]\n",
            "[0.22663409 0.7733659 ]\n",
            "[0.01913865 0.98086137]\n",
            "[0.9516692  0.04833083]\n",
            "[0.02923157 0.97076845]\n",
            "[0.9516842  0.04831575]\n",
            "[0.03996884 0.96003115]\n",
            "[0.95166034 0.04833967]\n",
            "[0.9514723  0.04852768]\n",
            "[0.09159617 0.9084038 ]\n",
            "[0.95131177 0.04868829]\n",
            "[0.01640066 0.9835993 ]\n",
            "[0.03445951 0.96554047]\n",
            "[0.03216212 0.96783787]\n",
            "[0.03043763 0.96956235]\n",
            "[0.95126545 0.04873452]\n",
            "[0.02752211 0.9724779 ]\n",
            "[0.95164436 0.04835569]\n",
            "[0.95176667 0.04823331]\n",
            "[0.01663164 0.98336834]\n",
            "[0.6561641 0.3438359]\n",
            "[0.86445343 0.13554658]\n",
            "[0.0203897 0.9796103]\n",
            "[0.26225427 0.73774576]\n",
            "[0.95131326 0.04868669]\n",
            "[0.02529602 0.974704  ]\n",
            "[0.9513736  0.04862638]\n",
            "[0.05835312 0.9416468 ]\n",
            "[0.0232249 0.9767751]\n",
            "[0.02319553 0.97680444]\n",
            "[0.95103675 0.04896325]\n",
            "[0.02719364 0.97280633]\n",
            "[0.9510181 0.0489819]\n",
            "[0.01955133 0.98044866]\n",
            "[0.9515424  0.04845769]\n",
            "[0.5171872  0.48281282]\n",
            "[0.5171872  0.48281282]\n",
            "[0.95184034 0.04815963]\n",
            "[0.9503772  0.04962284]\n",
            "[0.01715153 0.9828485 ]\n",
            "[0.9232808  0.07671919]\n",
            "[0.9516803  0.04831964]\n",
            "[0.9514844  0.04851566]\n",
            "[0.95064205 0.04935796]\n",
            "[0.9495175  0.05048257]\n",
            "[0.54789466 0.4521053 ]\n",
            "[0.9358486  0.06415136]\n",
            "[0.951683   0.04831698]\n",
            "[0.01660469 0.98339534]\n",
            "[0.01547309 0.9845269 ]\n",
            "[0.9501521  0.04984792]\n",
            "[0.02252853 0.9774714 ]\n",
            "[0.01829108 0.981709  ]\n",
            "[0.94568545 0.05431456]\n",
            "[0.07036459 0.9296354 ]\n",
            "[0.95125544 0.04874457]\n",
            "[0.01965575 0.9803443 ]\n",
            "[0.95170826 0.04829176]\n",
            "[0.19373319 0.80626684]\n",
            "[0.0342264 0.9657736]\n",
            "[0.7204612  0.27953875]\n",
            "[0.01504579 0.98495424]\n",
            "[0.05023759 0.94976234]\n",
            "[0.0405914 0.9594086]\n",
            "[0.95148396 0.04851605]\n",
            "[0.01996287 0.9800371 ]\n",
            "[0.95166683 0.04833314]\n",
            "[0.95165884 0.04834112]\n",
            "[0.03082794 0.9691721 ]\n",
            "[0.81480896 0.18519105]\n",
            "[0.9382751  0.06172489]\n",
            "[0.95079654 0.04920344]\n",
            "[0.02917186 0.97082806]\n",
            "[0.9517322  0.04826777]\n",
            "[0.03122081 0.9687792 ]\n",
            "[0.9516595  0.04834048]\n",
            "[0.6174683  0.38253167]\n",
            "[0.13142765 0.8685723 ]\n",
            "[0.01601935 0.9839806 ]\n",
            "[0.8813972  0.11860275]\n",
            "[0.8813972  0.11860275]\n",
            "[0.95136976 0.04863022]\n",
            "[0.01887964 0.9811204 ]\n",
            "[0.95130885 0.04869112]\n",
            "[0.38226238 0.6177376 ]\n",
            "[0.01990566 0.9800944 ]\n",
            "[0.9517858  0.04821415]\n",
            "[0.8872675  0.11273252]\n",
            "[0.02007972 0.9799203 ]\n",
            "[0.9515407  0.04845931]\n",
            "[0.9514375  0.04856258]\n",
            "[0.01576209 0.98423785]\n",
            "[0.02987538 0.97012454]\n",
            "[0.16064645 0.8393535 ]\n",
            "[0.76310706 0.23689292]\n",
            "[0.76310706 0.23689292]\n",
            "[0.95089483 0.04910516]\n",
            "[0.02294521 0.9770547 ]\n",
            "[0.01720983 0.98279023]\n",
            "[0.95174325 0.04825676]\n",
            "[0.95153975 0.04846022]\n",
            "[0.02619735 0.9738027 ]\n",
            "[0.9497123  0.05028776]\n",
            "[0.0244825 0.9755175]\n",
            "[0.9514302  0.04856974]\n",
            "[0.01775877 0.9822412 ]\n",
            "[0.02502451 0.97497547]\n",
            "[0.9513094  0.04869059]\n",
            "[0.02195352 0.9780465 ]\n",
            "[0.02195352 0.9780465 ]\n",
            "[0.03673422 0.96326584]\n",
            "[0.9515392  0.04846083]\n",
            "[0.6363654  0.36363456]\n",
            "[0.6363654  0.36363456]\n",
            "[0.63730913 0.36269093]\n",
            "[0.63730913 0.36269093]\n",
            "[0.02220805 0.9777919 ]\n",
            "[0.94529986 0.05470008]\n",
            "[0.01952682 0.98047316]\n",
            "[0.37779152 0.6222085 ]\n",
            "[0.01699638 0.9830036 ]\n",
            "[0.01699638 0.9830036 ]\n",
            "[0.02895145 0.9710486 ]\n",
            "[0.02547924 0.9745207 ]\n",
            "[0.9515661  0.04843393]\n",
            "[0.03319698 0.9668031 ]\n",
            "[0.02744832 0.97255164]\n",
            "[0.46969387 0.5303061 ]\n",
            "[0.0434986  0.95650136]\n",
            "[0.95084524 0.04915475]\n",
            "[0.02798363 0.97201633]\n",
            "[0.9514724  0.04852755]\n",
            "[0.8296453  0.17035477]\n",
            "[0.01710255 0.98289746]\n",
            "[0.9137919  0.08620807]\n",
            "[0.10375419 0.8962458 ]\n",
            "[0.02904272 0.9709573 ]\n",
            "[0.9513428  0.04865715]\n",
            "[0.04148932 0.95851076]\n",
            "[0.94374037 0.05625964]\n",
            "[0.03126768 0.96873236]\n",
            "[0.95145744 0.04854254]\n",
            "[0.9514002  0.04859976]\n",
            "[0.94668865 0.0533114 ]\n",
            "[0.9514949  0.04850513]\n",
            "[0.01725841 0.98274165]\n",
            "[0.9514488  0.04855127]\n",
            "[0.11827076 0.88172925]\n",
            "[0.9496727  0.05032726]\n",
            "[0.9513335  0.04866651]\n",
            "[0.19026187 0.80973816]\n",
            "[0.03298987 0.96701014]\n",
            "[0.9513139  0.04868607]\n",
            "[0.01807638 0.98192364]\n",
            "[0.01641308 0.98358697]\n",
            "[0.9268265  0.07317355]\n",
            "[0.49682847 0.5031715 ]\n",
            "[0.9516578  0.04834221]\n",
            "[0.0178025 0.9821975]\n",
            "[0.02347322 0.97652674]\n",
            "[0.9515435  0.04845642]\n",
            "[0.0151184  0.98488164]\n",
            "[0.9513193  0.04868074]\n",
            "[0.9514545  0.04854549]\n",
            "[0.06493421 0.9350658 ]\n",
            "[0.4062224 0.5937776]\n",
            "[0.19826117 0.80173886]\n",
            "[0.95113087 0.04886913]\n",
            "[0.04497431 0.95502573]\n",
            "[0.01774342 0.9822566 ]\n",
            "[0.9515405  0.04845944]\n",
            "[0.94907725 0.05092275]\n",
            "[0.03499606 0.965004  ]\n",
            "[0.48402166 0.51597834]\n",
            "[0.48402166 0.51597834]\n",
            "[0.02032795 0.97967213]\n",
            "[0.95090866 0.04909136]\n",
            "[0.15532988 0.8446701 ]\n",
            "[0.18038963 0.81961036]\n",
            "[0.08701228 0.9129877 ]\n",
            "[0.0150083 0.9849917]\n",
            "[0.01853285 0.9814672 ]\n",
            "[0.95161563 0.04838434]\n",
            "[0.95124036 0.04875962]\n",
            "[0.02016918 0.9798308 ]\n",
            "[0.01956799 0.98043203]\n",
            "[0.95150393 0.04849609]\n",
            "[0.0173156  0.98268443]\n",
            "[0.93231595 0.06768399]\n",
            "[0.9515681  0.04843184]\n",
            "[0.9515648  0.04843515]\n",
            "[0.95167696 0.04832302]\n",
            "[0.9490257  0.05097429]\n",
            "[0.9514728  0.04852711]\n",
            "[0.31331983 0.68668014]\n",
            "[0.02237717 0.9776228 ]\n",
            "[0.09589544 0.9041046 ]\n",
            "[0.9492968  0.05070325]\n",
            "[0.01776502 0.982235  ]\n",
            "[0.02074506 0.9792549 ]\n",
            "[0.94944793 0.05055207]\n",
            "[0.9512251  0.04877484]\n",
            "[0.9514613 0.0485387]\n",
            "[0.01698824 0.98301184]\n",
            "[0.2552964 0.7447036]\n",
            "[0.01874128 0.9812587 ]\n",
            "[0.95161587 0.04838413]\n",
            "[0.01571426 0.9842858 ]\n",
            "[0.36574593 0.6342541 ]\n",
            "[0.9514613  0.04853868]\n",
            "[0.9513644  0.04863558]\n",
            "[0.01809158 0.98190844]\n",
            "[0.9514957  0.04850429]\n",
            "[0.02042621 0.97957385]\n",
            "[0.89545614 0.10454381]\n",
            "[0.9515709  0.04842909]\n",
            "[0.1050904 0.8949096]\n",
            "[0.95140046 0.04859962]\n",
            "[0.05889789 0.94110215]\n",
            "[0.94898653 0.05101345]\n",
            "[0.9332467  0.06675334]\n",
            "[0.03911528 0.9608847 ]\n",
            "[0.20080064 0.79919934]\n",
            "[0.9515158  0.04848421]\n",
            "[0.02313377 0.9768662 ]\n",
            "[0.9511026 0.0488974]\n",
            "[0.94919217 0.0508078 ]\n",
            "[0.01817423 0.9818257 ]\n",
            "[0.08063392 0.9193661 ]\n",
            "[0.9514002  0.04859976]\n",
            "[0.48448354 0.5155164 ]\n",
            "[0.02159036 0.9784096 ]\n",
            "[0.95157915 0.0484209 ]\n",
            "[0.9514499  0.04855017]\n",
            "[0.02043338 0.97956663]\n",
            "[0.95121324 0.04878679]\n",
            "[0.95163757 0.04836244]\n",
            "[0.028418 0.971582]\n",
            "[0.9518562  0.04814381]\n",
            "[0.01664111 0.9833589 ]\n",
            "[0.03043773 0.96956223]\n",
            "[0.9514912  0.04850881]\n",
            "[0.01660847 0.9833915 ]\n",
            "[0.9515458  0.04845423]\n",
            "[0.614582   0.38541803]\n",
            "[0.05997233 0.9400277 ]\n",
            "[0.9512826  0.04871732]\n",
            "[0.0277992  0.97220075]\n",
            "[0.06865907 0.9313409 ]\n",
            "[0.89854187 0.1014581 ]\n",
            "[0.95121175 0.0487882 ]\n",
            "[0.0262499 0.9737501]\n",
            "[0.06900743 0.93099254]\n",
            "[0.95103973 0.04896025]\n",
            "[0.01582371 0.9841763 ]\n",
            "[0.9509065  0.04909351]\n",
            "[0.05448651 0.9455135 ]\n",
            "[0.9503379  0.04966209]\n",
            "[0.02683209 0.9731679 ]\n",
            "[0.02683209 0.9731679 ]\n",
            "[0.01963394 0.98036605]\n",
            "[0.06128698 0.938713  ]\n",
            "[0.9453774  0.05462262]\n",
            "[0.02110239 0.9788976 ]\n",
            "[0.01619576 0.9838043 ]\n",
            "[0.01804709 0.98195297]\n",
            "[0.9516678  0.04833222]\n",
            "[0.95090747 0.04909258]\n",
            "[0.01701548 0.98298454]\n",
            "[0.95151675 0.04848317]\n",
            "[0.8998536  0.10014644]\n",
            "[0.08491179 0.9150882 ]\n",
            "[0.0328148 0.9671852]\n",
            "[0.52499396 0.47500604]\n",
            "[0.9508908 0.0491093]\n",
            "[0.01635892 0.9836411 ]\n",
            "[0.02430022 0.9756998 ]\n",
            "[0.17297752 0.8270225 ]\n",
            "[0.9517207  0.04827934]\n",
            "[0.01759631 0.98240376]\n",
            "[0.0236375 0.9763625]\n",
            "[0.03259166 0.96740836]\n",
            "[0.03298558 0.9670144 ]\n",
            "[0.95150816 0.04849191]\n",
            "[0.89013594 0.10986409]\n",
            "[0.95154625 0.04845375]\n",
            "[0.02001157 0.97998846]\n",
            "[0.03763198 0.962368  ]\n",
            "[0.01987471 0.98012525]\n",
            "[0.9515101  0.04848992]\n",
            "[0.92068505 0.07931497]\n",
            "[0.9513142  0.04868572]\n",
            "[0.02036882 0.9796311 ]\n",
            "[0.95167017 0.04832982]\n",
            "[0.02137544 0.9786245 ]\n",
            "[0.9505763  0.04942363]\n",
            "[0.5734152  0.42658475]\n",
            "[0.9509975  0.04900251]\n",
            "[0.01650644 0.98349357]\n",
            "[0.4642301  0.53576994]\n",
            "[0.02035667 0.9796433 ]\n",
            "[0.03221377 0.9677862 ]\n",
            "[0.01881615 0.9811839 ]\n",
            "[0.95168054 0.04831949]\n",
            "[0.02259106 0.9774089 ]\n",
            "[0.01749257 0.9825074 ]\n",
            "[0.95164466 0.04835531]\n",
            "[0.01858193 0.9814181 ]\n",
            "[0.02240828 0.9775917 ]\n",
            "[0.0223801 0.9776199]\n",
            "[0.9516844  0.04831559]\n",
            "[0.02082392 0.97917604]\n",
            "[0.9514283  0.04857168]\n",
            "[0.95156837 0.04843162]\n",
            "[0.02360672 0.97639334]\n",
            "[0.19423832 0.80576164]\n",
            "[0.02968853 0.9703115 ]\n",
            "[0.9515552  0.04844485]\n",
            "[0.02058277 0.97941726]\n",
            "[0.95123106 0.04876895]\n",
            "[0.95163596 0.04836409]\n",
            "[0.01609273 0.9839072 ]\n",
            "[0.9491981  0.05080188]\n",
            "[0.02856546 0.97143453]\n",
            "[0.9380817  0.06191826]\n",
            "[0.9380817  0.06191826]\n",
            "[0.9516681 0.0483319]\n",
            "[0.49373576 0.5062642 ]\n",
            "[0.02212963 0.97787035]\n",
            "[0.8677117 0.1322883]\n",
            "[0.676351 0.323649]\n",
            "[0.676351 0.323649]\n",
            "[0.676351 0.323649]\n",
            "[0.676351 0.323649]\n",
            "[0.0543732 0.9456268]\n",
            "[0.36931604 0.630684  ]\n",
            "[0.95119816 0.04880188]\n",
            "[0.9511826  0.04881738]\n",
            "[0.95154    0.04846001]\n",
            "[0.01810594 0.9818941 ]\n",
            "[0.9515878 0.0484122]\n",
            "[0.02121826 0.97878176]\n",
            "[0.9445533  0.05544675]\n",
            "[0.95132416 0.04867582]\n",
            "[0.01700981 0.98299015]\n",
            "[0.92282254 0.07717753]\n",
            "[0.0219735  0.97802657]\n",
            "[0.9515769  0.04842316]\n",
            "[0.9486203  0.05137966]\n",
            "[0.9515549  0.04844517]\n",
            "[0.02364392 0.976356  ]\n",
            "[0.27384463 0.7261554 ]\n",
            "[0.9458425  0.05415752]\n",
            "[0.9513371  0.04866286]\n",
            "[0.02793006 0.97207   ]\n",
            "[0.9513443  0.04865569]\n",
            "[0.02780235 0.9721976 ]\n",
            "[0.9511823 0.0488177]\n",
            "[0.0194928  0.98050725]\n",
            "[0.9516151  0.04838492]\n",
            "[0.01939694 0.98060304]\n",
            "[0.02142015 0.9785799 ]\n",
            "[0.9515468  0.04845317]\n",
            "[0.02567921 0.9743208 ]\n",
            "[0.7133569 0.2866431]\n",
            "[0.9520736  0.04792638]\n",
            "[0.02692578 0.9730742 ]\n",
            "[0.01731017 0.98268986]\n",
            "[0.9509511 0.0490489]\n",
            "[0.9283448  0.07165518]\n",
            "[0.95151776 0.04848223]\n",
            "[0.02036304 0.97963697]\n",
            "[0.02058222 0.97941786]\n",
            "[0.95141757 0.04858239]\n",
            "[0.9491721  0.05082796]\n",
            "[0.02508063 0.9749194 ]\n",
            "[0.9059886 0.0940114]\n",
            "[0.60646    0.39354005]\n",
            "[0.94642353 0.0535765 ]\n",
            "[0.24007869 0.7599213 ]\n",
            "[0.14548196 0.85451806]\n",
            "[0.21666877 0.7833312 ]\n",
            "[0.3722058  0.62779427]\n",
            "[0.9257388  0.07426116]\n",
            "[0.95139915 0.04860085]\n",
            "[0.01898665 0.98101336]\n",
            "[0.9516028  0.04839721]\n",
            "[0.32410562 0.67589444]\n",
            "[0.13457057 0.86542946]\n",
            "[0.95092815 0.04907179]\n",
            "[0.6389671  0.36103293]\n",
            "[0.6389671  0.36103293]\n",
            "[0.640036 0.359964]\n",
            "[0.640036 0.359964]\n",
            "[0.95148754 0.04851244]\n",
            "[0.9504389  0.04956107]\n",
            "[0.01939776 0.98060226]\n",
            "[0.41827208 0.58172786]\n",
            "[0.9513929  0.04860707]\n",
            "[0.15845507 0.8415449 ]\n",
            "[0.95152164 0.04847839]\n",
            "[0.95164835 0.04835165]\n",
            "[0.03237139 0.96762854]\n",
            "[0.9513138  0.04868625]\n",
            "[0.95146227 0.04853774]\n",
            "[0.01665468 0.98334527]\n",
            "[0.01643152 0.9835685 ]\n",
            "[0.02213223 0.9778677 ]\n",
            "[0.4966595 0.5033405]\n",
            "[0.01885442 0.98114556]\n",
            "[0.95137435 0.04862562]\n",
            "[0.9511434 0.0488566]\n",
            "[0.9516287  0.04837127]\n",
            "[0.04442078 0.9555793 ]\n",
            "[0.8149169 0.1850831]\n",
            "[0.8149169 0.1850831]\n",
            "[0.93566245 0.06433757]\n",
            "[0.01646931 0.9835307 ]\n",
            "[0.64476997 0.35523003]\n",
            "[0.01895916 0.9810409 ]\n",
            "[0.95142835 0.04857159]\n",
            "[0.0455437 0.9544563]\n",
            "[0.65606993 0.34393007]\n",
            "[0.65606993 0.34393007]\n",
            "[0.9440703  0.05592979]\n",
            "[0.9517157  0.04828424]\n",
            "[0.02895914 0.97104084]\n",
            "[0.95173216 0.04826788]\n",
            "[0.95128673 0.04871326]\n",
            "[0.8144098 0.1855902]\n",
            "[0.9512971  0.04870291]\n",
            "[0.02616302 0.97383696]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/task1hindi-results.txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "outputId": "89a0f98d-9c80-4d02-c717-af2a00379b8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI2K1bqaR5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "outputId": "e8ce7e83-4818-4c34-d3c9-d83273055651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QowNXrYZSRMR",
        "colab_type": "code",
        "outputId": "d79bae87-206f-424b-d433-9fbd81dd9abb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "print(flat_predictions)\n",
        "print(\"************\")\n",
        "print(flat_true_labels)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1\n",
            " 1 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0\n",
            " 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0\n",
            " 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 1\n",
            " 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 1]\n",
            "************\n",
            "[1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "outputId": "965d5e9f-c27d-41ba-ff0d-72a3fa5d9aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-task1hindi'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task1hindi\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
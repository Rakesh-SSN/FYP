{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT TASK 1 HINDI",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT_TASK_1_HINDI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "56289ce6-a2a7-43f9-8d78-a8a85ed17e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "9dfaeaf9-f1a4-4a6c-abfd-6736d0387157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "42f958de-cca3-4f34-d8f6-df06a72b6d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/hindi/task1hindi.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1447</th>\n",
              "      <td>HIN1448</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>केंद्रीय मंत्री रविशंकर प्रसाद ने कहा कि एंटनी...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114</th>\n",
              "      <td>HIN1115</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>सूत्रों ने पहले ही इस बात के संकेत दिए थे कि ए...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1064</th>\n",
              "      <td>HIN1065</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>किन-किन ने ली शपथ?&lt;eol&gt;कौन-कौन बना मंत्री?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2287</th>\n",
              "      <td>HIN2288</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>मुझे भारत के गेंदबाज पसंद हैं और  इशांत शर्मा ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1537</th>\n",
              "      <td>HIN1538</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>पहले पड़ाव में वे बेल्जियम की राजधानी ब्रसेल्स...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>668</th>\n",
              "      <td>HIN0669</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>सेटलमेंट के लिए माल्या का छ्ह हज़ार आठ सौ अठ्सठ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1583</th>\n",
              "      <td>HIN1584</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>कोलकाता: मृतकों की संख्या चौबीस  हुई|&lt;eol&gt;वहीं...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2404</th>\n",
              "      <td>HIN2405</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>जम्मू-कश्मीर में सरकार गठन पर अगले कदम को लेकर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>HIN0498</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>सेना और एनडीआरएफ की मौजूदगी में राहत और बचाव क...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2480</th>\n",
              "      <td>HIN2481</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>प्रवर्तन निदेशालय ने माल्या के पासपोर्ट को रद ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "1447         HIN1448  ...  केंद्रीय मंत्री रविशंकर प्रसाद ने कहा कि एंटनी...\n",
              "1114         HIN1115  ...  सूत्रों ने पहले ही इस बात के संकेत दिए थे कि ए...\n",
              "1064         HIN1065  ...         किन-किन ने ली शपथ?<eol>कौन-कौन बना मंत्री?\n",
              "2287         HIN2288  ...  मुझे भारत के गेंदबाज पसंद हैं और  इशांत शर्मा ...\n",
              "1537         HIN1538  ...  पहले पड़ाव में वे बेल्जियम की राजधानी ब्रसेल्स...\n",
              "668          HIN0669  ...  सेटलमेंट के लिए माल्या का छ्ह हज़ार आठ सौ अठ्सठ...\n",
              "1583         HIN1584  ...  कोलकाता: मृतकों की संख्या चौबीस  हुई|<eol>वहीं...\n",
              "2404         HIN2405  ...  जम्मू-कश्मीर में सरकार गठन पर अगले कदम को लेकर...\n",
              "497          HIN0498  ...  सेना और एनडीआरएफ की मौजूदगी में राहत और बचाव क...\n",
              "2480         HIN2481  ...  प्रवर्तन निदेशालय ने माल्या के पासपोर्ट को रद ...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "c4120ec4-e872-4e56-d231-2478dbc6d5dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1673</th>\n",
              "      <td>माता वैष्णो देवी भी लड़कियों को मेडल मिलते देख...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1802</th>\n",
              "      <td>इसलिए उससे सेना को निपटना चाहिए।’’&lt;eol&gt;तीन दिव...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1351</th>\n",
              "      <td>सीबीआई यह भी पता लगाने की कोशिश में है कि इटली...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1714</th>\n",
              "      <td>उत्तराखंडः हाईकोर्ट  का सख्त कमेंट- कभी-कभी प्...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>आम आदमी पार्टी की सरकार को केंद्र पर ब्लेम लगा...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1673  माता वैष्णो देवी भी लड़कियों को मेडल मिलते देख...      0\n",
              "1802  इसलिए उससे सेना को निपटना चाहिए।’’<eol>तीन दिव...      0\n",
              "1351  सीबीआई यह भी पता लगाने की कोशिश में है कि इटली...      0\n",
              "1714  उत्तराखंडः हाईकोर्ट  का सख्त कमेंट- कभी-कभी प्...      0\n",
              "1001  आम आदमी पार्टी की सरकार को केंद्र पर ब्लेम लगा...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "609041c8-dc1a-4e11-9018-0d1ad5583de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "0f32e914-3c9a-495d-adce-f287289d92f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  भारतीय मुस्लिमों की वजह से नहीं पनप सकता आईएस|<eol>भारत में कभी वर्चस्व कायम नहीं कर सकता आईएस|\n",
            "Tokenized:  ['भारतीय', 'म', '##स', '##ल', '##िम', '##ो', 'की', 'व', '##ज', '##ह', 'स', 'न', '##ही', 'प', '##न', '##प', 'सकता', 'आई', '##ए', '##स', '|', '<', 'eo', '##l', '>', 'भारत', 'म', 'कभी', 'व', '##र', '##च', '##स', '##व', 'का', '##यम', 'न', '##ही', 'कर', 'सकता', 'आई', '##ए', '##स', '|']\n",
            "Token IDs:  [18725, 889, 13432, 11714, 50419, 13718, 10826, 895, 17413, 17110, 898, 884, 24667, 885, 11453, 18187, 26886, 44881, 22599, 13432, 196, 133, 13934, 10161, 135, 14311, 889, 50058, 895, 11549, 16940, 13432, 15070, 11081, 87136, 884, 24667, 16192, 26886, 44881, 22599, 13432, 196]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "06e11cd2-765b-4a30-f41b-98856a6d9392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  भारतीय मुस्लिमों की वजह से नहीं पनप सकता आईएस|<eol>भारत में कभी वर्चस्व कायम नहीं कर सकता आईएस|\n",
            "Token IDs: [101, 18725, 889, 13432, 11714, 50419, 13718, 10826, 895, 17413, 17110, 898, 884, 24667, 885, 11453, 18187, 26886, 44881, 22599, 13432, 196, 133, 13934, 10161, 135, 14311, 889, 50058, 895, 11549, 16940, 13432, 15070, 11081, 87136, 884, 24667, 16192, 26886, 44881, 22599, 13432, 196, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "997a522a-3f4d-4deb-ffa6-eefaf2a6d69b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "75be6091-41e4-469d-c7db-08d2b179b039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "9c671454-6b0b-44e0-f1b7-492f61104ac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "print(train_inputs)\n",
        "print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "#print(train_masks)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101    881  11549 ...      0      0      0]\n",
            " [   101    882  27391 ...      0      0      0]\n",
            " [   101    899  15552 ...  10826    889  78530]\n",
            " ...\n",
            " [   101    882  27391 ...  14265    886  13432]\n",
            " [   101    882  27391 ...    884 100915  16380]\n",
            " [   101    889  18351 ...      0      0      0]]\n",
            "[1 0 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "451f82f3-9feb-4256-f359-9ebecc7f7311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "outputId": "2dfb12de-4984-4e49-aeab-627e0f6307d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "outputId": "8cfe0136-d388-42ef-a8f0-687661239299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:18.\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epcoh took: 0:00:28\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifs2QgT9HeUe",
        "colab_type": "code",
        "outputId": "4c7f8da4-bbec-4661-f11f-f258ea6c8d5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3SW9eH+8ffzZO9FFiSBsLIgIQMJ\nigxBDAgKCMoyYCnVr1KtrUWpaL/HgijiwEFb2toS9h5KlSUIIsMEJJCEFUCJMSFkMUIGJL8/KPl9\nU2Yg5H6SXK9zOJ7c87rzOZDLO/f9eUxVVVVViIiIiIhIg2A2OoCIiIiIiNw6FXgRERERkQZEBV5E\nREREpAFRgRcRERERaUBU4EVEREREGhAVeBERERGRBkQFXkSkiZoxYwYhISHk5eXd1v5lZWWEhITw\n+uuv13Gy2lm4cCEhISF8//33huYQEakv1kYHEBFpykJCQm55202bNhEQEHAX04iISEOgAi8iYqDp\n06fX+DolJYXFixfzxBNPEBsbW2Odp6dnnZ77N7/5Db/+9a+xs7O7rf3t7OxITU3FysqqTnOJiMiN\nqcCLiBjo0UcfrfH1pUuXWLx4MZ06dbpq3fVUVVVx4cIFHB0da3Vua2trrK3v7MfA7ZZ/ERG5fXoG\nXkSkAdm6dSshISF8/vnnzJkzh4SEBDp27Mi8efMA2LNnDxMnTqRv375ERUURExPDqFGj2Lx581XH\nutYz8FeWnTx5krfffpv777+fjh07MnjwYLZv315j/2s9A/9/l3333XeMGDGCqKgo4uPjef3117lw\n4cJVOb799luGDRtGx44d6datG2+99Rbp6emEhIQwe/bs2/5enT59mtdff53u3bvToUMHevXqxZQp\nUyguLq6xXUlJCe+//z4PPfQQkZGRdO7cmYEDB/L+++/X2G7jxo2MGDGCLl26EBkZSa9evXj++ec5\nefLkbWcUEbkdugMvItIA/e1vf+Ps2bM89thjeHl5ERgYCMCXX37JyZMn6d+/P82bN6egoICVK1fy\nzDPP8NFHH9G3b99bOv7vfvc77Ozs+OUvf0lZWRn/+te/+J//+R82bNiAr6/vTfffv38/69atY+jQ\noTzyyCPs2LGDxYsXY2try+TJk6u327FjB+PHj8fT05Onn34aZ2dn1q5dy+7du2/vG/MfRUVFPPHE\nE2RnZzNs2DBCQ0PZv38/8+bNY9euXSxZsgQHBwcAXnvtNdauXcvgwYPp1KkTFRUVnDhxgp07d1Yf\n75tvvmHChAmEh4fzzDPP4OzsTG5uLtu3bycrK6v6+y8iUh9U4EVEGqBTp07xxRdf4O7uXmP5b37z\nm6sepXnyySd55JFH+POf/3zLBd7X15cPP/wQk8kEUH0nf+nSpUyYMOGm+x86dIhly5YRHh4OwIgR\nIxgzZgyLFy9m4sSJ2NraAjBt2jRsbGxYsmQJ/v7+AIwcOZLhw4ffUs7r+ctf/kJWVhZTp05l6NCh\n1cvbtWvH22+/Xf0/JFVVVXz11Vf06dOHadOmXfd4GzduBGDOnDm4uLhUL7+V74WISF3TIzQiIg3Q\nY489dlV5B2qU9wsXLlBYWEhZWRn33HMPGRkZlJeX39Lxx4wZU13eAWJjY7GxseHEiRO3tH/nzp2r\ny/sV8fHxlJeX8/PPPwPw008/cejQIR566KHq8g5ga2tLYmLiLZ3neq78pmDIkCE1lo8ePRoXFxc2\nbNgAgMlkwsnJiUOHDpGZmXnd47m4uFBVVcW6deu4dOnSHWUTEblTugMvItIAtWrV6prLT506xfvv\nv8/mzZspLCy8av3Zs2fx8vK66fH/+5EQk8mEm5sbRUVFt5TvWo+UXPkfjqKiIlq2bElWVhYAwcHB\nV217rWW3qqqqiuzsbOLj4zGba96nsrW1JSgoqPrcAK+++ip/+MMf6N+/Py1btqRLly488MAD9OzZ\ns/p/YsaMGcOWLVt49dVXeeutt4iLi+P++++nf//+eHh43HZWEZHboQIvItIAXXl++/+6dOkSY8eO\nJSsri8TERCIiInBxccFsNrNo0SLWrVtHZWXlLR3/v4vvFVVVVXe0f22OUV/69etHly5d2Lp1K7t3\n7+abb75hyZIldO3alb///e9YW1vTrFkzVq5cyXfffce3337Ld999x5QpU/jwww/5xz/+QYcOHYy+\nDBFpQlTgRUQaiQMHDpCZmclvf/tbnn766RrrrsxSY0latGgBwPHjx69ad61lt8pkMtGiRQuOHTtG\nZWVljf+ZKC8v58cffyQoKKjGPp6engwaNIhBgwZRVVXFm2++SVJSElu3buWBBx4ALk+72bVrV7p2\n7Qpc/n4PHTqUv/71r3z00Ue3nVdEpLb0DLyISCNxpaj+9x3utLQ0vv76ayMi3VBAQADt27dn3bp1\n1c/Fw+WSnZSUdEfH7tOnDzk5OaxatarG8gULFnD27FkefPBBACoqKjh37lyNbUwmE2FhYQDVU04W\nFBRcdY62bdtia2t7y48ViYjUFd2BFxFpJEJCQmjVqhV//vOfOXPmDK1atSIzM5MlS5YQEhJCWlqa\n0RGv8sorrzB+/Hgef/xxhg8fjpOTE2vXrq3xAu3teOaZZ1i/fj2TJ09m3759hISEcODAAVasWEH7\n9u0ZO3YscPl5/D59+tCnTx9CQkLw9PTk5MmTLFy4EA8PD3r06AHAxIkTOXPmDF27dqVFixaUlJTw\n+eefU1ZWxqBBg+702yAiUisq8CIijYStrS1/+9vfmD59OsuXL6esrIz27dvz3nvvkZKSYpEF/r77\n7mP27Nm8//77/OUvf8HNzY0BAwbQp08fRo0ahb29/W0d193dncWLF/PRRx+xadMmli9fjpeXF6NH\nj+bXv/519TsELi4ujB49mh07drBt2zYuXLiAt7c3ffv25emnn8bT0xOAIUOGsHr1alasWEFhYSEu\nLi60a9eOWbNm0bt37zr7foiI3ApTlaW9TSQiIk3emjVr+P3vf88nn3xCnz59jI4jImJR9Ay8iIgY\nprKy8qq56cvLy5kzZw62trbExcUZlExExHLpERoRETHMuXPn6N+/PwMHDqRVq1YUFBSwdu1ajhw5\nwoQJE675YVUiIk2dCryIiBjG3t6e++67j/Xr13P69GkAWrduzZ/+9Ccef/xxg9OJiFgmPQMvIiIi\nItKA6Bl4EREREZEGRAVeRERERKQB0TPwtVRYeJ7Kyvp/6sjLy5n8/HM331DqjcbEMmlcLI/GxDJp\nXCyPxsQyGTEuZrMJDw+n6643tMCXl5czc+ZMVq9ezZkzZwgNDeXFF1+ka9euN9zvo48+4uOPP75q\nebNmzdi+fftVy5cuXcqnn35KVlYWzZs3JzExkVGjRt1W5srKKkMK/JVzi2XRmFgmjYvl0ZhYJo2L\n5dGYWCZLGxdDC/wrr7zC+vXrSUxMpGXLlqxcuZLx48czd+5coqOjb7r/G2+8UeNT+q71iX2LFi3i\nj3/8IwkJCTz11FMkJyfzxhtvUFZWxi9+8Ys6vR4RERERkbvNsAKfmprK2rVrmTRpEmPHjgVg0KBB\nDBgwgBkzZjB//vybHqNfv364urped31paSnvv/8+vXv3ZubMmQA8/vjjVFZW8vHHHzNs2DBcXFzq\n5HpEREREROqDYS+xfvnll9jY2DBs2LDqZXZ2dgwdOpSUlBROnTp102NUVVVx7tw5rjcT5q5duygq\nKmLkyJE1lo8aNYrz58+zdevWO7sIEREREZF6ZliBz8jIIDg4GCenmg/oR0ZGUlVVRUZGxk2P0bNn\nT2JjY4mNjWXSpEkUFRXVWJ+eng5Ahw4daiyPiIjAbDZXrxcRERERaSgMe4QmLy8PX1/fq5Z7e3sD\n3PAOvKurK08++SRRUVHY2Niwc+dOFi9eTHp6OkuXLsXW1rb6HLa2tld9FPeVZbdyl19ERERExJIY\nVuBLS0uxsbG5armdnR0AZWVl1913zJgxNb5OSEigXbt2vPHGG6xatar647evd44r57nROa7Hy8u5\n1vvUFW9vPa9vaTQmlknjYnk0JpZJ42J5NCaWydLGxbACb29vT0VFxVXLr5TqK0X+Vo0YMYJ33nmH\nHTt2VBd4e3t7ysvLr7l9WVlZrc8BkJ9/zpCphLy9XcjLO1vv55Xr05hYJo2L5dGYWCaNi+XRmFgm\nI8bFbDbd8KaxYc/Ae3t7X/MRlry8PAB8fHxqdTyz2Yyvry/FxcU1zlFRUXHVs/Hl5eUUFRXV+hwi\nIiIiIkYzrMCHhoZy/Phxzp8/X2P5vn37qtfXRkVFBT///DMeHh7Vy8LCwgA4cOBAjW0PHDhAZWVl\n9XoRERERkYbCsAKfkJBARUUFS5curV5WXl7OihUriImJqX7BNTs7m8zMzBr7FhQUXHW8f/zjH5SV\nlXH//fdXL4uPj8fd3Z0FCxbU2HbhwoU4OjrSvXv3uryku2JHWg6/n7WdR363mt/P2s6OtByjI4mI\niIiIgQx7Bj4qKoqEhARmzJhBXl4eQUFBrFy5kuzsbKZNm1a93csvv8zu3bs5dOhQ9bJevXrRv39/\n2rdvj62tLbt27WLdunXExsYyYMCA6u3s7e15/vnneeONN3jhhRfo1q0bycnJrFmzhpdeeumGHwJl\nCXak5TDni4OUX6wEIP9MGXO+OAhA1wg/I6OJiIiIiEEMK/AA06dP54MPPmD16tUUFxcTEhLC7Nmz\niY2NveF+AwcOZM+ePXz55ZdUVFTQokULnn32WZ5++mmsrWte0qhRo7CxseHTTz9l06ZN+Pv78+qr\nr5KYmHg3L61OrPg6s7q8X1F+sZIVX2eqwIuIiIg0Uaaq632MqVxTfc5C84u3vrruuk9feaBeMsj1\nabYAy6RxsTwaE8ukcbE8GhPLpFlopFa8XK89zaWHS+2nvxQRERGRxkEF3oIN6dEGW+urh6i84hJZ\neecMSCQiIiIiRlOBt2BdI/wY0y8UL1c7TFy+Iz/4/mCsrc28OTeF1Mx8oyOKiIiISD0z9CVWubmu\nEX50jfCr8fzVfR39+XBZKjOX7WNE73b0iQs0OKWIiIiI1BfdgW+APF3teWV0DFFtmrFg4xHmrT/E\npcrKm+8oIiIiIg2eCnwDZW9rzYQhHUnoEsRXe37ig6WplJReNDqWiIiIiNxlKvANmNls4vFebRnb\nL5SDPxTy5rwUThVdMDqWiIiIiNxFKvCNQPeo5vz2iU4UnytjypxkjmQVGR1JRERERO4SFfhGIqyl\nB68mxuFkb807C/eyIy3H6EgiIiIicheowDcifp6OvJoYR9sWbvzts3RWbj1GpT5oV0RERKRRUYFv\nZJwdbPjtE53oFunPZ9+e4K+r0yivuGR0LBERERGpI5oHvhGytjLzVL9Q/L0cWbY5k9PFpTz/WEfc\nnO2MjiYiIiIid0h34Bspk8lEvy4teW5IR346fY4/JSVz8tQ5o2OJiIiIyB1SgW/kYtp7M2lULJWV\nVbw5L4V9R08bHUlERERE7oAKfBPQ0s+F18Z0xs/DkQ+Xp7L+u5NU6eVWERERkQZJBb6J8HCx45VR\nMcS082bRpiPMXX+Yi5cqjY4lIiIiIrWkAt+E2Nla8T+DO9AvPogte39i5tJ9lJRWGB1LRERERGpB\nBb6JMZtMDOvZlqf6h3LwxyKmzk3hVGGJ0bFERERE5BapwDdR90c256XhnThzvpwpSSkcPllkdCQR\nERERuQUq8E1YSJAHkxPjcHKw4Z2Fe9m+/2ejI4mIiIjITajAN3G+no5MToylfaA7/1ibwfKvM6nU\nDDUiIiIiFksFXnCyt+HFx6PoHtWctTt+4C+rDlBWccnoWCIiIiJyDSrwAoC1lZkxCSE88UBbUg7l\n8fb8PRSdKzM6loiIiIj8FxV4qWYymXjoniAmPNaRn/NL+NOcZH7MPWt0LBERERH5P1Tg5SrR7byZ\nNDoGgGnz9rD3SJ7BiURERETkChV4uaYgXxdeGxOHv5cjHy/fz5e7fqRKL7eKiIiIGE4FXq7L3dmO\nl0fFEBvizZLNR5nz5SEuXqo0OpaIiIhIk6YCLzdkZ2PFM4M6MODelmzdl837S/ZxvrTC6FgiIiIi\nTZYKvNyU2WRiSPc2jHs4jMMni5iSlEJuYYnRsURERESaJBV4uWX3dfTn9yOiOX+hgilzkjn0Y6HR\nkURERESaHBV4qZX2ge5MTozF1cmWGYu+Z1tqttGRRERERJoUFXipNR8PR159MpaQIHf++e+DLN1y\nlErNUCMiIiJSL1Tg5bY42tvwm2FR9OzUnC92/sislQcoK79kdCwRERGRRk8FXm6btZWZJx8KYUTv\nduw9ksdb8/dQeLbM6FgiIiIijZoKvNwRk8nEg50Def6xSHIKS5iSlMwPOWeNjiUiIiLSaKnAS52I\natuMP4yOxWyCafNT2HM4z+hIIiIiIo2SCrzUmUAfZyYnxtGimTOfrNjPF7t+oEovt4qIiIjUKRV4\nqVNuzna8PDKauFAflm7O5J9fHOTipUqjY4mIiIg0GtZGB5DGx9bGiqcfjcDP05HPvj3B6aILPDu4\nI84ONkZHExEREWnwdAde7gqzycTg7q0ZPzCcoz8VMzUpmZyCEqNjiYiIiDR4KvByV3WN8OP3I6Ip\nKbvI1KRkMn4oNDqSiIiISIOmAi93XbsAdyYnxuHmbMd7i79n675soyOJiIiINFgq8FIvvN0d+MPo\nWEJbevCvLw6yZPNRKis1Q42IiIhIbanAS71xtLfmN8Mi6RXTgi93/cgnK/dTWn7R6FgiIiIiDYoK\nvNQrK7OZJ/uGMLJPO74/epq35u2h4Eyp0bFEREREGgwVeDFEn7hAXhgaxamiC/wpKZnjP58xOpKI\niIhIg6ACL4aJbOPFH56Mxdps5u35e0g5dMroSCIiIiIWz9ACX15ezjvvvEO3bt2IjIzk8ccfZ8eO\nHbU+zvjx4wkJCWHq1KlXrQsJCbnmn4ULF9bFJcgdCvB2ZvKYOAJ9nPlk5QHW7jhBVZVebhURERG5\nHkM/ifWVV15h/fr1JCYm0rJlS1auXMn48eOZO3cu0dHRt3SMLVu2kJycfMNtunXrxiOPPFJjWVRU\n1G3nlrrl5mTLxJHRfPrvgyz/+hg5BSWMSQjF2kq/IBIRERH5b4YV+NTUVNauXcukSZMYO3YsAIMG\nDWLAgAHMmDGD+fPn3/QY5eXlTJs2jXHjxvHRRx9dd7vWrVvz6KOP1lV0uQtsrK341cBw/DwdWf3N\ncfKKSnlucAdcHG2NjiYiIiJiUQy7xfnll19iY2PDsGHDqpfZ2dkxdOhQUlJSOHXq5s9DJyUlUVpa\nyrhx4266bWlpKWVlZXeUWe4uk8nEo92C+dUj4RzLPsPUpBR+zj9vdCwRERERi2JYgc/IyCA4OBgn\nJ6cayyMjI6mqqiIjI+OG++fl5TFr1ixefPFFHBwcbrjtsmXL6NSpE5GRkQwcOJANGzbccX65e+LD\n/Zg4MprS8otMTUoh/USB0ZFERERELIZhBT4vLw8fH5+rlnt7ewPc9A78e++9R3Bw8E0fjYmOjubF\nF19k1qxZvP7665SXlzNhwgQ+//zz2w8vd13bFm5MTozDw9WO9xbvY8v3PxkdSURERMQiGPYMfGlp\nKTY2Nlctt7OzA7jh4y6pqamsWrWKuXPnYjKZbnieRYsW1fh68ODBDBgwgHfeeYeHH374pvv/Ny8v\n51ptX5e8vV0MO7cRvL1deO83PZg+N5mkLw9RXHKRpwZGYGWu3ZjdTU1tTBoKjYvl0ZhYJo2L5dGY\nWCZLGxfDCry9vT0VFRVXLb9S3K8U+f9WVVXF1KlT6du3L3FxcbU+r6OjI8OHD+fdd9/l2LFjtGnT\nplb75+efo7Ky/qc59PZ2IS/vbL2f1xI880g4i51sWb01kx+yi/nVI+HY2xo6gRLQtMfEkmlcLI/G\nxDJpXCyPxsQyGTEuZrPphjeNDXuExtvb+5qPyeTl5QFc8/EagA0bNpCamsqIESPIysqq/gNw7tw5\nsrKyKC0tveG5/f39ASguLr6TS5B6YmU2M/LB9ozu257UzHymzdtDwZkbj7GIiIhIY2VYgQ8NDeX4\n8eOcP19zlpF9+/ZVr7+W7OxsKisrGTNmDL17967+A7BixQp69+7N7t27b3jukydPAuDp6XmnlyH1\n6IGYAH4zLJLTxRf405xkjmWfMTqSiIiISL0z7DmEhIQEPv30U5YuXVo9D3x5eTkrVqwgJiYGX19f\n4HJhv3DhQvWjLg888AABAQFXHe+5556jV69eDB06lIiICAAKCgquKumFhYUsWLCAgIAAWrVqdfcu\nUO6KDq29+MPoWGYuS+XtBXv45YBwOode+7c1IiIiIo2RYQU+KiqKhIQEZsyYQV5eHkFBQaxcuZLs\n7GymTZtWvd3LL7/M7t27OXToEABBQUEEBQVd85iBgYH06dOn+uv58+ezadMmevbsSfPmzcnNzWXx\n4sUUFBTwySef3N0LlLumhbczk8fE8fHy/fx51QFyurdmQNeWtX4hWURERKQhMvRNwOnTp/PBBx+w\nevVqiouLCQkJYfbs2cTGxtbJ8aOjo9mzZw9Lly6luLgYR0dHOnXqxNNPP11n5xBjuDra8vsRnfjX\nFwdZufUYOfkljO0Xio21YU+FiYiIiNQLU1VVVf1PqdKAaRYay1JVVcXn355g5bbjtAtwY8KQjrg4\n2tbLuTUmlknjYnk0JpZJ42J5NCaWSbPQiNQxk8nEwPuCeebRCE7knGVKUjLZp8/ffEcRERGRBkoF\nXhqFe8J8mTgymrKKSqbOTSHteIHRkURERETuChV4aTTaNHdjcmIsXq52vL9kH5v3ZBkdSURERKTO\nqcBLo9LMzYFJo2Pp0NqTuesPs2DjYUPeWRARERG5W1TgpdFxsLPm+cci6ds5kI3JWXy4PJULZReN\njiUiIiJSJ1TgpVEym00M792OJx8K4cCxAt6cl8Lp4gtGxxIRERG5Yyrw0qj1im7Bi09EUXCmjClJ\nKWRmFxsdSUREROSOqMBLoxfRypNXn4zFzsbM2/P3sjsj1+hIIiIiIrdNBV6ahObNnJicGEewvwt/\nWZ3Gmm+Oo88wExERkYZIBV6aDBdHW14aHs29HfxY9c1x/vZZOhUXLxkdS0RERKRWrI0OIFKfbKzN\njHs4DH8vR5Z/fYy84gv8ekgkrk62RkcTERERuSW6Ay9Njslk4uGurXh2UAdO5p5jSlIyP+WdMzqW\niIiIyC1RgZcmKy7Uh5dHxVBxsZI356Vw4Fi+0ZFEREREbkoFXpq0YH9XXhsTRzM3B95fuo9NKVlG\nRxIRERG5IRV4afI8Xe2ZNDqGqDbNmL/hMPPXH+ZSZaXRsURERESuSQVeBLC3tWbCkI48dE8gm/Zk\nMXNZKiWlF42OJSIiInIVFXiR/zCbTTzxQDvGJISQcaKQafNSyCu6YHQsERERkRpU4EX+S49OLfjt\n41EUni1jSlIyR7OKjY4kIiIiUk0FXuQawlp58mpiLA521kxfuJedaTlGRxIREREBVOBFrsvfy4nJ\niXG0ae7K7M/SWbXtGFVVVUbHEhERkSZOBV7kBpwdbPjd8E7c19GPNdtP8Nc1aZRXXDI6loiIiDRh\n1kYHELF01lZmftE/DH8vJ5ZtySS/uJQJj0Xi7W10MhEREWmKdAde5BaYTCb6x7fkucEdOHnqHFPm\nfMeJn88YHUtERESaIBV4kVqIDfHhldExXKqsYuJHW0nNPG10JBEREWliVOBFaqmVnyuvjemMfzNn\nZi5LZUPySb3cKiIiIvVGBV7kNni42PH2c93o1LYZCzceYd6Gw1yqrDQ6loiIiDQBKvAit8nezprn\nhnSkX5cgNu/5iQ+WplJSWmF0LBEREWnkVOBF7oDZZGJYr7aM7RfKwR8KmTo3hVNFF4yOJSIiIo2Y\nCrxIHege1ZzfPdGJM+fLmTInmcMni4yOJCIiIo2UCrxIHQlt6cHkxDic7K2ZsWgv3x742ehIIiIi\n0gipwIvUIV9PR15NjKNtCzf+/nkGK7ZmUqkZakRERKQOqcCL1DFnBxt++0Qn7o/05/Nvf+Avq9Mo\nr7hkdCwRERFpJKyNDiDSGFlbmRnbLxR/LyeWbj5KfnEpzz/WETdnO6OjiYiISAOnO/Aid4nJZCKh\nSxAThnTkp9Pn+FNSMj/mnjU6loiIiDRwKvAid1l0e28mjYqlqgqmzd/D90dPGx1JREREGjAVeJF6\n0NLPhcmJcfh5OvLRslTW7/6RKr3cKiIiIrdBBV6knni42PHKqBhi2nuz6KujJK07xMVLlUbHEhER\nkQZGBV6kHtnZWPE/gzvwcNeWfP19Nu8v2cf50gqjY4mIiEgDogIvUs/MJhOP9WjDuIfDOHyyiDfn\npnCqsMToWCIiItJAqMCLGOS+jv68NLwTZ86XMyUphUM/FhodSURERBoAFXgRA4UEeTB5TBzODjbM\nWPQ92/f/bHQkERERsXAq8CIG8/Vw5NXEWNoHuvOPtRks/zqTSs1QIyIiItehAi9iAZzsbXjx8Sh6\ndGrO2h0/8OeVByiruGR0LBEREbFAKvAiFsLaykziQyEMf6Atew7n8db8PRSeLTM6loiIiFgYFXgR\nC2Iymeh7TxC/fiySnPwSpiQl80POWaNjiYiIiAVRgRexQJ3aNWPS6BgA3pq/h71H8gxOJCIiIpbC\n0AJfXl7OO++8Q7du3YiMjOTxxx9nx44dtT7O+PHjCQkJYerUqddcv3TpUvr160fHjh156KGHmD9/\n/p1GF7nrgnxdeG1MHM2bOfLx8v18uetHqvRyq4iISJNnaIF/5ZVXmDNnDo888givvvoqZrOZ8ePH\ns3fv3ls+xpYtW0hOTr7u+kWLFjF58mTat2/Pa6+9RlRUFG+88QaffvppXVyCyF3l7mzHxJExxIZ4\ns2TzUeZ8eZCLlyqNjiUiIiIGMqzAp6amsnbtWl566SUmTpzIE088wZw5c/D392fGjBm3dIzy8nKm\nTZvGuHHjrrm+tLSU999/n969ezNz5kwef/xxpk+fzsCBA/n44485e1bPFovls7Ox4plBHRhwbyu2\n7vuZ9xZ/z7kLFUbHEhEREYMYVuC//PJLbGxsGDZsWPUyOzs7hg4dSkpKCqdOnbrpMZKSkigtLb1u\ngd+1axdFRUWMHDmyxvJRo9qyFuAAACAASURBVEZx/vx5tm7demcXIVJPzCYTQ7q35pcDwjj6UzFT\n56aQW1BidCwRERExgGEFPiMjg+DgYJycnGosj4yMpKqqioyMjBvun5eXx6xZs3jxxRdxcHC45jbp\n6ekAdOjQocbyiIgIzGZz9XqRhuLeDv68NDya8xcqmJKUzMEfCo2OJCIiIvXMsAKfl5eHj4/PVcu9\nvb0BbnoH/r333iM4OJhHH330huewtbXF3d29xvIry27lLr+IpWkf6M7kMXG4Otny7uLv2ZaabXQk\nERERqUfWRp24tLQUGxubq5bb2dkBUFZ2/Q+wSU1NZdWqVcydOxeTyVTrc1w5z43OcT1eXs613qeu\neHu7GHZuuTajxsTb24X3XuzJ20nf8c9/H+TMhYsk9g/HbL7+34emRH9XLI/GxDJpXCyPxsQyWdq4\nGFbg7e3tqai4+kW8K6X6SpH/b1VVVUydOpW+ffsSFxd303OUl5dfc11ZWdl1z3Ej+fnnqKys/6n8\nvL1dyMvTS7eWxBLG5NlHI1iw8QjLNx/lWFYRvxoYgZ2tlaGZjGYJ4yI1aUwsk8bF8mhMLJMR42I2\nm25409iwR2i8vb2v+QhLXt7lD6y51uM1ABs2bCA1NZURI0aQlZVV/Qfg3LlzZGVlUVpaWn2OiooK\nioqKahyjvLycoqKi655DpKGwtjLzZN/2jOjTju+Pnmba/BQKz9b+N0siIiLScBhW4ENDQzl+/Djn\nz5+vsXzfvn3V668lOzubyspKxowZQ+/evav/AKxYsYLevXuze/duAMLCwgA4cOBAjWMcOHCAysrK\n6vUiDZnJZOLBuECefyyS3MIL/GnOd5zIOWN0LBEREblLDHuEJiEhgU8//ZSlS5cyduxY4PKd8RUr\nVhATE4Ovry9wubBfuHCBNm3aAPDAAw8QEBBw1fGee+45evXqxdChQ4mIiAAgPj4ed3d3FixYQLdu\n3aq3XbhwIY6OjnTv3v0uX6VI/Ylq24xXR8cyc9k+3pq/h/EDIogN8TY6loiIiNQxwwp8VFQUCQkJ\nzJgxg7y8PIKCgli5ciXZ2dlMmzateruXX36Z3bt3c+jQIQCCgoIICgq65jEDAwPp06dP9df29vY8\n//zzvPHGG7zwwgt069aN5ORk1qxZw0svvYSrq+vdvUiRehbg48zkMZ35aHkqn6zcz9CebejXJeiG\nL3uLiIhIw2JYgQeYPn06H3zwAatXr6a4uJiQkBBmz55NbGxsnZ1j1KhR2NjY8Omnn7Jp0yb8/f15\n9dVXSUxMrLNziFgSNydbJo6I5tN/Z7BsSyY5+SUkJoRgbWXYE3MiIiJSh0xVVVX1P6VKA6ZZaOQK\nSx+Tyqoq1nxznDXbTxAS6M5zQzri7HDtaVUbE0sfl6ZIY2KZNC6WR2NimTQLjYjUG7PJxKD7WzN+\nYDiZ2cVMSUrm5/zzN99RRERELJoKvEgj1zXCj4kjYrhQdpGpSSlknCgwOpKIiIjcARV4kSagbYAb\nkxPjcHex470l+9i6L9voSCIiInKbVOBFmghvdwf+MDqWsFYe/OuLgyz+6ogh73OIiIjInVGBF2lC\nHO2teWFoJL1jAli3+yQfr9hPaflFo2OJiIhILajAizQxVmYzo/q2Z9SD7dmXeZq35u2h4Eyp0bFE\nRETkFqnAizRRvWMD+M2wKE4VXeBPSckc//mM0ZFERETkFqjAizRhHVt78YcnY7GxMvP2/D0kHzxl\ndCQRERG5CRV4kSYuwNuZyYlxBPo6M2vVAdbuOIE+301ERMRyqcCLCK5OtkwcEU18uC/Lvz7GP9Zm\nUHGx0uhYIiIicg21LvA//PADW7durbFs3759PPPMMwwfPpzFixfXWTgRqT821laMHxjOoG7BfHsg\nh3cX7eVsSbnRsUREROS/WNd2hxkzZlBUVET37t0BKCgoYPz48ZSUlGBnZ8f//u//4uXlRZ8+feo8\nrIjcXSaTiUe6BePr6cg/1mYwNSmFF4ZF4u/lZHQ0ERER+Y9a34E/cOAA9957b/XXa9eu5dy5c6xY\nsYIdO3YQFRXFnDlz6jSkiNSvLuG+vDwymtLyi0xJSiHtRIHRkUREROQ/al3gCwoK8PHxqf5627Zt\nxMTE0L59e2xtbenfvz+ZmZl1GlJE6l+bFm5MHhOHp6sd7y/ex5a9PxkdSURERLiNAu/g4MDZs2cB\nuHTpEikpKcTFxVWvt7e359y5c3WXUEQM08zNgT+MjiUi2JOkdYdYuPEIlZWaoUZERMRItS7w7dq1\nY9WqVRQWFrJkyRJKSkq47777qtf/9NNPeHp61mlIETGOg501zw/tSJ+4ADYkn+TD5alcKLtodCwR\nEZEmq9YFfty4cRw+fJh7772XN954g7CwsBp34Ldv3054eHidhhQRY1mZzYzs054n+7bnwLECps3b\nQ35xqdGxREREmqRaz0LTs2dP5syZw6ZNm3B2dmb06NGYTCYACgsL8fPzY9CgQXUeVESM1ysmAG8P\nB/686gB/Skrm+cciad3c1ehYIiIiTYqpSh+5WCv5+ecMeQbY29uFvLyz9X5eub6mPCY/nT7PzKX7\nKD5fzriHw7gnzNfoSNWa8rhYKo2JZdK4WB6NiWUyYlzMZhNeXs7XX18XJ7l48SLr1q1jyZIl5OXl\n1cUhRcSCtWjmxOQxcbT0c+Evq9P4bPtxdC9ARESkftT6EZrp06eza9culi9fDkBVVRVPPfUUycnJ\nVFVV4e7uzpIlSwgKCqrzsCJiOVwdbfn98Gj+9UUGK7cdJ6eghLH9wrCxrpP7AiIiInIdtf5Ju23b\nthovrX711Vd89913jBs3jnfffReA2bNn111CEbFYNtZmfjkgnMHdW7MjLZd3Fu3lTEm50bFEREQa\ntVrfgc/JyaFly5bVX2/evJmAgABeeuklAI4cOcJnn31WdwlFxKKZTCYG3tsKP09H/v55OlPmJPPC\nsChaNHMyOpqIiEijVOs78BUVFVhb///ev2vXLu69997qrwMDA/UcvEgT1DnUh5dHxlB+sZI35yZz\n4Hi+0ZFEREQapVoXeD8/P/bu3Qtcvtt+8uRJOnfuXL0+Pz8fR0fHuksoIg1G6+auvJYYh5erAx8s\nSWXzniyjI4mIiDQ6tX6E5uGHH2bWrFkUFBRw5MgRnJ2d6dGjR/X6jIwMvcAq0oR5udkzaXQMs9ek\nMXf9YX7OL+GJ3m2xMuvlVhERkbpQ65+oTz/9NIMHD+b777/HZDLx9ttv4+p6+YNczp49y1dffUXX\nrl3rPKiINBwOdtb8+rFI+nYOZGNKFh8u28+FsotGxxIREWkUan0H3tbWljfffPOa65ycnPjmm2+w\nt7e/42Ai0rCZzSaG926Hn6cj89Yf5s15KbzwWCTN3B2MjiYiItKg1envtM1mMy4uLtjY2NTlYUWk\nAesZ3YIXn4ii4EwZU5KSOfpTsdGRREREGrTbKvAlJSV8+OGHDBw4kOjoaKKjoxk4cCAfffQRJSUl\ndZ1RRBq4iFaeTE6Mxd7WmukL9rIrPdfoSCIiIg1WrQt8UVERw4YNY9asWeTn5xMWFkZYWBj5+fl8\n8sknDBs2jKKioruRVUQaMH8vJ15NjKW1vwt/XZPG6m+OU1VVZXQsERGRBqfWBf7DDz/k2LFjvPba\na2zbto0FCxawYMECtm3bxuuvv87x48f5+OOP70ZWEWngXBxt+d3waO7r4Mfqb44z+7N0Ki5eMjqW\niIhIg1LrAv/VV18xbNgwRo0ahZWVVfVyKysrRo4cyWOPPcbGjRvrNKSINB421mZ+8XAYj/Voza70\nXKYv3Evx+XKjY4mIiDQYtS7wp0+fJiws7Lrrw8PDOX369B2FEpHGzWQy8XDXVjw7qAMnc88xZU4y\nWXnnjI4lIiLSINS6wDdr1oyMjIzrrs/IyKBZs2Z3FEpEmoa4UB9eHhXDxcpK3pybQmpmvtGRRERE\nLF6tC3yvXr1YtmwZixYtorKysnp5ZWUlixcvZvny5TzwwAN1GlJEGq9gf1deS4zDx92Bmcv2sTH5\npNGRRERELJqpqpbTQBQWFjJ8+HB+/PFHPD09CQ4OBuD48eMUFBQQFBTEokWL8PDwuCuBjZaff47K\nyvqfOcPb24W8vLP1fl65Po1J3Sotv8jsNel8f/Q0D8S0YESfdliZaz/TrcbF8mhMLJPGxfJoTCyT\nEeNiNpvw8nK+/vraHtDDw4Ply5fzq1/9Cnd3d/bv38/+/fvx8PDgV7/6FcuXL2+05V1E7h57W2sm\nDOlIwj1BfLXnJ2YuTaWk9KLRsURERCxOre/A38yiRYtISkri3//+d10e1mLoDrxcoTG5e7buy2bu\nukP4ejrywtBIvN0dbnlfjYvl0ZhYJo2L5dGYWKZGcQf+ZgoLCzl+/HhdH1ZEmpDuUc357eNRFJ0t\nY0pSMkezio2OJCIiYjHqvMCLiNSFsFaeTB4Th4OdNdMX7mFHWo7RkURERCyCCryIWCw/T0cmJ8bR\nprkbf/ssnZVbj1FZt0/9iYiINDgq8CJi0ZwdbPjd8E50i/Tns29P8NfVaZRXXDI6loiIiGGsjQ4g\nInIz1lZmnuoXir+XI8s2Z5J/ppRfD+mIm7Od0dFERETq3S0V+H/+85+3fMA9e/bcdhgRkesxmUz0\n69ISXw9HZn+WxpSkZJ4fGkWgz/Xf0hcREWmMbqnAv/3227U6qMlkuq0wIiI3E9Pem0mjYpm5bB9v\nzkvhmUciiGrbzOhYIiIi9eaWCnxSUtJdOXl5eTkzZ85k9erVnDlzhtDQUF588UW6du16w/3WrFnD\nsmXLyMzMpLi4GB8fH7p06cKECRNo0aJFjW1DQkKueYz//d//ZcSIEXV2LSJSf1r6ufDamM58uCyV\nD5enMvyBdjg5WLNy6zEKzpTh6WrHkB5t6BrhZ3RUERGROndLBf6ee+65Kyd/5ZVXWL9+PYmJibRs\n2ZKVK1cyfvx45s6dS3R09HX3O3jwIL6+vvTo0QM3Nzeys7NZsmQJW7ZsYc2aNXh7e9fYvlu3bjzy\nyCM1lkVFRd2VaxKR+uHhYscro2L42+fpLNx0BLPJVD1DTf6ZMuZ8cRBAJV5ERBodw15iTU1NZe3a\ntUyaNImxY8cCMGjQIAYMGMCMGTOYP3/+dfedOHHiVct69+7NkCFDWLNmDePGjauxrnXr1jz66KN1\nml9EjGdna8Wzgzsw4f2tlJbXnJmm/GIlK77OVIEXEZFGx7BpJL/88ktsbGwYNmxY9TI7OzuGDh1K\nSkoKp06dqtXxmjdvDsCZM2euub60tJSysrLbDywiFslsMl1V3q/IP6O/8yIi0vgYVuAzMjIIDg7G\nycmpxvLIyEiqqqrIyMi46TGKiorIz89n//79TJo0CeCaz88vW7aMTp06ERkZycCBA9mwYUPdXISI\nWAQv1+tPJzl7TRr7jp7m4qXKekwkIiJy9xj2CE1eXh6+vr5XLb/y/Pqt3IF/6KGHKCoqAsDd3Z3X\nX3+d+Pj4GttER0fTv39/AgIC+Pnnn0lKSmLChAm8++67DBgwoA6uRESMNqRHG+Z8cZDyi/+/pFtb\nmWjXwo39x/LZmZ6Ls4MNnUN9iI/wpU0LN8yaLUtERBoowwp8aWkpNjY2Vy23s7t8J+1WHnf5+OOP\nKSkp4fjx46xZs4bz589ftc2iRYtqfD148GAGDBjAO++8w8MPP1zrKS+9vIybc9rb28Wwc8u1aUws\nwyM9XXB1sSfpiwxOF16gmYcDif3C6BkbSMXFSvYeOsXXe7LYfiCHzXt/wsfDgR4xAfSICaCln6vR\n8ZsE/V2xTBoXy6MxsUyWNi6GFXh7e3sqKiquWn6luF8p8jfSuXNnAHr06EHv3r0ZOHAgjo6OjB49\n+rr7ODo6Mnz4cN59912OHTtGmzZtapU7P/8clZVVtdqnLnh7u5CXd7bezyvXpzGxLBFB7rz9dNca\n43Llv8E+TgQnhPBErzbsPZLHzvRcln91lKWbjhDg7UzXCF/uCfPFy83eyEtotPR3xTJpXCyPxsQy\nGTEuZrPphjeNDSvw3t7e13xMJi8vDwAfH59aHS8wMJCIiAg+++yzGxZ4AH9/fwCKi4trdQ4Radgc\n7Ky5t4M/93bwp/h8OckHT7EzLYelWzJZuiWT9oHuxIf7Ehfqg7PD1b8hFBERsQSGFfjQ0FDmzp3L\n+fPna7zIum/fvur1tVVaWsqFCxduut3JkycB8PT0rPU5RKRxcHOypXdsAL1jAzhVdIFdaTnsTM8l\nad0h5m84TMfWXsRH+BLVthl2NlZGxxUREalm2Cw0CQkJVFRUsHTp0upl5eXlrFixgpiYmOoXXLOz\ns8nMzKyxb0FBwVXHO3DgAAcPHiQiIuKG2xUWFrJgwQICAgJo1apVHV2NiDRkPu4ODLwvmCm/7MIf\nx3bmwbhAfsg9y19Wp/Gbj77hb5+lc+BYPpcqNZONiIgYz7A78FFRUSQkJDBjxgzy8vIICgpi5cqV\nZGdnM23atOrtXn75ZXbv3s2hQ4eql/Xq1Yt+/frRvn17HB0dOXr0KMuXL8fJyYlnn322erv58+ez\nadMmevbsSfPmzcnNzWXx4sUUFBTwySef1Ov1iojlM5lMtPRzoaWfC0N7tuHwySJ2pueQfDCPHWk5\nuDra0DnMl/hwX1o3d631S/AiIiJ1wbACDzB9+nQ++OADVq9eTXFxMSEhIcyePZvY2Ngb7jdy5Eh2\n7NjBxo0bKS0txdvbm4SEBJ599lkCAwOrt4uOjmbPnj0sXbqU4uJiHB0d6dSpE08//fRNzyEiTZvZ\nbCK0pQehLT0Y9WDI5eko03L4+vtsNqVk4e1uT5dwP7pG+OLv5XTzA4qIiNQRU1VVVf1PqdKAaRYa\nuUJjYpnu9rhcKLtIyqE8dqXnkP5DIVVVEOTrTHy4H13CffFwufkMWk2N/q5YJo2L5dGYWCbNQiMi\n0sA52FnTLdKfbpH+FJ0r47uMU+xMz2HJ5qMs3XyUkCB34iP8iAvxxtFeM9mIiEjdU4EXEblN7s52\nPNg5kAc7B5JTUMKu9Fx2puXwry8OMm/9ISLbNCM+3Jeotl7YWGsmGxERqRsq8CIidcDP05FHuwXz\nyH2tOJFzlp1puezOyGXP4Twc7KyIae9NfIQfYUEemM16+VVERG6fCryISB0ymUwE+7sS7O/KEw+0\nJePHQnal5ZJy+BTb9+fg5mTLPWG+xEf40srPRTPZiIhIranAi4jcJWaziYhWnkS08mR03/akZuaz\nMz2XzXuz2JB8El8PB7qE+9I1wg9fT0ej44qISAOhAi8iUg9sbayIC/UhLtSHktIKkg/lsTMth8+2\nn2DN9hO08nMhPsKPe8J8cHfWTDYiInJ9KvAiIvXM0d6G7lHN6R7VnMKzZexKz2VXei6LNh1h8VdH\nCGvpQZdwX2Lb++Bor3+mRUSkJv1kEBExkIeLHQldgkjoEkT26fOXZ7JJz+Gf/z7I3HWHiWrrRXy4\nH5FtvLCxNhsdV0RELIAKvIiIhWjezInB3Vsz6P5gjv18hp1puXyXkUvKoTwc7KyJC7k8k01IoLtm\nshERacJU4EVELIzJZKJNczfaNHdjeO+2ZJwoZGd6LrsPnmJb6s94uNhxT5gP8eF+BPk6ayYbEZEm\nRgVeRMSCWZnNdGjtRYfWXjxZcYl9R0+zMy2XjclZrNt9En8vR7qE+xIf7ouPh2ayERFpClTgRUQa\nCDsbK+4J8+WeMF/OXagg+dApdqblsmrbcVZtO06b5q50Cb+83tXJ1ui4IiJyl6jAi4g0QM4ONvTs\n1IKenVpQcKb0Py+/5rJg4xEWbTpKeCsP4iN8iW7njYOd/qkXEWlM9K+6iEgD5+lqT7/4lvSLb8lP\neefY+Z9pKf/+eQa21ofo1K4ZXcJ96djaC2srzWQjItLQqcCLiDQiLbydeayHM0O6tybzpzPsSM/h\nu4xT7M44hZO9NXGhPsSH+9Iu0B2zXn4VEWmQVOBFRBohk8lE2wA32ga4MaJ3O9JPFLAzLZcdaTl8\n/X02nq52dAnzpUu4L4E+mslGRKQhUYEXEWnkrK3MRLZpRmSbZpSVX2LvkTx2puey/ruTfLHrR1o0\nc6qeyaaZu4PRcUVE5CZU4EVEmhA7WyviI/yIj/DjbEk5yQdPsSM9lxVbj7Fi6zHaBrgRH+5L51Af\nXBw1k42IiCVSgRcRaaJcHG3pFRNAr5gAThddYFdGLjvTcpm3/jALNx4hItiT+PDLM9nY2VoZHVdE\nRP5DBV5ERGjm7sDDXVvRP74lWXnn2ZmWw66MXGZ/lo+tjZmYdt50CfclIthTM9mIiBhMBV5ERKqZ\nTCYCfZwJ9GnLYz3bcORkEbvSc/nu4Cl2pufi7GBD51Af4iN8adPCTTPZiIgYQAVeRESuyWwyERLk\nQUiQByMfbM+BYwXsTM9h+/6f2bz3J7xc7YmPuDyTTYC3s9FxRUSaDBV4ERG5KWsrM53aNaNTu2Zc\nKLtYPZPNFzt/ZO2OHwjwdr5c5sN88XKzNzquiEijpgIvIiK14mBnzb0d/Lm3gz/F58v5LuPyJ78u\n25LJsi2ZtA90Jz7cl7hQH5wdbIyOKyLS6KjAi4jIbXNzsqVPXCB94gI5VVjCrvRcdqbnkrTuEPM3\nHKZjay/6xrci2NcJOxvNZCMiUhdU4EVEpE74eDgy8L5gBtzbih9zz7EzPYdd6blMn5eMna0VMe28\n6RrhS1grD6zMmslGROR2qcCLiEidMplMtPRzoaWfC8N6tiX3bBnrvj1O8sE8dqTl4OpoQ+ewy5/8\n2rq5KybNZCMiUisq8CIicteYzSYi23rj72bPqAdD2H8sn51pOXz9fTabUrLwdrenS7gfXSN88fdy\nMjquiEiDoAIvIiL1wsbaTEx7b2Lae1NSepE9h/PYlZ7D2h0n+PzbEwT5OhMf7keXcF88XOyMjisi\nYrFU4EVEpN452lvTLdKfbpH+FJ0rY3fGKXal57Bk81GWbj5KSJA78RF+xIZ442SvmWxERP4vFXgR\nETGUu7MdfTsH0rdzIDkF/5nJJi2Hf31xkHnrD9GxtRddI/yIbOOFrWayERFRgRcREcvh5+nIo92C\neeS+VpzIOcvOtFx2Z+Sy98hpHOysiGnvTXyEH2FBHpjNevlVRJomFXgREbE4JpOJYH9Xgv1deeKB\ntmT8WMjOtBxSDuWxfX8Obk623BPmS3yEL638XDSTjYg0KSrwIiJi0cxmExGtPIlo5cmTfS+RmpnP\nzvRcNu/NYkPySXw9HOgS7kvXCD98PR2NjisictepwIuISINha2NFXKgPcaE+nC+tIOVQHjvTcvhs\n+wnWbD9BKz8X4iP8uCfMB3dnzWQjIo2TCryIiDRITvY2dI9qTveo5hSeLbv88mt6Dos2HWHxV0cI\na+lBl3BfYtv74GivH3ci0njoXzQREWnwPFzsSOgSREKXILJPn2dnei670nP4578PMnfdYaLaehEf\nfnkmGxtrs9FxRUTuiAq8iIg0Ks2bOTGke2sG3x/Msewz7EzP5buMXFIO5eFgZ01cyOWZbEIC3TWT\njYg0SCrwIiLSKJlMJtq0cKNNCzeG925LxolCdqTlsvvgKbal/oy7sy1dwn2JD/cjyNdZM9mISIOh\nAi8iIo2eldlMh9ZedGjtRVnFJfYdPc3OtFw2JmexbvdJ/L0c/1PmffHx0Ew2ImLZVOBFRKRJsbOx\n4p4wX+4J8+XchQqSD51iZ1ouq7YdZ9W247Rp7kqX8MvrXZ1sjY4rInIVFXgREWmynB1s6NmpBT07\ntaDgTCm70nPZkZbLgo1HWLTpKOGtPIiP8CW6nTcOdvqRKSKWQf8aiYiIAJ6u9vSLb0m/+JZk5Z27\nPC1lWi5//zwDW+tDdGrXjC7hvnRs7YW1lWayERHjqMCLiIj8lwBvZwJ6ODOke2uO/lT8n5lsTrE7\n4xRO9tbEhfoQH+5Lu0B3zHr5VUTqmQq8iIjIdZhMJtoFuNMuwJ0RvduRfqKAnWm57EjL4evvs/F0\ntaNLmC9dwn0J9NFMNiJSP1TgRUREboG1lZnINs2IbNOMsvJL7D2Sx870XNZ/d5Ivdv1Ii2ZO1TPZ\nNHN3MDquiDRihhb48vJyZs6cyerVqzlz5gyhoaG8+OKLdO3a9Yb7rVmzhmXLlpGZmUlxcTE+Pj50\n6dKFCRMm0KJFi6u2X7p0KZ9++ilZWVk0b96cxMRERo0adbcuS0REGjk7WyviI/yIj/DjTEk5KQdP\nsSM9lxVbj7Fi6zHatnAjPsKXuFAfXB01k42I1C1DC/wrr7zC+vXrSUxMpGXLlqxcuZLx48czd+5c\noqOjr7vfwYMH8fX1pUePHri5uZGdnc2SJUvYsmULa9aswdvbu3rbRYsW8cc//pGEhASeeuopkpOT\neeONNygrK+MXv/hFfVymiIg0Yq6OtvSKCaBXTACniy6wK+Pyy6/z1h9m4cYjRAR7Eh9+eSYbO1sr\no+OKSCNgqqqqqjLixKmpqQwbNoxJkyYxduxYAMrKyhgwYAA+Pj7Mnz+/VsdLS0tjyJAhTJw4kXHj\nxgFQWlpKjx49iI2NZdasWdXbvvTSS3z11Vd8/fXXuLi41Oo8+fnnqKys/2+Zt7cLef+vvTuPjqrM\n0zj+rUoq+1qhEkJ2IlkIkEAOTdgUF+yI2IDL2MriaMNoo91KT89BxumZaXvUOd3uaJ9BoceGY7uA\nRARbQYEWZR1BwpIEJBAghixkhYQsJHf+SFJtTAKBLFUhz+cczrHeujf13vxyvU9u3ve9Jef6/HOl\nc6qJc1JdnM9ArIlhGOSXVLPrcCG7s4soq6rDzWJmzDAb44aHkBRjdfhKNgOxLs5ONXFOjqiL2Wwi\nKMin0/cddgf+008/xWKxcM8999jb3N3dufvuu3nppZcoLi4mODi4y19vyJAhAFRVVdnbdu/eTUVF\nBffff3+bbWfPns369evZtm0bt99+ezePREREpC2TyUREsA8Rwddx15RYvj1dwa6sIr7OKWZXVhE+\nnhbGJgSTlhRCbJi/0VrZvQAAIABJREFUVrIRkSvisACfnZ1NTEwM3t7ebdpHjRqFYRhkZ2dfNsBX\nVFTQ2NhIQUEBr7/+OkCb8fNZWVkAjBgxos1+SUlJmM1msrKyFOBFRKRXmU0m4iMDiY8MZPbUOA4e\nL2V3VhHbD55h6zffEeTnQVpS80o24bbO77iJiLRyWIAvKSkhJCSkXXvr+PXi4uLLfo0f//jHVFRU\nABAQEMC///u/k5aW1uYz3NzcCAgIaLNfa1tXPkNERKSnuLqYGT3MxuhhNi7UXWxeyeZwEZ/sOsXH\nO08SbvNpDvOJIQT5ezi6uyLipBwW4Gtra7FYLO3a3d3dgebx8Jfz2muvUVNTw4kTJ/joo4+orq7u\n0me0fk5XPuOHLjUeqbfZbFc2Xl96n2rinFQX56OadCwyPJAZN8ZRfq6Wr/YX8MU3+az5Wy5r/pZL\n0tAgbhgTzqTkIfj20ko2qovzUU2ck7PVxWEB3sPDg4aGhnbtraG6NchfytixYwG44YYbuPnmm7nj\njjvw8vJizpw59s+or6/vcN+6uroufcYPaRKrtFJNnJPq4nxUk65JS7CRlmCjuLyG3VlF7Moq4o9r\nMlm29gAjhwaRlhRC8nWDcLf0zEo2qovzUU2ckyaxfo/NZutwCEtJSQnAFU1gBYiIiCApKYn169fb\nA7zNZqOhoYGKioo2w2jq6+upqKi44s8QERHpbcGBXtwxMYbpE6I5VXSeXVmF7M4qYv+xs7i7uTBm\nmI20pBCGRwfiYnbsSjYi4hgOC/AJCQmsWrWK6urqNhNZMzMz7e9fqdraWi5cuGB/nZiYCMChQ4eY\nNGmSvf3QoUM0NTXZ3xcREXE2JpOJqMG+RA325Z4p13HkdAW7Dhfy9ZESdh4uxNfLwo8SQkhLCmHo\nED9MWslGZMBw2K/u6enpNDQ0sHr1antbfX09a9euZcyYMfYJrgUFBeTm5rbZt6ysrN3XO3ToEDk5\nOSQlJdnb0tLSCAgI4C9/+Uubbd955x28vLy4/vrre/KQREREeoXZbCIxKpAHpyXy8i8m8eiskcRH\nBPBFZgHPrNrLk8t2snbbcc6UVl/+i4lIv+ewO/DJycmkp6fz/PPPU1JSQmRkJBkZGRQUFPDcc8/Z\nt1u8eDF79uzhyJEj9rYbb7yR2267jbi4OLy8vDh27BgffPAB3t7eLFy40L6dh4cHv/zlL3n66ad5\n/PHHmTRpEl9//TUfffQRv/71r/Hz8+vTYxYREekui6uZ1HgbqfE2amovsu9oCbuyCvl4Zx4bduQR\nGeJD2vDBjBseQqDvlc/1EhHn57AAD/D73/+el19+mXXr1lFZWUl8fDxvvPEGqampl9zv/vvvZ+fO\nnXz++efU1tZis9lIT09n4cKFREREtNl29uzZWCwW/vSnP7F582ZCQ0N56qmnmDdvXm8emoiISK/z\n8nBl0qhQJo0KpeJ8HXuyi9mdVcj7W4+xeusx4iMDSEsaTGq8DW+P5lXZdh4uZO0XuZRV1WH1c+fO\nG2IZnzTYwUciIlfCZBhG3y+p0o9pFRpppZo4J9XF+agmfa+wrIZdhwvZlVVEcfkFXF1MjBwaRJCf\nB9syC6i/2GTf1s3VzAO3JSjEOwGdK85Jq9CIiIhIrxts9WLm5KHMmBRDXuE5dh0uYk92Ed98e7bd\ntvUXm1j7Ra4CvEg/ovWnRERErlEmk4mYUD/uu2UYLzw6sdPtSqvqOHSilLqGxj7snYhcLd2BFxER\nGQDMZhNBfu6UVnX8FPIX38vE1cXEdWH+JEYFkhhtJSbUV2vNizghBXgREZEB4s4bYvnzJzntxsDf\nP3UYAT4eZJ8sIzuvnIwvT5Dx5Qk83FyIjwggMdrK8OhAwgZ5a715ESegAC8iIjJAtI5z72wVmlGx\nQQCcq6kn51QFWXnNgT4ztxQAP2+35rvzUYEMjwpkUICnYw5EZIBTgBcRERlAxicNZnzS4EuurOHr\n5cbYhGDGJgQDcLbyAtl55WSfLCfrZDm7s4oAsAV4kBjVfHc+ISoQPy+3PjsOkYFMAV5EREQuaZC/\nJ5OTPZmcPATDMCg4W01WS6Dfk13EtswCACKCfZrvzkcHEhcRgIebYoZIb9CZJSIiIl1mMpkIs/kQ\nZvNh6tgIGpuayDtzjqyT5WTnlbFlXz6b/u80LmYTQ4f4tQR6K0OH+OHqogmxIj1BAV5ERESumovZ\nTGyYP7Fh/twxIZq6hkaO5VeS1TIhdv32PD7anoe7xYVhEf4MbxlyEx7sg1kTYkWuigK8iIiI9Bh3\niwtJMVaSYqwAVNc2kHOyguyTZWTllfP+8WMA+HhaSGiZDJsYHUhwgKdWuBHpIgV4ERER6TXeHhZS\n422kxtsAKD9X17y6zcnmMfRf5xQDEOTnbp8QmxgViL+PuyO7LeLUFOBFRESkzwT6ujNxZCgTR4Zi\nGAaFZTXNq9vklbPvaAlfHTwDQNgg75YHSgUSHxGIl4cii0grnQ0iIiLiECaTidAgb0KDvLlpTDhN\nTQYni841353PK+OLzAI+35uP2WQiJtSXxOhAEqOsXBfmh8XVxdHdF3EYBXgRERFxCmaziZhQP2JC\n/ZiWFkXDxSZyv/v7hNi/7jzFhh0nsbiaGRbub1/hJirEF7NZ4+dl4FCAFxEREadkcTWTENX8kCiu\nhwt1FzlyqqI50J8s54MvjvPBF8fxcnclofUJsdGBDLZ6aUKsXNMU4EVERKRf8HR3JWXYIFKGDQKg\n8nyd/emw2S1j6AECfNzaTIi1+nk4stsiPU4BXkRERPolfx930pIGk5Y0GMMwKKm4YA/zB4+XsvNw\nIQCDrV4kRjcvWZkQFYi3h8XBPRfpHgV4ERER6fdMJhPBgV4EB3oxJSWMJsMgv/g8WXnNy1XuOFjI\n1n3fYQIiB/syvGX8/HXh/rhbNCFW+hcFeBEREbnmmE0mIkN8iQzxJX1cJBcbmzheUNWyZGUZm/7v\nNJ/sPoWri4nrwvxblqy0EhPqi4vZ7Ojui1ySAryIiIhc81xdzMRFBBAXEcCMSTHU1l/k6OlKsltW\nuMn48gQZX57A092F+IhA+xr0YYO8NSFWnI4CvIiIiAw4Hm6ujIoNYlRsEADnaurJOVXR/JTYvHL2\nHzsLgJ+3W/PqNi2BfpC/pyO7LQIowIuIiIjg6+XG2IRgxiYEA3C28gLZLePns06WszurCIDgAM+W\nB0o1//P1cnNkt2WAUoAXERER+YFB/p5MTvZkcvIQDMOg4Gy1fULs7qwivthfAEBEsE/LcpVW4iL8\n8XBTtJLep58yERERkUswmUyE2XwIs/kwdWwEjU1N5J0517JkZRmb9+azcc9pXMwmhg7xsz8hdugQ\nP1xdNCFWep4CvIiIiMgVcDGbiQ3zJzbMnzsmRFPX0Mix/MrmJ8TmlbN+ex4fbc/D3eJCXESA/Qmx\n4cE+mDUhVnqAAryIiIhIN7hbXEiKsZIUYwWguraBnJMVZJ8sI6vloVIAPp4WEr43ITY4wFMr3MhV\nUYAXERER6UHeHhZS422kxtsAKD9X17y6zcnmMfRf5xQDEOTnYX9CbGJUIDabryO7Lf2IAryIiIhI\nLwr0dWfiyFAmjgzFMAwKy2paHihVzr4jJXx14AzQ/ITYuDB/EqMDiY8IxMtDMU06pp8MERERkT5i\nMpkIDfImNMibm8aE09RkcLLoHNknyzlWUMUXmQV8vjcfs8lETKhvy5KVVq4L88Pi6uLo7ouTUIAX\nERERcRCz2URMqB8xoX7YbL4UnKkk97u/T4j9685TbNhxEourmWHh/gyPtpIYFUhUiC9ms8bPD1QK\n8CIiIiJOwuJqJiEqkISoQLgeLtRd5MipCnugX/O3XAC83F1JaBk7Pzw6kMFWL02IHUAU4EVERESc\nlKe7KynDBpEybBAAlefr7E+Hzc4rZ9/REqB5nH2iPdBbCfR1d2S3pZcpwIuIiIj0E/4+7qQlDSYt\naTCGYVBScYGslgmxB3JL2XGoEIDBVq+WFW6sJEQF4O1hcXDPpScpwIuIiIj0QyaTieBAL4IDvZiS\nEkaTYZBffJ6svOblKnccLGTrvu8wmSAqxNce6IeF++Nm0YTY/kwBXkREROQaYDaZiAzxJTLEl/Rx\nkVxsbOJ4QZV9DfpNe07zya5TuLqYuC7Mn8RoK8OjAokO9cXFbHZ09+UKKMCLiIiIXINcXczERQQQ\nFxHAzMlQW3+Ro6cryW6ZEJux7TgZgKe7C/ERzePnE6MDCRvkrQmxTk4BXkRERGQA8HBzZVRsEKNi\ngwCoqqknp+XpsNl55ew/dhYAP283+9NhE6MDGeTv6chuSwcU4EVEREQGID8vN36UGMKPEkMAOFt5\ngeyW8fNZJ8vZlVUEQHCAZ8sDpZr/+Xq5ObLbggK8iIiIiACD/D2ZnOzJ5OQhGIbBd2er7YF+d1YR\nX+wvACAy2Mf+hNi4CH883BQn+5q+4yIiIiLShslkItzmQ7jNh6ljI2hsaiLvzLmW9efL2Lw3n417\nTuNiNjF0iJ99/fmhQ/xwddGE2N6mAC8iIiIil+RiNhMb5k9smD93TIimrqGRY/mV9ifErt+ex0fb\n83C3uBAXEWB/Qmx4sA9mTYjtcQrwIiIiInJF3C0uJMVYSYqxAnD+QgNHTv39CbEHj5cC4ONpsU+G\nHR4ViC3AUyvc9AAFeBERERHpFh9PC6nxwaTGBwNQVlXbvLpNy7//yykGIMjPwx7mE6Ot+HtrQuzV\nUIAXERERkR5l9fNg4shQJo4MxTAMCstq7E+I3XekhK8OnAEgzObdPNwmykp8ZACe7oqmXaHvkoiI\niIj0GpPJRGiQN6FB3tycGk5Tk8HJonMt68+X8cX+Aj7/Oh+zyURMqG/LHXorsWH+WFw1IbYjCvAi\nIiIi0mfMZhMxoX7EhPoxLS2KhouNHPuuyv6E2L/uPMWGHSdxczUzLNyfxGgriVGBRIX4YjZr/Dwo\nwIuIiIiIA1lcXewPieJ6qKm9yNHTFfYVbtb8LRcAbw9X4iObV7dJjApksNVrwE6IdWiAr6+v55VX\nXmHdunVUVVWRkJDAokWLGD9+/CX327RpE3/96185cOAApaWlhIaGcuONN7Jw4UJ8fX3bbBsfH9/h\n1/jP//xP7rvvvh47FhERERHpPi8PV1KGDSJl2CAAKs/X2Z8Om51Xzr6jJQAE+rrbg//waCuBvu6O\n7HafcmiAf/LJJ9m0aRPz5s0jKiqKjIwMFixYwKpVqxg9enSn+/3mN78hODiYGTNmMGTIEI4cOcKq\nVav48ssv+eCDD3B3b1vASZMm8ZOf/KRNW3Jycq8ck4iIiIj0HH8fd9KSBpOWNBjDMCipuEDWyXKy\n8so5kFvKjkOFAIQGebUEeisJUQF4e1gc3PPe47AAf+DAAT7++GOWLFnCP/7jPwIwc+ZMpk+fzvPP\nP8/bb7/d6b6vvvoq48aNa9M2YsQIFi9ezMcff8ydd97Z5r2hQ4cyY8aMHj8GEREREek7JpOJ4EAv\nggO9mJISRpNhkF983r7CzfaDhWzZ9x0mE0SF/H1C7LBwf9wsLo7ufo9xWID/9NNPsVgs3HPPPfY2\nd3d37r77bl566SWKi4sJDg7ucN8fhneAW265BYDc3NwO96mtrcVkMrW7Oy8iIiIi/ZPZZCIyxJfI\nEF/Sx0VysbGJ4wVVZOWVkX2ynE17TvPJrlO4upi5LsyPxGgrw6MCiQ71xcXcf1e4cViAz87OJiYm\nBm9v7zbto0aNwjAMsrOzOw3wHTl79iwAgYGB7d5bs2YNq1atwjAM4uLi+OUvf8nUqVO7dwAiIiIi\n4lRcXczERQQQFxHAzMlQW3+Ro6cr7SvcZGw7Tgbg6e5CfETr+PlAhgzybjchdufhQtZ+kUtZVR1W\nP3fuvCGW8UmDHXNgP+CwAF9SUkJISEi7dpvNBkBxcfEVfb0333wTFxcXbr311jbto0ePZtq0aYSH\nh3PmzBlWrlzJY489xgsvvMD06dOv/gBERERExKl5uLkyKjaIUbFBAFTV1JPT+oTYvHL2H2u+Aezv\n7WafEJsYHci3+ZX8+ZMc6i82AVBaVcefP8kBcIoQ77AAX1tbi8XSfnJB6xCXurq6Ln+t9evXs2bN\nGh5++GEiIyPbvPfuu++2eT1r1iymT5/OH/7wB26//fYrXn4oKMjnirbvSTab7+U3kj6lmjgn1cX5\nqCbOSXVxPqpJ77IBsVFB3N7yurishsxvS8j89iyZx0rYlVUENK9V39RktNm3/mITH351gp9MGda3\nne6AwwK8h4cHDQ0N7dpbg3tXx6p//fXXPPXUU0yZMoXHH3/8stt7eXnx05/+lBdeeIHjx48TGxt7\nRf0uLT3frqB9wWbzpaTkXJ9/rnRONXFOqovzUU2ck+rifFSTvmcCUoZaSRlqxbh1GN+drSY7r5x3\nNn/b4fYl5Rf6pEZms+mSN40dNnrfZrN1OEympKR5bc+ujH/Pycnh5z//OfHx8bz00ku4uHRtdnFo\naCgAlZWVV9BjEREREblWmUwmwm0+TB0bQZBfxzeSO2vvaw4L8AkJCZw4cYLq6uo27ZmZmfb3L+XU\nqVPMnz8fq9XKsmXL8PLy6vJnnz59GgCr1XqFvRYRERGRa92dN8Ti5to2Jru5mrnzhisbudFbHBbg\n09PTaWhoYPXq1fa2+vp61q5dy5gxY+wTXAsKCtotDVlSUsJDDz2EyWRixYoVnQbxsrKydm3l5eX8\n5S9/ITw8nOjo6J47IBERERG5JoxPGswDtyUQ5OeOieY77w/cluAUE1jBgWPgk5OTSU9P5/nnn6ek\npITIyEgyMjIoKCjgueees2+3ePFi9uzZw5EjR+xt8+fP5/Tp08yfP5+9e/eyd+9e+3uRkZH2p7i+\n/fbbbN68mSlTpjBkyBCKiop47733KCsr4/XXX++7gxURERGRfmV80mDGJw12yrkJDgvwAL///e95\n+eWXWbduHZWVlcTHx/PGG2+Qmpp6yf1ycpqX8Vm+fHm792bNmmUP8KNHj2bfvn2sXr2ayspKvLy8\nSElJ4eGHH77sZ4iIiIiIOCOTYRh9v6RKP6ZVaKSVauKcVBfno5o4J9XF+agmzskRdXHaVWhERERE\nROTKKcCLiIiIiPQjCvAiIiIiIv2IAryIiIiISD+iAC8iIiIi0o8owIuIiIiI9CMK8CIiIiIi/YhD\nH+TUH5nNpgH52dIx1cQ5qS7ORzVxTqqL81FNnFNf1+Vyn6cHOYmIiIiI9CMaQiMiIiIi0o8owIuI\niIiI9CMK8CIiIiIi/YgCvIiIiIhIP6IALyIiIiLSjyjAi4iIiIj0IwrwIiIiIiL9iAK8iIiIiEg/\nogAvIiIiItKPKMCLiIiIiPQjro7uwEBWX1/PK6+8wrp166iqqiIhIYFFixYxfvz4y+5bVFTEs88+\ny/bt22lqaiItLY0lS5YQERHRBz2/dl1tTZYuXcprr73Wrn3QoEFs3769t7o7IBQXF7Ny5UoyMzM5\ndOgQNTU1rFy5knHjxnVp/9zcXJ599ln27duHxWLhxhtvZPHixVit1l7u+bWtO3V58sknycjIaNee\nnJzM+++/3xvdHRAOHDhARkYGu3fvpqCggICAAEaPHs0TTzxBVFTUZffXdaXndacmuq70noMHD/I/\n//M/ZGVlUVpaiq+vLwkJCTz66KOMGTPmsvs7w7miAO9ATz75JJs2bWLevHlERUWRkZHBggULWLVq\nFaNHj+50v+rqaubNm0d1dTWPPPIIrq6uvPXWW8ybN48PP/wQf3//PjyKa8vV1qTV008/jYeHh/31\n9/9brs6JEyd48803iYqKIj4+nm+++abL+xYWFjJ79mz8/PxYtGgRNTU1/OlPf+Lo0aO8//77WCyW\nXuz5ta07dQHw9PTkt7/9bZs2/VLVPcuXL2ffvn2kp6cTHx9PSUkJb7/9NjNnzmTNmjXExsZ2uq+u\nK72jOzVppetKzzt9+jSNjY3cc8892Gw2zp07x/r165kzZw5vvvkmEydO7HRfpzlXDHGIzMxMIy4u\nzvjf//1fe1ttba1xyy23GPfff/8l933jjTeM+Ph44/Dhw/a2Y8eOGYmJicbLL7/cW12+5nWnJq++\n+qoRFxdnVFZW9nIvB55z584ZZWVlhmEYxmeffWbExcUZu3bt6tK+//Ef/2GkpKQYhYWF9rbt27cb\ncXFxxurVq3ulvwNFd+qyePFiIzU1tTe7NyDt3bvXqKura9N24sQJY8SIEcbixYsvua+uK72jOzXR\ndaVv1dTUGBMmTDD+6Z/+6ZLbOcu5ojHwDvLpp59isVi455577G3u7u7cfffd7N27l+Li4k733bhx\nIykpKQwfPtzeFhsby/jx4/nkk096td/Xsu7UpJVhGJw/fx7DMHqzqwOKj48PgYGBV7Xvpk2buOmm\nmwgJCbG3TZgwgejoaJ0r3dSdurRqbGzk/PnzPdQjGTNmDG5ubm3aoqOjGTZsGLm5uZfcV9eV3tGd\nmrTSdaVveHp6YrVaqaqquuR2znKuKMA7SHZ2NjExMXh7e7dpHzVqFIZhkJ2d3eF+TU1NHDlyhBEj\nRrR7b+TIkeTl5XHhwoVe6fO17mpr8n1TpkwhNTWV1NRUlixZQkVFRW91Vy6jqKiI0tLSDs+VUaNG\ndame0nuqq6vt58q4ceN47rnnqKurc3S3rjmGYXD27NlL/rKl60rf6kpNvk/Xld5z/vx5ysrKOH78\nOC+++CJHjx695Jw3ZzpXNAbeQUpKStrcFWxls9kAOr3bW1FRQX19vX27H+5rGAYlJSVERkb2bIcH\ngKutCYCfnx9z584lOTkZi8XCrl27eO+998jKymL16tXt7sBI72utV2fnSmlpKY2Njbi4uPR11wY8\nm83G/PnzSUxMpKmpia1bt/LWW2+Rm5vL8uXLHd29a8pHH31EUVERixYt6nQbXVf6VldqArqu9IV/\n/dd/ZePGjQBYLBZ++tOf8sgjj3S6vTOdKwrwDlJbW9vhBDp3d3eATu9EtbZ3dOK27ltbW9tT3RxQ\nrrYmAA888ECb1+np6QwbNoynn36aDz/8kH/4h3/o2c7KZXX1XPnhX1yk9/3zP/9zm9fTp08nJCSE\nFStWsH379ktOIJOuy83N5emnnyY1NZUZM2Z0up2uK32nqzUBXVf6wqOPPsq9995LYWEh69ato76+\nnoaGhk5/OXKmc0VDaBzEw8ODhoaGdu2tPxytPwg/1NpeX1/f6b6aoX51rrYmnbnvvvvw9PRk586d\nPdI/uTI6V/qXhx56CEDnSw8pKSnh4Ycfxt/fn1deeQWzufPLvc6VvnElNemMris9Kz4+nokTJ3LX\nXXexYsUKDh8+zJIlSzrd3pnOFQV4B7HZbB0OySgpKQEgODi4w/0CAgJwc3Ozb/fDfU0mU4d/2pHL\nu9qadMZsNhMSEkJlZWWP9E+uTGu9OjtXgoKCNHzGiQwaNAiLxaLzpQecO3eOBQsWcO7cOZYvX37Z\na4KuK73vSmvSGV1Xeo/FYuHmm29m06ZNnd5Fd6ZzRQHeQRISEjhx4gTV1dVt2jMzM+3vd8RsNhMX\nF8ehQ4favXfgwAGioqLw9PTs+Q4PAFdbk840NDRw5syZbq/UIVcnJCQEq9Xa6bmSmJjogF5JZwoL\nC2loaNBa8N1UV1fHI488Ql5eHsuWLWPo0KGX3UfXld51NTXpjK4rvau2thbDMNrlgFbOdK4owDtI\neno6DQ0NrF692t5WX1/P2rVrGTNmjH0yZUFBQbulpn784x+zf/9+srKy7G3Hjx9n165dpKen980B\nXIO6U5OysrJ2X2/FihXU1dUxefLk3u24AHDq1ClOnTrVpu3WW29ly5YtFBUV2dt27txJXl6ezpU+\n8sO61NXVdbh05B//+EcAJk2a1Gd9u9Y0NjbyxBNPsH//fl555RVSUlI63E7Xlb7TnZroutJ7Ovre\nnj9/no0bNxIaGkpQUBDg3OeKydDCog7z+OOPs3nzZh544AEiIyPJyMjg0KFD/PnPfyY1NRWAuXPn\nsmfPHo4cOWLf7/z588yaNYsLFy7w4IMP4uLiwltvvYVhGHz44Yf6zbwbrrYmycnJTJs2jbi4ONzc\n3Ni9ezcbN24kNTWVlStX4uqq+eLd0RrucnNz2bBhA3fddRfh4eH4+fkxZ84cAG666SYAtmzZYt/v\nzJkzzJw5k4CAAObMmUNNTQ0rVqwgNDRUqzj0gKupS35+PrNmzWL69OkMHTrUvgrNzp07mTZtGi+9\n9JJjDuYa8Mwzz7By5UpuvPFGbrvttjbveXt7c8sttwC6rvSl7tRE15XeM2/ePNzd3Rk9ejQ2m40z\nZ86wdu1aCgsLefHFF5k2bRrg3OeKArwD1dXV8fLLL7N+/XoqKyuJj4/nV7/6FRMmTLBv09EPDzT/\nufnZZ59l+/btNDU1MW7cOJ566ikiIiL6+jCuKVdbk3/7t39j3759nDlzhoaGBsLCwpg2bRoPP/yw\nJn/1gPj4+A7bw8LC7MGwowAP8O233/Lf//3f7N27F4vFwpQpU1iyZImGavSAq6lLVVUVv/vd78jM\nzKS4uJimpiaio6OZNWsW8+bN07yEbmj9f1NHvl8TXVf6TndqoutK71mzZg3r1q3j2LFjVFVV4evr\nS0pKCg899BA/+tGP7Ns587miAC8iIiIi0o9oDLyIiIiISD+iAC8iIiIi0o8owIuIiIiI9CMK8CIi\nIiIi/YgCvIiIiIhIP6IALyIiIiLSjyjAi4iIiIj0IwrwIiLi9ObOnWt/KJSIyECn5/CKiAxQu3fv\nZt68eZ2+7+LiQlZWVh/2SEREukIBXkRkgJs+fTrXX399u3azWX+kFRFxRgrwIiID3PDhw5kxY4aj\nuyEiIl2k2ysiInJJ+fn5xMfHs3TpUjZs2MAdd9zByJEjmTJlCkuXLuXixYvt9snJyeHRRx9l3Lhx\njBw5kmnTpvHmm2/S2NjYbtuSkhL+67/+i5tvvpkRI0Ywfvx4HnzwQbZv395u26KiIn71q18xduxY\nkpOT+dnPfsYByEvnAAAD7ElEQVSJEyd65bhFRJyV7sCLiAxwFy5coKysrF27m5sbPj4+9tdbtmzh\n9OnTzJ49m0GDBrFlyxZee+01CgoKeO655+zbHTx4kLlz5+Lq6mrfduvWrTz//PPk5OTwwgsv2LfN\nz8/nvvvuo7S0lBkzZjBixAguXLhAZmYmO3bsYOLEifZta2pqmDNnDsnJySxatIj8/HxWrlzJwoUL\n2bBhAy4uLr30HRIRcS4K8CIiA9zSpUtZunRpu/YpU6awbNky++ucnBzWrFlDUlISAHPmzOGxxx5j\n7dq13HvvvaSkpADwzDPPUF9fz7vvvktCQoJ92yeeeIINGzZw9913M378eAB++9vfUlxczPLly5k8\neXKbz29qamrzury8nJ/97GcsWLDA3ma1WvnDH/7Ajh072u0vInKtUoAXERng7r33XtLT09u1W63W\nNq8nTJhgD+8AJpOJ+fPn8/nnn/PZZ5+RkpJCaWkp33zzDVOnTrWH99Ztf/7zn/Ppp5/y2WefMX78\neCoqKvjyyy+ZPHlyh+H7h5NozWZzu1Vz0tLSADh58qQCvIgMGArwIiIDXFRUFBMmTLjsdrGxse3a\nrrvuOgBOnz4NNA+J+X779w0dOhSz2Wzf9tSpUxiGwfDhw7vUz+DgYNzd3du0BQQEAFBRUdGlryEi\nci3QJFYREekXLjXG3TCMPuyJiIhjKcCLiEiX5Obmtms7duwYABEREQCEh4e3af++48eP09TUZN82\nMjISk8lEdnZ2b3VZROSapAAvIiJdsmPHDg4fPmx/bRgGy5cvB+CWW24BICgoiNGjR7N161aOHj3a\nZts33ngDgKlTpwLNw1+uv/56tm3bxo4dO9p9nu6qi4h0TGPgRUQGuKysLNatW9fhe63BHCAhIYEH\nHniA2bNnY7PZ2Lx5Mzt27GDGjBmMHj3avt1TTz3F3LlzmT17Nvfffz82m42tW7fy1VdfMX36dPsK\nNAC/+c1vyMrKYsGCBcycOZOkpCTq6urIzMwkLCyMf/mXf+m9AxcR6acU4EVEBrgNGzawYcOGDt/b\ntGmTfez5TTfdRExMDMuWLePEiRMEBQWxcOFCFi5c2GafkSNH8u677/Lqq6/yzjvvUFNTQ0REBL/+\n9a956KGH2mwbERHBBx98wOuvv862bdtYt24dfn5+JCQkcO+99/bOAYuI9HMmQ3+jFBGRS8jPz+fm\nm2/mscce4xe/+IWjuyMiMuBpDLyIiIiISD+iAC8iIiIi0o8owIuIiIiI9CMaAy8iIiIi0o/oDryI\niIiISD+iAC8iIiIi0o8owIuIiIiI9CMK8CIiIiIi/YgCvIiIiIhIP6IALyIiIiLSj/w/1ieLpfRU\nPVwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "outputId": "b7da2b8c-47f6-444c-8684-5a53da312842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task1hindi-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "outputId": "9c1a3658-050f-4ebf-cb48-0616beb6ca8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "#print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 900 test sentences...\n",
            "    DONE.\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/task1hindi-results.txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "outputId": "039717a2-72e4-4c73-dfbb-b865aacc8325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] ):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] ):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    # elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "    #   #print(\" Semi Paraphrase     \",end=\"\")\n",
        "    #   #count_NP+=1      \n",
        "    #   print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "    #   x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(1)\t 1\t [-1.6438307  1.9184935] \t1\ttrue\n",
            "(0)\t 2\t [ 2.0707335 -2.0819454] \t0\ttrue\n",
            "(1)\t 3\t [-1.6212598  1.9013627] \t1\ttrue\n",
            "(0)\t 4\t [ 1.7443249 -1.8412107] \t0\ttrue\n",
            "(0)\t 5\t [ 1.8615649 -2.0052323] \t0\ttrue\n",
            "(1)\t 6\t [-1.7131191  1.669416 ] \t1\ttrue\n",
            "(0)\t 7\t [ 2.107905 -2.11603 ] \t0\ttrue\n",
            "(0)\t 8\t [ 1.7579662 -1.8302615] \t0\ttrue\n",
            "(1)\t 9\t [-1.8403392  1.779405 ] \t1\ttrue\n",
            "(0)\t 10\t [ 2.0689793 -2.1397016] \t0\ttrue\n",
            "(0)\t 11\t [ 1.6493807 -1.6628263] \t0\ttrue\n",
            "(1)\t 12\t [ 1.2332069 -1.3003455] \t0\tfalse\n",
            "(0)\t 13\t [ 2.0643125 -2.0412042] \t0\ttrue\n",
            "(0)\t 14\t [ 1.9831188 -1.866666 ] \t0\ttrue\n",
            "(1)\t 15\t [-1.741417   2.0210927] \t1\ttrue\n",
            "(0)\t 16\t [-1.8959496  1.9299197] \t1\tfalse\n",
            "(0)\t 17\t [-0.19323812 -0.08428665] \t1\tfalse\n",
            "(0)\t 18\t [ 1.4736418 -1.4964331] \t0\ttrue\n",
            "(1)\t 19\t [-1.6019588  1.8794247] \t1\ttrue\n",
            "(1)\t 20\t [-1.7745076  2.029062 ] \t1\ttrue\n",
            "(0)\t 21\t [-0.43743634  0.2741822 ] \t1\tfalse\n",
            "(0)\t 22\t [ 2.092904  -2.1327288] \t0\ttrue\n",
            "(0)\t 23\t [ 0.02812064 -0.28360167] \t0\ttrue\n",
            "(1)\t 24\t [-1.9449269  2.1219716] \t1\ttrue\n",
            "(0)\t 25\t [ 2.0132437 -2.12286  ] \t0\ttrue\n",
            "(0)\t 26\t [ 2.142679  -2.1436422] \t0\ttrue\n",
            "(1)\t 27\t [-1.493452   1.8353466] \t1\ttrue\n",
            "(0)\t 28\t [ 1.8539444 -1.9160268] \t0\ttrue\n",
            "(0)\t 29\t [ 2.0504324 -2.072597 ] \t0\ttrue\n",
            "(1)\t 30\t [-1.5571231  1.8489815] \t1\ttrue\n",
            "(1)\t 31\t [-1.9653722  1.9886019] \t1\ttrue\n",
            "(0)\t 32\t [ 2.1858542 -2.2110436] \t0\ttrue\n",
            "(1)\t 33\t [-1.8917323  1.9912717] \t1\ttrue\n",
            "(0)\t 34\t [-0.52389693  0.22496699] \t1\tfalse\n",
            "(0)\t 35\t [ 1.8858935 -2.0138967] \t0\ttrue\n",
            "(0)\t 36\t [ 2.108075  -2.1264458] \t0\ttrue\n",
            "(0)\t 37\t [-0.13466951  0.03688107] \t1\tfalse\n",
            "(1)\t 38\t [-1.8018813  1.994404 ] \t1\ttrue\n",
            "(0)\t 39\t [ 2.1355345 -2.1363633] \t0\ttrue\n",
            "(1)\t 40\t [-1.7471145  2.0207288] \t1\ttrue\n",
            "(0)\t 41\t [ 0.35166547 -0.35517377] \t0\ttrue\n",
            "(1)\t 42\t [-1.8451635  2.0828362] \t1\ttrue\n",
            "(0)\t 43\t [ 1.7495718 -1.768253 ] \t0\ttrue\n",
            "(0)\t 44\t [ 1.8623238 -1.6970776] \t0\ttrue\n",
            "(0)\t 45\t [ 1.6755906 -1.729335 ] \t0\ttrue\n",
            "(0)\t 46\t [-0.7240226  0.6658231] \t1\tfalse\n",
            "(1)\t 47\t [-1.4691405  1.7784176] \t1\ttrue\n",
            "(1)\t 48\t [-2.0099394  2.0715625] \t1\ttrue\n",
            "(0)\t 49\t [-0.63418233  0.49760127] \t1\tfalse\n",
            "(0)\t 50\t [ 2.1601143 -2.1727743] \t0\ttrue\n",
            "(1)\t 51\t [-1.7306472  1.9727429] \t1\ttrue\n",
            "(0)\t 52\t [ 2.0515647 -2.1802995] \t0\ttrue\n",
            "(1)\t 53\t [-0.5048522   0.43275148] \t1\ttrue\n",
            "(1)\t 54\t [-1.8612882  2.0543633] \t1\ttrue\n",
            "(0)\t 55\t [ 2.0684254 -2.0124516] \t0\ttrue\n",
            "(0)\t 56\t [ 1.2597222 -1.2349836] \t0\ttrue\n",
            "(0)\t 57\t [ 2.0368333 -2.0258653] \t0\ttrue\n",
            "(0)\t 58\t [ 1.9546217 -1.7942677] \t0\ttrue\n",
            "(1)\t 59\t [-1.5593961  1.7964501] \t1\ttrue\n",
            "(0)\t 60\t [-0.14548366  0.01049534] \t1\tfalse\n",
            "(0)\t 61\t [ 2.1165798 -2.0313787] \t0\ttrue\n",
            "(1)\t 62\t [-1.6301447  1.8748473] \t1\ttrue\n",
            "(0)\t 63\t [-1.8919263  1.9164144] \t1\tfalse\n",
            "(1)\t 64\t [-1.4932696  1.8206036] \t1\ttrue\n",
            "(0)\t 65\t [ 1.9908955 -2.004143 ] \t0\ttrue\n",
            "(0)\t 66\t [-0.5805168  0.3810626] \t1\tfalse\n",
            "(1)\t 67\t [-1.5583249  1.8437966] \t1\ttrue\n",
            "(1)\t 68\t [-1.512598   1.8182288] \t1\ttrue\n",
            "(0)\t 69\t [-1.9786701  2.0998147] \t1\tfalse\n",
            "(0)\t 70\t [ 1.4955498 -1.4002817] \t0\ttrue\n",
            "(1)\t 71\t [-1.4854736  1.8249073] \t1\ttrue\n",
            "(1)\t 72\t [-1.4903957  1.8042345] \t1\ttrue\n",
            "(0)\t 73\t [ 2.1039264 -2.214207 ] \t0\ttrue\n",
            "(0)\t 74\t [ 1.929763 -1.859583] \t0\ttrue\n",
            "(1)\t 75\t [-2.0024498  2.1320329] \t1\ttrue\n",
            "(0)\t 76\t [ 2.0718791 -2.1357834] \t0\ttrue\n",
            "(1)\t 77\t [-1.7455988  1.9937454] \t1\ttrue\n",
            "(0)\t 78\t [ 0.5990434 -0.6277236] \t0\ttrue\n",
            "(1)\t 79\t [-1.5669609  1.887609 ] \t1\ttrue\n",
            "(0)\t 80\t [ 2.1374712 -2.1782732] \t0\ttrue\n",
            "(1)\t 81\t [-1.5729427  1.8351084] \t1\ttrue\n",
            "(0)\t 82\t [-0.4310028  0.245196 ] \t1\tfalse\n",
            "(0)\t 83\t [ 1.9949937 -2.0008016] \t0\ttrue\n",
            "(1)\t 84\t [-1.8991375  2.128149 ] \t1\ttrue\n",
            "(0)\t 85\t [ 1.598403  -1.6317331] \t0\ttrue\n",
            "(1)\t 86\t [-0.88626695  0.78255713] \t1\ttrue\n",
            "(1)\t 87\t [-1.802286  2.04041 ] \t1\ttrue\n",
            "(0)\t 88\t [ 2.0975962 -2.144227 ] \t0\ttrue\n",
            "(1)\t 89\t [-0.8158447  0.7363595] \t1\ttrue\n",
            "(0)\t 90\t [-1.9489262  2.103965 ] \t1\tfalse\n",
            "(0)\t 91\t [ 2.1544287 -2.128429 ] \t0\ttrue\n",
            "(1)\t 92\t [-1.842337   2.0785604] \t1\ttrue\n",
            "(1)\t 93\t [-1.7172823  1.9901521] \t1\ttrue\n",
            "(0)\t 94\t [-1.2484417  1.1881629] \t1\tfalse\n",
            "(0)\t 95\t [ 1.7162653 -1.6635824] \t0\ttrue\n",
            "(1)\t 96\t [-1.6457328  1.8590021] \t1\ttrue\n",
            "(0)\t 97\t [ 1.9835209 -2.0155084] \t0\ttrue\n",
            "(1)\t 98\t [-1.8749957  2.0296628] \t1\ttrue\n",
            "(0)\t 99\t [ 0.03786778 -0.23296049] \t0\ttrue\n",
            "(1)\t 100\t [-0.9805058  1.0169562] \t1\ttrue\n",
            "(0)\t 101\t [-0.5043124  0.4185409] \t1\tfalse\n",
            "(1)\t 102\t [ 0.05254326 -0.40898594] \t0\tfalse\n",
            "(0)\t 103\t [ 0.05254326 -0.40898594] \t0\ttrue\n",
            "(0)\t 104\t [ 1.6858344 -1.5060586] \t0\ttrue\n",
            "(1)\t 105\t [-2.0001087  2.0557086] \t1\ttrue\n",
            "(1)\t 106\t [-1.6790739  1.9461801] \t1\ttrue\n",
            "(0)\t 107\t [ 1.6717627 -1.6353811] \t0\ttrue\n",
            "(0)\t 108\t [ 2.0165634 -1.966973 ] \t0\ttrue\n",
            "(1)\t 109\t [-1.8559271  1.9220691] \t1\ttrue\n",
            "(0)\t 110\t [ 2.1805704 -2.1520867] \t0\ttrue\n",
            "(1)\t 111\t [-1.6949065  1.9261404] \t1\ttrue\n",
            "(1)\t 112\t [ 0.12212242 -0.39657316] \t0\tfalse\n",
            "(0)\t 113\t [ 0.12212242 -0.39657316] \t0\ttrue\n",
            "(0)\t 114\t [ 2.0754838 -2.0763757] \t0\ttrue\n",
            "(1)\t 115\t [-1.2815391  1.2799189] \t1\ttrue\n",
            "(1)\t 116\t [-1.8895166  2.0582147] \t1\ttrue\n",
            "(0)\t 117\t [ 1.9269428 -2.0510032] \t0\ttrue\n",
            "(0)\t 118\t [ 2.0462036 -2.1889439] \t0\ttrue\n",
            "(1)\t 119\t [-1.6416931  1.9075686] \t1\ttrue\n",
            "(0)\t 120\t [ 2.0650048 -2.1204255] \t0\ttrue\n",
            "(1)\t 121\t [-1.4683596  1.751701 ] \t1\ttrue\n",
            "(1)\t 122\t [-1.732494   1.8064474] \t1\ttrue\n",
            "(0)\t 123\t [ 2.1428902 -2.2117178] \t0\ttrue\n",
            "(0)\t 124\t [ 2.148523  -2.1358967] \t0\ttrue\n",
            "(1)\t 125\t [-1.7380445  1.9677159] \t1\ttrue\n",
            "(1)\t 126\t [-1.6717421  1.8760234] \t1\ttrue\n",
            "(0)\t 127\t [ 1.7763454 -1.8786298] \t0\ttrue\n",
            "(0)\t 128\t [ 1.9965006 -2.0996501] \t0\ttrue\n",
            "(0)\t 129\t [ 2.0620103 -2.1624744] \t0\ttrue\n",
            "(0)\t 130\t [ 2.118835  -2.1730218] \t0\ttrue\n",
            "(1)\t 131\t [-1.8545581  2.0564036] \t1\ttrue\n",
            "(0)\t 132\t [ 1.9282366 -2.0809588] \t0\ttrue\n",
            "(1)\t 133\t [-1.8562692  1.9939811] \t1\ttrue\n",
            "(0)\t 134\t [ 2.0770912 -2.0650096] \t0\ttrue\n",
            "(0)\t 135\t [ 2.0670514 -2.1214328] \t0\ttrue\n",
            "(1)\t 136\t [-1.6983904  1.9395257] \t1\ttrue\n",
            "(1)\t 137\t [-1.8565389  2.0173461] \t1\ttrue\n",
            "(0)\t 138\t [ 2.049065  -2.1234674] \t0\ttrue\n",
            "(1)\t 139\t [-1.6997426  1.8781081] \t1\ttrue\n",
            "(0)\t 140\t [ 1.0686746 -1.1224314] \t0\ttrue\n",
            "(0)\t 141\t [ 2.0697863 -2.0778432] \t0\ttrue\n",
            "(1)\t 142\t [ 0.25262097 -0.18224733] \t0\tfalse\n",
            "(0)\t 143\t [ 0.15756875 -0.34196097] \t0\ttrue\n",
            "(1)\t 144\t [ 1.7875421 -1.884734 ] \t0\tfalse\n",
            "(0)\t 145\t [ 1.8686682 -1.8961595] \t0\ttrue\n",
            "(1)\t 146\t [-1.8935336  1.9886354] \t1\ttrue\n",
            "(0)\t 147\t [ 1.446356  -1.4799818] \t0\ttrue\n",
            "(1)\t 148\t [-1.6445963  1.9541785] \t1\ttrue\n",
            "(1)\t 149\t [-1.5303099  1.8169764] \t1\ttrue\n",
            "(0)\t 150\t [ 2.0879395 -2.1519363] \t0\ttrue\n",
            "(0)\t 151\t [ 2.1342108 -2.2269511] \t0\ttrue\n",
            "(1)\t 152\t [-1.8204029  2.0153997] \t1\ttrue\n",
            "(0)\t 153\t [ 1.4236215 -1.3529857] \t0\ttrue\n",
            "(0)\t 154\t [ 2.0660243 -2.2140331] \t0\ttrue\n",
            "(1)\t 155\t [-1.1735343  1.2829185] \t1\ttrue\n",
            "(0)\t 156\t [ 1.7207227 -1.7772634] \t0\ttrue\n",
            "(0)\t 157\t [ 0.15631081 -0.46866643] \t0\ttrue\n",
            "(1)\t 158\t [ 0.15631081 -0.46866643] \t0\tfalse\n",
            "(1)\t 159\t [-1.583976   1.8796271] \t1\ttrue\n",
            "(0)\t 160\t [-1.7434745  1.8257918] \t1\tfalse\n",
            "(0)\t 161\t [ 1.9636905 -1.9346858] \t0\ttrue\n",
            "(0)\t 162\t [-1.5517317  1.5066253] \t1\tfalse\n",
            "(1)\t 163\t [-1.7947334  1.988161 ] \t1\ttrue\n",
            "(1)\t 164\t [-1.8105237  2.0064769] \t1\ttrue\n",
            "(0)\t 165\t [ 2.0613842 -2.1023557] \t0\ttrue\n",
            "(1)\t 166\t [-1.7301657  1.8802687] \t1\ttrue\n",
            "(0)\t 167\t [ 2.101818  -2.1133766] \t0\ttrue\n",
            "(1)\t 168\t [-1.9785966  2.0939407] \t1\ttrue\n",
            "(0)\t 169\t [ 2.1022372 -2.0725834] \t0\ttrue\n",
            "(0)\t 170\t [ 0.09059852 -0.3744537 ] \t0\ttrue\n",
            "(1)\t 171\t [ 0.09059852 -0.3744537 ] \t0\tfalse\n",
            "(0)\t 172\t [ 1.0378028 -1.018645 ] \t0\ttrue\n",
            "(0)\t 173\t [ 2.0980237 -2.072092 ] \t0\ttrue\n",
            "(1)\t 174\t [-1.6103249  1.9192656] \t1\ttrue\n",
            "(0)\t 175\t [ 1.8618984 -1.8759979] \t0\ttrue\n",
            "(0)\t 176\t [ 2.1704955 -2.1948338] \t0\ttrue\n",
            "(1)\t 177\t [-1.810832  2.039606] \t1\ttrue\n",
            "(1)\t 178\t [-2.0237815  2.113967 ] \t1\ttrue\n",
            "(0)\t 179\t [ 2.0761847 -2.1083076] \t0\ttrue\n",
            "(1)\t 180\t [-0.2533192   0.11450435] \t1\ttrue\n",
            "(0)\t 181\t [ 2.0219321 -2.0626254] \t0\ttrue\n",
            "(0)\t 182\t [ 2.143143  -2.2163424] \t0\ttrue\n",
            "(1)\t 183\t [-1.5922772  1.8788692] \t1\ttrue\n",
            "(1)\t 184\t [ 1.8287244 -1.707067 ] \t0\tfalse\n",
            "(0)\t 185\t [-1.5315332  1.4551591] \t1\tfalse\n",
            "(0)\t 186\t [ 0.4647871 -0.5808656] \t0\ttrue\n",
            "(1)\t 187\t [-1.8961902  2.116566 ] \t1\ttrue\n",
            "(1)\t 188\t [-1.6071947  1.8993782] \t1\ttrue\n",
            "(0)\t 189\t [ 2.1520813 -2.1382842] \t0\ttrue\n",
            "(0)\t 190\t [ 2.0615697 -2.2032175] \t0\ttrue\n",
            "(1)\t 191\t [-1.7260122  1.9348834] \t1\ttrue\n",
            "(1)\t 192\t [-1.7242327  1.9797938] \t1\ttrue\n",
            "(0)\t 193\t [ 1.3393542 -1.2678419] \t0\ttrue\n",
            "(1)\t 194\t [-1.8051196  2.0040917] \t1\ttrue\n",
            "(0)\t 195\t [-0.68171155  0.4243159 ] \t1\tfalse\n",
            "(1)\t 196\t [-1.7502791  2.018431 ] \t1\ttrue\n",
            "(0)\t 197\t [ 1.6175314 -1.6257296] \t0\ttrue\n",
            "(1)\t 198\t [-1.8454045  2.0721738] \t1\ttrue\n",
            "(0)\t 199\t [ 1.106871  -1.1114323] \t0\ttrue\n",
            "(0)\t 200\t [ 0.21170114 -0.5005679 ] \t0\ttrue\n",
            "(0)\t 201\t [ 0.5311327  -0.73715466] \t0\ttrue\n",
            "(1)\t 202\t [-1.7973689  2.0561662] \t1\ttrue\n",
            "(1)\t 203\t [ 2.0933657 -2.1430936] \t0\tfalse\n",
            "(0)\t 204\t [ 1.9155337 -1.9381527] \t0\ttrue\n",
            "(0)\t 205\t [ 2.0827138 -2.149039 ] \t0\ttrue\n",
            "(1)\t 206\t [-1.5288126  1.8489848] \t1\ttrue\n",
            "(1)\t 207\t [-1.5624392  1.8268856] \t1\ttrue\n",
            "(0)\t 208\t [ 0.7482113  -0.81500316] \t0\ttrue\n",
            "(1)\t 209\t [-1.829218   2.0384836] \t1\ttrue\n",
            "(0)\t 210\t [ 2.1288984 -2.182955 ] \t0\ttrue\n",
            "(0)\t 211\t [ 1.5250602 -1.7497422] \t0\ttrue\n",
            "(1)\t 212\t [-1.8304158  2.0584157] \t1\ttrue\n",
            "(1)\t 213\t [-1.8606533  2.0469496] \t1\ttrue\n",
            "(0)\t 214\t [ 2.1023042 -2.1757822] \t0\ttrue\n",
            "(0)\t 215\t [ 0.03744917 -0.07269772] \t0\ttrue\n",
            "(1)\t 216\t [-1.985024  1.960678] \t1\ttrue\n",
            "(0)\t 217\t [-1.945127   2.0307546] \t1\tfalse\n",
            "(1)\t 218\t [-1.982377   2.0724366] \t1\ttrue\n",
            "(0)\t 219\t [-1.7881283  1.7863919] \t1\tfalse\n",
            "(1)\t 220\t [-1.9574795  1.9990267] \t1\ttrue\n",
            "(1)\t 221\t [-1.7576311  1.9702281] \t1\ttrue\n",
            "(0)\t 222\t [ 2.0836625 -2.2216156] \t0\ttrue\n",
            "(1)\t 223\t [-1.9037511  2.081927 ] \t1\ttrue\n",
            "(0)\t 224\t [ 2.0510619 -2.141651 ] \t0\ttrue\n",
            "(0)\t 225\t [ 2.0510619 -2.141651 ] \t0\ttrue\n",
            "(1)\t 226\t [-1.6917621  1.7283648] \t1\ttrue\n",
            "(0)\t 227\t [-1.5242183  1.4977821] \t1\tfalse\n",
            "(1)\t 228\t [-1.8893393  1.8464339] \t1\ttrue\n",
            "(1)\t 229\t [-0.5543427   0.44859293] \t1\ttrue\n",
            "(0)\t 230\t [-0.5543427   0.44859293] \t1\tfalse\n",
            "(0)\t 231\t [ 2.0280707 -2.134953 ] \t0\ttrue\n",
            "(1)\t 232\t [-1.9756421  1.9608067] \t1\ttrue\n",
            "(1)\t 233\t [-1.5678961  1.540548 ] \t1\ttrue\n",
            "(0)\t 234\t [-1.042344   0.8582357] \t1\tfalse\n",
            "(1)\t 235\t [-1.937065  2.091539] \t1\ttrue\n",
            "(0)\t 236\t [ 2.1321368 -2.166305 ] \t0\ttrue\n",
            "(0)\t 237\t [ 0.9027108 -1.0431896] \t0\ttrue\n",
            "(1)\t 238\t [-1.9692585  2.02769  ] \t1\ttrue\n",
            "(1)\t 239\t [-1.5658618  1.858591 ] \t1\ttrue\n",
            "(0)\t 240\t [ 2.0731425 -2.1878157] \t0\ttrue\n",
            "(0)\t 241\t [ 2.0477557 -2.1114728] \t0\ttrue\n",
            "(1)\t 242\t [-1.5592943  1.8132659] \t1\ttrue\n",
            "(1)\t 243\t [ 0.2165334  -0.40335548] \t0\tfalse\n",
            "(0)\t 244\t [ 2.1328871 -2.2129142] \t0\ttrue\n",
            "(1)\t 245\t [-1.7190149  1.969566 ] \t1\ttrue\n",
            "(0)\t 246\t [-1.400679   1.4652466] \t1\tfalse\n",
            "(1)\t 247\t [-1.5159051  1.833429 ] \t1\ttrue\n",
            "(0)\t 248\t [ 1.4111615 -1.4032207] \t0\ttrue\n",
            "(0)\t 249\t [-0.95671463  0.71188307] \t1\tfalse\n",
            "(1)\t 250\t [ 1.8509662 -1.7054695] \t0\tfalse\n",
            "(0)\t 251\t [ 2.1472282 -2.1971688] \t0\ttrue\n",
            "(1)\t 252\t [-1.5827101  1.8724718] \t1\ttrue\n",
            "(0)\t 253\t [ 1.1634245 -1.1157992] \t0\ttrue\n",
            "(1)\t 254\t [ 1.3178693 -1.3035746] \t0\tfalse\n",
            "(0)\t 255\t [ 1.8256896 -1.8670985] \t0\ttrue\n",
            "(1)\t 256\t [-1.9565006  2.0934238] \t1\ttrue\n",
            "(0)\t 257\t [ 2.089898  -2.1139975] \t0\ttrue\n",
            "(1)\t 258\t [-0.65124214  0.40150854] \t1\ttrue\n",
            "(1)\t 259\t [-1.5846001  1.6095375] \t1\ttrue\n",
            "(0)\t 260\t [ 2.0385783 -2.053905 ] \t0\ttrue\n",
            "(1)\t 261\t [-1.6017609  1.8923922] \t1\ttrue\n",
            "(0)\t 262\t [ 1.7701757 -1.7766505] \t0\ttrue\n",
            "(1)\t 263\t [-0.34265968  0.00823891] \t1\ttrue\n",
            "(0)\t 264\t [-0.34265968  0.00823891] \t1\tfalse\n",
            "(1)\t 265\t [-0.6903794  0.5957314] \t1\ttrue\n",
            "(0)\t 266\t [ 2.0236628 -2.0506685] \t0\ttrue\n",
            "(1)\t 267\t [-1.6476005  1.9217287] \t1\ttrue\n",
            "(0)\t 268\t [ 2.0550797 -2.1119242] \t0\ttrue\n",
            "(0)\t 269\t [ 1.8560107 -1.9606115] \t0\ttrue\n",
            "(1)\t 270\t [-1.2293144  1.1521367] \t1\ttrue\n",
            "(0)\t 271\t [ 2.0606458 -2.1093678] \t0\ttrue\n",
            "(1)\t 272\t [-1.8721436  1.9934059] \t1\ttrue\n",
            "(1)\t 273\t [-1.8192444  2.02412  ] \t1\ttrue\n",
            "(0)\t 274\t [-0.93960845  0.81826746] \t1\tfalse\n",
            "(0)\t 275\t [ 1.444495  -1.4104999] \t0\ttrue\n",
            "(1)\t 276\t [-1.5268474  1.8215666] \t1\ttrue\n",
            "(0)\t 277\t [ 0.13932906 -0.03113622] \t0\ttrue\n",
            "(1)\t 278\t [-2.024876   2.0570939] \t1\ttrue\n",
            "(1)\t 279\t [-1.4145453  1.4420339] \t1\ttrue\n",
            "(0)\t 280\t [ 2.123725 -2.088141] \t0\ttrue\n",
            "(1)\t 281\t [-1.5816005  1.8611425] \t1\ttrue\n",
            "(0)\t 282\t [ 2.0582213 -2.185997 ] \t0\ttrue\n",
            "(0)\t 283\t [ 1.5918132 -1.5732852] \t0\ttrue\n",
            "(1)\t 284\t [ 1.6485935 -1.7354968] \t0\tfalse\n",
            "(0)\t 285\t [ 2.1101894 -2.1968315] \t0\ttrue\n",
            "(0)\t 286\t [-1.9159393  1.8889651] \t1\tfalse\n",
            "(1)\t 287\t [-2.0006514  2.113891 ] \t1\ttrue\n",
            "(0)\t 288\t [-0.1933337  -0.00511391] \t1\tfalse\n",
            "(1)\t 289\t [-1.8571821  2.0239582] \t1\ttrue\n",
            "(1)\t 290\t [-1.8672457  2.0335429] \t1\ttrue\n",
            "(0)\t 291\t [-1.5469869  1.5477133] \t1\tfalse\n",
            "(0)\t 292\t [-0.258486   -0.10208875] \t1\tfalse\n",
            "(1)\t 293\t [-0.258486   -0.10208875] \t1\ttrue\n",
            "(0)\t 294\t [ 2.0111194 -2.1196942] \t0\ttrue\n",
            "(1)\t 295\t [-1.8296238  2.0680068] \t1\ttrue\n",
            "(0)\t 296\t [ 1.982609  -1.9474111] \t0\ttrue\n",
            "(0)\t 297\t [ 2.1053827 -2.0704525] \t0\ttrue\n",
            "(1)\t 298\t [-1.6988465  1.9117122] \t1\ttrue\n",
            "(0)\t 299\t [ 2.1262813 -2.1171405] \t0\ttrue\n",
            "(1)\t 300\t [-1.9321297  2.0962374] \t1\ttrue\n",
            "(0)\t 301\t [ 1.5931069 -1.4433731] \t0\ttrue\n",
            "(1)\t 302\t [-1.575014   1.8363262] \t1\ttrue\n",
            "(1)\t 303\t [-1.5989451  1.8890262] \t1\ttrue\n",
            "(0)\t 304\t [ 2.097846  -2.1286173] \t0\ttrue\n",
            "(1)\t 305\t [-1.9311105  2.084546 ] \t1\ttrue\n",
            "(0)\t 306\t [-0.49512067  0.21173196] \t1\tfalse\n",
            "(0)\t 307\t [ 1.7608495 -1.7160617] \t0\ttrue\n",
            "(1)\t 308\t [-1.5640677  1.890364 ] \t1\ttrue\n",
            "(0)\t 309\t [ 1.042426  -1.0929116] \t0\ttrue\n",
            "(0)\t 310\t [-0.91346115  0.79214275] \t1\tfalse\n",
            "(1)\t 311\t [-1.8242598  1.9919162] \t1\ttrue\n",
            "(1)\t 312\t [-1.4591883  1.4546164] \t1\ttrue\n",
            "(0)\t 313\t [ 0.5902374 -0.7596845] \t0\ttrue\n",
            "(1)\t 314\t [-0.3478431   0.38563973] \t1\ttrue\n",
            "(0)\t 315\t [ 2.130468  -2.1333656] \t0\ttrue\n",
            "(1)\t 316\t [-2.0018187  1.9948528] \t1\ttrue\n",
            "(0)\t 317\t [ 1.9859017 -2.0910723] \t0\ttrue\n",
            "(0)\t 318\t [ 1.9928493 -2.0482922] \t0\ttrue\n",
            "(1)\t 319\t [-1.9814744  2.120043 ] \t1\ttrue\n",
            "(0)\t 320\t [-1.7480862  1.7727501] \t1\tfalse\n",
            "(1)\t 321\t [-1.5257981  1.7734575] \t1\ttrue\n",
            "(1)\t 322\t [-1.6913075  1.9364202] \t1\ttrue\n",
            "(0)\t 323\t [ 2.0783708 -2.002356 ] \t0\ttrue\n",
            "(0)\t 324\t [ 2.1472101 -2.2012033] \t0\ttrue\n",
            "(1)\t 325\t [-2.0225883  2.0950608] \t1\ttrue\n",
            "(0)\t 326\t [ 2.148458  -2.1886325] \t0\ttrue\n",
            "(0)\t 327\t [-0.19718413 -0.02529596] \t1\tfalse\n",
            "(1)\t 328\t [-0.19718413 -0.02529596] \t1\ttrue\n",
            "(1)\t 329\t [-1.7382065  1.9352602] \t1\ttrue\n",
            "(0)\t 330\t [ 2.065686  -2.0576553] \t0\ttrue\n",
            "(0)\t 331\t [ 2.1314034 -2.1477046] \t0\ttrue\n",
            "(1)\t 332\t [-1.5166204  1.6150551] \t1\ttrue\n",
            "(0)\t 333\t [-1.8914837  1.9528002] \t1\tfalse\n",
            "(1)\t 334\t [-1.8735471  2.017203 ] \t1\ttrue\n",
            "(0)\t 335\t [-0.6706113   0.51434714] \t1\tfalse\n",
            "(1)\t 336\t [-1.7822686  1.9935914] \t1\ttrue\n",
            "(0)\t 337\t [ 2.1166413 -2.14264  ] \t0\ttrue\n",
            "(0)\t 338\t [ 2.0485256 -2.169516 ] \t0\ttrue\n",
            "(0)\t 339\t [ 1.6946472 -1.6835899] \t0\ttrue\n",
            "(1)\t 340\t [-1.6594918  1.8982482] \t1\ttrue\n",
            "(1)\t 341\t [-1.8818166  2.0284984] \t1\ttrue\n",
            "(0)\t 342\t [ 2.1588159 -2.1288457] \t0\ttrue\n",
            "(0)\t 343\t [-1.014986   0.9030577] \t1\tfalse\n",
            "(1)\t 344\t [-1.6146713  1.8508061] \t1\ttrue\n",
            "(1)\t 345\t [-1.6622168  1.9302797] \t1\ttrue\n",
            "(0)\t 346\t [ 1.5458885 -1.5264142] \t0\ttrue\n",
            "(1)\t 347\t [-0.23770542  0.15346307] \t1\ttrue\n",
            "(0)\t 348\t [ 2.1370385 -2.160251 ] \t0\ttrue\n",
            "(0)\t 349\t [ 0.17076015 -0.30488685] \t0\ttrue\n",
            "(1)\t 350\t [ 0.17076015 -0.30488685] \t0\tfalse\n",
            "(1)\t 351\t [ 0.30072302 -0.5341953 ] \t0\tfalse\n",
            "(0)\t 352\t [ 0.30072302 -0.5341953 ] \t0\ttrue\n",
            "(0)\t 353\t [ 1.8662018 -1.8896044] \t0\ttrue\n",
            "(1)\t 354\t [-1.6709899  1.8910666] \t1\ttrue\n",
            "(1)\t 355\t [-2.009324   2.0192204] \t1\ttrue\n",
            "(0)\t 356\t [ 2.0895069 -2.137424 ] \t0\ttrue\n",
            "(0)\t 357\t [ 1.9783422 -1.9238094] \t0\ttrue\n",
            "(1)\t 358\t [-1.9154737  2.0870798] \t1\ttrue\n",
            "(1)\t 359\t [-1.7190236  1.6332567] \t1\ttrue\n",
            "(0)\t 360\t [ 2.1029806 -2.1349597] \t0\ttrue\n",
            "(1)\t 361\t [-1.6229299  1.8789626] \t1\ttrue\n",
            "(0)\t 362\t [ 0.35354558 -0.46776664] \t0\ttrue\n",
            "(1)\t 363\t [-2.0167196  2.1075406] \t1\ttrue\n",
            "(0)\t 364\t [ 2.1918898 -2.0918353] \t0\ttrue\n",
            "(1)\t 365\t [-1.9059902  2.0951643] \t1\ttrue\n",
            "(0)\t 366\t [ 2.0993557 -1.899825 ] \t0\ttrue\n",
            "(0)\t 367\t [-1.1626817  1.1281915] \t1\tfalse\n",
            "(1)\t 368\t [-1.6347195  1.6118999] \t1\ttrue\n",
            "(1)\t 369\t [-1.9636266  1.9926072] \t1\ttrue\n",
            "(0)\t 370\t [ 2.122648  -2.1717513] \t0\ttrue\n",
            "(0)\t 371\t [ 2.1996555 -2.1063366] \t0\ttrue\n",
            "(1)\t 372\t [-2.0458422  2.0191553] \t1\ttrue\n",
            "(1)\t 373\t [-1.6837708  1.9565643] \t1\ttrue\n",
            "(0)\t 374\t [ 2.0892448 -2.0619884] \t0\ttrue\n",
            "(0)\t 375\t [ 1.7931668 -1.7480409] \t0\ttrue\n",
            "(1)\t 376\t [-1.8066905  2.037593 ] \t1\ttrue\n",
            "(1)\t 377\t [-1.1707276  1.0794569] \t1\ttrue\n",
            "(0)\t 378\t [ 2.134094 -2.114122] \t0\ttrue\n",
            "(1)\t 379\t [ 2.125611 -2.109455] \t0\tfalse\n",
            "(0)\t 380\t [ 1.9980289 -1.9500033] \t0\ttrue\n",
            "(0)\t 381\t [ 2.1494923 -2.189238 ] \t0\ttrue\n",
            "(1)\t 382\t [ 0.17609017 -0.36377448] \t0\tfalse\n",
            "(0)\t 383\t [ 0.17609017 -0.36377448] \t0\ttrue\n",
            "(0)\t 384\t [ 1.7568774 -1.8665181] \t0\ttrue\n",
            "(0)\t 385\t [ 2.1683187 -2.2495706] \t0\ttrue\n",
            "(0)\t 386\t [ 2.1118703 -2.1331358] \t0\ttrue\n",
            "(1)\t 387\t [-1.4948854  1.8082386] \t1\ttrue\n",
            "(0)\t 388\t [ 2.1468813 -2.1579735] \t0\ttrue\n",
            "(1)\t 389\t [ 0.26866454 -0.37989086] \t0\tfalse\n",
            "(0)\t 390\t [ 0.26866454 -0.37989086] \t0\ttrue\n",
            "(0)\t 391\t [ 1.7644272 -1.7768986] \t0\ttrue\n",
            "(0)\t 392\t [ 1.3920426 -1.378724 ] \t0\ttrue\n",
            "(1)\t 393\t [-1.7798535  2.0072057] \t1\ttrue\n",
            "(0)\t 394\t [-0.2913833  0.0224156] \t1\tfalse\n",
            "(1)\t 395\t [ 1.8244203 -1.7802764] \t0\tfalse\n",
            "(0)\t 396\t [ 2.1353898 -2.0864456] \t0\ttrue\n",
            "(1)\t 397\t [-1.5772204  1.8734471] \t1\ttrue\n",
            "(0)\t 398\t [-1.8704659  1.9289486] \t1\tfalse\n",
            "(1)\t 399\t [-1.6271608  1.595826 ] \t1\ttrue\n",
            "(0)\t 400\t [ 2.1493058 -2.1858494] \t0\ttrue\n",
            "(0)\t 401\t [ 2.072248  -2.2155724] \t0\ttrue\n",
            "(1)\t 402\t [-1.706673   1.9279248] \t1\ttrue\n",
            "(1)\t 403\t [-0.6971907  0.5675863] \t1\ttrue\n",
            "(0)\t 404\t [ 0.10175904 -0.28747448] \t0\ttrue\n",
            "(0)\t 405\t [ 1.4924254 -1.6160123] \t0\ttrue\n",
            "(1)\t 406\t [-1.638057   1.9114218] \t1\ttrue\n",
            "(1)\t 407\t [-1.6305246  1.9247653] \t1\ttrue\n",
            "(0)\t 408\t [ 2.1252334 -2.086701 ] \t0\ttrue\n",
            "(0)\t 409\t [ 1.2116513 -1.2063344] \t0\ttrue\n",
            "(0)\t 410\t [ 2.153189 -2.195395] \t0\ttrue\n",
            "(1)\t 411\t [-1.6250226  1.884045 ] \t1\ttrue\n",
            "(0)\t 412\t [ 2.0473866 -2.1647644] \t0\ttrue\n",
            "(0)\t 413\t [ 2.1393793 -2.0773163] \t0\ttrue\n",
            "(1)\t 414\t [-1.9130837  2.0665665] \t1\ttrue\n",
            "(1)\t 415\t [-1.6561264  1.921408 ] \t1\ttrue\n",
            "(0)\t 416\t [ 2.0500433 -2.1042886] \t0\ttrue\n",
            "(1)\t 417\t [-1.5037767  1.8098246] \t1\ttrue\n",
            "(0)\t 418\t [ 2.1235528 -2.078608 ] \t0\ttrue\n",
            "(0)\t 419\t [ 1.7793128 -1.8155321] \t0\ttrue\n",
            "(1)\t 420\t [-1.7905227  1.8674088] \t1\ttrue\n",
            "(1)\t 421\t [-1.8851372  2.0788848] \t1\ttrue\n",
            "(0)\t 422\t [ 1.4598237 -1.5150739] \t0\ttrue\n",
            "(1)\t 423\t [-1.3155209  1.2902578] \t1\ttrue\n",
            "(0)\t 424\t [ 1.0124674  -0.90863895] \t0\ttrue\n",
            "(0)\t 425\t [ 1.0736526 -0.9744621] \t0\ttrue\n",
            "(1)\t 426\t [-1.8540909  1.8412112] \t1\ttrue\n",
            "(0)\t 427\t [-1.0092555  0.9784237] \t1\tfalse\n",
            "(1)\t 428\t [-1.5843105  1.8125521] \t1\ttrue\n",
            "(0)\t 429\t [ 1.809201  -1.9172819] \t0\ttrue\n",
            "(1)\t 430\t [ 0.30687696 -0.34866357] \t0\tfalse\n",
            "(0)\t 431\t [ 1.5393459 -1.3230795] \t0\ttrue\n",
            "(0)\t 432\t [ 0.6121563 -0.6299885] \t0\ttrue\n",
            "(1)\t 433\t [-1.7425379  1.7535452] \t1\ttrue\n",
            "(1)\t 434\t [-1.6326264  1.5615989] \t1\ttrue\n",
            "(0)\t 435\t [-0.3136145  0.2671241] \t1\tfalse\n",
            "(0)\t 436\t [ 2.090602  -2.1599965] \t0\ttrue\n",
            "(0)\t 437\t [ 0.44466665 -0.53022367] \t0\ttrue\n",
            "(1)\t 438\t [-0.9713884   0.93058753] \t1\ttrue\n",
            "(0)\t 439\t [-1.520089   1.8092226] \t1\tfalse\n",
            "(1)\t 440\t [-1.8421172  2.0429692] \t1\ttrue\n",
            "(1)\t 441\t [-1.5699598  1.8391653] \t1\ttrue\n",
            "(0)\t 442\t [ 2.1167467 -2.1306682] \t0\ttrue\n",
            "(0)\t 443\t [ 2.0853112 -2.1330998] \t0\ttrue\n",
            "(1)\t 444\t [-1.5625585  1.8402646] \t1\ttrue\n",
            "(0)\t 445\t [ 0.5889606  -0.90002817] \t0\ttrue\n",
            "(0)\t 446\t [ 2.1402102 -2.1970837] \t0\ttrue\n",
            "(0)\t 447\t [ 1.7599864 -1.76919  ] \t0\ttrue\n",
            "(0)\t 448\t [ 0.86805385 -0.92529655] \t0\ttrue\n",
            "(0)\t 449\t [ 1.5552552 -1.5121607] \t0\ttrue\n",
            "(1)\t 450\t [-1.790937   2.0149698] \t1\ttrue\n",
            "(1)\t 451\t [-0.00270718 -0.24129616] \t0\tfalse\n",
            "(0)\t 452\t [-0.00270718 -0.24129616] \t0\ttrue\n",
            "(1)\t 453\t [-1.8737662  2.066438 ] \t1\ttrue\n",
            "(0)\t 454\t [ 0.6527798 -0.9642921] \t0\ttrue\n",
            "(0)\t 455\t [ 1.4146706 -1.4192442] \t0\ttrue\n",
            "(0)\t 456\t [ 2.0167608 -1.9867699] \t0\ttrue\n",
            "(1)\t 457\t [-1.5451899  1.8284451] \t1\ttrue\n",
            "(1)\t 458\t [-1.4352998  1.7588308] \t1\ttrue\n",
            "(0)\t 459\t [ 2.1653254 -2.139145 ] \t0\ttrue\n",
            "(0)\t 460\t [ 1.57783   -1.4796578] \t0\ttrue\n",
            "(1)\t 461\t [-1.8116822  1.9267222] \t1\ttrue\n",
            "(0)\t 462\t [ 2.2050488 -2.2254593] \t0\ttrue\n",
            "(0)\t 463\t [ 2.098885 -2.05858 ] \t0\ttrue\n",
            "(1)\t 464\t [-1.7034341  1.869654 ] \t1\ttrue\n",
            "(0)\t 465\t [ 2.0356417 -2.2063298] \t0\ttrue\n",
            "(1)\t 466\t [-1.6327735  1.9025501] \t1\ttrue\n",
            "(0)\t 467\t [ 1.8626151 -1.8345085] \t0\ttrue\n",
            "(0)\t 468\t [ 1.9308184 -1.7978514] \t0\ttrue\n",
            "(0)\t 469\t [ 2.1188245 -2.0539968] \t0\ttrue\n",
            "(0)\t 470\t [ 1.6899194 -1.707244 ] \t0\ttrue\n",
            "(1)\t 471\t [-1.5751392  1.8520383] \t1\ttrue\n",
            "(0)\t 472\t [ 2.1387258 -2.2219403] \t0\ttrue\n",
            "(1)\t 473\t [-1.9524891  1.9818715] \t1\ttrue\n",
            "(0)\t 474\t [ 1.9631759 -2.0700538] \t0\ttrue\n",
            "(1)\t 475\t [-1.6136606  1.686204 ] \t1\ttrue\n",
            "(0)\t 476\t [ 2.1609428 -2.0897217] \t0\ttrue\n",
            "(0)\t 477\t [ 1.911678  -1.8943474] \t0\ttrue\n",
            "(1)\t 478\t [-0.98327166  1.0574269 ] \t1\ttrue\n",
            "(0)\t 479\t [ 1.9649838 -2.0326886] \t0\ttrue\n",
            "(1)\t 480\t [-1.8081492  2.0190904] \t1\ttrue\n",
            "(1)\t 481\t [-1.8137757  1.9251795] \t1\ttrue\n",
            "(0)\t 482\t [-1.3816581  1.3896406] \t1\tfalse\n",
            "(1)\t 483\t [-1.4502267  1.7726642] \t1\ttrue\n",
            "(0)\t 484\t [ 2.0343032 -2.0993786] \t0\ttrue\n",
            "(1)\t 485\t [-1.5851465  1.8478596] \t1\ttrue\n",
            "(0)\t 486\t [ 1.8987594 -2.0136786] \t0\ttrue\n",
            "(0)\t 487\t [ 2.1580336 -2.2207477] \t0\ttrue\n",
            "(1)\t 488\t [-1.5817797  1.8363943] \t1\ttrue\n",
            "(0)\t 489\t [-1.2295045  1.1181904] \t1\tfalse\n",
            "(1)\t 490\t [-0.9478718  0.8776604] \t1\ttrue\n",
            "(1)\t 491\t [-1.981815   2.1304631] \t1\ttrue\n",
            "(0)\t 492\t [ 0.8173045 -0.8299948] \t0\ttrue\n",
            "(0)\t 493\t [ 2.1402097 -2.0933511] \t0\ttrue\n",
            "(1)\t 494\t [-2.0131588  2.0639005] \t1\ttrue\n",
            "(0)\t 495\t [ 2.0545893 -2.0906081] \t0\ttrue\n",
            "(0)\t 496\t [-1.3001784  1.3100318] \t1\tfalse\n",
            "(1)\t 497\t [-1.9212593  1.9704317] \t1\ttrue\n",
            "(1)\t 498\t [-1.5764071  1.8589356] \t1\ttrue\n",
            "(0)\t 499\t [ 1.991987  -2.0406175] \t0\ttrue\n",
            "(1)\t 500\t [-1.6840128  1.93619  ] \t1\ttrue\n",
            "(0)\t 501\t [ 1.3776317 -1.4229112] \t0\ttrue\n",
            "(1)\t 502\t [-1.9564247  2.1217806] \t1\ttrue\n",
            "(0)\t 503\t [ 2.101248 -2.127043] \t0\ttrue\n",
            "(0)\t 504\t [ 0.06439329 -0.34912294] \t0\ttrue\n",
            "(1)\t 505\t [ 0.06439329 -0.34912294] \t0\tfalse\n",
            "(0)\t 506\t [ 2.1025786 -2.140223 ] \t0\ttrue\n",
            "(0)\t 507\t [ 1.7222552 -1.6099191] \t0\ttrue\n",
            "(1)\t 508\t [-1.9664432  2.0911183] \t1\ttrue\n",
            "(0)\t 509\t [ 2.1011767 -2.1162531] \t0\ttrue\n",
            "(0)\t 510\t [ 2.1758769 -2.1094723] \t0\ttrue\n",
            "(0)\t 511\t [ 2.148869 -2.164342] \t0\ttrue\n",
            "(0)\t 512\t [ 1.18464   -1.3528197] \t0\ttrue\n",
            "(1)\t 513\t [ 1.9329967 -1.9918044] \t0\tfalse\n",
            "(0)\t 514\t [ 0.38084146 -0.31014982] \t0\ttrue\n",
            "(0)\t 515\t [ 1.4984285 -1.4393537] \t0\ttrue\n",
            "(0)\t 516\t [ 2.118092  -2.1186857] \t0\ttrue\n",
            "(1)\t 517\t [-1.9627377  2.1155915] \t1\ttrue\n",
            "(1)\t 518\t [-1.8459152  2.0542364] \t1\ttrue\n",
            "(0)\t 519\t [ 1.4109677 -1.4183933] \t0\ttrue\n",
            "(0)\t 520\t [-2.0006835  2.0001497] \t1\tfalse\n",
            "(1)\t 521\t [-1.845496   2.0205357] \t1\ttrue\n",
            "(0)\t 522\t [ 1.4136729 -1.4045836] \t0\ttrue\n",
            "(1)\t 523\t [-1.3960868  1.4923294] \t1\ttrue\n",
            "(0)\t 524\t [ 1.8125484 -1.8062414] \t0\ttrue\n",
            "(1)\t 525\t [-1.80711    1.9897908] \t1\ttrue\n",
            "(0)\t 526\t [ 2.093788  -2.0822077] \t0\ttrue\n",
            "(0)\t 527\t [-1.4510121  1.5397619] \t1\tfalse\n",
            "(1)\t 528\t [-1.52636   1.746429] \t1\ttrue\n",
            "(0)\t 529\t [ 0.47283992 -0.5545746 ] \t0\ttrue\n",
            "(1)\t 530\t [-1.7589744  2.0177538] \t1\ttrue\n",
            "(0)\t 531\t [-1.6133065  1.6364908] \t1\tfalse\n",
            "(1)\t 532\t [-1.9308056  1.9741851] \t1\ttrue\n",
            "(0)\t 533\t [ 2.1045213 -2.1962054] \t0\ttrue\n",
            "(1)\t 534\t [-1.8527822  2.030457 ] \t1\ttrue\n",
            "(0)\t 535\t [ 2.156336  -2.1155148] \t0\ttrue\n",
            "(0)\t 536\t [ 2.1611378 -2.113269 ] \t0\ttrue\n",
            "(1)\t 537\t [-1.5473787  1.8187517] \t1\ttrue\n",
            "(1)\t 538\t [-1.2940873  1.3067797] \t1\ttrue\n",
            "(0)\t 539\t [ 2.0774853 -2.0166256] \t0\ttrue\n",
            "(0)\t 540\t [ 1.7915992 -1.742637 ] \t0\ttrue\n",
            "(1)\t 541\t [-1.5431688  1.8324589] \t1\ttrue\n",
            "(0)\t 542\t [ 2.1409895 -2.2096212] \t0\ttrue\n",
            "(1)\t 543\t [-1.6419822  1.8950056] \t1\ttrue\n",
            "(0)\t 544\t [ 2.1602762 -2.1022828] \t0\ttrue\n",
            "(0)\t 545\t [ 0.28926244 -0.6048805 ] \t0\ttrue\n",
            "(0)\t 546\t [-1.3433913  1.412282 ] \t1\tfalse\n",
            "(1)\t 547\t [-1.7598239  2.0286236] \t1\ttrue\n",
            "(1)\t 548\t [ 0.19115111 -0.43054417] \t0\tfalse\n",
            "(0)\t 549\t [ 0.19115111 -0.43054417] \t0\ttrue\n",
            "(0)\t 550\t [ 2.0180352 -2.0030003] \t0\ttrue\n",
            "(1)\t 551\t [-1.7894335  2.019619 ] \t1\ttrue\n",
            "(0)\t 552\t [ 1.9549145 -1.9273775] \t0\ttrue\n",
            "(1)\t 553\t [-1.0375117   0.92408407] \t1\ttrue\n",
            "(1)\t 554\t [-1.9246273  2.0514708] \t1\ttrue\n",
            "(0)\t 555\t [ 2.0952663 -2.1130977] \t0\ttrue\n",
            "(0)\t 556\t [ 1.5297035 -1.6756047] \t0\ttrue\n",
            "(1)\t 557\t [-1.605105   1.8746333] \t1\ttrue\n",
            "(0)\t 558\t [ 1.9591169 -1.9335178] \t0\ttrue\n",
            "(0)\t 559\t [ 2.1117022 -2.1881676] \t0\ttrue\n",
            "(1)\t 560\t [-1.6765088  1.9526274] \t1\ttrue\n",
            "(1)\t 561\t [-1.8932543  1.9890865] \t1\ttrue\n",
            "(0)\t 562\t [-1.3608291  1.4988219] \t1\tfalse\n",
            "(0)\t 563\t [-0.11311178 -0.05403936] \t1\tfalse\n",
            "(1)\t 564\t [-0.11311178 -0.05403936] \t1\ttrue\n",
            "(0)\t 565\t [ 1.7369307 -1.7635843] \t0\ttrue\n",
            "(1)\t 566\t [-1.7582587  1.9270135] \t1\ttrue\n",
            "(1)\t 567\t [-1.6408067  1.8813355] \t1\ttrue\n",
            "(0)\t 568\t [ 2.1589367 -2.1231818] \t0\ttrue\n",
            "(0)\t 569\t [ 2.1253762 -2.1451433] \t0\ttrue\n",
            "(1)\t 570\t [-1.6138941  1.8770059] \t1\ttrue\n",
            "(0)\t 571\t [ 1.4014188 -1.4919966] \t0\ttrue\n",
            "(1)\t 572\t [-1.1359828  1.2601199] \t1\ttrue\n",
            "(0)\t 573\t [ 1.923905  -1.9982657] \t0\ttrue\n",
            "(1)\t 574\t [-1.5724801  1.8539001] \t1\ttrue\n",
            "(1)\t 575\t [-1.573742   1.8622046] \t1\ttrue\n",
            "(0)\t 576\t [ 1.8603345 -1.9859786] \t0\ttrue\n",
            "(1)\t 577\t [-1.6690068  1.7076846] \t1\ttrue\n",
            "(0)\t 578\t [-1.6690068  1.7076846] \t1\tfalse\n",
            "(1)\t 579\t [-1.998505   1.9650544] \t1\ttrue\n",
            "(0)\t 580\t [ 2.1697378 -2.1812246] \t0\ttrue\n",
            "(0)\t 581\t [ 0.09343249 -0.312199  ] \t0\ttrue\n",
            "(1)\t 582\t [ 0.09343249 -0.312199  ] \t0\tfalse\n",
            "(0)\t 583\t [ 0.1309417  -0.34643716] \t0\ttrue\n",
            "(1)\t 584\t [ 0.1309417  -0.34643716] \t0\tfalse\n",
            "(1)\t 585\t [-1.6556574  1.9438181] \t1\ttrue\n",
            "(0)\t 586\t [ 1.4328585 -1.567364 ] \t0\ttrue\n",
            "(1)\t 587\t [-1.5371425  1.841619 ] \t1\ttrue\n",
            "(0)\t 588\t [-1.9056491  1.896633 ] \t1\tfalse\n",
            "(1)\t 589\t [-1.7037579  1.9593781] \t1\ttrue\n",
            "(0)\t 590\t [-1.7037579  1.9593781] \t1\tfalse\n",
            "(0)\t 591\t [-1.8079152  1.7933636] \t1\tfalse\n",
            "(1)\t 592\t [-1.9740901  2.0626173] \t1\ttrue\n",
            "(0)\t 593\t [ 1.8844148 -1.9823344] \t0\ttrue\n",
            "(1)\t 594\t [-1.8199201  1.8914957] \t1\ttrue\n",
            "(1)\t 595\t [-2.001526   2.0069215] \t1\ttrue\n",
            "(0)\t 596\t [-1.1337606  1.1250552] \t1\tfalse\n",
            "(1)\t 597\t [-1.8075038  1.8568425] \t1\ttrue\n",
            "(0)\t 598\t [ 1.9382244 -1.9608808] \t0\ttrue\n",
            "(1)\t 599\t [-1.920596   1.9345571] \t1\ttrue\n",
            "(0)\t 600\t [ 1.7682359 -1.668626 ] \t0\ttrue\n",
            "(0)\t 601\t [-0.37510958  0.29273996] \t1\tfalse\n",
            "(1)\t 602\t [-1.7964021  2.0086794] \t1\ttrue\n",
            "(0)\t 603\t [ 1.878055  -1.7419823] \t0\ttrue\n",
            "(1)\t 604\t [-1.2228621  1.190445 ] \t1\ttrue\n",
            "(1)\t 605\t [-1.4892944  1.8289748] \t1\ttrue\n",
            "(0)\t 606\t [ 0.7659478 -0.7688463] \t0\ttrue\n",
            "(1)\t 607\t [-1.7517413  1.7732805] \t1\ttrue\n",
            "(0)\t 608\t [ 0.8895297 -1.035654 ] \t0\ttrue\n",
            "(1)\t 609\t [-1.680864   1.9506758] \t1\ttrue\n",
            "(0)\t 610\t [ 2.1127527 -2.1953726] \t0\ttrue\n",
            "(0)\t 611\t [ 2.1666956 -2.1581845] \t0\ttrue\n",
            "(0)\t 612\t [-0.6250013   0.53236127] \t1\tfalse\n",
            "(0)\t 613\t [ 2.1526456 -2.1774168] \t0\ttrue\n",
            "(1)\t 614\t [-1.8373972  2.0440054] \t1\ttrue\n",
            "(0)\t 615\t [ 2.118537  -2.2024608] \t0\ttrue\n",
            "(1)\t 616\t [-1.2692779  1.2152215] \t1\ttrue\n",
            "(0)\t 617\t [ 0.65662366 -0.7007417 ] \t0\ttrue\n",
            "(0)\t 618\t [ 1.9900286 -1.979127 ] \t0\ttrue\n",
            "(0)\t 619\t [-0.92169523  0.99713016] \t1\tfalse\n",
            "(1)\t 620\t [-1.4653894  1.7671045] \t1\ttrue\n",
            "(0)\t 621\t [ 2.035174  -2.0683744] \t0\ttrue\n",
            "(1)\t 622\t [-1.8252821  2.0455534] \t1\ttrue\n",
            "(1)\t 623\t [-1.6771531  1.9338299] \t1\ttrue\n",
            "(0)\t 624\t [ 1.5844644 -1.5508634] \t0\ttrue\n",
            "(0)\t 625\t [-0.8830702   0.85585433] \t1\tfalse\n",
            "(0)\t 626\t [ 2.1365006 -2.1374476] \t0\ttrue\n",
            "(1)\t 627\t [-1.8986009  2.0910919] \t1\ttrue\n",
            "(1)\t 628\t [-1.7784506  1.8450826] \t1\ttrue\n",
            "(0)\t 629\t [ 2.1714756 -2.2236652] \t0\ttrue\n",
            "(1)\t 630\t [-1.8291056  2.08195  ] \t1\ttrue\n",
            "(0)\t 631\t [ 1.9394896 -2.04022  ] \t0\ttrue\n",
            "(0)\t 632\t [ 2.071638 -2.132726] \t0\ttrue\n",
            "(1)\t 633\t [-0.61360586  0.40197346] \t1\ttrue\n",
            "(0)\t 634\t [ 0.04343924 -0.30411702] \t0\ttrue\n",
            "(1)\t 635\t [-1.0187182  0.8509879] \t1\ttrue\n",
            "(0)\t 636\t [ 1.354718  -1.3716606] \t0\ttrue\n",
            "(1)\t 637\t [-1.9723811  2.0468974] \t1\ttrue\n",
            "(1)\t 638\t [-1.5631021  1.8190951] \t1\ttrue\n",
            "(0)\t 639\t [ 2.0492966 -2.1756842] \t0\ttrue\n",
            "(0)\t 640\t [ 1.1204097 -1.2515591] \t0\ttrue\n",
            "(1)\t 641\t [-1.7049398  1.7291108] \t1\ttrue\n",
            "(0)\t 642\t [ 0.33134654 -0.6158731 ] \t0\ttrue\n",
            "(1)\t 643\t [ 0.33134654 -0.6158731 ] \t0\tfalse\n",
            "(1)\t 644\t [-1.8886343  2.0159872] \t1\ttrue\n",
            "(0)\t 645\t [ 1.8804543 -1.8136297] \t0\ttrue\n",
            "(0)\t 646\t [-0.9130803   0.66202736] \t1\tfalse\n",
            "(1)\t 647\t [-1.306906   1.2004054] \t1\ttrue\n",
            "(0)\t 648\t [-1.0123459  1.0515486] \t1\tfalse\n",
            "(1)\t 649\t [-1.8970597  2.0848587] \t1\ttrue\n",
            "(1)\t 650\t [-1.9767439  2.0907762] \t1\ttrue\n",
            "(0)\t 651\t [ 2.1194026 -2.2418792] \t0\ttrue\n",
            "(0)\t 652\t [ 1.752009  -1.8692147] \t0\ttrue\n",
            "(1)\t 653\t [-1.8432677  1.8547311] \t1\ttrue\n",
            "(1)\t 654\t [-1.6198634  1.8945532] \t1\ttrue\n",
            "(0)\t 655\t [ 2.028148 -2.063716] \t0\ttrue\n",
            "(1)\t 656\t [-1.6457306  1.9022862] \t1\ttrue\n",
            "(0)\t 657\t [ 1.184837  -1.2179524] \t0\ttrue\n",
            "(1)\t 658\t [ 2.0475063 -1.8950663] \t0\tfalse\n",
            "(0)\t 659\t [ 2.084967 -2.157025] \t0\ttrue\n",
            "(1)\t 660\t [ 1.9437674 -1.9151474] \t0\tfalse\n",
            "(0)\t 661\t [ 2.002089  -1.9217323] \t0\ttrue\n",
            "(0)\t 662\t [ 2.1119175 -2.0968   ] \t0\ttrue\n",
            "(1)\t 663\t [ 0.00386955 -0.1665743 ] \t0\tfalse\n",
            "(1)\t 664\t [-1.5014318  1.7918702] \t1\ttrue\n",
            "(0)\t 665\t [-1.7225598  1.7523139] \t1\tfalse\n",
            "(0)\t 666\t [ 0.48967412 -0.688347  ] \t0\ttrue\n",
            "(1)\t 667\t [-1.6573545  1.9562709] \t1\ttrue\n",
            "(1)\t 668\t [-1.9531379  1.9953359] \t1\ttrue\n",
            "(0)\t 669\t [ 2.007904  -1.9227505] \t0\ttrue\n",
            "(0)\t 670\t [ 2.0102053 -2.0099807] \t0\ttrue\n",
            "(0)\t 671\t [ 1.7291778 -1.8831416] \t0\ttrue\n",
            "(1)\t 672\t [-1.7358212  1.9790756] \t1\ttrue\n",
            "(0)\t 673\t [ 1.0588249 -1.0808262] \t0\ttrue\n",
            "(1)\t 674\t [-1.7243385  1.9917524] \t1\ttrue\n",
            "(0)\t 675\t [ 2.12552  -2.247714] \t0\ttrue\n",
            "(1)\t 676\t [-1.7044048  1.925152 ] \t1\ttrue\n",
            "(0)\t 677\t [-0.39540458  0.205227  ] \t1\tfalse\n",
            "(0)\t 678\t [ 1.8429418 -1.8483244] \t0\ttrue\n",
            "(0)\t 679\t [ 1.8958212 -1.9486274] \t0\ttrue\n",
            "(1)\t 680\t [-1.94342    2.1243136] \t1\ttrue\n",
            "(0)\t 681\t [ 1.94901   -2.1055799] \t0\ttrue\n",
            "(1)\t 682\t [-1.5433829  1.8460095] \t1\ttrue\n",
            "(0)\t 683\t [ 1.1175404 -1.094802 ] \t0\ttrue\n",
            "(0)\t 684\t [ 2.1127071 -2.0632503] \t0\ttrue\n",
            "(1)\t 685\t [-1.261575   1.2910359] \t1\ttrue\n",
            "(0)\t 686\t [ 2.1188679 -2.2106383] \t0\ttrue\n",
            "(1)\t 687\t [-1.9177366  1.9985958] \t1\ttrue\n",
            "(0)\t 688\t [-0.29499438  0.23402296] \t1\tfalse\n",
            "(0)\t 689\t [ 1.4474812 -1.366244 ] \t0\ttrue\n",
            "(1)\t 690\t [-1.7681383  1.8405654] \t1\ttrue\n",
            "(1)\t 691\t [-1.1948968  1.1134747] \t1\ttrue\n",
            "(0)\t 692\t [ 1.9582326 -1.8774217] \t0\ttrue\n",
            "(1)\t 693\t [-1.6066225  1.894298 ] \t1\ttrue\n",
            "(0)\t 694\t [ 1.9218841 -1.9789219] \t0\ttrue\n",
            "(0)\t 695\t [ 0.87440556 -1.0533572 ] \t0\ttrue\n",
            "(1)\t 696\t [-1.8303962  2.042913 ] \t1\ttrue\n",
            "(1)\t 697\t [-1.046937   0.9728925] \t1\ttrue\n",
            "(0)\t 698\t [ 2.118383  -2.1963158] \t0\ttrue\n",
            "(0)\t 699\t [ 1.8559397 -1.9832244] \t0\ttrue\n",
            "(0)\t 700\t [-1.9490002  2.0937748] \t1\tfalse\n",
            "(1)\t 701\t [ 2.0326385 -2.021382 ] \t0\tfalse\n",
            "(0)\t 702\t [ 2.1312788 -2.1972039] \t0\ttrue\n",
            "(1)\t 703\t [-1.8236511  2.042279 ] \t1\ttrue\n",
            "(0)\t 704\t [ 1.9960523 -1.9316925] \t0\ttrue\n",
            "(0)\t 705\t [ 2.0370762 -1.9916792] \t0\ttrue\n",
            "(1)\t 706\t [-1.9696862  2.0654027] \t1\ttrue\n",
            "(0)\t 707\t [ 2.076751  -2.0440893] \t0\ttrue\n",
            "(1)\t 708\t [-1.8178517  2.0747862] \t1\ttrue\n",
            "(1)\t 709\t [-1.8634864  1.8652111] \t1\ttrue\n",
            "(0)\t 710\t [ 2.107788  -2.2072232] \t0\ttrue\n",
            "(1)\t 711\t [-1.7128023  1.9692456] \t1\ttrue\n",
            "(0)\t 712\t [ 2.1598485 -2.2070882] \t0\ttrue\n",
            "(0)\t 713\t [ 1.9187087 -1.9013518] \t0\ttrue\n",
            "(1)\t 714\t [-0.5048349   0.26822713] \t1\ttrue\n",
            "(0)\t 715\t [ 2.1340978 -2.1539917] \t0\ttrue\n",
            "(1)\t 716\t [-1.5814875  1.8583665] \t1\ttrue\n",
            "(1)\t 717\t [-0.99541044  0.7954644 ] \t1\ttrue\n",
            "(0)\t 718\t [ 0.01730515 -0.17899013] \t0\ttrue\n",
            "(0)\t 719\t [ 1.9142746 -1.9427106] \t0\ttrue\n",
            "(1)\t 720\t [-1.92066    2.0452714] \t1\ttrue\n",
            "(0)\t 721\t [-1.4248695  1.3293194] \t1\tfalse\n",
            "(0)\t 722\t [ 1.9720536 -2.0715744] \t0\ttrue\n",
            "(1)\t 723\t [-1.7393234  1.9891647] \t1\ttrue\n",
            "(0)\t 724\t [ 1.3268846 -1.4087456] \t0\ttrue\n",
            "(1)\t 725\t [-1.4439503  1.380198 ] \t1\ttrue\n",
            "(0)\t 726\t [ 1.8293681 -1.9836714] \t0\ttrue\n",
            "(1)\t 727\t [-1.7611383  1.7837268] \t1\ttrue\n",
            "(0)\t 728\t [-1.7611383  1.7837268] \t1\tfalse\n",
            "(0)\t 729\t [-1.8393925  1.9754213] \t1\tfalse\n",
            "(1)\t 730\t [-1.531284   1.4926871] \t1\ttrue\n",
            "(0)\t 731\t [ 0.5866469  -0.54720443] \t0\ttrue\n",
            "(1)\t 732\t [-1.9567276  2.079308 ] \t1\ttrue\n",
            "(1)\t 733\t [-1.8247211  2.0445297] \t1\ttrue\n",
            "(0)\t 734\t [-1.9192984  2.1451352] \t1\tfalse\n",
            "(0)\t 735\t [ 2.0367362 -2.195915 ] \t0\ttrue\n",
            "(0)\t 736\t [ 1.8694983 -1.8382174] \t0\ttrue\n",
            "(1)\t 737\t [-1.8856691  2.122326 ] \t1\ttrue\n",
            "(0)\t 738\t [ 2.1215086 -2.0970967] \t0\ttrue\n",
            "(0)\t 739\t [ 0.20510349 -0.3288165 ] \t0\ttrue\n",
            "(1)\t 740\t [-1.408523   1.4583197] \t1\ttrue\n",
            "(0)\t 741\t [-1.8191562  1.9210445] \t1\tfalse\n",
            "(1)\t 742\t [ 0.45223215 -0.62321377] \t0\tfalse\n",
            "(0)\t 743\t [ 1.7392017 -1.8129367] \t0\ttrue\n",
            "(1)\t 744\t [-1.581657   1.8734429] \t1\ttrue\n",
            "(1)\t 745\t [-1.9229835  2.0213668] \t1\ttrue\n",
            "(0)\t 746\t [-1.863012   1.8423759] \t1\tfalse\n",
            "(0)\t 747\t [ 2.1586428 -2.1068237] \t0\ttrue\n",
            "(1)\t 748\t [-1.6988913  1.9763256] \t1\ttrue\n",
            "(1)\t 749\t [-1.6309844  1.871023 ] \t1\ttrue\n",
            "(0)\t 750\t [-0.5468102  0.4127896] \t1\tfalse\n",
            "(1)\t 751\t [-1.6402009  1.8732076] \t1\ttrue\n",
            "(0)\t 752\t [ 1.7220297 -1.6566721] \t0\ttrue\n",
            "(0)\t 753\t [-0.85640043  0.732862  ] \t1\tfalse\n",
            "(0)\t 754\t [ 2.124907  -2.1096573] \t0\ttrue\n",
            "(1)\t 755\t [-1.7899107  2.0330229] \t1\ttrue\n",
            "(0)\t 756\t [-1.3764124  1.5539632] \t1\tfalse\n",
            "(1)\t 757\t [-1.8225378  2.0292883] \t1\ttrue\n",
            "(0)\t 758\t [ 1.9800304 -2.124094 ] \t0\ttrue\n",
            "(0)\t 759\t [ 1.5496119 -1.3259563] \t0\ttrue\n",
            "(0)\t 760\t [ 1.8761444 -2.1116173] \t0\ttrue\n",
            "(1)\t 761\t [-1.8090148  1.8102123] \t1\ttrue\n",
            "(0)\t 762\t [ 2.142273  -2.1146045] \t0\ttrue\n",
            "(1)\t 763\t [-1.59645    1.8806791] \t1\ttrue\n",
            "(0)\t 764\t [ 1.5407507 -1.6158838] \t0\ttrue\n",
            "(0)\t 765\t [ 0.1502692  -0.48408183] \t0\ttrue\n",
            "(0)\t 766\t [ 1.9116071 -1.8941406] \t0\ttrue\n",
            "(1)\t 767\t [-1.7815833  2.05502  ] \t1\ttrue\n",
            "(0)\t 768\t [ 1.6739818 -1.5690718] \t0\ttrue\n",
            "(1)\t 769\t [-1.6949916  1.9355968] \t1\ttrue\n",
            "(1)\t 770\t [-1.6640081  1.6095837] \t1\ttrue\n",
            "(0)\t 771\t [-2.004919   2.0918114] \t1\tfalse\n",
            "(0)\t 772\t [ 2.1223347 -2.2040007] \t0\ttrue\n",
            "(1)\t 773\t [-1.9217008  2.0885062] \t1\ttrue\n",
            "(0)\t 774\t [-1.8541179  2.0451841] \t1\tfalse\n",
            "(0)\t 775\t [ 2.1435473 -2.254665 ] \t0\ttrue\n",
            "(1)\t 776\t [-1.6193949  1.8818473] \t1\ttrue\n",
            "(1)\t 777\t [-1.5696683  1.813041 ] \t1\ttrue\n",
            "(1)\t 778\t [-1.5416162  1.8226814] \t1\ttrue\n",
            "(0)\t 779\t [ 2.0572348 -2.1588287] \t0\ttrue\n",
            "(1)\t 780\t [-1.5909195  1.8818443] \t1\ttrue\n",
            "(0)\t 781\t [ 2.112854  -2.1826394] \t0\ttrue\n",
            "(0)\t 782\t [ 2.1576445 -2.2450738] \t0\ttrue\n",
            "(1)\t 783\t [-1.5703604  1.8497732] \t1\ttrue\n",
            "(0)\t 784\t [-1.2903659  1.1842844] \t1\tfalse\n",
            "(1)\t 785\t [-1.9461285  2.0323844] \t1\ttrue\n",
            "(0)\t 786\t [ 2.123654  -2.0591357] \t0\ttrue\n",
            "(1)\t 787\t [-1.9216431  2.0837271] \t1\ttrue\n",
            "(0)\t 788\t [ 2.1175256 -2.1621885] \t0\ttrue\n",
            "(0)\t 789\t [ 2.116254 -2.110834] \t0\ttrue\n",
            "(1)\t 790\t [-1.869667   2.0570455] \t1\ttrue\n",
            "(0)\t 791\t [ 1.9699647 -2.059424 ] \t0\ttrue\n",
            "(1)\t 792\t [-1.551028   1.8631898] \t1\ttrue\n",
            "(1)\t 793\t [ 1.2574471 -1.3877375] \t0\tfalse\n",
            "(0)\t 794\t [ 1.2574471 -1.3877375] \t0\ttrue\n",
            "(0)\t 795\t [ 2.1562533 -2.1397877] \t0\ttrue\n",
            "(1)\t 796\t [-1.8173364  1.7949449] \t1\ttrue\n",
            "(1)\t 797\t [-1.8572832  1.9110353] \t1\ttrue\n",
            "(0)\t 798\t [ 0.8479633 -1.0071028] \t0\ttrue\n",
            "(0)\t 799\t [-0.02481497 -0.22611867] \t0\ttrue\n",
            "(1)\t 800\t [-0.02481497 -0.22611867] \t0\tfalse\n",
            "(0)\t 801\t [-0.02481497 -0.22611867] \t0\ttrue\n",
            "(1)\t 802\t [-0.02481497 -0.22611867] \t0\tfalse\n",
            "(1)\t 803\t [-1.8421828  1.8675932] \t1\ttrue\n",
            "(0)\t 804\t [-1.1711696  1.0423483] \t1\tfalse\n",
            "(0)\t 805\t [ 2.0846045 -2.034461 ] \t0\ttrue\n",
            "(1)\t 806\t [ 2.0861993 -1.9933366] \t0\tfalse\n",
            "(0)\t 807\t [ 2.1449394 -2.1229396] \t0\ttrue\n",
            "(1)\t 808\t [-1.9905447  2.1059515] \t1\ttrue\n",
            "(0)\t 809\t [ 2.1056895 -2.20914  ] \t0\ttrue\n",
            "(1)\t 810\t [-1.6893452  1.9301453] \t1\ttrue\n",
            "(0)\t 811\t [ 1.5564165 -1.5094235] \t0\ttrue\n",
            "(0)\t 812\t [ 2.0869024 -2.1873372] \t0\ttrue\n",
            "(1)\t 813\t [-1.8125292  2.050017 ] \t1\ttrue\n",
            "(0)\t 814\t [-1.2498837  1.1389216] \t1\tfalse\n",
            "(1)\t 815\t [-1.5319232  1.8246896] \t1\ttrue\n",
            "(0)\t 816\t [ 2.088547 -2.144655] \t0\ttrue\n",
            "(1)\t 817\t [-0.6333013   0.49047893] \t1\ttrue\n",
            "(0)\t 818\t [ 1.971092  -2.0906317] \t0\ttrue\n",
            "(1)\t 819\t [-1.6811814  1.6252537] \t1\ttrue\n",
            "(0)\t 820\t [-0.30229017  0.21626131] \t1\tfalse\n",
            "(0)\t 821\t [ 0.55377066 -0.7840067 ] \t0\ttrue\n",
            "(0)\t 822\t [ 2.0079463 -2.0295453] \t0\ttrue\n",
            "(0)\t 823\t [-1.6696075  1.6591768] \t1\tfalse\n",
            "(0)\t 824\t [ 1.7576404 -1.73578  ] \t0\ttrue\n",
            "(1)\t 825\t [-1.7386776  1.8004289] \t1\ttrue\n",
            "(0)\t 826\t [ 1.8047161 -1.8429216] \t0\ttrue\n",
            "(1)\t 827\t [-1.8818846  2.004294 ] \t1\ttrue\n",
            "(0)\t 828\t [ 2.1418839 -2.1790366] \t0\ttrue\n",
            "(1)\t 829\t [-1.5959277  1.8976084] \t1\ttrue\n",
            "(1)\t 830\t [-1.5215273  1.8505791] \t1\ttrue\n",
            "(0)\t 831\t [ 2.1465375 -2.0830023] \t0\ttrue\n",
            "(1)\t 832\t [-1.4778168  1.7538486] \t1\ttrue\n",
            "(0)\t 833\t [-0.05409175 -0.10438292] \t0\ttrue\n",
            "(0)\t 834\t [ 2.0425744 -2.0609255] \t0\ttrue\n",
            "(1)\t 835\t [-1.6181788  1.5613302] \t1\ttrue\n",
            "(1)\t 836\t [-1.8727326  2.080938 ] \t1\ttrue\n",
            "(0)\t 837\t [ 1.7747159 -1.7783482] \t0\ttrue\n",
            "(0)\t 838\t [ 0.36252648 -0.48347402] \t0\ttrue\n",
            "(0)\t 839\t [ 2.0059052 -2.0543187] \t0\ttrue\n",
            "(1)\t 840\t [-1.7487186  2.0266244] \t1\ttrue\n",
            "(1)\t 841\t [-1.8741654  2.09498  ] \t1\ttrue\n",
            "(0)\t 842\t [ 1.9546821 -2.0626707] \t0\ttrue\n",
            "(0)\t 843\t [ 1.6980032 -1.767833 ] \t0\ttrue\n",
            "(1)\t 844\t [-1.904294   2.0921397] \t1\ttrue\n",
            "(0)\t 845\t [ 1.2948091 -1.284531 ] \t0\ttrue\n",
            "(1)\t 846\t [ 0.3704106  -0.46193552] \t0\tfalse\n",
            "(0)\t 847\t [ 1.9188714 -1.8815504] \t0\ttrue\n",
            "(1)\t 848\t [-0.49964684  0.34227398] \t1\ttrue\n",
            "(1)\t 849\t [-0.87569356  0.8289818 ] \t1\ttrue\n",
            "(0)\t 850\t [-0.73294723  0.66824   ] \t1\tfalse\n",
            "(1)\t 851\t [-1.3577346  1.2790037] \t1\ttrue\n",
            "(0)\t 852\t [ 1.0685933 -1.1074293] \t0\ttrue\n",
            "(0)\t 853\t [ 1.6559246 -1.6216401] \t0\ttrue\n",
            "(1)\t 854\t [-1.7832539  2.0445223] \t1\ttrue\n",
            "(0)\t 855\t [ 2.0665064 -1.986465 ] \t0\ttrue\n",
            "(0)\t 856\t [ 0.31195027 -0.26215705] \t0\ttrue\n",
            "(1)\t 857\t [-1.0857552  1.1059353] \t1\ttrue\n",
            "(0)\t 858\t [ 1.8153968 -1.8322897] \t0\ttrue\n",
            "(1)\t 859\t [ 0.1127221  -0.34056392] \t0\tfalse\n",
            "(0)\t 860\t [ 0.1127221  -0.34056392] \t0\ttrue\n",
            "(0)\t 861\t [ 0.09126098 -0.30832183] \t0\ttrue\n",
            "(1)\t 862\t [ 0.09126098 -0.30832183] \t0\tfalse\n",
            "(0)\t 863\t [ 1.7660633 -1.769022 ] \t0\ttrue\n",
            "(0)\t 864\t [ 1.4766694 -1.6147666] \t0\ttrue\n",
            "(1)\t 865\t [-1.7043864  1.9495949] \t1\ttrue\n",
            "(0)\t 866\t [ 0.02983535 -0.17652076] \t0\ttrue\n",
            "(1)\t 867\t [ 1.9997399 -2.018718 ] \t0\tfalse\n",
            "(0)\t 868\t [ 1.6783404 -1.6433446] \t0\ttrue\n",
            "(0)\t 869\t [ 2.1411161 -2.0970635] \t0\ttrue\n",
            "(0)\t 870\t [ 1.8861381 -1.8240477] \t0\ttrue\n",
            "(1)\t 871\t [-2.019345  2.027266] \t1\ttrue\n",
            "(0)\t 872\t [ 1.9635135 -1.9627573] \t0\ttrue\n",
            "(0)\t 873\t [ 2.113354  -2.2150433] \t0\ttrue\n",
            "(1)\t 874\t [-1.724639   1.9915751] \t1\ttrue\n",
            "(1)\t 875\t [-1.9286579  2.1357808] \t1\ttrue\n",
            "(0)\t 876\t [-1.8891579  1.8693386] \t1\tfalse\n",
            "(0)\t 877\t [-0.9761857  0.9088709] \t1\tfalse\n",
            "(1)\t 878\t [-1.6960242  1.9203941] \t1\ttrue\n",
            "(1)\t 879\t [ 2.073589 -2.168035] \t0\tfalse\n",
            "(0)\t 880\t [ 1.9772274 -2.0439448] \t0\ttrue\n",
            "(0)\t 881\t [ 2.148295  -2.2036119] \t0\ttrue\n",
            "(1)\t 882\t [-0.6882051   0.62414247] \t1\ttrue\n",
            "(1)\t 883\t [ 0.09710555 -0.390206  ] \t0\tfalse\n",
            "(0)\t 884\t [ 0.09710555 -0.390206  ] \t0\ttrue\n",
            "(0)\t 885\t [ 1.74703   -1.8282715] \t0\ttrue\n",
            "(1)\t 886\t [-1.854503   2.0827036] \t1\ttrue\n",
            "(0)\t 887\t [ 0.29512212 -0.4716057 ] \t0\ttrue\n",
            "(1)\t 888\t [-1.8202729  2.000452 ] \t1\ttrue\n",
            "(0)\t 889\t [ 2.0308018 -2.0606134] \t0\ttrue\n",
            "(1)\t 890\t [-1.6435648  1.7616898] \t1\ttrue\n",
            "(0)\t 891\t [ 0.18935673 -0.45241305] \t0\ttrue\n",
            "(1)\t 892\t [ 0.18935673 -0.45241305] \t0\tfalse\n",
            "(1)\t 893\t [-0.62569743  0.4713408 ] \t1\ttrue\n",
            "(0)\t 894\t [ 2.1228366 -2.1712363] \t0\ttrue\n",
            "(1)\t 895\t [-1.9704846  1.9750154] \t1\ttrue\n",
            "(0)\t 896\t [ 2.1588783 -2.1713185] \t0\ttrue\n",
            "(0)\t 897\t [ 1.6721454 -1.5952928] \t0\ttrue\n",
            "(1)\t 898\t [ 1.6390721 -1.589609 ] \t0\tfalse\n",
            "(0)\t 899\t [ 2.1163447 -2.1203053] \t0\ttrue\n",
            "(1)\t 900\t [-1.5800191  1.8839139] \t1\ttrue\n",
            "Number of true predictions: 768\n",
            "Number of false predictions: 132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIgsWWCTKdY",
        "colab_type": "code",
        "outputId": "48b20713-6cb6-418d-8ae1-8cf637a59e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.23862375138734 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print('True positives: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "outputId": "5df25529-c7ae-4857-a126-f8f94e35a0e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI2K1bqaR5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "outputId": "dff13dbc-4975-44df-e746-b75e28885eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QowNXrYZSRMR",
        "colab_type": "code",
        "outputId": "c5e9719f-8013-4b16-c522-5be0c036980b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "print(flat_predictions)\n",
        "print(\"************\")\n",
        "print(flat_true_labels)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1\n",
            " 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1\n",
            " 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1\n",
            " 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0\n",
            " 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 1 1 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 1 0 1 0 0 0 0 1]\n",
            "************\n",
            "[1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "outputId": "d961eb32-7891-4d4f-a83d-83f3746274f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-task1hindi'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task1hindi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task1hindi/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1hindi/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1hindi/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}
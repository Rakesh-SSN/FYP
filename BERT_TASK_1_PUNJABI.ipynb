{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT TASK 1 PUNJABI",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1157db526165427e9197c667d0038dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f575100eae684ebcb1112f1d7c1bda6a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_09d5945030dc4e45b6204c4bc4149b27",
              "IPY_MODEL_d829be394b2d478eae71d575424d1d9b"
            ]
          }
        },
        "f575100eae684ebcb1112f1d7c1bda6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09d5945030dc4e45b6204c4bc4149b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0326856aeb9d4391bf812fd096e55ffd",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49d79de9235240d8ae3695a0f190cc73"
          }
        },
        "d829be394b2d478eae71d575424d1d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_99a60fda3bed43fcaa9081bb2ce44c7f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 996k/996k [00:00&lt;00:00, 1.94MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_606d474cc1364e438f203a0c036faf66"
          }
        },
        "0326856aeb9d4391bf812fd096e55ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49d79de9235240d8ae3695a0f190cc73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99a60fda3bed43fcaa9081bb2ce44c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "606d474cc1364e438f203a0c036faf66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91c0e7c798a948cb9b982a1f7e110382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_787fbd6c5c6341a48520894c6cbcf6d6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2bc147c0e85d438185cd752a56b4abc3",
              "IPY_MODEL_64d9c7bbdd334e16bceb48a91e3ee2ad"
            ]
          }
        },
        "787fbd6c5c6341a48520894c6cbcf6d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2bc147c0e85d438185cd752a56b4abc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e3091f253714497a95ca1ca263ccc0c5",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 569,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 569,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ff0e9f9c4eb426fb343b8a4b4300347"
          }
        },
        "64d9c7bbdd334e16bceb48a91e3ee2ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a15bd5534bb54fc6b322c66ae946c52d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 569/569 [00:00&lt;00:00, 13.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94981692526649df811d277af24e5390"
          }
        },
        "e3091f253714497a95ca1ca263ccc0c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ff0e9f9c4eb426fb343b8a4b4300347": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a15bd5534bb54fc6b322c66ae946c52d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94981692526649df811d277af24e5390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "005c0a61c726467481ed666671210c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bd6d9a8e254f4948b0f48ccb94072c79",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_30afa040f5c24033812aa806a292f59a",
              "IPY_MODEL_edabb13daca44a3ab15e9fe58d2af3c5"
            ]
          }
        },
        "bd6d9a8e254f4948b0f48ccb94072c79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "30afa040f5c24033812aa806a292f59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0197b1ec3a9b4c75a574a594527805e7",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d1d899dcbaf44c89051320e5c84362e"
          }
        },
        "edabb13daca44a3ab15e9fe58d2af3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d588f8870b8043168cd272b1040764b1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 714M/714M [00:22&lt;00:00, 31.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b48fec7dc344b4aaccfe3d337230daa"
          }
        },
        "0197b1ec3a9b4c75a574a594527805e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d1d899dcbaf44c89051320e5c84362e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d588f8870b8043168cd272b1040764b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b48fec7dc344b4aaccfe3d337230daa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakesh-SSN/FYP/blob/master/BERT_TASK_1_PUNJABI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "outputId": "07cc18ff-4ba5-4c84-9c43-7de22d79c807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P4\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 12.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 54.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 52.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=73c32f44242219599548d1a8ac20ea84e32cbdd69156353c9eae0dc5a683eabe\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "outputId": "5b51531e-56ae-4d66-cb71-e3e3479b748c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "outputId": "571344a0-98fa-461b-e155-2c02e6a5f318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/punjabi/task1punjabi.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 1,700\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>PUN0348</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>ਭਾਰਤ-ਕਤਰ ਵਿਚਕਾਰ 7 ਸਮਝੌਤਿਆਂ 'ਤੇ ਦਸਤਖਤ&lt;eol&gt;ਕਤਰ ਅ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>PUN0198</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>ਤੇਜ਼ ਗੇਂਦਬਾਜ਼ਾਂ ਲਈ ਸਵਰਗ ਹੈ ਸਬੀਨਾ ਪਾਰਕ, 18 ਸਾਲ ਤੋ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1660</th>\n",
              "      <td>PUN1661</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ਇਹ ਪ੍ਰਗਟਾਵਾ ਕਰਦਿਆਂ ਪਾਰਟੀ ਦੇ ਸਕੱਤਰ ਜਨਰਲ ਅਤੇ ਮੈਂ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1270</th>\n",
              "      <td>PUN1271</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ਕ੍ਰਿਸ ਗੇਲ ਨੇ 700 ਛੱਕਿਆਂ ਦਾ ਵੱਡਾ ਰਿਕਾਰਡ ਬਣਾਇਆ&lt;e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>PUN0197</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>ਅਮਰੀਕੀ ਰਾਸ਼ਟਰਪਤੀ ਚੋਣਾਂ ਵਿਚ ਹਿਲੇਰੀ ਨੇ ਹਾਸਲ ਕੀਤੀ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>592</th>\n",
              "      <td>PUN0593</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>ਅੱਜ ਲਗਾਤਾਰ ਦੂਜੇ ਦਿਨ ਨਕਲੀ ਗਊ ਰੱਖਿਅਕਾਂ ਨੂੰ ਨਿਸ਼ਾ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>PUN0110</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>ਬੀ.ਐਸ.ਐਨ.ਐਲ ਲਗਾ ਰਿਹਾਹ ਹੈ 21,000 ਨਵੇਂ ਟਾਵਰ&lt;eol&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1666</th>\n",
              "      <td>PUN1667</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ਇਸ ਤੋਂ ਇਲਾਵਾ ਮੰਡਲ ਖੇਤਰ ਦੇ ਪਰਲਾਦਪੁਰ ਵਿਚ  ਘਰ ਦੇ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1200</th>\n",
              "      <td>PUN1201</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ਤੇਜ਼ ਗੇਂਦਬਾਜ਼ਾਂ ਲਈ ਸਵਰਗ ਹੈ ਸਬੀਨਾ ਪਾਰਕ, 18 ਸਾਲ ਤੋ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>943</th>\n",
              "      <td>PUN0944</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>ਉਸ ਨੇ ਦੱਸਿਆ, ‘ਭਲਕੇ ਸ਼ਰਮੀਲਾ ਨੂੰ ਜੁਡੀਸ਼ਲ ਮੈਜਿਸਟਰੇਟ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "347          PUN0348  ...  ਭਾਰਤ-ਕਤਰ ਵਿਚਕਾਰ 7 ਸਮਝੌਤਿਆਂ 'ਤੇ ਦਸਤਖਤ<eol>ਕਤਰ ਅ...\n",
              "197          PUN0198  ...  ਤੇਜ਼ ਗੇਂਦਬਾਜ਼ਾਂ ਲਈ ਸਵਰਗ ਹੈ ਸਬੀਨਾ ਪਾਰਕ, 18 ਸਾਲ ਤੋ...\n",
              "1660         PUN1661  ...  ਇਹ ਪ੍ਰਗਟਾਵਾ ਕਰਦਿਆਂ ਪਾਰਟੀ ਦੇ ਸਕੱਤਰ ਜਨਰਲ ਅਤੇ ਮੈਂ...\n",
              "1270         PUN1271  ...  ਕ੍ਰਿਸ ਗੇਲ ਨੇ 700 ਛੱਕਿਆਂ ਦਾ ਵੱਡਾ ਰਿਕਾਰਡ ਬਣਾਇਆ<e...\n",
              "196          PUN0197  ...  ਅਮਰੀਕੀ ਰਾਸ਼ਟਰਪਤੀ ਚੋਣਾਂ ਵਿਚ ਹਿਲੇਰੀ ਨੇ ਹਾਸਲ ਕੀਤੀ ...\n",
              "592          PUN0593  ...  ਅੱਜ ਲਗਾਤਾਰ ਦੂਜੇ ਦਿਨ ਨਕਲੀ ਗਊ ਰੱਖਿਅਕਾਂ ਨੂੰ ਨਿਸ਼ਾ...\n",
              "109          PUN0110  ...  ਬੀ.ਐਸ.ਐਨ.ਐਲ ਲਗਾ ਰਿਹਾਹ ਹੈ 21,000 ਨਵੇਂ ਟਾਵਰ<eol>...\n",
              "1666         PUN1667  ...  ਇਸ ਤੋਂ ਇਲਾਵਾ ਮੰਡਲ ਖੇਤਰ ਦੇ ਪਰਲਾਦਪੁਰ ਵਿਚ  ਘਰ ਦੇ ...\n",
              "1200         PUN1201  ...  ਤੇਜ਼ ਗੇਂਦਬਾਜ਼ਾਂ ਲਈ ਸਵਰਗ ਹੈ ਸਬੀਨਾ ਪਾਰਕ, 18 ਸਾਲ ਤੋ...\n",
              "943          PUN0944  ...  ਉਸ ਨੇ ਦੱਸਿਆ, ‘ਭਲਕੇ ਸ਼ਰਮੀਲਾ ਨੂੰ ਜੁਡੀਸ਼ਲ ਮੈਜਿਸਟਰੇਟ...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "outputId": "58bc54e8-a185-471f-a504-16c7839854ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1231</th>\n",
              "      <td>ਰੀਓ ਉਲੰਪਿਕ ਵਿਚ ਅੱਜ ਆਇਰਲੈਂਡ ਨਾਲ ਮੁਕਾਬਲਾ ਕਰੇਗੀ ਭ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>ਸੂਬਾ ਪ੍ਰਧਾਨ ਨੇ ਕਿਹਾ ਕਿ ਇਨ੍ਹਾਂ ਦੋਨਾਂ ਆਗੂਆਂ ਦੀ ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>905</th>\n",
              "      <td>ਪੀਆਰਟੀਸੀ ਡਿੱਪੂ ਸੰਗਰੂਰ ਦੇ ਜਨਰਲ ਮੈਨੇਜਰ ਮਨਿੰਦਰਜੀਤ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1471</th>\n",
              "      <td>ਸਿਵਲ ਹਸਪਤਾਲ ਵਿੱਚ ਦਾਖ਼ਲ ਲੰਗਰ ਲਗਾਉਣ ਵਾਲੇ ਸ਼ੰਮੀ ਕੁ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>957</th>\n",
              "      <td>ਆਸਟਰੇਲੀਆ ਦੇ ਵਿਦੇਸ਼ ਮਾਮਲਿਆਂ ਅਤੇ ਵਪਾਰ ਬਾਰੇ ਵਿਭਾਗ ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1231  ਰੀਓ ਉਲੰਪਿਕ ਵਿਚ ਅੱਜ ਆਇਰਲੈਂਡ ਨਾਲ ਮੁਕਾਬਲਾ ਕਰੇਗੀ ਭ...      0\n",
              "906    ਸੂਬਾ ਪ੍ਰਧਾਨ ਨੇ ਕਿਹਾ ਕਿ ਇਨ੍ਹਾਂ ਦੋਨਾਂ ਆਗੂਆਂ ਦੀ ...      0\n",
              "905   ਪੀਆਰਟੀਸੀ ਡਿੱਪੂ ਸੰਗਰੂਰ ਦੇ ਜਨਰਲ ਮੈਨੇਜਰ ਮਨਿੰਦਰਜੀਤ...      0\n",
              "1471  ਸਿਵਲ ਹਸਪਤਾਲ ਵਿੱਚ ਦਾਖ਼ਲ ਲੰਗਰ ਲਗਾਉਣ ਵਾਲੇ ਸ਼ੰਮੀ ਕੁ...      0\n",
              "957   ਆਸਟਰੇਲੀਆ ਦੇ ਵਿਦੇਸ਼ ਮਾਮਲਿਆਂ ਅਤੇ ਵਪਾਰ ਬਾਰੇ ਵਿਭਾਗ ...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "outputId": "c2333220-9869-4a5f-c98f-2ab0b82db1ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "1157db526165427e9197c667d0038dd0",
            "f575100eae684ebcb1112f1d7c1bda6a",
            "09d5945030dc4e45b6204c4bc4149b27",
            "d829be394b2d478eae71d575424d1d9b",
            "0326856aeb9d4391bf812fd096e55ffd",
            "49d79de9235240d8ae3695a0f190cc73",
            "99a60fda3bed43fcaa9081bb2ce44c7f",
            "606d474cc1364e438f203a0c036faf66"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1157db526165427e9197c667d0038dd0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=995526, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "outputId": "10ee0188-f87c-4524-c383-43341bd3ccee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  ਰੀਓ ਉਲੰਪਿਕ ਜਾਵੇਗਾ ਨਰਸਿੰਘ<eol>ਨਰਸਿੰਘ ਜਾਵੇਗਾ ਰੀਓ ਉਲਪਿੰਕ\n",
            "Tokenized:  ['ਰ', '##ੀ', '##ਓ', 'ਉ', '##ਲ', '##ਪ', '##ਿਕ', 'ਜਾ', '##ਵ', '##ਗ', '##ਾ', 'ਨ', '##ਰ', '##ਸ', '##ਿ', '##ਘ', '<', 'eo', '##l', '>', 'ਨ', '##ਰ', '##ਸ', '##ਿ', '##ਘ', 'ਜਾ', '##ਵ', '##ਗ', '##ਾ', 'ਰ', '##ੀ', '##ਓ', 'ਉ', '##ਲ', '##ਪ', '##ਿਕ']\n",
            "Token IDs:  [1041, 13094, 111248, 1011, 15423, 31948, 26674, 62130, 32143, 48874, 13768, 1034, 13475, 19836, 45378, 111250, 133, 13934, 10161, 135, 1034, 13475, 19836, 45378, 111250, 62130, 32143, 48874, 13768, 1041, 13094, 111248, 1011, 15423, 31948, 26674]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "outputId": "af38fc46-6d1e-4ed8-a869-12e4a9458230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  ਰੀਓ ਉਲੰਪਿਕ ਜਾਵੇਗਾ ਨਰਸਿੰਘ<eol>ਨਰਸਿੰਘ ਜਾਵੇਗਾ ਰੀਓ ਉਲਪਿੰਕ\n",
            "Token IDs: [101, 1041, 13094, 111248, 1011, 15423, 31948, 26674, 62130, 32143, 48874, 13768, 1034, 13475, 19836, 45378, 111250, 133, 13934, 10161, 135, 1034, 13475, 19836, 45378, 111250, 62130, 32143, 48874, 13768, 1041, 13094, 111248, 1011, 15423, 31948, 26674, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "outputId": "649f2e1b-40fa-4395-cca0-75e93a3d6a41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "outputId": "b2c1896c-800d-4595-9f43-47ceee9d7e00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "outputId": "b83c1662-a944-40b5-e779-aa9c9a3791df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "print(train_inputs)\n",
        "print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "#print(train_masks)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101   1035  72497 ...  14673  15423   1041]\n",
            " [   101   1039  22236 ...    102      0      0]\n",
            " [   101  46043   1035 ...   1037  48560   1032]\n",
            " ...\n",
            " [   101   1044  21205 ...      0      0      0]\n",
            " [   101   1032  19836 ... 108041   1041  14673]\n",
            " [   101   1032  64812 ...  12666   1044  72497]]\n",
            "[0 1 0 ... 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "outputId": "c83d2cfa-9a25-4325-e837-17f8dd38aeaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "91c0e7c798a948cb9b982a1f7e110382",
            "787fbd6c5c6341a48520894c6cbcf6d6",
            "2bc147c0e85d438185cd752a56b4abc3",
            "64d9c7bbdd334e16bceb48a91e3ee2ad",
            "e3091f253714497a95ca1ca263ccc0c5",
            "4ff0e9f9c4eb426fb343b8a4b4300347",
            "a15bd5534bb54fc6b322c66ae946c52d",
            "94981692526649df811d277af24e5390",
            "005c0a61c726467481ed666671210c22",
            "bd6d9a8e254f4948b0f48ccb94072c79",
            "30afa040f5c24033812aa806a292f59a",
            "edabb13daca44a3ab15e9fe58d2af3c5",
            "0197b1ec3a9b4c75a574a594527805e7",
            "0d1d899dcbaf44c89051320e5c84362e",
            "d588f8870b8043168cd272b1040764b1",
            "9b48fec7dc344b4aaccfe3d337230daa"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91c0e7c798a948cb9b982a1f7e110382",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=569, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "005c0a61c726467481ed666671210c22",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=714314041, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "outputId": "039fde85-ed12-4255-dc54-9c69d8ca3e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "outputId": "823f99da-2faa-4808-8682-09ddddb78c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     43.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     43.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     43.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     43.    Elapsed: 0:00:19.\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epcoh took: 0:00:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifs2QgT9HeUe",
        "colab_type": "code",
        "outputId": "c77f253f-9d67-4c07-dd40-f7f201fd214e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeVjWVf7/8ed9ww3IJoI3CAKKC4so\nyGY6Wq6jaGqbmFtY0/izaZym5ttkjdZ8L7Mss2yfGWfGGTRz17Sc3CtbTAUNRAHXSkQFQXBjU/j9\nYfIdcgMFPjfyelwX1zWcz3LeH86gL0/nfG5TZWVlJSIiIiIi0iiYjS5ARERERERqTgFeRERERKQR\nUYAXEREREWlEFOBFRERERBoRBXgRERERkUZEAV5EREREpBFRgBcRaaJmzZpFSEgIeXl5N3V9aWkp\nISEhvPDCC3VcWe0sXLiQkJAQvvvuO0PrEBFpKPZGFyAi0pSFhITU+NxNmzbh7+9fj9WIiEhjoAAv\nImKgmTNnVvs+JSWFxYsX8+CDDxITE1PtmKenZ532/eSTT/K73/0OR0fHm7re0dGRtLQ07Ozs6rQu\nERG5PgV4ERED3XPPPdW+v3jxIosXL6Zr165XHLuWyspKiouLcXZ2rlXf9vb22Nvf2l8DNxv+RUTk\n5mkNvIhII7JlyxZCQkL45JNPSEpKIj4+ni5duvDBBx8AsHPnTp555hkGDhxIZGQk0dHRjB07ls8+\n++yKe11tDfzltiNHjvDqq69y55130qVLF+677z6+/vrratdfbQ38f7ft2LGD0aNHExkZSffu3Xnh\nhRcoLi6+oo5vvvmGhIQEunTpQq9evXjllVfYu3cvISEhzJkz56Z/VidPnuSFF17grrvuonPnzvTt\n25fp06dTVFRU7bzz588ze/ZsBg0aREREBHFxcQwbNozZs2dXO2/jxo2MHj2aO+64g4iICPr27csT\nTzzBkSNHbrpGEZGboRl4EZFG6O9//ztnzpzhgQcewMvLi4CAAADWrl3LkSNHGDJkCH5+fhQUFLBy\n5Uoee+wx3nnnHQYOHFij+//P//wPjo6O/PrXv6a0tJR///vf/OY3v2HDhg34+Pjc8Prdu3ezbt06\nRowYwfDhw9m6dSuLFy/GwcGBqVOnVp23detWJkyYgKenJxMnTsTV1ZU1a9awffv2m/vB/KSwsJAH\nH3yQnJwcEhISCA0NZffu3XzwwQds27aNJUuW0KxZMwCef/551qxZw3333UfXrl0pLy/n+++/59tv\nv62631dffcWkSZPo1KkTjz32GK6urpw4cYKvv/6a7Ozsqp+/iEhDUIAXEWmEcnNz+fTTT/Hw8KjW\n/uSTT16xlOahhx5i+PDh/OUvf6lxgPfx8eHtt9/GZDIBVM3kL126lEmTJt3w+qysLJYtW0anTp0A\nGD16NOPHj2fx4sU888wzODg4ADBjxgwsFgtLlizB19cXgDFjxjBq1Kga1Xktf/3rX8nOzuall15i\nxIgRVe0dO3bk1VdfrfoHSWVlJZs3b2bAgAHMmDHjmvfbuHEjAElJSbi5uVW11+RnISJS17SERkSk\nEXrggQeuCO9AtfBeXFzMqVOnKC0tpVu3bmRkZFBWVlaj+48fP74qvAPExMRgsVj4/vvva3R9XFxc\nVXi/rHv37pSVlXHs2DEAjh49SlZWFoMGDaoK7wAODg4kJibWqJ9rufxfCu6///5q7ePGjcPNzY0N\nGzYAYDKZcHFxISsri4MHD17zfm5ublRWVrJu3TouXrx4S7WJiNwqzcCLiDRCbdu2vWp7bm4us2fP\n5rPPPuPUqVNXHD9z5gxeXl43vP/Pl4SYTCaaN29OYWFhjeq72pKSy//gKCwspE2bNmRnZwMQFBR0\nxblXa6upyspKcnJy6N69O2Zz9XkqBwcHAgMDq/oGmDJlCn/6058YMmQIbdq04Y477qBfv3706dOn\n6h8x48eP5/PPP2fKlCm88sorxMbGcueddzJkyBBatGhx07WKiNwMBXgRkUbo8vrt/3bx4kUefvhh\nsrOzSUxMJDw8HDc3N8xmM4sWLWLdunVUVFTU6P4/D76XVVZW3tL1tblHQxk8eDB33HEHW7ZsYfv2\n7Xz11VcsWbKEHj168I9//AN7e3tatmzJypUr2bFjB9988w07duxg+vTpvP322/zzn/+kc+fORj+G\niDQhCvAiIreJ9PR0Dh48yB/+8AcmTpxY7djlt9TYktatWwNw+PDhK45dra2mTCYTrVu35tChQ1RU\nVFT7x0RZWRk//vgjgYGB1a7x9PTk3nvv5d5776WyspKXX36ZefPmsWXLFvr16wdceu1mjx496NGj\nB3Dp5z1ixAj+9re/8c4779x0vSIitaU18CIit4nLQfXnM9x79uzhiy++MKKk6/L39yc4OJh169ZV\nrYuHSyF73rx5t3TvAQMGcPz4cT766KNq7R9++CFnzpzhl7/8JQDl5eWcPXu22jkmk4mwsDCAqldO\nFhQUXNFHhw4dcHBwqPGyIhGRuqIZeBGR20RISAht27blL3/5C6dPn6Zt27YcPHiQJUuWEBISwp49\ne4wu8QrPPvssEyZMYOTIkYwaNQoXFxfWrFlTbQPtzXjsscdYv349U6dOJTU1lZCQENLT01mxYgXB\nwcE8/PDDwKX1+AMGDGDAgAGEhITg6enJkSNHWLhwIS1atKB3794APPPMM5w+fZoePXrQunVrzp8/\nzyeffEJpaSn33nvvrf4YRERqRQFeROQ24eDgwN///ndmzpzJ8uXLKS0tJTg4mDfeeIOUlBSbDPA9\ne/Zkzpw5zJ49m7/+9a80b96coUOHMmDAAMaOHYuTk9NN3dfDw4PFixfzzjvvsGnTJpYvX46Xlxfj\nxo3jd7/7XdUeAjc3N8aNG8fWrVv58ssvKS4uxmq1MnDgQCZOnIinpycA999/P6tWrWLFihWcOnUK\nNzc3OnbsyPvvv0///v3r7OchIlITpkpb200kIiJN3urVq/njH//Ie++9x4ABA4wuR0TEpmgNvIiI\nGKaiouKKd9OXlZWRlJSEg4MDsbGxBlUmImK7tIRGREQMc/bsWYYMGcKwYcNo27YtBQUFrFmzhv37\n9zNp0qSrfliViEhTpwAvIiKGcXJyomfPnqxfv56TJ08C0K5dO1588UVGjhxpcHUiIrZJa+BFRERE\nRBoRrYEXEREREWlEFOBFRERERBoRrYGvpVOnzlFR0fCrjry8XMnPP3vjE6XBaExsk8bF9mhMbJPG\nxfZoTGyTEeNiNpto0cLlmscV4GupoqLSkAB/uW+xLRoT26RxsT0aE9ukcbE9GhPbZGvjoiU0IiIi\nIiKNiAK8iIiIiEgjogAvIiIiItKIKMCLiIiIiDQiCvAiIiIiIo2IAryIiIiISCOiAC8iIiIi0ogo\nwIuIiIiINCIK8CIiIiIijYg+idXGbd1znBVfHKTgdCme7o7c37s9PcJbGV2WiIiIiBhEAd6Gbd1z\nnKRPMym7UAFA/ulSkj7NBFCIFxEREWmitITGhq344mBVeL+s7EIFK744aFBFIiIiImI0QwN8WVkZ\nr732Gr169SIiIoKRI0eydevWG173zjvvEBIScsVXz549r3r+0qVLGTx4MF26dGHQoEEsWLCgrh+l\nXuSfLq1Vu4iIiIjc/gxdQvPss8+yfv16EhMTadOmDStXrmTChAnMnz+fqKioG14/bdo0nJycqr7/\n7/992aJFi/jzn/9MfHw8jzzyCMnJyUybNo3S0lJ+9atf1enz1DUvd8erhvUWro4GVCMiIiIitsCw\nAJ+WlsaaNWt47rnnePjhhwG49957GTp0KLNmzarRLPngwYNxd3e/5vGSkhJmz55N//79eeuttwAY\nOXIkFRUVvPvuuyQkJODm5lYnz1Mf7u/dvtoa+MtKyi/w/fHTtG117WcXERERkduTYUto1q5di8Vi\nISEhoarN0dGRESNGkJKSQm5u7g3vUVlZydmzZ6msrLzq8W3btlFYWMiYMWOqtY8dO5Zz586xZcuW\nW3uIetYjvBXjB4fi5e6IiUsz8vf3boezoz2vfLCT5Mwb/4xERERE5PZi2Ax8RkYGQUFBuLi4VGuP\niIigsrKSjIwMvL29r3uPPn36cP78eVxcXBg0aBCTJ0/Gw8Oj6vjevXsB6Ny5c7XrwsPDMZvN7N27\nl7vvvruOnqh+9AhvRY/wVlitbuTlnQHgzgg/3luxm/c/Sue+O4MY+ou2mEwmgysVERERkYZgWIDP\ny8vDx8fninar1Qpw3Rl4d3d3HnroISIjI7FYLHz77bcsXryYvXv3snTpUhwcHKr6cHBwqBbqgaq2\nmszy26LmLg78cXRX/v1pFiu/PExO/nkeGRyKg8XO6NJEREREpJ4ZFuBLSkqwWCxXtDs6XtqgWVp6\n7TetjB8/vtr38fHxdOzYkWnTpvHRRx8xcuTI6/ZxuZ/r9XEtXl6utb6mrlit1dfrP/dIN5Zt3s+8\n/2RQeLaMKY90o4X7lRt5pf78fEzENmhcbI/GxDZpXGyPxsQ22dq4GBbgnZycKC8vv6L9cqi+HORr\navTo0bz22mts3bq1KsA7OTlRVlZ21fNLS0tr3QdAfv5ZKiquvua+Pv33Epr/1ifCFzdHe/7+yR6e\nnP05TzwQQaCPbf2f7HZ1rTERY2lcbI/GxDZpXGyPxsQ2GTEuZrPpupPGhm1itVqtV13CkpeXB3DD\n9e8/Zzab8fHxoaioqFof5eXlFBYWVju3rKyMwsLCWvdhq2JCrPxpXAyVlfDyByns3JdndEkiIiIi\nUk8MC/ChoaEcPnyYc+fOVWtPTU2tOl4b5eXlHDt2jBYtWlS1hYWFAZCenl7t3PT0dCoqKqqO3w4C\nfdx4YXws/lZX3l2xmzVbv7/m23lEREREpPEyLMDHx8dTXl7O0qVLq9rKyspYsWIF0dHRVRtcc3Jy\nOHjwYLVrCwoKrrjfP//5T0pLS7nzzjur2rp3746HhwcffvhhtXMXLlyIs7Mzd911V10+kuGauzry\nzOgo7ujkw/IvDvGPTzIo/9k75EVERESkcTNsDXxkZCTx8fHMmjWLvLw8AgMDWblyJTk5OcyYMaPq\nvMmTJ7N9+3aysrKq2vr27cuQIUMIDg7GwcGBbdu2sW7dOmJiYhg6dGjVeU5OTjzxxBNMmzaN3//+\n9/Tq1Yvk5GRWr17N008/fd0PgWqsHCx2/L9hnfDzcmbll4fJKyxm0v1dcHdxMLo0EREREakDhgV4\ngJkzZ/Lmm2+yatUqioqKCAkJYc6cOcTExFz3umHDhrFz507Wrl1LeXk5rVu35vHHH2fixInY21d/\npLFjx2KxWJg7dy6bNm3C19eXKVOmkJiYWJ+PZiiTycSwnkH4ernwj0/28mJSMk+MiCDA27g36IiI\niIhI3TBVaqF0rdjaW2hu5Pvjp3l7WRrFZReZOCycrh1b1kN1TZPeFmCbNC62R2NimzQutkdjYpv0\nFhppcG1bufP8+Dh8PZ15Z3kaa7f9qM2tIiIiIo2YAnwT0MLNkcljo4kN9WbJZwf4138ytblVRERE\npJEydA28NBxHix2P3ROOr5czq7/+ntxT53n8/i64O2tzq4iIiEhjohn4JsRkMnHvne2YODycw8fP\nMD0pmaN5Z40uS0RERERqQQG+Cbqjkw+Tx0RTfrGCl+ankHYw3+iSRERERKSGFOCbqHZ+7jyfGIt3\ni2a8tSyV9du1uVVERESkMVCAb8I83Z14bmwM0R2tLNp8gKS1WVy4qM2tIiIiIrZMAb6Jc3Sw4zf3\ndWboL9qwJTWHNxZ/x9nicqPLEhEREZFrUIAXzCYT99/VngnDOnHg6GmmJyVzLP+c0WWJiIiIyFUo\nwEuVHuGtmDwmipLyi0yfl0L6YW1uFREREbE1CvBSTfvWzXk+MZaWzZ2YvSSVjclHtLlVRERExIYo\nwMsVvJo78dy4aCLbt+TDjfv5YP0+bW4VERERsREK8HJVTg72THqgC4O7B/LZrqPMXpKqza0iIiIi\nNkABXq7JbDKR0KcDj94dxv7sQl6al8zxgvNGlyUiIiLSpCnAyw317OLLH0dHcb70AtOTktn7fYHR\nJYmIiIg0WQrwUiMd/T14PjGWFu6OvLE4lc92HTW6JBEREZEmSQFeaqylRzP+NC6Gzu08mb8uiwXr\n93GxQptbRURERBqSArzUSjNHe554IIL4boFs2pnNm0vTOF+iza0iIiIiDUUBXmrNbDYxsl8HHhkc\nSuYPp3hpfgonTmlzq4iIiEhDUICXm3ZnpB9Pj+rKmfPlTE9KJvOHU0aXJCIiInLbU4CXWxIS2IKp\n42Np7urI64u/44vvtLlVREREpD4pwMst8/5pc2tY2xYkrc1i4cb9VFRUGl2WiIiIyG1JAV7qhLOT\nPb8fEcEvYwPYkHyEt5alUVx6weiyRERERG47CvBSZ+zMZkYP6EhifAh7vy/gpfkp5BYWG12WiIiI\nyG1FAV7qXJ+urfnDg10pOlvK9KRksn7U5lYRERGRuqIAL/UirE0LpibG4trMwqxF3/FlWo7RJYmI\niIjcFhTgpd74eDozNTGG0EAP/vWfTJZsPqDNrSIiIiK3SAFe6pWzk4UnR0bSP9qftdt/5J3l2twq\nIiIicisU4KXe2ZnNjB0YzLiBwew+VMDLH6RwUptbRURERG6KArw0mH7R/jz1YCSnTpfy4rxk9mcX\nGl2SiIiISKOjAC8NKrytJ1MSY3B2tOe1hbv4Jv2Y0SWJiIiINCoK8NLgfL1cmJIYS0d/D/7xSQbL\nPj9IRaU2t4qIiIjUhAK8GMK1mYWnRkbSp6sf//n2B95bsZuSMm1uFREREbkRBXgxjL2dmYcGhTBm\nQEe+O3CSGR/sJL+oxOiyRERERGyaArwYymQyMSA2gCcTIjlZVMyL85I5eLTI6LJEREREbJYCvNiE\nLu28mPJQLI4WM69+uItv9xw3uiQRERERm6QALzbDr6ULz4+Po72fO3M+3suKLYe0uVVERETkZxTg\nxaa4NrPwP6O6cmeEL5988z1/+Sid0rKLRpclIiIiYjMMDfBlZWW89tpr9OrVi4iICEaOHMnWrVtr\nfZ8JEyYQEhLCSy+9dMWxkJCQq34tXLiwLh5B6oG9nZmHB4cyql8Hdu7L45UFOyk4rc2tIiIiIgD2\nRnb+7LPPsn79ehITE2nTpg0rV65kwoQJzJ8/n6ioqBrd4/PPPyc5Ofm65/Tq1Yvhw4dXa4uMjLzp\nuqX+mUwmBnYLxMfTmb+t3sOL85J54oEIgnzdjS5NRERExFCGzcCnpaWxZs0ann76aZ555hkefPBB\nkpKS8PX1ZdasWTW6R1lZGTNmzODRRx+97nnt2rXjnnvuqfbVtm3bOngKqW+RHVryp4disNiZeWXB\nTrZnnDC6JBERERFDGRbg165di8ViISEhoarN0dGRESNGkJKSQm5u7g3vMW/ePEpKSm4Y4AFKSkoo\nLS29pZrFGP5WV6aOj6VtKzf+umoPq746TKU2t4qIiEgTZViAz8jIICgoCBcXl2rtERERVFZWkpGR\ncd3r8/LyeP/993nqqado1qzZdc9dtmwZXbt2JSIigmHDhrFhw4Zbrl8alruzA0+PiqJnl1as+uow\nf1u9h7JybW4VERGRpsewNfB5eXn4+Phc0W61WgFuOAP/xhtvEBQUxD333HPd86KiohgyZAj+/v4c\nO3aMefPmMWnSJF5//XWGDh168w8gDc5ib+ZXQ8Lwa+nCss8OkldYzKT7I2jh5mh0aSIiIiINxrAA\nX1JSgsViuaLd0fFSGLvecpe0tDQ++ugj5s+fj8lkum4/ixYtqvb9fffdx9ChQ3nttde4++67b3j9\nz3l5udbq/LpktboZ1rctSRzamZC2XsxakMLLH6Qw9Vd30MHfw5BaNCa2SeNiezQmtknjYns0JrbJ\n1sbFsADv5OREeXn5Fe2Xg/vlIP9zlZWVvPTSSwwcOJDY2Nha9+vs7MyoUaN4/fXXOXToEO3bt6/V\n9fn5Z6moaPj111arG3l5Zxq8X1vVzseV58bF8PayVCa/8yW/HtqJ2FDvBq1BY2KbNC62R2NimzQu\ntkdjYpuMGBez2XTdSWPD1sBbrdarLpPJy8sDwNv76mFsw4YNpKWlMXr0aLKzs6u+AM6ePUt2djYl\nJdd/Z7ivry8ARUVFt/IIYrAAb1emjo8jwMeV9z9K5+OvtblVREREbn+GBfjQ0FAOHz7MuXPnqrWn\npqZWHb+anJwcKioqGD9+PP3796/6AlixYgX9+/dn+/bt1+37yJEjAHh6et7qY4jBmrs48MzoKHqE\n+7Dyy8P8/eO9lF/Q5lYRERG5fRm2hCY+Pp65c+eydOlSHn74YeDSe91XrFhBdHR01QbXnJwciouL\nq5a69OvXD39//yvu99vf/pa+ffsyYsQIwsPDASgoKLgipJ86dYoPP/wQf39/vQv+NmGxt+PXQzvh\n19KF5V8cIrewmN/d34XmrtrcKiIiIrcfwwJ8ZGQk8fHxzJo1i7y8PAIDA1m5ciU5OTnMmDGj6rzJ\nkyezfft2srKyAAgMDCQwMPCq9wwICGDAgAFV3y9YsIBNmzbRp08f/Pz8OHHiBIsXL6agoID33nuv\nfh9QGpTJZOLuHm1p5enC3z/5v09uDfSxrU0nIiIiIrfKsAAPMHPmTN58801WrVpFUVERISEhzJkz\nh5iYmDq5f1RUFDt37mTp0qUUFRXh7OxM165dmThxYp31IbYlJsRKy+YxvL08jZc/SOH/DQsnOthq\ndFkiIiIidcZUqV1/taK30DQOhWdLeWf5br4/dpoH+rRn8B2BtX5l6I1oTGyTxsX2aExsk8bF9mhM\nbJPeQiPSQDxcHZk8Joq4MG+WfX6Qf67JoPxChdFliYiIiNwyQ5fQiNQnB4sdE4eH49fShY++PExu\nYTGT7uuCu4uD0aWJiIiI3DTNwMttzWQyMbxnEI/f25kfj5/hxaRksnPPGl2WiIiIyE1TgJcmITbU\nm8ljo7lYUcFLH6Tw3YGTRpckIiIiclMU4KXJCPJ15/nxcfh6OvPOsjTWbvtRn9wqIiIijY4CvDQp\nLdwcmTw2mphQb5Z8doB//SeTCxe1uVVEREQaDwV4aXIcLXY8dk84w3u25avdx5i1cBdnzpcZXZaI\niIhIjSjAS5NkNpm49852TBwezqFjlza3Hj15zuiyRERERG5IAV6atDs6+fDs2GjKL1Tw8vxk0g7m\nG12SiIiIyHUpwEuT187PnefHx2L1aMZby1JZv+OINreKiIiIzVKAFwE83Z14bmwM0R2tLNq0n6S1\nWdrcKiIiIjZJAV7kJ44Odvzmvs7c3aMNW1JzeGPxd5wtLje6LBEREZFqFOBF/ovZZOKB3u2ZMLQT\nB46eZnpSMsfytblVREREbIcCvMhV9OjcimfGRFFSdoHp81JIP6zNrSIiImIbFOBFrqFD6+ZMHR+L\nl7sTby5JY1NKttEliYiIiCjAi1xPy+bN+NND0US092LBhn3MX6fNrSIiImIsBXiRG3BysGfSA10Y\n3D2Qz3YdZfaSVM6VaHOriIiIGEMBXqQGzCYTCX068OjdYew7Usj0eSkczTtrdFkiIiLSBCnAi9RC\nzy6+/HF0FOeKy/mft7aw9/sCo0sSERGRJkYBXqSWggM8eH58LC2bO/HG4lQ+23XU6JJERESkCVGA\nF7kJVo9mzPzdnXRu58n8dVks2LCPixXa3CoiIiL1TwFe5CY5O1l44oEIBnULYFNKNm8uTeO8NreK\niIhIPVOAF7kFZrOJB/t15OHBoWT+cIqX5qdw4tR5o8sSERGR25gCvEgduCvSj6dHdeXM+XKmJyWT\n+cMpo0sSERGR25QCvEgdCQlswdTEGNxdHHh98XdsSc0xuiQRERG5DSnAi9Qh7xbOTHkolrA2Lfj3\np5ks2rSfiopKo8sSERGR24gCvEgdc3ay5/cJEQyI9Wf9jiO8vTyN4tILRpclIiIitwkFeJF6YGc2\nM2ZAMImDQthzuICX5qeQW1hsdFkiIiJyG1CAF6lHfaJa84eRkRSdLWV6UjL7jhQaXZKIiIg0cgrw\nIvUsrK0nUxNjcWlm4bWFu/gq7ZjRJYmIiEgjpgAv0gB8PJ2ZmhhDSKAHc/+TwZLPDmhzq4iIiNwU\nBXiRBuLiZOGpkZH0i27N2m0/8u6K3drcKiIiIrWmAC/SgOzMZsYNDGHsL4NJO5jPjA9SOFmkza0i\nIiJScwrwIgboH+PPUyMjyT99aXPrgewio0sSERGRRkIBXsQg4UGeTE2MwcnRnpkLd/JNuja3ioiI\nyI0pwIsYyNfLhamJsXRo3Zx/fJLB8i8OUlGpza0iIiJybQrwIgZzbWbhDw92pXdXP9Zs/YH3Vuym\npEybW0VEROTqFOBFbIC9nZnEQSGMHtCR7w6c5JUPdlJwusToskRERMQGGRrgy8rKeO211+jVqxcR\nERGMHDmSrVu31vo+EyZMICQkhJdeeumqx5cuXcrgwYPp0qULgwYNYsGCBbdaukidM5lM/DI2gCcT\nIskrKmZaUjIHj2pzq4iIiFRnaIB/9tlnSUpKYvjw4UyZMgWz2cyECRPYtWtXje/x+eefk5ycfM3j\nixYtYurUqQQHB/P8888TGRnJtGnTmDt3bl08gkid69LOiz89FIujxcyrH+7i273HjS5JREREbIhh\nAT4tLY01a9bw9NNP88wzz/Dggw+SlJSEr68vs2bNqtE9ysrKmDFjBo8++uhVj5eUlDB79mz69+/P\nW2+9xciRI5k5cybDhg3j3Xff5cyZM3X5SCJ1pnXLS5tb2/m5M2f1XlZuOaTNrSIiIgIYGODXrl2L\nxWIhISGhqs3R0ZERI0aQkpJCbm7uDe8xb948SkpKrhngt23bRmFhIWPGjKnWPnbsWM6dO8eWLVtu\n7SFE6pGbswNPj+rKnRG+fPzN9/zlo3RKyy4aXZaIiIgYzLAAn5GRQVBQEC4uLtXaIyIiqKysJCMj\n47rX5+Xl8f777/PUU0/RrFmzq56zd+9eADp37lytPTw8HLPZXHVcxFbZ25l5eHAoo/p1YGdWHq8s\n2MmpM6VGlyUiIiIGMizA5+Xl4e3tfUW71WoFuOEM/BtvvEFQUBD33HPPdftwcHDAw8OjWvvltprM\n8osYzWQyMbBbIE+MiOD4qfAngicAACAASURBVPNMS9rB4WOnjS5LREREDGJvVMclJSVYLJYr2h0d\nHQEoLb32LGNaWhofffQR8+fPx2Qy1bqPy/1cr49r8fJyrfU1dcVqdTOsb7m6hhyTAVY3Orb1Ytrc\nbby6YCdPjo7mzq6tG6z/xkS/K7ZHY2KbNC62R2Nim2xtXAwL8E5OTpSXl1/RfjlUXw7yP1dZWclL\nL73EwIEDiY2NvWEfZWVlVz1WWlp6zT6uJz//LBUVDb+Z0Gp1Iy9Pm25tiRFj4mxv4k9jo3l35W5m\nzk8m63A+w3u2ve4/ZJsa/a7YHo2JbdK42B6NiW0yYlzMZtN1J40NW0JjtVqvuoQlLy8P4KrLawA2\nbNhAWloao0ePJjs7u+oL4OzZs2RnZ1NSUlLVR3l5OYWFhdXuUVZWRmFh4TX7ELFl7i4O/HFUFD07\nt2LVV4f52+o9lJVrc6uIiEhTYViADw0N5fDhw5w7d65ae2pqatXxq8nJyaGiooLx48fTv3//qi+A\nFStW0L9/f7Zv3w5AWFgYAOnp6dXukZ6eTkVFRdVxkcbGYm/mV3eHkdCnPTsycnn1w50UntXmVhER\nkabAsCU08fHxzJ07l6VLl/Lwww8Dl2bGV6xYQXR0ND4+PsClwF5cXEz79u0B6NevH/7+/lfc77e/\n/S19+/ZlxIgRhIeHA9C9e3c8PDz48MMP6dWrV9W5CxcuxNnZmbvuuquen1Kk/phMJgZ3b0MrT2fm\nfLyXF5OSeeKBCNq0sq11eiIiIlK3DAvwkZGRxMfHM2vWLPLy8ggMDGTlypXk5OQwY8aMqvMmT57M\n9u3bycrKAiAwMJDAwMCr3jMgIIABAwZUfe/k5MQTTzzBtGnT+P3vf0+vXr1ITk5m9erVPP3007i7\nu9fvQ4o0gKhgK8+Ni+ad5WnM+CCFXw/tRGyoloeJiIjcrgwL8AAzZ87kzTffZNWqVRQVFRESEsKc\nOXOIiYmpsz7Gjh2LxWJh7ty5bNq0CV9fX6ZMmUJiYmKd9SFitEAfN6aOj+PdFWm8/1E6993VjqE9\n2mhzq4iIyG3IVFmpz2evDb2FRi6zxTEpv3CRf32aybd7TtA93IdHBodisbczuqwGZYvj0tRpTGyT\nxsX2aExsky2+hcbQGXgRqVsWezsmDO2En5cLK7YcIu9UMZPu70Jz19q/MlVERERsk2FvoRGR+mEy\nmRj6i7b89r7OHMk7y4vzkvnxhGZ0REREbhcK8CK3qZgQb54bG0NlJcz4YCe79uUZXZKIiIjUAQV4\nkdtYm1ZuPD8+Fr+WLry7Yjf/+fYHtO1FRESkcVOAF7nNebg6MnlMFHFh3iz7/CD/XJNB+YUKo8sS\nERGRm6RNrCJNgIPFjonDw/Fr6cJHXx4mt7CYSfd1wd3FwejSREREpJY0Ay/SRJhMJob3DOI393bm\nx+NneDEpmezcs0aXJSIiIrWkAC/SxMSFejN5bDQXKyp46YMUvjtw0uiSREREpBYU4EWaoCBfd54f\nH0crT2feWZbG2m0/anOriIhII6EAL9JEtXBz5Nmx0cSEWFny2QH+9WkmFy5qc6uIiIitU4AXacIc\nLXY8dm9nhvdsy1dpx5i16DvOnC8zuiwRERG5DgV4kSbObDJx753t+H/DO3Eo5zTT5yVz9OQ5o8sS\nERGRa1CAFxEAundqxeSxUZSVV/Dy/GTSDuYbXZKIiIhchQK8iFRp79ec58fHYm3ejLeWpbJhxxFt\nbhUREbExtQ7wP/zwA1u2bKnWlpqaymOPPcaoUaNYvHhxnRUnIg3P092J58bFENXRysJN+5m3Lkub\nW0VERGxIrT+JddasWRQWFnLXXXcBUFBQwIQJEzh//jyOjo787//+L15eXgwYMKDOixWRhuHoYMfj\n93Vm5ZZDrNn6AycKzvP4fV1wbWYxujQREZEmr9Yz8Onp6fziF7+o+n7NmjWcPXuWFStWsHXrViIj\nI0lKSqrTIkWk4ZlNJh7o3Z4JQztx4GgR0+clcyxfm1tFRESMVusAX1BQgLe3d9X3X375JdHR0QQH\nB+Pg4MCQIUM4ePBgnRYpIsbp0bkVz4yJpqT0AtPnpbDncIHRJYmIiDRptQ7wzZo148yZMwBcvHiR\nlJQUYmNjq447OTlx9uzZuqtQRAzXoXVzpo6PxcvdkdlLUtmUkm10SSIiIk1WrQN8x44d+eijjzh1\n6hRLlizh/Pnz9OzZs+r40aNH8fT0rNMiRcR4LZs347lxMUS092LBhn3MX6/NrSIiIkao9SbWRx99\nlMcff7xqHXxYWFi1Gfivv/6aTp061V2FImIzmjnaM+n+Liz/4iCfbvuREwXn+c29nXFx0uZWERGR\nhlLrAN+nTx+SkpLYtGkTrq6ujBs3DpPJBMCpU6do1aoV9957b50XKiK2wWw2kdC3A75eLiStzWT6\nvBSeHBGBj6ez0aWJiIg0CaZKfUpLreTnn6WiouF/ZFarG3l5Zxq8X7k2jQnsO1LIuyt2U1lZyeP3\ndiasrfHL5zQutkdjYps0LrZHY2KbjBgXs9mEl5frtY/XRScXLlxg3bp1LFmyhLy8vLq4pYg0AsEB\nHjw/PhYPV0deX5zK57uOGl2SiIjIba/WS2hmzpzJtm3bWL58OQCVlZU88sgjJCcnU1lZiYeHB0uW\nLCEwMLDOixUR22P1aMafHorhb6v3MG9dFjknz/Fg/w7YmetkfkBERER+ptZ/w3755ZfVNq1u3ryZ\nHTt28Oijj/L6668DMGfOnLqrUERsXjNHe554IIKBcQFsTMnmraVpnC8pN7osERGR21KtZ+CPHz9O\nmzZtqr7/7LPP8Pf35+mnnwZg//79fPzxx3VXoYg0CmaziVH9O+LX0oX567J4aX4KT4yIwKeFNreK\niIjUpVrPwJeXl2Nv/3+5f9u2bVWvlAQICAjQOniRJuyuSD/+58GunD5XxvSkZLJ+PGV0SSIiIreV\nWgf4Vq1asWvXLuDSbPuRI0eIi4urOp6fn4+zs2bcRJqy0DYteH58LO4uDsxa9B1bUnOMLklEROS2\nUeslNHfffTfvv/8+BQUF7N+/H1dXV3r37l11PCMjQxtYRQTvFs5MeSiWv65K59+fZpJz8hwj+3bA\nbDYZXZqIiEijVusZ+IkTJ3Lffffx3XffYTKZePXVV3F3dwfgzJkzbN68mR49etR5oSLS+Dg72fP7\nhAgGxPizfscR3l6eRnHpBaPLEhERadRqPQPv4ODAyy+/fNVjLi4ufPXVVzg5Od1yYSJye7Azmxnz\ny2D8WrqwYMM+Xv5pc6vVo5nRpYmIiDRKdfqiZrPZjJubGxaLpS5vKyK3gT5RrfnDyEgKz5byYlIy\n+44UGl2SiIhIo3RTAf78+fO8/fbbDBs2jKioKKKiohg2bBjvvPMO58+fr+saReQ2EdbWkymJsbg0\ns/Dawl18lXbM6JJEREQanVoH+MLCQhISEnj//ffJz88nLCyMsLAw8vPzee+990hISKCwUDNrInJ1\nrTydmZoYQ0igB3P/k8GSzw5QUVFpdFkiIiKNRq3XwL/99tscOnSI559/nlGjRmFnZwfAxYsXWbx4\nMdOnT+fdd99l6tSpdV6siNweXJwsPJkQycJN+1m77UeO559nwrBONHOs9R9JIiIiTU6tZ+A3b95M\nQkICY8eOrQrvAHZ2dowZM4YHHniAjRs31mmRInL7sbcz89DAEMb+Mpi0g/nM+CCFk0XFRpclIiJi\n82od4E+ePElYWNg1j3fq1ImTJ0/eUlEi0nT0j/HnyZER5J8uZXpSMgeyi4wuSURExKbVOsC3bNmS\njIyMax7PyMigZcuWt1SUiDQtnYO8mJoYg5OjPTMX7uSbdG1uFRERuZZaB/i+ffuybNkyFi1aREVF\nRVV7RUUFixcvZvny5fTr169G9yorK+O1116jV69eREREMHLkSLZu3XrD61avXk1iYiI9e/akc+fO\n9OvXj+eee46jR49ecW5ISMhVvxYuXFjzhxaReufr5cLUxFg6tG7OPz7JYPkXB6mo1OZWERGRnzNV\nVtbub8hTp04xatQofvzxRzw9PQkKCgLg8OHDFBQUEBgYyKJFi2jRosUN7/WHP/yB9evXk5iYSJs2\nbVi5ciXp6enMnz+fqKioa143c+ZM8vLyCA0NpXnz5uTk5LBkyRIuXrzI6tWrsVqtVeeGhITQq1cv\nhg8fXu0ekZGRtG3btjaPDkB+/llD3phhtbqRl3emwfuVa9OY1I8LFyv4YP0+tqTmEB1s5ddDw3By\nqPnmVo2L7dGY2CaNi+3RmNgmI8bFbDbh5eV6zeO1DvAAZ8+e5e9//zsbN24kOzsbgICAAPr378+E\nCRNwdb12h5elpaWRkJDAc889x8MPPwxAaWkpQ4cOxdvbmwULFtSqpj179nD//ffzzDPP8Oijj1a1\nh4SEkJiYyJQpU2p1v2tRgJfLNCb1p7Kyko3J2SzavJ8AqytPjIjA071mn/CscbE9GhPbpHGxPRoT\n22SLAf6mPsjJ1dWVp556ijVr1pCamkpqaiqffPIJTz31FJ988glDhgy54T3Wrl2LxWIhISGhqs3R\n0ZERI0aQkpJCbm5urWry8/MD4PTp01c9XlJSQmlpaa3uKSLGMJlM/DIugCcTIskrKmZaUjIHc7S5\nVUREBG4ywF/PqVOnOHz48A3Py8jIICgoCBcXl2rtERERVFZWXnej7GWFhYXk5+eze/dunnvuOQB6\n9OhxxXnLli2ja9euREREMGzYMDZs2FDDpxERI3Vp58WfHorF0WLm1QW7+HbvcaNLEhERMZxhn5qS\nl5eHj4/PFe2X16/XZAZ+0KBBVZ/66uHhwQsvvED37t2rnRMVFcWQIUPw9/fn2LFjzJs3j0mTJvH6\n668zdOjQOngSEalPrVte2tz63ordzFm9l2Mnz3PPnUGYTSajSxMRETGEYQG+pKQEi8VyRbujoyNA\njZa7vPvuu5w/f57Dhw+zevVqzp07d8U5ixYtqvb9fffdx9ChQ3nttde4++67MdUyBFxvPVJ9s1rd\nDOtbrk5j0jCswCu/u4u/LE/l42++p+BsGU+Ojrrm5laNi+3RmNgmjYvt0ZjYJlsbF8MCvJOTE+Xl\n5Ve0Xw7ul4P89cTFxQHQu3dv+vfvz7Bhw3B2dmbcuHHXvMbZ2ZlRo0bx+uuvc+jQIdq3b1+rurWJ\nVS7TmDS8UX3b4+nqwJLNB8jOPcMTD0TQwq36nxUaF9ujMbFNGhfbozGxTbfNJta6YLVar7pMJi8v\nDwBvb+9a3S8gIIDw8HA+/vjjG57r6+sLQFGRNsWJNCYmk4lB3QL53YgIjhecZ1rSDg4fu/rGdRER\nkdtVjWbg//Wvf9X4hjt37qzReaGhocyfP59z585V28iamppadby2SkpKKC4uvuF5R44cAcDT07PW\nfYiI8bp2aMmUcTG8tSyNVxbs5NG7w+gWduWeGhERkdtRjQL8q6++Wqub1mRdeXx8PHPnzmXp0qVV\n74EvKytjxYoVREdHV21wzcnJobi4uNpSl4KCgivCd3p6OpmZmdVeYXm1806dOsWHH36Iv7//TX2Q\nk4jYBn9vV54fH8u7K3fz11V72JFxgu+Pn6HgdCme7o7c37s9PcJbGV2miIhInatRgJ83b16ddxwZ\nGUl8fDyzZs0iLy+PwMBAVq5cSU5ODjNmzKg6b/LkyWzfvp2srKyqtr59+zJ48GCCg4NxdnbmwIED\nLF++HBcXFx5//PGq8xYsWMCmTZvo06cPfn5+nDhxgsWLF1NQUMB7771X588kIg3L3cWBP46KYtai\nnaTsO1nVnn+6lKRPMwEU4kVE5LZTowDfrVu3eul85syZvPnmm6xatYqioiJCQkKYM2cOMTEx171u\nzJgxbN26lY0bN1JSUoLVaiU+Pp7HH3+cgICAqvOioqLYuXMnS5cupaioCGdnZ7p27crEiRNv2IeI\nNA4WezMFp698a1XZhQqWf35QAV5ERG47psrKyoZ/pUojprfQyGUaE9vxq1c2X/NY93Af4kK96Rzk\nicXergGrksv0u2KbNC62R2Nim2zxLTSGvUZSRKSueLk7kn+VWXhHix27D+bz7Z4TNHO0o2sHK3Fh\n3oS39cRib9hLuERERG6JAryINHr3925P0qeZlF2oqGpzsDeTGB9CXKg3mT+cYntmLrv25bF1z3Ga\nOdoT3bElcWHedGrrib2dwryIiDQeCvAi0uhdXue+4ouDV30LTed2XnRu58WFQSHs/f4UOzJPsHPf\nSb5OP46Lkz1RwVa6hXoT2qaFwryIiNg8BXgRuS30CG9Fj/BW112raG9nJqK9FxHtvUgcVMHe7wvY\nkZlLSlYuX6Udw8XJnpgQK3GhPoS28cDOrDAvIiK2RwFeRJoki72ZyA4tiezQkvILFaQfzmdHZi7b\nMnLZknoM12aWn8K8NyGBCvMiImI7FOBFpMmz2JuJ6mglqqOVsvKLpB++NDP/7Z4TfPFdDm7OFmJC\nvOkW6k1wgAdm840/rE5ERKS+KMCLiPwXB4sd0cFWooMvhfndhy7NzH+TfozPdx3F3cWB2J9m5jv6\nK8yLiEjDU4AXEbkGB4sdMSHexIR4U1p2kbRD+ezIOMFXacfYvPMozV0diA3xJi7Umw7+zTGbFOZF\nRKT+KcCLiNSAo4MdcaGXwnpJ2QXSDuazIyOXLak5bErJpoWbIzEhVrqF+dDOz11hXkRE6o0CvIhI\nLTk52NMtzIduYT4Ul14g9eBJdmTk8vmuHDYmZ+Pp7nhpZj7Mm3a+7pgU5kVEpA4pwIuI3IJmjvZ0\n79SK7p1aUVx6ge8OXArzm3dms37HEbzcnS7N3Id507aVm8K8iIjcMgV4EZE60szRvup99OdLytm1\n/yQ7MnPZkHyEtdt/pGVzJ+LCvOkW6kOgj6vCvIiI3BQFeBGReuDsZKFnF196dvHlXEk5u/ZdCvPr\ntx/h029/xNujGXFhl9bUB3grzIuISM0pwIuI1DMXJwu9InzpFeHL2eJydu3LY3tmLp9++yNrtv6A\nT4vLYd4Hf6uLwryIiFyXAryISANybWbhzkg/7oz048z5Mnbuy2NHZi5rtv7AJ9/8QCtP56o18/5W\nV6PLFRERG6QALyJiEDdnB3p3bU3vrq05fe7/wvwnW7/n42++x6+lS9WrK/1auhhdroiI2AgFeBER\nG+Du4kCfqNb0iWpN0bkyUrJy2ZGRy+qvDrPqq8O0tv5fmPf1UpgXEWnKFOBFRGxMcxcH+kX70y/a\nn8KzpaRk5bEj4wSrvjzMR18ext/q+tPbbLzx8XQ2ulwREWlgCvAiIjbMw9WR/jH+9I/x59SZUpKz\nctmRmcvKLYdYueUQgT6uVTPz3i0U5kVEmgIFeBGRRqKFmyO/jA3gl7EBFJwuITkrjx2ZJ1j+xSGW\nf3GINq3c6BbqTWyoN1aPZkaXKyIi9UQBXkSkEfJ0d2JgXAAD4wLILyphR+almfmlnx9k6ecHCfJ1\nIy7Uh9hQKy2bK8yLiNxOFOBFRBo5r+ZOxN8RSPwdgZwsLGbHTxtgl3x2gCWfHaC9nztxP83Me7o7\nGV2uiIjcIgV4EZHbSEuPZgy+ow2D72hDbmExyZmXwvyizQdYtPkAHVo3rwrzLdwcjS5XRERuggK8\niMhtytujGUO6t2FI9zacKDhftcxm4ab9LNy0n47+l8J8TIjCvIhIY6IALyLSBPh4OjP0F20Z+ou2\nHMs/d2lmPjOXDzfuZ+HG/XQM8Lg0Mx9ipbmrwryIiC1TgBcRaWJ8vVwY1jOIYT2DyDl5Kcxvz8xl\nwYZ9fLhxHyEBHsSF+RATbMXdxcHockVE5GcU4EVEmjC/li4M7xXE8F5BHM07W7XMZv66LD5Yn0Vo\nYAviwryJCbbi5qwwLyJiCxTgRUQEgNZWV1pbXbmnVxBH886xPTOXHRknmLc2iw/W7SOszaWZ+ehg\nK67NLEaXKyLSZCnAi4hINSaTCX9vV/y9XbnvziCO5P40M5+Ry78/zWT+uizC2rYgLtSb6GArLk4K\n8yIiDUkBXkRErslkMhHo40agjxv339WOH09cCvPbM07wr/9kMm9tFuFBnsSFehPVsSXOCvMiIvVO\nAV5ERGrEZDLRppUbbVq58UDvdnx//EzVzHzawQzszCY6B3nSLcyHrh1b0sxRf8WIiNQH/ekqIiK1\nZjKZCPJ1J8jXnYQ+7Tl87Aw7Mk+wIzOX1IP52NuZ6dLu0sx8/+769FcRkbqkAC8iIrfEZDLRzs+d\ndn7uJPTtwOGc02zPyCU5K5dd+0/yr08z6dLOi7hQbyI7eOHkoL96RERuhf4UFRGROmM2mWjfujnt\nWzfnwf4dOHi0iPQfCtmyK5ud+/Kw2JuJaP9TmG/fEkcHO6NLFhFpdBTgRUSkXphNJjr6e/CLqADu\n+UUbDmQXseOnmfmUrDwcLGYi27ckLtSbLu29cLQozIuI1IQCvIiI1DuzyURwgAfBAR6MHtCR/dmF\nbM/MJeWnD45ytNgR2cGLuFAfurTzxEFhXkTkmhTgRUSkQZnNJkICWxAS2IKxA4LJOlLIjowTJGfl\nsT0jF0cHO6I6XJqZ79zOE4u9wryIyH9TgBcREcOYzSbC2rQgrE0Lxg4MJvPHQnZk5LJzXx7f7j1B\nM0c7unawEhfmTXhbTyz2ZqNLFhExnKEBvqysjLfeeotVq1Zx+vRpQkNDeeqpp+jRo8d1r1u9ejXL\nli3j4MGDFBUV4e3tzR133MGkSZNo3br1FecvXbqUuXPnkp2djZ+fH4mJiYwdO7a+HktERG6CndlM\neFtPwtt6Mm5gMJk/nqoK81v3HKeZoz3RHVsSF+ZNp7ae2NspzItI02RogH/22WdZv349iYmJtGnT\nhpUrVzJhwgTmz59PVFTUNa/LzMzEx8eH3r1707x5c3JycliyZAmff/45q1evxmq1Vp27aNEi/vzn\nPxMfH88jjzxCcnIy06ZNo7S0lF/96lcN8ZgiIlJL9nZmOgd50TnIi4cGhbD3+1PsyDzBzn0n+Tr9\nOM6O9kQHX5qZD2vTQmFeRJoUU2VlZaURHaelpZGQkMBzzz3Hww8/DEBpaSlDhw7F29ubBQsW1Op+\ne/bs4f777+eZZ57h0UcfBaCkpITevXsTExPD+++/X3Xu008/zebNm/niiy9wc3OrVT/5+WepqGj4\nH5nV6kZe3pkG71euTWNimzQutqcux+TCxQr2HC5gR2Yuu/bnUVx6ERcne2JCrMSF+hDaxgM7s8J8\nTeh3xfZoTGyTEeNiNpvw8nK95nHDZuDXrl2LxWIhISGhqs3R0ZERI0Ywe/ZscnNz8fb2rvH9/Pz8\nADh9+nRV27Zt2ygsLGTMmDHVzh07diwff/wxW7Zs4e67777FJxERkYZib2cmskNLIju0pPzC5TB/\ngu0ZuWxJPYZrM8tPYd6bkECFeRG5PRkW4DMyMggKCsLFxaVae0REBJWVlWRkZNwwwBcWFnLx4kVy\ncnJ47733AKqtn9+7dy8AnTt3rnZdeHg4ZrOZvXv3KsCLiDRSFnszXTu2pGvHlpRfuMjuQ5dm5r/d\nc4IvvsvBzdlCTIj3pTAf4IHZbDK6ZBGROmFYgM/Ly8PHx+eK9svr13Nzc294j0GDBlFYWAiAh4cH\nL7zwAt27d6/Wh4ODAx4eHtWuu9xWkz5ERMT2WeztiA62Eh1spaz8IrsP5bMjM5dv0o/x+a6juLs4\nEBNipVuoNx39FeZFpHEzLMCXlJRgsViuaHd0dAQurYe/kXfffZfz589z+PBhVq9ezblz52rUx+V+\natLHz11vPVJ9s1prt15f6p/GxDZpXGxPQ49Jaz8P4nu1p6TsAikZuXyZepSvdx/ns51H8XR35BcR\nfvSKbE1YW88mHeb1u2J7NCa2ydbGxbAA7+TkRHl5+RXtl0P15SB/PXFxcQD07t2b/v37M2zYMJyd\nnRk3blxVH2VlZVe9trS0tEZ9/Jw2scplGhPbpHGxPUaPSbCfG8F+oYzr35HUgyfZkZnLum9/4JOv\nDuPh6kBsqDfdQn1o19ods6nphHmjx0WupDGxTdrE+l+sVutVl7Dk5eUB1GoDK0BAQADh4eF8/PHH\nVQHearVSXl5OYWFhtWU0ZWVlFBYW1roPERFpvBwd7OgW5kO3MB+KSy9cCvMZuXy+K4eNydm0cHMk\nLtSbuDBv2vm6Y2pCYV5EGhfDAnxoaCjz58/n3Llz1TaypqamVh2vrZKSEoqLi6u+DwsLAyA9PZ1e\nvXpVtaenp1NRUVF1XEREmpZmjvZ079SK7p1aUVx6ge8OXArzm3dms37HEbzcHYkL9SEuzJu2rdwU\n5kXEphj2fq34+HjKy8tZunRpVVtZWRkrVqwgOjq6aoNrTk4OBw8erHZtQUHBFfdLT08nMzOT8PDw\nqrbu3bvj4eHBhx9+WO3chQsX4uzszF133VWXjyQiIo1QM0d7eoS34okREbz5uzv59dAwWltd2ZB8\nhBeTkpn8160s/ewA3x8/jUEfnSIiUo1hM/CRkZHEx8cza9Ys8vLyCAwMZOXKleTk5DBjxoyq8yZP\nnsz27dvJysqqauvbty+DBw8mODgYZ2dnDhw4wPLly3FxceHxxx+vOs/JyYknnniCadOm8fvf/55e\nvXqRnJzM6tWrefrpp3F3d2/QZxYREdvm7GTPLzr78ovOvpwrKWfXvktr5tfvOMKn237E26MZcWGX\nXk0Z4O2qmXkRMYRhAR5g5syZvPnmm6xatYqioiJCQkKYM2cOMTEx171uzJgxbN26lY0bN1JSUoLV\naiU+Pp7HH3+cgICAaueOHTsWi8XC3Llz2bRpE76+vkyZMoXExMT6fDQREWnkXJws9IrwpVeEL/+/\nvTsPb6rK3wD+JmnSPW2Tpvua0I3uFIQCYtmciijghsriiDAoOqM4Mw8yzjwzOqPOM66IzjMIOArj\nuICUCo5QBBQsIArYFrpB0kJr9xS6bzT390fa/KxtobRNk7Tv5y9zck/7DafX+/b03HMbWzpwprAa\n3+VX4YsTl/D58YvwChKraAAAIABJREFU9ugO894IUDkzzBPRiBEJ/HvgDeEuNNSNY2KdOC7WZ7SN\nSUNzO86cr8HJvErkXbwMQQB8FE6mG2D9PW0jzI+2cRkNOCbWibvQEBER2ThXJxlmxPthRrwf6pvb\ncbrAODO/93gx9hwrhq+yO8x7w9/T+bpfj4joRjHAExERDZLcSYaURH+kJPqjrqkdpwuq8F1+FfZk\nFuOzzGL4q5yNYT7SC75KhnkiGh4M8ERERMPAzVmGmRMCMHNCAOoa2/B918x8+tEi7D5ahACVCyZF\neeGmSC94K5wsXS4R2TAGeCIiomHm5mKP2UkBmJ0UgMsNbfi+a2Y+7YgOaUd0CPJyMe1m4+XBME9E\nN4YBnoiIyIw8XO0xd2Ig5k4MRG19a9fMfCU+/VqHT7/WIdjHFTdFemFipBdU7o6WLpeIbAADPBER\n0QhRyB1w66RA3DopEPq6VtPM/I6vtNjxlRahvq6YFOmNiZEqeLoxzBNR3xjgiYiILEDp5oBf3BSE\nX9wUhJorLfiuoArf5VXhk8MX8MnhC1D7yU03wCrkDpYul4isCAM8ERGRhXm6O+K2ycG4bXIwqq60\n4Pt8Y5j/+NAFfHzoAsb5u2FS1zIbD1d7S5dLRBbGAE9ERGRFvNwdMW9KMOZNCUbl5WZTmP/w4Hl8\nePA8wgKMYT4pgmGeaKxigCciIrJS3h5OuD05BLcnh6CithnfdYX5/355Hh9+eR5hge7GmfkIFdxc\nGOaJxgoGeCIiIhvgo3DCHVNDcMfUEJTVNBln5vOr8MGBQvz3QCEigtxNM/NyZ5mlyyUiM2KAJyIi\nsjF+ns64c3oo7pweih+rG40z8/lV2J5RiP8cKERkkAcmRXlhQrgKcieGeaLRhgGeiIjIhvmrXOCv\ncsGC6aH4saYJ3+VV4WR+FbbtK8B/9hciKtgdk6K8MSFcBRdHKY6fq8Cur7WorW+DQm6Pu27RIDna\nx9Ifg4huAAM8ERHRKCASiRCgckGAygULbw5FSdX/z8y/90U+tu8vgK/CCeW1zeg0CAAAfX0b3v8i\nHwAY4olsCAM8ERHRKCMSiRDk7Yogb1fcNUONS5XGML/v24voyu4m7VcN2PW1lgGeyIaILV0AERER\nmY9IJEKwjyvuSdH0Cu/d9PVt+E9GAbK1NWjr6BzZAonohnEGnoiIaIxQyu2hr2/r1S61E+ObnHIc\nOv0jpHZiRAZ5IE6jRKxGCS93RwtUSkTXwgBPREQ0Rtx1iwbvf5GP9qsGU5vMToyHbovExAgVCkvq\nkK3VI1unxwcHCoEDxu0ru8N8eIA7pHb84z2RpTHAExERjRHd69z724UmOlSB6FAFHkAYKi83I6cr\nzB86/SMyviuBvVSC8SEeiNUoEadWQiF3sOTHIRqzGOCJiIjGkORoHyRH+0ClckV1dUO/x3l7OMF7\nohPmTAxEW0cn8i9eRrZOj+wLepw5XwMACFA5m8K8xt8NdhLOzhONBAZ4IiIiuiZ7qQTx4zwRP84T\nwlwB5fpmZGv1yNHpkXGyBF+cuARHeztEhyoQp1YiVq2Am4u9pcsmGrUY4ImIiGjARCIR/Dyd4efp\njNTJQWhpu4rc4svI0dUgW6vH9/lVAIBgH1fEqZWI0ygR6iuHWCyycOVEowcDPBEREQ2ao70dkiJU\nSIpQQRAElFQ1IkenR7ZWj73Hi7HnWDFcHKWIURtn52PUSrg4Si1dNpFNY4AnIiKiYfHTB0jdnhyC\nxpYO5BbXmpbbnDhXCZEIUPvJu2bnPRHo7QKxiLPzRDeCAZ6IiIjMwsVRipuivHFTlDcMgoCLFQ3G\nbSq1euw+WoS0o0Vwc5YhtmupzfgQBZwcGE2IrodnCREREZmdWCRCqK8cob5yLJgeivqmdpwtMob5\n04XV+CanHBKxCOP83Uz7zvt7OkPE2XmiXhjgiYiIaMTJnWWYGuOLqTG+6DQYoCurN83O7/hKix1f\naaGQ2xt3tdEoERXsAQcZYwsRwABPREREFiYRixEW4I6wAHfcfYsGlxvakKPTI0erx4ncSnz1Qxns\nJCJEBLojVuOJOI0S3h6OnJ2nMYsBnoiIiKyKh6s9ZsT7YUa8H652GnC+tM70VNiPDp7HRwfPw8vd\n0fgQKY0SEYHukEklli6baMQwwBMREZHVspOIERXsgahgD9w3axxqrrSYtqk8mlWGg6dKIbMTIzLY\nA3FdT4X1dHe0dNlEZsUAT0RERDbD090RMycEYOaEALR3dKKw5Ipp7Xy2Vg8A8FU6mcJ8WKA77CRi\nC1dNNLwY4ImIiMgmyaQSxHQ9HOrBuUBlbXNXkK/BwVOl2H+yBPYyCaJDFMadbdRKeLjaW7psoiFj\ngCciIqJRwVvhhLkKJ8ydFIjW9qvIu3gZObpaZGtrcLqwGgAQ6OViCvMafzkkYs7Ok+1hgCciIqJR\nx0Fmh8QwFRLDVBCEcJTVNJmW2Xxx4hI+P34RTvZ2iFErEKs2Bnq5s8zSZRMNCAM8ERERjWoikQj+\nKhf4q1xw25RgNLdeRW5xLbK1euTo9DiZVwURgBBf166nwnoixNcVYm5TSVaKAZ6IiIjGFCcHO0yM\n9MLESC8YBAEllY3I1tYgW6fHnsxifJZZDFcnKWJCjdtURocq4OIotXTZRCYM8ERERDRmiUUiBPu4\nItjHFXdMC0VDczvOFdUiW2ecnT9+rgIiEaDxd0Oc2hjoA71c+BApsigGeCIiIqIurk4yTIn2wZRo\nHxgMAorK641r53V67Dqiw64jOri7yLqW2igxPkQBR3vGKRpZ/IkjIiIi6oNYLILG3w0afzcsmqFG\nXWObcVcbnR7fF1TjaHY5JGIRwgLcEKfxRKxGCT+lE2fnyewsGuDb29uxYcMGpKeno76+HpGRkVi7\ndi2Sk5Ov2S8jIwP/+9//kJ2dDb1eD19fX8ycORNr1qyBq6trj2MjIiL6/Bp/+ctf8MADDwzbZyEi\nIqLRzc3FHtPjfDE9zhdXOw3Q/lhnXGqj1eOTwxfwyeELUModjNtUapSICvKAvUxi6bJpFLJogH/m\nmWeQkZGB5cuXIzg4GGlpaVi1ahW2b9+OxMTEfvv96U9/gpeXFxYsWAA/Pz8UFBRg+/btOHr0KD79\n9FPY2/d8SMP06dNx55139miLj483y2ciIiKi0c9OIkZEkAcigjxwb8o41Na3msL8sbMVOHzmR9hJ\nxIgMckesxrjcxtvDydJl0yhhsQCfnZ2Nzz//HOvXr8cvf/lLAMDChQsxf/58vPLKK/jggw/67fvm\nm29i8uTJPdpiYmKwbt06fP7557jrrrt6vKdWq7FgwYJh/wxEREREAKCQOyAlwR8pCf7ouGpAYekV\n5HTtO//hl+fx4Zfn4e3haArzEYHukNpxdp4Gx2IBft++fZBKpbj33ntNbfb29rjnnnvw+uuvo6qq\nCl5eXn32/Xl4B4A5c+YAALRabZ99WltbIRKJes3OExEREQ0nqZ0Y0SEKRIcocP/sMFRdbu56Iqwe\nX/9Qhi+/L4VMKsb4YIUx0KuVULo5WLpssiEWC/B5eXkIDQ2Fs7Nzj/a4uDgIgoC8vLx+A3xfampq\nAAAeHh693tu5cye2b98OQRAQHh6O3/zmN5g7d+7QPgARERHRAHh5OGF2khNmJwWgraMTBZcum54K\n+8MFY37x93TG5BhfjPN1xbgAN9hJxBaumqyZxQJ8dXU1vL29e7WrVCoAQFVV1Q19vc2bN0MikeDW\nW2/t0Z6YmIh58+YhICAA5eXl2LZtG5544gm8+uqrmD9//uA/ABEREdENspdKEKfxRJzGE4IgoKK2\n2RTmPzuqxdVOAY72EowPUSBObbwZ1t2FqweoJ4sF+NbWVkilvZ9q1r3Epa2tbcBfa8+ePdi5cydW\nr16NoKCgHu999NFHPV4vWrQI8+fPx8svv4zbb7/9hrd6Uipdbuj44aRSuV7/IBpRHBPrxHGxPhwT\n68RxsTwvLzniIn0AAM2tHcg6X4NT+ZX4Pq8SpwqqAQBqfzdMjPLGxEhvhAd7QCLmNpUjzdrOFYsF\neAcHB3R0dPRq7w7uA12r/v333+PZZ59FSkoKnnzyyese7+TkhPvvvx+vvvoqdDodNBrNDdWt1zfC\nYBBuqM9wUKlcUV3dMOLfl/rHMbFOHBfrwzGxThwX66NSuWKcjwvG+bjgvlvUKK1uQra2BjlaPXYe\nPI9PviyEs4MdYtVKxKqViFYrIHeSWbrsUc8S54pYLLrmpLHFArxKpepzmUx1tfG3zYGsf8/Pz8dj\njz2GiIgIvP7665BIBnY3t6+vLwCgrq7uBiomIiIiGhkikQiBXi4I9HLB7ckhaGrtwLmiWuRo9cjR\n6XEitxIiAKF+ctNSm2AfV4j5EKkxwWIBPjIyEtu3b0dTU1OPG1mzsrJM71/LpUuXsHLlSigUCmza\ntAlOTgPfW7WkpAQAoFAoBlE5ERER0chydpDipihv3BTlDYMg4GJFg3GbSp0e6d8UYfc3RZA7SY2z\n8xolYkIVcHLovVSZRgeLBfjU1FS8++672LFjh2kf+Pb2duzatQsTJkww3eBaVlaGlpaWHktdqqur\nsWLFCohEImzdurXfIF5bW9vrvcuXL+O///0vAgICEBISYpbPRkRERGQuYpEIob5yhPrKcef0UNQ3\nt+OcrhbZOuOuNplnKyAWiTDOX96177wnAlTON3zfH1kviwX4+Ph4pKam4pVXXkF1dTWCgoKQlpaG\nsrIyvPTSS6bj1q1bh5MnT6KgoMDUtnLlSpSUlGDlypU4deoUTp06ZXovKCjI9BTXDz74AAcPHkRK\nSgr8/PxQWVmJjz/+GLW1tXj77bdH7sMSERERmYncSYbkGB8kx/ig02BAUVkDsnU1yNbq8enXOnz6\ntQ4ervaIVSsRr1EiKsQDDjKLRUAaBhYdvX/84x944403kJ6ejrq6OkREROCdd95BUlLSNfvl5+cD\nALZs2dLrvUWLFpkCfGJiIk6fPo0dO3agrq4OTk5OSEhIwOrVq6/7PYiIiIhsjUQsxrgAN4wLcMNd\nMzS43NCGszrjUpuTeZU4klUGiViEiCB309p5H4UTZ+dtjEgQhJHfUsWGcRca6sYxsU4cF+vDMbFO\nHBfrY+4xudppwIXSOmTr9MjR6vFjTZPx+7o7IE7tiViNEpFB7pBJB7YpyFjBXWiIiIiIyCLsJGJE\nBnsgMtgD980ch5q6FuTojDvbHM0pw8HTpZDaiREV7GG6GdbL3dHSZVMfGOCJiIiIxiBPN0fMTPTH\nzER/dFztREHJFWRrjbPz2Vo9cADwVTohVq1EnEaJsAB3SO3Eli6bwABPRERENOZJ7SSICVUiJlQJ\nzAEqa5tNS20Onf4RGd+VwF4mwfhgD8RpjA+SUsgdLF32mMUAT0REREQ9eCucMFfhhLkTA9HW3om8\nS5e7ZuZrcOZ8DQAgQOWCOI1xdl7jL4dEzNn5kcIAT0RERET9spdJkDDOEwnjPCEI4SjTN5vC/P6T\nl/C/ExfhaG+HmFAF4jRKxKiVcHOWWbrsUY0BnoiIiIgGRCQSwd/TGf6ezkidHISWtqvILa5FdtdT\nYb/LrwIAhPi4GpfaaJQI9ZFDLOY2lcOJAZ6IiIiIBsXR3g5JEV5IivCCIAgoqWo0hfk9x4rxWWYx\nXByliFUrEKsxrrF3cZRaumybxwBPREREREMmEokQ5O2KIG9XzJ8agsaWDpwrMs7O5+j0OH6uEiIR\noPFzQ6xGiTi1EkHeLnyI1CAwwBMRERHRsHNxlGLyeG9MHu8Ng0FAcUUDsrU1yNHpkXZEh7QjOri5\nyIzbVKqVGB+igJMDo+lA8F+JiIiIiMxKLBZB7SeH2k+OhTerUdfUjrM648z86YJqfJNdDolYhLCA\n/5+d9/N05ux8PxjgiYiIiGhEuTnLMC3WF9NifdFpMED7Yz1ydMYHSO04rMWOw1oo5faI1XgiTq1E\nVLAH7GUSS5dtNRjgiYiIiMhiJGIxwgPdER7ojrtv0aC2vhVnu9bOHz9Xga/O/Ag7iQgRQR6I63oq\nrLfCydJlWxQDPBERERFZDYXcATPi/TAj3g9XOw04X3IF2V2z8x8ePI8PD56Hl4ejKcxHBLlDaje2\nZucZ4ImIiIjIKtlJxIgKUSAqRIHFs8JQfaXFtNTmSFYZvjxVCpmdGFHBHsZ959VKeLo7Wrpss2OA\nJyIiIiKboHJ3xKwJAZg1IQDtHZ0oKLli3HdeW4MsrR4A4OfpjDi18SFSYQFusJOILVz18GOAJyIi\nIiKbI5NKEKs2zro/OCcMFbXNyNHVIkdbgy9PlWDfyUtwkEkQHWJ8iFSsWgkPV3tLlz0sGOCJiIiI\nyKaJRCL4Kp3hq3TGrZMC0dp+FXkXL3fNzutxqrAaABDk5WLcplKjhNpPDonYNmfnGeCJiIiIaFRx\nkNkhMUyFxDAVBEHAj9VNphthvzhxCZ8fvwhnBztEhyoQp1EiRq2E3EnW42scP1eBXV9rUVvfBoXc\nHnfdokFytI+FPlFPDPBERERENGqJRCIEeLkgwMsF86YEo7m1A+eKL3c9FbYWJ/OqIAIQ4itHXNfs\nfLm+Cdv2FaD9qgEAoK9vw/tf5AOAVYR4BngiIiIiGjOcHKSYFOmFSZFeMAgCLlU2IFurR45Wj8++\nKUL6N0UQiQBB6Nmv/aoBu77WMsATEREREVmKWCRCiI8cIT5y3DktFA3N7ThbVIvNe3L7PF5f3zbC\nFfbNNlfuExERERENM1cnGZKjfaCU971bTX/tI40BnoiIiIjoJ+66RQOZXc+YLLMT465bNBaqqCcu\noSEiIiIi+onude7chYaIiIiIyEYkR/sgOdoHKpUrqqsbLF1OD1xCQ0RERERkQxjgiYiIiIhsCAM8\nEREREZENYYAnIiIiIrIhDPBERERERDaEAZ6IiIiIyIYwwBMRERER2RAGeCIiIiIiG8IAT0RERERk\nQ/gk1hskFovG5PemvnFMrBPHxfpwTKwTx8X6cEys00iPy/W+n0gQBGGEaiEiIiIioiHiEhoiIiIi\nIhvCAE9EREREZEMY4ImIiIiIbAgDPBERERGRDWGAJyIiIiKyIQzwREREREQ2hAGeiIiIiMiGMMAT\nEREREdkQBngiIiIiIhvCAE9EREREZEPsLF3AWNbe3o4NGzYgPT0d9fX1iIyMxNq1a5GcnHzdvpWV\nlXjxxReRmZkJg8GAKVOmYP369QgMDByBykevwY7Jxo0b8dZbb/Vq9/T0RGZmprnKHROqqqqwbds2\nZGVl4ezZs2hubsa2bdswefLkAfXXarV48cUXcfr0aUilUsycORPr1q2DQqEwc+Wj21DG5ZlnnkFa\nWlqv9vj4eHzyySfmKHdMyM7ORlpaGr799luUlZXB3d0diYmJeOqppxAcHHzd/ryuDL+hjAmvK+aT\nk5ODf/3rX8jNzYVer4erqysiIyPx+OOPY8KECdftbw3nCgO8BT3zzDPIyMjA8uXLERwcjLS0NKxa\ntQrbt29HYmJiv/2ampqwfPlyNDU14dFHH4WdnR3ee+89LF++HLt374abm9sIforRZbBj0u3555+H\ng4OD6fVP/5sGp6ioCJs3b0ZwcDAiIiJw5syZAfetqKjAkiVLIJfLsXbtWjQ3N+Pdd99FYWEhPvnk\nE0ilUjNWProNZVwAwNHREc8991yPNv5SNTRbtmzB6dOnkZqaioiICFRXV+ODDz7AwoULsXPnTmg0\nmn778rpiHkMZk268rgy/kpISdHZ24t5774VKpUJDQwP27NmDpUuXYvPmzZg2bVq/fa3mXBHIIrKy\nsoTw8HDh3//+t6mttbVVmDNnjvDggw9es+8777wjRERECOfOnTO1XbhwQYiKihLeeOMNc5U86g1l\nTN58800hPDxcqKurM3OVY09DQ4NQW1srCIIgHDhwQAgPDxdOnDgxoL5//vOfhYSEBKGiosLUlpmZ\nKYSHhws7duwwS71jxVDGZd26dUJSUpI5yxuTTp06JbS1tfVoKyoqEmJiYoR169Zdsy+vK+YxlDHh\ndWVkNTc3C1OnThV+9atfXfM4azlXuAbeQvbt2wepVIp7773X1GZvb4977rkHp06dQlVVVb999+/f\nj4SEBIwfP97UptFokJycjC+++MKsdY9mQxmTboIgoLGxEYIgmLPUMcXFxQUeHh6D6puRkYFZs2bB\n29vb1DZ16lSEhITwXBmioYxLt87OTjQ2Ng5TRTRhwgTIZLIebSEhIQgLC4NWq71mX15XzGMoY9KN\n15WR4ejoCIVCgfr6+mseZy3nCgO8heTl5SE0NBTOzs492uPi4iAIAvLy8vrsZzAYUFBQgJiYmF7v\nxcbGori4GC0tLWapebQb7Jj8VEpKCpKSkpCUlIT169fjypUr5iqXrqOyshJ6vb7PcyUuLm5A40nm\n09TUZDpXJk+ejJdeegltbW2WLmvUEQQBNTU11/xli9eVkTWQMfkpXlfMp7GxEbW1tdDpdHjttddQ\nWFh4zXverOlc4Rp4C6muru4xK9hNpVIBQL+zvVeuXEF7e7vpuJ/3FQQB1dXVCAoKGt6Cx4DBjgkA\nyOVyLFu2DPHx8ZBKpThx4gQ+/vhj5ObmYseOHb1mYMj8userv3NFr9ejs7MTEolkpEsb81QqFVau\nXImoqCgYDAYcPnwY7733HrRaLbZs2WLp8kaVzz77DJWVlVi7dm2/x/C6MrIGMiYArysj4Q9/+AP2\n798PAJBKpbj//vvx6KOP9nu8NZ0rDPAW0tra2ucNdPb29gDQ70xUd3tfJ25339bW1uEqc0wZ7JgA\nwEMPPdTjdWpqKsLCwvD8889j9+7duO+++4a3WLqugZ4rP/+LC5nfb3/72x6v58+fD29vb2zduhWZ\nmZnXvIGMBk6r1eL5559HUlISFixY0O9xvK6MnIGOCcDrykh4/PHHsXjxYlRUVCA9PR3t7e3o6Ojo\n95cjazpXuITGQhwcHNDR0dGrvfuHo/sH4ee629vb2/vtyzvUB2ewY9KfBx54AI6Ojjh+/Piw1Ec3\nhueKbVmxYgUA8HwZJtXV1Vi9ejXc3NywYcMGiMX9X+55royMGxmT/vC6MrwiIiIwbdo03H333di6\ndSvOnTuH9evX93u8NZ0rDPAWolKp+lySUV1dDQDw8vLqs5+7uztkMpnpuJ/3FYlEff5ph65vsGPS\nH7FYDG9vb9TV1Q1LfXRjuserv3NFqVRy+YwV8fT0hFQq5fkyDBoaGrBq1So0NDRgy5Yt170m8Lpi\nfjc6Jv3hdcV8pFIpZs+ejYyMjH5n0a3pXGGAt5DIyEgUFRWhqampR3tWVpbp/b6IxWKEh4fj7Nmz\nvd7Lzs5GcHAwHB0dh7/gMWCwY9Kfjo4OlJeXD3mnDhocb29vKBSKfs+VqKgoC1RF/amoqEBHRwf3\ngh+itrY2PProoyguLsamTZugVquv24fXFfMazJj0h9cV82ptbYUgCL1yQDdrOlcY4C0kNTUVHR0d\n2LFjh6mtvb0du3btwoQJE0w3U5aVlfXaauoXv/gFfvjhB+Tm5pradDodTpw4gdTU1JH5AKPQUMak\ntra219fbunUr2tracPPNN5u3cAIAXLp0CZcuXerRduutt+LQoUOorKw0tR0/fhzFxcU8V0bIz8el\nra2tz60j//nPfwIApk+fPmK1jTadnZ146qmn8MMPP2DDhg1ISEjo8zheV0bOUMaE1xXz6evftrGx\nEfv374evry+USiUA6z5XRAI3FrWYJ598EgcPHsRDDz2EoKAgpKWl4ezZs3j//feRlJQEAFi2bBlO\nnjyJgoICU7/GxkYsWrQILS0tePjhhyGRSPDee+9BEATs3r2bv5kPwWDHJD4+HvPmzUN4eDhkMhm+\n/fZb7N+/H0lJSdi2bRvs7Hi/+FB0hzutVou9e/fi7rvvRkBAAORyOZYuXQoAmDVrFgDg0KFDpn7l\n5eVYuHAh3N3dsXTpUjQ3N2Pr1q3w9fXlLg7DYDDjUlpaikWLFmH+/PlQq9WmXWiOHz+OefPm4fXX\nX7fMhxkFXnjhBWzbtg0zZ87Ebbfd1uM9Z2dnzJkzBwCvKyNpKGPC64r5LF++HPb29khMTIRKpUJ5\neTl27dqFiooKvPbaa5g3bx4A6z5XGOAtqK2tDW+88Qb27NmDuro6RERE4Omnn8bUqVNNx/T1wwMY\n/9z84osvIjMzEwaDAZMnT8azzz6LwMDAkf4Yo8pgx+SPf/wjTp8+jfLycnR0dMDf3x/z5s3D6tWr\nefPXMIiIiOiz3d/f3xQM+wrwAHD+/Hn8/e9/x6lTpyCVSpGSkoL169dzqcYwGMy41NfX469//Suy\nsrJQVVUFg8GAkJAQLFq0CMuXL+d9CUPQ/f+mvvx0THhdGTlDGRNeV8xn586dSE9Px4ULF1BfXw9X\nV1ckJCRgxYoVuOmmm0zHWfO5wgBPRERERGRDuAaeiIiIiMiGMMATEREREdkQBngiIiIiIhvCAE9E\nREREZEMY4ImIiIiIbAgDPBERERGRDWGAJyIiIiKyIQzwRERk9ZYtW2Z6KBQR0VjH5/ASEY1R3377\nLZYvX97v+xKJBLm5uSNYERERDQQDPBHRGDd//nzMmDGjV7tYzD/SEhFZIwZ4IqIxbvz48ViwYIGl\nyyAiogHi9AoREV1TaWkpIiIisHHjRuzduxd33HEHYmNjkZKSgo0bN+Lq1au9+uTn5+Pxxx/H5MmT\nERsbi3nz5mHz5s3o7OzsdWx1dTX+9re/Yfbs2YiJiUFycjIefvhhZGZm9jq2srISTz/9NCZNmoT4\n+Hg88sgjKCoqMsvnJiKyVpyBJyIa41paWlBbW9urXSaTwcXFxfT60KFDKCkpwZIlS+Dp6YlDhw7h\nrbfeQllZGV566SXTcTk5OVi2bBns7OxMxx4+fBivvPIK8vPz8eqrr5qOLS0txQMPPAC9Xo8FCxYg\nJiYGLS0tyMrKwrFjxzBt2jTTsc3NzVi6dCni4+Oxdu1alJaWYtu2bVizZg327t0LiURipn8hIiLr\nwgBPRDTGbdwZ949UAAADRElEQVS4ERs3buzVnpKSgk2bNple5+fnY+fOnYiOjgYALF26FE888QR2\n7dqFxYsXIyEhAQDwwgsvoL29HR999BEiIyNNxz711FPYu3cv7rnnHiQnJwMAnnvuOVRVVWHLli24\n+eabe3x/g8HQ4/Xly5fxyCOPYNWqVaY2hUKBl19+GceOHevVn4hotGKAJyIa4xYvXozU1NRe7QqF\nosfrqVOnmsI7AIhEIqxcuRJffvklDhw4gISEBOj1epw5cwZz5841hffuYx977DHs27cPBw4cQHJy\nMq5cuYKjR4/i5ptv7jN8//wmWrFY3GvXnClTpgAALl68yABPRGMGAzwR0RgXHByMqVOnXvc4jUbT\nq23cuHEAgJKSEgDGJTE/bf8ptVoNsVhsOvbSpUsQBAHjx48fUJ1eXl6wt7fv0ebu7g4AuHLlyoC+\nBhHRaMCbWImIyCZca427IAgjWAkRkWUxwBMR0YBotdpebRcuXAAABAYGAgACAgJ6tP+UTqeDwWAw\nHRsUFASRSIS8vDxzlUxENCoxwBMR0YAcO3YM586dM70WBAFbtmwBAMyZMwcAoFQqkZiYiMOHD6Ow\nsLDHse+88w4AYO7cuQCMy19mzJiBI0eO4NixY72+H2fViYj6xjXwRERjXG5uLtLT0/t8rzuYA0Bk\nZCQeeughLFmyBCqVCgcPHsSxY8ewYMECJCYmmo579tlnsWzZMixZsgQPPvggVCoVDh8+jG+++Qbz\n58837UADAH/605+Qm5uLVatWYeHChYiOjkZbWxuysrLg7++P3//+9+b74ERENooBnohojNu7dy/2\n7t3b53sZGRmmteezZs1CaGgoNm3ahKKiIiiVSqxZswZr1qzp0Sc2NhYfffQR3nzzTXz44Ydobm5G\nYGAgfve732HFihU9jg0MDMSnn36Kt99+G0eOHEF6ejrkcjkiIyOxePFi83xgIiIbJxL4N0oiIrqG\n0tJSzJ49G0888QR+/etfW7ocIqIxj2vgiYiIiIhsCAM8EREREZENYYAnIiIiIrIhXANPRERERGRD\nOANPRERERGRDGOCJiIiIiGwIAzwRERERkQ1hgCciIiIisiEM8ERERERENoQBnoiIiIjIhvwfJi3C\nO+D7mBIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "outputId": "4634a6b4-88e8-40a8-ccd9-ba33f812e0f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/task1punjabi-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "outputId": "9bb9e0a0-8760-4237-f0b8-021bbac2020f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "#print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 500 test sentences...\n",
            "    DONE.\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/task1punjabi-results.txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "outputId": "ad05e2cd-d3b9-414d-f91b-9e10a028208e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] ):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] ):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    # elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "    #   #print(\" Semi Paraphrase     \",end=\"\")\n",
        "    #   #count_NP+=1      \n",
        "    #   print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "    #   x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(1)\t 1\t [ 0.4395841 -0.3949595] \t0\tfalse\n",
            "(0)\t 2\t [ 2.0134618 -2.0504131] \t0\ttrue\n",
            "(0)\t 3\t [-0.10603503  0.00139187] \t1\tfalse\n",
            "(0)\t 4\t [-1.4648659  1.5550133] \t1\tfalse\n",
            "(0)\t 5\t [ 2.00312  -2.036718] \t0\ttrue\n",
            "(0)\t 6\t [ 0.04988813 -0.00445753] \t0\ttrue\n",
            "(1)\t 7\t [-2.6069822  2.6403801] \t1\ttrue\n",
            "(0)\t 8\t [ 2.1355796 -2.0325055] \t0\ttrue\n",
            "(0)\t 9\t [ 2.0878968 -2.009596 ] \t0\ttrue\n",
            "(1)\t 10\t [-1.6112905  1.6292827] \t1\ttrue\n",
            "(1)\t 11\t [-2.589262  2.630895] \t1\ttrue\n",
            "(1)\t 12\t [-2.4469771  2.5763993] \t1\ttrue\n",
            "(0)\t 13\t [ 0.93532234 -1.177584  ] \t0\ttrue\n",
            "(1)\t 14\t [-2.618046   2.6863093] \t1\ttrue\n",
            "(0)\t 15\t [ 1.9846337 -1.9420576] \t0\ttrue\n",
            "(0)\t 16\t [ 0.49747333 -0.6205631 ] \t0\ttrue\n",
            "(0)\t 17\t [ 0.7055649  -0.65112543] \t0\ttrue\n",
            "(0)\t 18\t [ 0.4867156 -0.4242077] \t0\ttrue\n",
            "(0)\t 19\t [ 2.052277  -1.9170943] \t0\ttrue\n",
            "(0)\t 20\t [ 2.0646856 -1.957302 ] \t0\ttrue\n",
            "(0)\t 21\t [ 0.24331328 -0.3231169 ] \t0\ttrue\n",
            "(0)\t 22\t [ 0.4542266  -0.47898751] \t0\ttrue\n",
            "(0)\t 23\t [ 0.83177143 -0.87910384] \t0\ttrue\n",
            "(0)\t 24\t [-0.26701003  0.23229443] \t1\tfalse\n",
            "(1)\t 25\t [-2.5924227  2.6365418] \t1\ttrue\n",
            "(0)\t 26\t [-0.2348946   0.16439822] \t1\tfalse\n",
            "(0)\t 27\t [ 0.6225731 -0.5508296] \t0\ttrue\n",
            "(0)\t 28\t [ 0.96946675 -0.977019  ] \t0\ttrue\n",
            "(0)\t 29\t [ 0.7418011 -0.7911751] \t0\ttrue\n",
            "(0)\t 30\t [ 2.0493617 -2.0076542] \t0\ttrue\n",
            "(0)\t 31\t [ 2.100397 -2.046166] \t0\ttrue\n",
            "(1)\t 32\t [-2.6120934  2.6071317] \t1\ttrue\n",
            "(0)\t 33\t [ 0.49326348 -0.47709996] \t0\ttrue\n",
            "(0)\t 34\t [ 2.0971575 -2.0192313] \t0\ttrue\n",
            "(0)\t 35\t [ 0.8025983  -0.88466066] \t0\ttrue\n",
            "(0)\t 36\t [ 0.4869181  -0.48234144] \t0\ttrue\n",
            "(0)\t 37\t [ 0.70956796 -0.8788669 ] \t0\ttrue\n",
            "(1)\t 38\t [-2.6307588  2.676139 ] \t1\ttrue\n",
            "(0)\t 39\t [ 0.29005823 -0.23219652] \t0\ttrue\n",
            "(0)\t 40\t [ 2.0291524 -1.9018816] \t0\ttrue\n",
            "(1)\t 41\t [-2.6031826  2.6465647] \t1\ttrue\n",
            "(1)\t 42\t [-2.6027493  2.6004944] \t1\ttrue\n",
            "(1)\t 43\t [-1.6689041  1.7220575] \t1\ttrue\n",
            "(0)\t 44\t [ 0.75121886 -0.8209753 ] \t0\ttrue\n",
            "(1)\t 45\t [-2.6619492  2.688055 ] \t1\ttrue\n",
            "(1)\t 46\t [-2.6065533  2.6387434] \t1\ttrue\n",
            "(0)\t 47\t [-0.01169816 -0.0379362 ] \t0\ttrue\n",
            "(0)\t 48\t [ 1.5426987 -1.6619872] \t0\ttrue\n",
            "(0)\t 49\t [ 0.4163104  -0.45105302] \t0\ttrue\n",
            "(1)\t 50\t [-2.5675187  2.6643057] \t1\ttrue\n",
            "(0)\t 51\t [ 1.9978846 -2.049254 ] \t0\ttrue\n",
            "(0)\t 52\t [ 2.0241494 -1.9393858] \t0\ttrue\n",
            "(1)\t 53\t [ 0.5074549  -0.54363894] \t0\tfalse\n",
            "(1)\t 54\t [-2.4084196  2.5229123] \t1\ttrue\n",
            "(1)\t 55\t [ 0.624874  -0.5470001] \t0\tfalse\n",
            "(0)\t 56\t [ 2.0809085 -1.993106 ] \t0\ttrue\n",
            "(0)\t 57\t [ 0.71431804 -0.617534  ] \t0\ttrue\n",
            "(0)\t 58\t [ 0.24845643 -0.27105194] \t0\ttrue\n",
            "(0)\t 59\t [ 0.15296288 -0.19610173] \t0\ttrue\n",
            "(0)\t 60\t [ 1.4616667 -1.3359841] \t0\ttrue\n",
            "(1)\t 61\t [-2.595364   2.7106361] \t1\ttrue\n",
            "(0)\t 62\t [ 0.3582122  -0.40835628] \t0\ttrue\n",
            "(0)\t 63\t [ 2.0423262 -1.9717886] \t0\ttrue\n",
            "(0)\t 64\t [ 0.47407296 -0.5580757 ] \t0\ttrue\n",
            "(0)\t 65\t [-0.23223159  0.27447504] \t1\tfalse\n",
            "(0)\t 66\t [ 0.537581   -0.53329253] \t0\ttrue\n",
            "(0)\t 67\t [ 0.7088408 -0.9477941] \t0\ttrue\n",
            "(0)\t 68\t [ 0.8531427 -0.78453  ] \t0\ttrue\n",
            "(0)\t 69\t [ 1.9617113 -1.9824922] \t0\ttrue\n",
            "(0)\t 70\t [ 1.9949846 -1.9327592] \t0\ttrue\n",
            "(1)\t 71\t [-2.599163  2.645794] \t1\ttrue\n",
            "(0)\t 72\t [ 0.33699903 -0.38118166] \t0\ttrue\n",
            "(1)\t 73\t [-2.5861204  2.6699486] \t1\ttrue\n",
            "(0)\t 74\t [ 1.9729033 -1.8747915] \t0\ttrue\n",
            "(0)\t 75\t [-0.09250569 -0.02715067] \t1\tfalse\n",
            "(0)\t 76\t [ 2.0222063 -2.0023806] \t0\ttrue\n",
            "(1)\t 77\t [-1.8553797  1.8895193] \t1\ttrue\n",
            "(0)\t 78\t [ 0.39070547 -0.32947475] \t0\ttrue\n",
            "(1)\t 79\t [-2.4239821  2.5501103] \t1\ttrue\n",
            "(1)\t 80\t [-1.7638088  1.8396239] \t1\ttrue\n",
            "(0)\t 81\t [ 0.62463385 -0.6004539 ] \t0\ttrue\n",
            "(1)\t 82\t [-2.6497746  2.7021198] \t1\ttrue\n",
            "(1)\t 83\t [-2.300846   2.3646832] \t1\ttrue\n",
            "(1)\t 84\t [-2.6244822  2.6482806] \t1\ttrue\n",
            "(0)\t 85\t [-1.4146458  1.4279164] \t1\tfalse\n",
            "(1)\t 86\t [-2.635899  2.692962] \t1\ttrue\n",
            "(1)\t 87\t [-2.5951972  2.6484838] \t1\ttrue\n",
            "(1)\t 88\t [-2.556538   2.6014316] \t1\ttrue\n",
            "(1)\t 89\t [ 0.5608696  -0.48416775] \t0\tfalse\n",
            "(1)\t 90\t [-1.6163844  1.6827276] \t1\ttrue\n",
            "(1)\t 91\t [-2.618498  2.629599] \t1\ttrue\n",
            "(1)\t 92\t [-2.6062932  2.6469505] \t1\ttrue\n",
            "(1)\t 93\t [-2.55559    2.6571758] \t1\ttrue\n",
            "(1)\t 94\t [-2.571165   2.6673093] \t1\ttrue\n",
            "(1)\t 95\t [-2.604476   2.6686873] \t1\ttrue\n",
            "(1)\t 96\t [-2.5400195  2.582677 ] \t1\ttrue\n",
            "(1)\t 97\t [-2.600267   2.6560335] \t1\ttrue\n",
            "(1)\t 98\t [-1.8165864  1.8832885] \t1\ttrue\n",
            "(0)\t 99\t [ 1.4118148 -1.4245673] \t0\ttrue\n",
            "(1)\t 100\t [-2.4795625  2.6001105] \t1\ttrue\n",
            "(1)\t 101\t [-2.3715692  2.4823732] \t1\ttrue\n",
            "(1)\t 102\t [-0.05048604 -0.05331651] \t0\tfalse\n",
            "(1)\t 103\t [ 0.40571636 -0.37294787] \t0\tfalse\n",
            "(1)\t 104\t [-2.6015213  2.6975086] \t1\ttrue\n",
            "(1)\t 105\t [-2.2299857  2.3173296] \t1\ttrue\n",
            "(0)\t 106\t [ 0.0196231  -0.03546414] \t0\ttrue\n",
            "(1)\t 107\t [-2.5335488  2.649591 ] \t1\ttrue\n",
            "(0)\t 108\t [ 0.24375698 -0.222316  ] \t0\ttrue\n",
            "(1)\t 109\t [-2.2566433  2.4405582] \t1\ttrue\n",
            "(1)\t 110\t [-2.271615  2.400836] \t1\ttrue\n",
            "(0)\t 111\t [ 0.84933    -0.97231966] \t0\ttrue\n",
            "(0)\t 112\t [-1.7263068  1.7577575] \t1\tfalse\n",
            "(0)\t 113\t [ 1.0912029 -1.1900103] \t0\ttrue\n",
            "(0)\t 114\t [ 1.7283024 -1.7935728] \t0\ttrue\n",
            "(0)\t 115\t [ 0.60624474 -0.7537742 ] \t0\ttrue\n",
            "(0)\t 116\t [ 0.12397683 -0.21372943] \t0\ttrue\n",
            "(1)\t 117\t [-2.6096883  2.6475723] \t1\ttrue\n",
            "(0)\t 118\t [ 0.9507682 -0.9915097] \t0\ttrue\n",
            "(0)\t 119\t [ 0.58062106 -0.6040894 ] \t0\ttrue\n",
            "(1)\t 120\t [-2.338473   2.4608536] \t1\ttrue\n",
            "(1)\t 121\t [-2.592921   2.6539135] \t1\ttrue\n",
            "(1)\t 122\t [-1.8796471  1.969993 ] \t1\ttrue\n",
            "(1)\t 123\t [ 0.8261463 -1.074562 ] \t0\tfalse\n",
            "(1)\t 124\t [-2.538528   2.6674514] \t1\ttrue\n",
            "(0)\t 125\t [ 1.9637243 -2.0019524] \t0\ttrue\n",
            "(0)\t 126\t [ 0.59303135 -0.48243216] \t0\ttrue\n",
            "(1)\t 127\t [-2.5452847  2.6068938] \t1\ttrue\n",
            "(0)\t 128\t [ 0.2185927  -0.20396888] \t0\ttrue\n",
            "(0)\t 129\t [-1.3804574  1.3542427] \t1\tfalse\n",
            "(0)\t 130\t [-1.2574211  1.0827938] \t1\tfalse\n",
            "(0)\t 131\t [-0.52420205  0.47772598] \t1\tfalse\n",
            "(1)\t 132\t [-2.3203218  2.4567776] \t1\ttrue\n",
            "(1)\t 133\t [ 0.64924794 -0.7587023 ] \t0\tfalse\n",
            "(0)\t 134\t [ 2.0367286 -1.9753104] \t0\ttrue\n",
            "(0)\t 135\t [ 0.46957758 -0.46831957] \t0\ttrue\n",
            "(0)\t 136\t [ 1.8431848 -1.8662885] \t0\ttrue\n",
            "(0)\t 137\t [ 1.9176253 -1.9063948] \t0\ttrue\n",
            "(0)\t 138\t [ 1.9933585 -1.9061557] \t0\ttrue\n",
            "(0)\t 139\t [ 2.0595913 -1.9615746] \t0\ttrue\n",
            "(0)\t 140\t [ 2.015306  -1.9690975] \t0\ttrue\n",
            "(1)\t 141\t [-2.3269308  2.2946024] \t1\ttrue\n",
            "(0)\t 142\t [ 0.72170246 -0.76070315] \t0\ttrue\n",
            "(0)\t 143\t [ 2.0215876 -1.9656358] \t0\ttrue\n",
            "(1)\t 144\t [-2.558031   2.6179142] \t1\ttrue\n",
            "(1)\t 145\t [-2.6178298  2.6675725] \t1\ttrue\n",
            "(1)\t 146\t [ 0.63363665 -0.5852039 ] \t0\tfalse\n",
            "(1)\t 147\t [ 0.9225789 -1.0905267] \t0\tfalse\n",
            "(1)\t 148\t [ 0.6423422 -0.5944279] \t0\tfalse\n",
            "(0)\t 149\t [ 0.741972  -0.7529629] \t0\ttrue\n",
            "(1)\t 150\t [ 0.6495502 -0.5978717] \t0\tfalse\n",
            "(1)\t 151\t [-2.5660927  2.6459727] \t1\ttrue\n",
            "(1)\t 152\t [ 0.2649061  -0.25718635] \t0\tfalse\n",
            "(1)\t 153\t [-2.5118213  2.6162288] \t1\ttrue\n",
            "(1)\t 154\t [-2.4840798  2.5967045] \t1\ttrue\n",
            "(0)\t 155\t [-0.86251825  0.9054757 ] \t1\tfalse\n",
            "(1)\t 156\t [-2.623273   2.6947196] \t1\ttrue\n",
            "(0)\t 157\t [ 0.21695393 -0.18886352] \t0\ttrue\n",
            "(1)\t 158\t [-2.3683722  2.5021677] \t1\ttrue\n",
            "(1)\t 159\t [-2.5789032  2.679463 ] \t1\ttrue\n",
            "(1)\t 160\t [-2.4203913  2.4817948] \t1\ttrue\n",
            "(1)\t 161\t [-1.9779611  1.895073 ] \t1\ttrue\n",
            "(0)\t 162\t [-2.6605988  2.6796556] \t1\tfalse\n",
            "(0)\t 163\t [ 0.7744194 -0.7986574] \t0\ttrue\n",
            "(1)\t 164\t [ 0.5663643  -0.59076315] \t0\tfalse\n",
            "(0)\t 165\t [-0.14482576  0.18506004] \t1\tfalse\n",
            "(1)\t 166\t [-2.60852    2.6392074] \t1\ttrue\n",
            "(1)\t 167\t [-2.507063   2.5916557] \t1\ttrue\n",
            "(1)\t 168\t [-2.6086304  2.6707087] \t1\ttrue\n",
            "(1)\t 169\t [-2.633533   2.6877637] \t1\ttrue\n",
            "(1)\t 170\t [ 0.6372124 -0.6535997] \t0\tfalse\n",
            "(0)\t 171\t [-1.0525057  1.202541 ] \t1\tfalse\n",
            "(0)\t 172\t [ 1.2637324 -1.3459928] \t0\ttrue\n",
            "(0)\t 173\t [ 2.1615624 -2.0001192] \t0\ttrue\n",
            "(1)\t 174\t [ 0.1738509 -0.1504277] \t0\tfalse\n",
            "(1)\t 175\t [-1.3782724  1.4317269] \t1\ttrue\n",
            "(0)\t 176\t [ 1.2887496 -1.3495687] \t0\ttrue\n",
            "(1)\t 177\t [-2.6197948  2.6734812] \t1\ttrue\n",
            "(0)\t 178\t [ 2.0972302 -2.0124898] \t0\ttrue\n",
            "(0)\t 179\t [ 0.16674036 -0.07981098] \t0\ttrue\n",
            "(0)\t 180\t [ 0.4434339  -0.48556504] \t0\ttrue\n",
            "(0)\t 181\t [ 0.59826654 -0.5507197 ] \t0\ttrue\n",
            "(1)\t 182\t [-2.5877204  2.704032 ] \t1\ttrue\n",
            "(0)\t 183\t [ 1.7622123 -1.733044 ] \t0\ttrue\n",
            "(0)\t 184\t [ 0.14532907 -0.1392998 ] \t0\ttrue\n",
            "(1)\t 185\t [-1.6737157  1.852296 ] \t1\ttrue\n",
            "(0)\t 186\t [ 0.52006453 -0.41081348] \t0\ttrue\n",
            "(1)\t 187\t [-2.610259   2.6517892] \t1\ttrue\n",
            "(1)\t 188\t [-2.640619   2.6990829] \t1\ttrue\n",
            "(1)\t 189\t [-1.8205744  1.9534059] \t1\ttrue\n",
            "(1)\t 190\t [-2.1231341  2.1702206] \t1\ttrue\n",
            "(0)\t 191\t [ 0.23918122 -0.16864598] \t0\ttrue\n",
            "(0)\t 192\t [-2.243085   2.3467336] \t1\tfalse\n",
            "(0)\t 193\t [ 0.35358644 -0.3997898 ] \t0\ttrue\n",
            "(1)\t 194\t [-2.244152   2.3137665] \t1\ttrue\n",
            "(0)\t 195\t [ 0.38129592 -0.386372  ] \t0\ttrue\n",
            "(0)\t 196\t [-1.5336112  1.455563 ] \t1\tfalse\n",
            "(0)\t 197\t [ 0.20081201 -0.15717867] \t0\ttrue\n",
            "(1)\t 198\t [ 1.5295044 -1.4309183] \t0\tfalse\n",
            "(0)\t 199\t [ 0.8058328 -0.7837255] \t0\ttrue\n",
            "(1)\t 200\t [-2.5861735  2.6612005] \t1\ttrue\n",
            "(0)\t 201\t [-2.6164079  2.6261182] \t1\tfalse\n",
            "(0)\t 202\t [ 0.9622772  -0.93000257] \t0\ttrue\n",
            "(0)\t 203\t [ 0.7993264 -1.0065267] \t0\ttrue\n",
            "(0)\t 204\t [ 1.9837483 -1.8828417] \t0\ttrue\n",
            "(0)\t 205\t [ 0.44937164 -0.45715007] \t0\ttrue\n",
            "(1)\t 206\t [-2.6257606  2.67174  ] \t1\ttrue\n",
            "(0)\t 207\t [ 0.70828396 -0.69742966] \t0\ttrue\n",
            "(1)\t 208\t [-2.618256   2.6515436] \t1\ttrue\n",
            "(1)\t 209\t [-2.5754104  2.6526608] \t1\ttrue\n",
            "(0)\t 210\t [ 0.5333605  -0.56024605] \t0\ttrue\n",
            "(0)\t 211\t [ 0.8565611  -0.81275886] \t0\ttrue\n",
            "(1)\t 212\t [-2.598339   2.6767979] \t1\ttrue\n",
            "(1)\t 213\t [ 0.9920065 -0.9425879] \t0\tfalse\n",
            "(1)\t 214\t [-2.5825698  2.642263 ] \t1\ttrue\n",
            "(0)\t 215\t [ 0.080304   -0.02008465] \t0\ttrue\n",
            "(0)\t 216\t [ 0.5934914  -0.47140628] \t0\ttrue\n",
            "(1)\t 217\t [-2.5928717  2.7158916] \t1\ttrue\n",
            "(0)\t 218\t [ 0.79262906 -0.75101197] \t0\ttrue\n",
            "(1)\t 219\t [-2.1934452  2.3761668] \t1\ttrue\n",
            "(1)\t 220\t [-2.1089249  2.1065118] \t1\ttrue\n",
            "(1)\t 221\t [-1.8687963  1.9099107] \t1\ttrue\n",
            "(1)\t 222\t [ 0.67235345 -0.86563146] \t0\tfalse\n",
            "(1)\t 223\t [ 1.0259293 -1.2064611] \t0\tfalse\n",
            "(0)\t 224\t [ 0.84483045 -0.7934781 ] \t0\ttrue\n",
            "(1)\t 225\t [-1.2557733  1.2660475] \t1\ttrue\n",
            "(0)\t 226\t [-0.00200273  0.09245068] \t1\tfalse\n",
            "(1)\t 227\t [-2.4388103  2.5900478] \t1\ttrue\n",
            "(1)\t 228\t [ 1.6079777 -1.661425 ] \t0\tfalse\n",
            "(1)\t 229\t [-2.600607   2.6909509] \t1\ttrue\n",
            "(1)\t 230\t [-2.6118436  2.661201 ] \t1\ttrue\n",
            "(0)\t 231\t [ 1.9712311 -1.876527 ] \t0\ttrue\n",
            "(0)\t 232\t [-0.00055814 -0.09572145] \t0\ttrue\n",
            "(1)\t 233\t [ 0.7010353 -0.8256348] \t0\tfalse\n",
            "(0)\t 234\t [-0.04810398  0.08551036] \t1\tfalse\n",
            "(0)\t 235\t [ 0.6258145 -0.7360954] \t0\ttrue\n",
            "(1)\t 236\t [-2.652906   2.6954627] \t1\ttrue\n",
            "(1)\t 237\t [-2.607057   2.6682026] \t1\ttrue\n",
            "(0)\t 238\t [ 0.5891455  -0.56682783] \t0\ttrue\n",
            "(0)\t 239\t [-2.5975318  2.6697257] \t1\tfalse\n",
            "(1)\t 240\t [-2.6214128  2.6407876] \t1\ttrue\n",
            "(1)\t 241\t [-2.3129144  2.4074292] \t1\ttrue\n",
            "(1)\t 242\t [ 0.38483006 -0.25599527] \t0\tfalse\n",
            "(1)\t 243\t [ 0.33144018 -0.369673  ] \t0\tfalse\n",
            "(1)\t 244\t [-2.2730172  2.3853736] \t1\ttrue\n",
            "(0)\t 245\t [ 0.12770285 -0.13543637] \t0\ttrue\n",
            "(0)\t 246\t [ 1.4358491 -1.3523018] \t0\ttrue\n",
            "(0)\t 247\t [ 1.684518  -1.6073307] \t0\ttrue\n",
            "(0)\t 248\t [ 1.9908816 -2.001099 ] \t0\ttrue\n",
            "(1)\t 249\t [ 0.7923663 -0.8382722] \t0\tfalse\n",
            "(1)\t 250\t [-2.6217742  2.7072601] \t1\ttrue\n",
            "(1)\t 251\t [-2.567754   2.6204524] \t1\ttrue\n",
            "(0)\t 252\t [ 0.03126545 -0.22333357] \t0\ttrue\n",
            "(0)\t 253\t [ 0.19017667 -0.19136712] \t0\ttrue\n",
            "(0)\t 254\t [ 1.8812622 -1.7956114] \t0\ttrue\n",
            "(1)\t 255\t [ 0.43806165 -0.42560917] \t0\tfalse\n",
            "(0)\t 256\t [ 2.0451927 -1.899382 ] \t0\ttrue\n",
            "(1)\t 257\t [-2.587625   2.6579742] \t1\ttrue\n",
            "(0)\t 258\t [ 2.0285058 -1.967965 ] \t0\ttrue\n",
            "(0)\t 259\t [ 0.5919583 -0.6072907] \t0\ttrue\n",
            "(0)\t 260\t [ 0.7262096 -0.7705808] \t0\ttrue\n",
            "(1)\t 261\t [-2.5851092  2.6313791] \t1\ttrue\n",
            "(0)\t 262\t [ 1.9774424 -1.9850668] \t0\ttrue\n",
            "(0)\t 263\t [ 0.6448633 -0.9128215] \t0\ttrue\n",
            "(1)\t 264\t [-2.64045    2.6453424] \t1\ttrue\n",
            "(1)\t 265\t [-2.1665773  2.2922578] \t1\ttrue\n",
            "(1)\t 266\t [-2.6082826  2.6703281] \t1\ttrue\n",
            "(1)\t 267\t [-2.4231799  2.544972 ] \t1\ttrue\n",
            "(0)\t 268\t [ 2.0901136 -1.9361169] \t0\ttrue\n",
            "(0)\t 269\t [ 1.995664  -1.9727418] \t0\ttrue\n",
            "(1)\t 270\t [ 0.5641321 -0.5317934] \t0\tfalse\n",
            "(1)\t 271\t [-2.1895392  2.2463326] \t1\ttrue\n",
            "(1)\t 272\t [ 0.17537081 -0.10028171] \t0\tfalse\n",
            "(1)\t 273\t [-2.5379386  2.6567974] \t1\ttrue\n",
            "(0)\t 274\t [ 2.0160162 -1.9987189] \t0\ttrue\n",
            "(1)\t 275\t [-2.24966    2.3515363] \t1\ttrue\n",
            "(1)\t 276\t [-2.6279218  2.654224 ] \t1\ttrue\n",
            "(1)\t 277\t [-2.6126678  2.6413558] \t1\ttrue\n",
            "(0)\t 278\t [ 2.0532794 -1.9635369] \t0\ttrue\n",
            "(1)\t 279\t [-2.2843528  2.3428075] \t1\ttrue\n",
            "(0)\t 280\t [ 0.74491745 -0.7106608 ] \t0\ttrue\n",
            "(0)\t 281\t [-0.10188476  0.09384112] \t1\tfalse\n",
            "(1)\t 282\t [-2.4542007  2.567847 ] \t1\ttrue\n",
            "(0)\t 283\t [ 2.1077287 -1.9665171] \t0\ttrue\n",
            "(1)\t 284\t [-2.5437474  2.692717 ] \t1\ttrue\n",
            "(1)\t 285\t [ 0.7953984 -0.701369 ] \t0\tfalse\n",
            "(1)\t 286\t [ 0.8314316  -0.76514715] \t0\tfalse\n",
            "(0)\t 287\t [ 0.9690795 -0.9834233] \t0\ttrue\n",
            "(1)\t 288\t [-2.5792403  2.6296883] \t1\ttrue\n",
            "(0)\t 289\t [-0.25443795  0.3208547 ] \t1\tfalse\n",
            "(0)\t 290\t [ 2.015259  -1.9560198] \t0\ttrue\n",
            "(1)\t 291\t [-1.7245488  1.7234195] \t1\ttrue\n",
            "(1)\t 292\t [-2.5785694  2.6302187] \t1\ttrue\n",
            "(1)\t 293\t [-2.4427474  2.4236424] \t1\ttrue\n",
            "(1)\t 294\t [-2.3883147  2.453365 ] \t1\ttrue\n",
            "(1)\t 295\t [-2.4420903  2.531297 ] \t1\ttrue\n",
            "(0)\t 296\t [ 0.6878152 -0.7820067] \t0\ttrue\n",
            "(1)\t 297\t [ 0.3790119  -0.32784253] \t0\tfalse\n",
            "(0)\t 298\t [ 0.462759  -0.6895972] \t0\ttrue\n",
            "(0)\t 299\t [ 1.0987252 -1.1223261] \t0\ttrue\n",
            "(1)\t 300\t [-2.0356567  1.9883198] \t1\ttrue\n",
            "(0)\t 301\t [ 0.57477665 -0.6061899 ] \t0\ttrue\n",
            "(0)\t 302\t [ 0.30310762 -0.23062167] \t0\ttrue\n",
            "(0)\t 303\t [ 2.0155327 -1.9990387] \t0\ttrue\n",
            "(1)\t 304\t [-2.5892942  2.6388373] \t1\ttrue\n",
            "(0)\t 305\t [ 0.1523524  -0.05309669] \t0\ttrue\n",
            "(0)\t 306\t [ 0.5891215 -0.5442861] \t0\ttrue\n",
            "(1)\t 307\t [-0.34514132  0.4144211 ] \t1\ttrue\n",
            "(0)\t 308\t [-0.474558    0.45470828] \t1\tfalse\n",
            "(1)\t 309\t [-2.5949955  2.6484852] \t1\ttrue\n",
            "(0)\t 310\t [ 1.971212  -1.8977166] \t0\ttrue\n",
            "(0)\t 311\t [ 1.1085241 -1.0933244] \t0\ttrue\n",
            "(0)\t 312\t [ 2.063232  -1.9819987] \t0\ttrue\n",
            "(1)\t 313\t [-2.6118534  2.6361628] \t1\ttrue\n",
            "(0)\t 314\t [ 0.616062  -0.8261692] \t0\ttrue\n",
            "(1)\t 315\t [-1.5768327  1.5184559] \t1\ttrue\n",
            "(0)\t 316\t [ 2.0842617 -1.9799393] \t0\ttrue\n",
            "(0)\t 317\t [ 0.22611775 -0.19798118] \t0\ttrue\n",
            "(1)\t 318\t [-2.5067904  2.6169343] \t1\ttrue\n",
            "(0)\t 319\t [ 0.86961436 -0.7810021 ] \t0\ttrue\n",
            "(0)\t 320\t [ 2.0866168 -1.9878236] \t0\ttrue\n",
            "(0)\t 321\t [ 0.6976244  -0.76970094] \t0\ttrue\n",
            "(0)\t 322\t [ 0.18310177 -0.16026759] \t0\ttrue\n",
            "(1)\t 323\t [-1.5204787  1.5383822] \t1\ttrue\n",
            "(1)\t 324\t [ 0.8348592 -1.1915601] \t0\tfalse\n",
            "(0)\t 325\t [ 0.8446286 -1.0558219] \t0\ttrue\n",
            "(1)\t 326\t [-2.355056   2.4926205] \t1\ttrue\n",
            "(0)\t 327\t [ 2.077484  -1.9659667] \t0\ttrue\n",
            "(0)\t 328\t [ 0.36270157 -0.30114803] \t0\ttrue\n",
            "(1)\t 329\t [-2.6070414  2.6402555] \t1\ttrue\n",
            "(0)\t 330\t [ 0.8185711  -0.80462253] \t0\ttrue\n",
            "(1)\t 331\t [-1.5673498  1.581905 ] \t1\ttrue\n",
            "(0)\t 332\t [ 1.6720494 -1.6960922] \t0\ttrue\n",
            "(1)\t 333\t [-2.587164   2.6462986] \t1\ttrue\n",
            "(0)\t 334\t [ 1.0069895 -1.0684816] \t0\ttrue\n",
            "(0)\t 335\t [ 0.5190138 -0.4489199] \t0\ttrue\n",
            "(1)\t 336\t [-0.56060153  0.62174046] \t1\ttrue\n",
            "(0)\t 337\t [ 0.00866873 -0.01378479] \t0\ttrue\n",
            "(0)\t 338\t [ 0.6144664  -0.63962287] \t0\ttrue\n",
            "(1)\t 339\t [-2.4830732  2.5966039] \t1\ttrue\n",
            "(1)\t 340\t [ 0.52484566 -0.6193313 ] \t0\tfalse\n",
            "(0)\t 341\t [ 0.4966213  -0.52293915] \t0\ttrue\n",
            "(1)\t 342\t [ 0.5264029 -0.5194096] \t0\tfalse\n",
            "(1)\t 343\t [-2.640819  2.696328] \t1\ttrue\n",
            "(1)\t 344\t [-2.2501478  2.272637 ] \t1\ttrue\n",
            "(1)\t 345\t [-1.6316372  1.6148369] \t1\ttrue\n",
            "(1)\t 346\t [ 0.03112554 -0.00725812] \t0\tfalse\n",
            "(1)\t 347\t [-2.6086688  2.6576834] \t1\ttrue\n",
            "(1)\t 348\t [-2.644033   2.6531634] \t1\ttrue\n",
            "(1)\t 349\t [-0.28417063  0.222708  ] \t1\ttrue\n",
            "(0)\t 350\t [ 0.4782515  -0.44411558] \t0\ttrue\n",
            "(0)\t 351\t [ 2.0468125 -2.0586715] \t0\ttrue\n",
            "(1)\t 352\t [-2.4452832  2.543036 ] \t1\ttrue\n",
            "(1)\t 353\t [-2.5652094  2.6777725] \t1\ttrue\n",
            "(1)\t 354\t [-2.6260486  2.6693597] \t1\ttrue\n",
            "(1)\t 355\t [-2.1841304  2.3121097] \t1\ttrue\n",
            "(0)\t 356\t [ 0.78476745 -0.7322667 ] \t0\ttrue\n",
            "(0)\t 357\t [ 1.1457697 -1.1957259] \t0\ttrue\n",
            "(0)\t 358\t [-0.11186838  0.15558386] \t1\tfalse\n",
            "(1)\t 359\t [-0.27803516  0.33601883] \t1\ttrue\n",
            "(0)\t 360\t [-0.04421606  0.11306997] \t1\tfalse\n",
            "(0)\t 361\t [ 1.5201373 -1.6084757] \t0\ttrue\n",
            "(0)\t 362\t [ 0.21548232 -0.18560715] \t0\ttrue\n",
            "(1)\t 363\t [ 1.6469619 -1.6310627] \t0\tfalse\n",
            "(1)\t 364\t [-2.5141664  2.5740268] \t1\ttrue\n",
            "(0)\t 365\t [ 2.1004515 -1.9363184] \t0\ttrue\n",
            "(1)\t 366\t [-2.6057332  2.6286302] \t1\ttrue\n",
            "(1)\t 367\t [-2.6203485  2.660458 ] \t1\ttrue\n",
            "(1)\t 368\t [-2.445939   2.5539951] \t1\ttrue\n",
            "(1)\t 369\t [-2.384771   2.4544542] \t1\ttrue\n",
            "(1)\t 370\t [-2.4433491  2.5563223] \t1\ttrue\n",
            "(0)\t 371\t [ 0.38258222 -0.41083843] \t0\ttrue\n",
            "(0)\t 372\t [ 0.2211751 -0.1798388] \t0\ttrue\n",
            "(0)\t 373\t [ 0.39954934 -0.38187772] \t0\ttrue\n",
            "(1)\t 374\t [-2.60088   2.641242] \t1\ttrue\n",
            "(1)\t 375\t [ 0.6513848 -0.5963661] \t0\tfalse\n",
            "(1)\t 376\t [ 0.54851955 -0.61966664] \t0\tfalse\n",
            "(1)\t 377\t [-2.270323  2.393941] \t1\ttrue\n",
            "(0)\t 378\t [ 1.9734135 -1.965296 ] \t0\ttrue\n",
            "(1)\t 379\t [-2.593916   2.6659484] \t1\ttrue\n",
            "(1)\t 380\t [-2.4444466  2.549064 ] \t1\ttrue\n",
            "(1)\t 381\t [-2.6465974  2.7007313] \t1\ttrue\n",
            "(0)\t 382\t [ 1.6627157 -1.7397895] \t0\ttrue\n",
            "(1)\t 383\t [-2.5735507  2.6763391] \t1\ttrue\n",
            "(0)\t 384\t [ 0.26575148 -0.36858654] \t0\ttrue\n",
            "(1)\t 385\t [-2.6299407  2.717568 ] \t1\ttrue\n",
            "(1)\t 386\t [-2.3545315  2.4515154] \t1\ttrue\n",
            "(1)\t 387\t [-2.6196623  2.6790028] \t1\ttrue\n",
            "(0)\t 388\t [ 2.1691988 -2.0492084] \t0\ttrue\n",
            "(0)\t 389\t [ 0.8968647 -0.8897354] \t0\ttrue\n",
            "(1)\t 390\t [-2.625277   2.6472712] \t1\ttrue\n",
            "(1)\t 391\t [-2.5857034  2.629788 ] \t1\ttrue\n",
            "(1)\t 392\t [-0.4006483   0.26865217] \t1\ttrue\n",
            "(0)\t 393\t [ 1.8011663 -1.7046982] \t0\ttrue\n",
            "(0)\t 394\t [ 1.9959775 -1.9136535] \t0\ttrue\n",
            "(0)\t 395\t [ 0.5035996  -0.41615573] \t0\ttrue\n",
            "(0)\t 396\t [-0.9380297  0.7815053] \t1\tfalse\n",
            "(0)\t 397\t [ 0.5266525  -0.49524269] \t0\ttrue\n",
            "(0)\t 398\t [ 0.22194293 -0.24743289] \t0\ttrue\n",
            "(0)\t 399\t [ 0.46135002 -0.4091113 ] \t0\ttrue\n",
            "(0)\t 400\t [-2.5983586  2.6689599] \t1\tfalse\n",
            "(0)\t 401\t [ 0.8236303 -0.8382357] \t0\ttrue\n",
            "(1)\t 402\t [-2.5349956  2.6539507] \t1\ttrue\n",
            "(0)\t 403\t [ 0.34311408 -0.49839413] \t0\ttrue\n",
            "(0)\t 404\t [ 0.7580432 -0.6975733] \t0\ttrue\n",
            "(0)\t 405\t [ 2.0837874 -2.0718372] \t0\ttrue\n",
            "(1)\t 406\t [ 0.58875006 -0.58287525] \t0\tfalse\n",
            "(0)\t 407\t [ 0.6976913 -0.7069569] \t0\ttrue\n",
            "(0)\t 408\t [ 0.36788186 -0.30460125] \t0\ttrue\n",
            "(0)\t 409\t [ 1.7983077 -1.7288724] \t0\ttrue\n",
            "(0)\t 410\t [ 2.06263  -1.955742] \t0\ttrue\n",
            "(1)\t 411\t [ 0.7312453  -0.66792244] \t0\tfalse\n",
            "(0)\t 412\t [ 1.8976556 -1.9071885] \t0\ttrue\n",
            "(0)\t 413\t [ 0.35220852 -0.4197387 ] \t0\ttrue\n",
            "(0)\t 414\t [ 0.44751272 -0.33859062] \t0\ttrue\n",
            "(0)\t 415\t [ 0.8061439 -0.9949718] \t0\ttrue\n",
            "(0)\t 416\t [ 1.9838667 -2.0039692] \t0\ttrue\n",
            "(0)\t 417\t [ 2.046768  -1.8536456] \t0\ttrue\n",
            "(0)\t 418\t [ 2.1338735 -1.9392244] \t0\ttrue\n",
            "(1)\t 419\t [ 0.66932064 -0.6022826 ] \t0\tfalse\n",
            "(1)\t 420\t [-2.6230311  2.6208675] \t1\ttrue\n",
            "(1)\t 421\t [ 0.35386613 -0.3533522 ] \t0\tfalse\n",
            "(0)\t 422\t [ 2.138279  -1.9890653] \t0\ttrue\n",
            "(1)\t 423\t [-2.576796  2.659019] \t1\ttrue\n",
            "(1)\t 424\t [-2.537037  2.594565] \t1\ttrue\n",
            "(0)\t 425\t [ 0.93505883 -0.9645361 ] \t0\ttrue\n",
            "(0)\t 426\t [ 2.0281491 -1.9992654] \t0\ttrue\n",
            "(1)\t 427\t [ 0.7379537  -0.70005345] \t0\tfalse\n",
            "(1)\t 428\t [-2.5868473  2.623639 ] \t1\ttrue\n",
            "(0)\t 429\t [ 0.18785067 -0.15091512] \t0\ttrue\n",
            "(0)\t 430\t [ 2.1388972 -2.1181214] \t0\ttrue\n",
            "(0)\t 431\t [ 0.69495374 -0.7116938 ] \t0\ttrue\n",
            "(0)\t 432\t [ 0.37588692 -0.4457544 ] \t0\ttrue\n",
            "(0)\t 433\t [ 0.5377114  -0.56060326] \t0\ttrue\n",
            "(1)\t 434\t [-2.605211   2.6509438] \t1\ttrue\n",
            "(0)\t 435\t [ 0.7311686  -0.85918623] \t0\ttrue\n",
            "(1)\t 436\t [-2.6320262  2.675795 ] \t1\ttrue\n",
            "(1)\t 437\t [-2.6089501  2.6680834] \t1\ttrue\n",
            "(0)\t 438\t [ 0.5805637  -0.47164682] \t0\ttrue\n",
            "(1)\t 439\t [-2.522452   2.5969958] \t1\ttrue\n",
            "(0)\t 440\t [ 2.049275  -1.9299937] \t0\ttrue\n",
            "(0)\t 441\t [ 2.0464494 -1.944574 ] \t0\ttrue\n",
            "(1)\t 442\t [-2.3967    2.496684] \t1\ttrue\n",
            "(1)\t 443\t [-2.32661    2.4238596] \t1\ttrue\n",
            "(1)\t 444\t [ 0.12711295 -0.15768088] \t0\tfalse\n",
            "(1)\t 445\t [-2.5872169  2.6498418] \t1\ttrue\n",
            "(0)\t 446\t [ 0.6971361 -0.8745032] \t0\ttrue\n",
            "(1)\t 447\t [-2.6607962  2.6816924] \t1\ttrue\n",
            "(1)\t 448\t [-2.5881681  2.613574 ] \t1\ttrue\n",
            "(1)\t 449\t [-1.197765   1.1243013] \t1\ttrue\n",
            "(1)\t 450\t [-2.5742002  2.6341524] \t1\ttrue\n",
            "(1)\t 451\t [ 0.14572391 -0.18059562] \t0\tfalse\n",
            "(1)\t 452\t [ 0.61729723 -0.6521955 ] \t0\tfalse\n",
            "(0)\t 453\t [ 0.03838488 -0.0098535 ] \t0\ttrue\n",
            "(0)\t 454\t [ 0.19530563 -0.2538867 ] \t0\ttrue\n",
            "(0)\t 455\t [ 0.83124983 -0.74582964] \t0\ttrue\n",
            "(0)\t 456\t [ 2.0510068 -2.0676618] \t0\ttrue\n",
            "(1)\t 457\t [-2.6368084  2.691782 ] \t1\ttrue\n",
            "(1)\t 458\t [-1.6391562  1.611898 ] \t1\ttrue\n",
            "(1)\t 459\t [ 0.88505626 -0.8446836 ] \t0\tfalse\n",
            "(1)\t 460\t [-2.552096   2.6306458] \t1\ttrue\n",
            "(1)\t 461\t [-2.6114082  2.6072726] \t1\ttrue\n",
            "(1)\t 462\t [-2.4650154  2.6304035] \t1\ttrue\n",
            "(1)\t 463\t [-2.634464   2.7074478] \t1\ttrue\n",
            "(1)\t 464\t [-2.6002488  2.6449027] \t1\ttrue\n",
            "(0)\t 465\t [ 2.0626283 -1.8718333] \t0\ttrue\n",
            "(0)\t 466\t [ 0.04187891 -0.00770333] \t0\ttrue\n",
            "(1)\t 467\t [ 0.51928955 -0.5186716 ] \t0\tfalse\n",
            "(0)\t 468\t [ 2.1163335 -2.0673864] \t0\ttrue\n",
            "(1)\t 469\t [-2.4907718  2.567496 ] \t1\ttrue\n",
            "(0)\t 470\t [ 1.50444   -1.5970868] \t0\ttrue\n",
            "(1)\t 471\t [-1.1252605  1.0726789] \t1\ttrue\n",
            "(1)\t 472\t [-2.5782752  2.6970558] \t1\ttrue\n",
            "(0)\t 473\t [ 0.5789916  -0.56436205] \t0\ttrue\n",
            "(1)\t 474\t [-1.4402381  1.5174965] \t1\ttrue\n",
            "(1)\t 475\t [-2.6375961  2.6655257] \t1\ttrue\n",
            "(1)\t 476\t [-1.8374635  1.8345784] \t1\ttrue\n",
            "(0)\t 477\t [ 0.6882198 -0.6816264] \t0\ttrue\n",
            "(1)\t 478\t [ 0.95418054 -0.9788844 ] \t0\tfalse\n",
            "(0)\t 479\t [ 1.7010132 -1.780044 ] \t0\ttrue\n",
            "(1)\t 480\t [ 0.7336494 -0.7206928] \t0\tfalse\n",
            "(0)\t 481\t [ 0.51447445 -0.52887493] \t0\ttrue\n",
            "(0)\t 482\t [ 0.22870968 -0.24592254] \t0\ttrue\n",
            "(0)\t 483\t [ 2.1318185 -1.9908544] \t0\ttrue\n",
            "(1)\t 484\t [-2.6487908  2.6843338] \t1\ttrue\n",
            "(1)\t 485\t [ 0.23507439 -0.21152222] \t0\tfalse\n",
            "(0)\t 486\t [ 0.23540989 -0.20008826] \t0\ttrue\n",
            "(1)\t 487\t [-2.5725245  2.63627  ] \t1\ttrue\n",
            "(0)\t 488\t [ 1.8921132 -1.8082758] \t0\ttrue\n",
            "(1)\t 489\t [-2.3828316  2.4870052] \t1\ttrue\n",
            "(1)\t 490\t [-2.5964804  2.6269078] \t1\ttrue\n",
            "(1)\t 491\t [ 0.0788354  -0.33112592] \t0\tfalse\n",
            "(0)\t 492\t [ 0.62490577 -0.6725737 ] \t0\ttrue\n",
            "(1)\t 493\t [ 0.30679494 -0.3210914 ] \t0\tfalse\n",
            "(1)\t 494\t [-2.6290143  2.6905966] \t1\ttrue\n",
            "(1)\t 495\t [-2.5883646  2.6992483] \t1\ttrue\n",
            "(1)\t 496\t [-2.364223   2.4977536] \t1\ttrue\n",
            "(1)\t 497\t [-2.5858333  2.6786203] \t1\ttrue\n",
            "(0)\t 498\t [-0.8299218  0.6910344] \t1\tfalse\n",
            "(1)\t 499\t [-2.6085835  2.6796908] \t1\ttrue\n",
            "(1)\t 500\t [-0.48492646  0.54293394] \t1\ttrue\n",
            "Number of true predictions: 418\n",
            "Number of false predictions: 82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIgsWWCTKdY",
        "colab_type": "code",
        "outputId": "bdfb42f2-f81b-4d9c-8c56-56627be08de5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 83.43313373253493 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print('True positives: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "outputId": "e150dc68-e6c2-4d8e-ef12-c2d8b48265b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI2K1bqaR5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "outputId": "916a15da-4c7b-433d-d704-d7ae4506b353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "outputId": "1e12358c-0cad-4856-efd9-e4c30384ea99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-task1punjabi'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task1punjabi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task1punjabi/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1punjabi/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1punjabi/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}